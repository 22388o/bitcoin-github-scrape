[
  {
    "sha": "7cc960f8f57e7fe90ee7aa0ccd3e3c6c89ec5a25",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo3Y2M5NjBmOGY1N2U3ZmU5MGVlN2FhMGNjZDNlM2M2Yzg5ZWM1YTI1",
    "commit": {
      "author": {
        "name": "Peter Todd",
        "email": "pete@petertodd.org",
        "date": "2013-06-25T13:57:59Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-06-25T13:59:31Z"
      },
      "message": "Truncate oversize 'tx' messages before relaying/storing.\n\nFixes a memory exhaustion attack on low-memory peers.",
      "tree": {
        "sha": "5e48c278206e93cab621cefb69863e166e8a2f49",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/5e48c278206e93cab621cefb69863e166e8a2f49"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/7cc960f8f57e7fe90ee7aa0ccd3e3c6c89ec5a25",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7cc960f8f57e7fe90ee7aa0ccd3e3c6c89ec5a25",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/7cc960f8f57e7fe90ee7aa0ccd3e3c6c89ec5a25",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7cc960f8f57e7fe90ee7aa0ccd3e3c6c89ec5a25/comments",
    "author": {
      "login": "petertodd",
      "id": 7042,
      "node_id": "MDQ6VXNlcjcwNDI=",
      "avatar_url": "https://avatars.githubusercontent.com/u/7042?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/petertodd",
      "html_url": "https://github.com/petertodd",
      "followers_url": "https://api.github.com/users/petertodd/followers",
      "following_url": "https://api.github.com/users/petertodd/following{/other_user}",
      "gists_url": "https://api.github.com/users/petertodd/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/petertodd/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/petertodd/subscriptions",
      "organizations_url": "https://api.github.com/users/petertodd/orgs",
      "repos_url": "https://api.github.com/users/petertodd/repos",
      "events_url": "https://api.github.com/users/petertodd/events{/privacy}",
      "received_events_url": "https://api.github.com/users/petertodd/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "09e437ba4e5cb7fcc53020c1ceb2451e0ff1606b",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/09e437ba4e5cb7fcc53020c1ceb2451e0ff1606b",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/09e437ba4e5cb7fcc53020c1ceb2451e0ff1606b"
      }
    ],
    "stats": {
      "total": 10,
      "additions": 10,
      "deletions": 0
    },
    "files": [
      {
        "sha": "226d32295de48cb66de3f399ba3bf406fc7aefd5",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/7cc960f8f57e7fe90ee7aa0ccd3e3c6c89ec5a25/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/7cc960f8f57e7fe90ee7aa0ccd3e3c6c89ec5a25/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=7cc960f8f57e7fe90ee7aa0ccd3e3c6c89ec5a25",
        "patch": "@@ -3508,6 +3508,16 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n         CInv inv(MSG_TX, tx.GetHash());\n         pfrom->AddInventoryKnown(inv);\n \n+        // Truncate messages to the size of the tx in them\n+        unsigned int nSize = ::GetSerializeSize(tx, SER_NETWORK, PROTOCOL_VERSION);\n+        unsigned int oldSize = vMsg.size();\n+        if (nSize < oldSize) {\n+            vMsg.resize(nSize);\n+            printf(\"truncating oversized TX %s (%u -> %u)\\n\",\n+                   tx.GetHash().ToString().c_str(),\n+                   oldSize, nSize);\n+        }\n+\n         bool fMissingInputs = false;\n         CValidationState state;\n         if (tx.AcceptToMemoryPool(state, true, true, &fMissingInputs))"
      }
    ]
  },
  {
    "sha": "65c20dbf558d9a8e9856017568016c92473dada4",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo2NWMyMGRiZjU1OGQ5YThlOTg1NjAxNzU2ODAxNmM5MjQ3M2RhZGE0",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2013-06-23T22:23:28Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-06-25T14:21:02Z"
      },
      "message": "Dump addresses every 15 minutes instead of 10 seconds",
      "tree": {
        "sha": "4b05eec1d44b4cf843cea6c23f68f13ba715d0f9",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/4b05eec1d44b4cf843cea6c23f68f13ba715d0f9"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/65c20dbf558d9a8e9856017568016c92473dada4",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/65c20dbf558d9a8e9856017568016c92473dada4",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/65c20dbf558d9a8e9856017568016c92473dada4",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/65c20dbf558d9a8e9856017568016c92473dada4/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "7cc960f8f57e7fe90ee7aa0ccd3e3c6c89ec5a25",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7cc960f8f57e7fe90ee7aa0ccd3e3c6c89ec5a25",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/7cc960f8f57e7fe90ee7aa0ccd3e3c6c89ec5a25"
      }
    ],
    "stats": {
      "total": 9,
      "additions": 6,
      "deletions": 3
    },
    "files": [
      {
        "sha": "30215d1bc595833da41f3cbdcdeb2cf35314ce3a",
        "filename": "src/net.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/65c20dbf558d9a8e9856017568016c92473dada4/src/net.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/65c20dbf558d9a8e9856017568016c92473dada4/src/net.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/net.cpp?ref=65c20dbf558d9a8e9856017568016c92473dada4",
        "patch": "@@ -21,6 +21,9 @@\n #include <miniupnpc/upnperrors.h>\n #endif\n \n+// Dump addresses to peers.dat every 15 minutes (900s)\n+#define DUMP_ADDRESSES_INTERVAL 900\n+\n using namespace std;\n using namespace boost;\n \n@@ -1855,7 +1858,7 @@ void StartNode(boost::thread_group& threadGroup)\n     threadGroup.create_thread(boost::bind(&TraceThread<void (*)()>, \"msghand\", &ThreadMessageHandler));\n \n     // Dump network addresses\n-    threadGroup.create_thread(boost::bind(&LoopForever<void (*)()>, \"dumpaddr\", &DumpAddresses, 10000));\n+    threadGroup.create_thread(boost::bind(&LoopForever<void (*)()>, \"dumpaddr\", &DumpAddresses, DUMP_ADDRESSES_INTERVAL * 1000));\n }\n \n bool StopNode()"
      },
      {
        "sha": "3f3dd0f4871c4af1c327b7c72a66d831307767c5",
        "filename": "src/util.h",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/65c20dbf558d9a8e9856017568016c92473dada4/src/util.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/65c20dbf558d9a8e9856017568016c92473dada4/src/util.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/util.h?ref=65c20dbf558d9a8e9856017568016c92473dada4",
        "patch": "@@ -531,7 +531,7 @@ inline uint32_t ByteReverse(uint32_t value)\n // Standard wrapper for do-something-forever thread functions.\n // \"Forever\" really means until the thread is interrupted.\n // Use it like:\n-//   new boost::thread(boost::bind(&LoopForever<void (*)()>, \"dumpaddr\", &DumpAddresses, 10000));\n+//   new boost::thread(boost::bind(&LoopForever<void (*)()>, \"dumpaddr\", &DumpAddresses, 900000));\n // or maybe:\n //    boost::function<void()> f = boost::bind(&FunctionWithArg, argument);\n //    threadGroup.create_thread(boost::bind(&LoopForever<boost::function<void()> >, \"nothing\", f, milliseconds));\n@@ -544,8 +544,8 @@ template <typename Callable> void LoopForever(const char* name,  Callable func,\n     {\n         while (1)\n         {\n-            func();\n             MilliSleep(msecs);\n+            func();\n         }\n     }\n     catch (boost::thread_interrupted)"
      }
    ]
  },
  {
    "sha": "40809aed657502e9de158e2cfe2c659a316f2f90",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo0MDgwOWFlZDY1NzUwMmU5ZGUxNThlMmNmZTJjNjU5YTMxNmYyZjkw",
    "commit": {
      "author": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-06-25T14:27:24Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-06-25T14:27:24Z"
      },
      "message": "Bump version numbers for 0.8.3 release",
      "tree": {
        "sha": "f9a9c50a415c6ce26915ce1fa36686b9360a95cc",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/f9a9c50a415c6ce26915ce1fa36686b9360a95cc"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/40809aed657502e9de158e2cfe2c659a316f2f90",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/40809aed657502e9de158e2cfe2c659a316f2f90",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/40809aed657502e9de158e2cfe2c659a316f2f90",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/40809aed657502e9de158e2cfe2c659a316f2f90/comments",
    "author": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "65c20dbf558d9a8e9856017568016c92473dada4",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/65c20dbf558d9a8e9856017568016c92473dada4",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/65c20dbf558d9a8e9856017568016c92473dada4"
      }
    ],
    "stats": {
      "total": 31,
      "additions": 21,
      "deletions": 10
    },
    "files": [
      {
        "sha": "05de05184f5b49f5cd150d969648729197fd42da",
        "filename": "bitcoin-qt.pro",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/40809aed657502e9de158e2cfe2c659a316f2f90/bitcoin-qt.pro",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/40809aed657502e9de158e2cfe2c659a316f2f90/bitcoin-qt.pro",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/bitcoin-qt.pro?ref=40809aed657502e9de158e2cfe2c659a316f2f90",
        "patch": "@@ -1,7 +1,7 @@\n TEMPLATE = app\n TARGET = bitcoin-qt\n macx:TARGET = \"Bitcoin-Qt\"\n-VERSION = 0.8.2\n+VERSION = 0.8.3\n INCLUDEPATH += src src/json src/qt\n QT += network\n DEFINES += QT_GUI BOOST_THREAD_USE_LIB BOOST_SPIRIT_THREADSAFE"
      },
      {
        "sha": "d449211ce72492a0c4c81c96f3ae614e0f57e19b",
        "filename": "contrib/verifysfbinaries/verify.sh",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/40809aed657502e9de158e2cfe2c659a316f2f90/contrib/verifysfbinaries/verify.sh",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/40809aed657502e9de158e2cfe2c659a316f2f90/contrib/verifysfbinaries/verify.sh",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/contrib/verifysfbinaries/verify.sh?ref=40809aed657502e9de158e2cfe2c659a316f2f90",
        "patch": "@@ -18,7 +18,7 @@ WORKINGDIR=\"/tmp/bitcoin\"\n TMPFILE=\"hashes.tmp\"\n \n #this URL is used if a version number is not specified as an argument to the script\n-SIGNATUREFILE=\"http://downloads.sourceforge.net/project/bitcoin/Bitcoin/bitcoin-0.8.2/SHA256SUMS.asc\"\n+SIGNATUREFILE=\"http://downloads.sourceforge.net/project/bitcoin/Bitcoin/bitcoin-0.8.3/SHA256SUMS.asc\"\n \n SIGNATUREFILENAME=\"SHA256SUMS.asc\"\n RCSUBDIR=\"test/\""
      },
      {
        "sha": "db5970f2db698b0b65123ff4d53111421b619081",
        "filename": "doc/README.md",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/40809aed657502e9de158e2cfe2c659a316f2f90/doc/README.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/40809aed657502e9de158e2cfe2c659a316f2f90/doc/README.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/README.md?ref=40809aed657502e9de158e2cfe2c659a316f2f90",
        "patch": "@@ -1,4 +1,4 @@\n-Bitcoin 0.8.2 BETA\n+Bitcoin 0.8.3 BETA\n ====================\n \n Copyright (c) 2009-2013 Bitcoin Developers\n@@ -43,4 +43,4 @@ Other Pages\n - [Release Notes](release-notes.md)\n - [Multiwallet Qt Development](multiwallet-qt.md)\n - [Unit Tests](unit-tests.md)\n-- [Translation Process](translation_process.md)\n\\ No newline at end of file\n+- [Translation Process](translation_process.md)"
      },
      {
        "sha": "c968b3899f00c30fd41d6086d1a1b3de7ec54246",
        "filename": "doc/README_windows.txt",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/40809aed657502e9de158e2cfe2c659a316f2f90/doc/README_windows.txt",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/40809aed657502e9de158e2cfe2c659a316f2f90/doc/README_windows.txt",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/README_windows.txt?ref=40809aed657502e9de158e2cfe2c659a316f2f90",
        "patch": "@@ -1,4 +1,4 @@\n-Bitcoin 0.8.2 BETA\r\n+Bitcoin 0.8.3 BETA\r\n \r\n Copyright (c) 2009-2013 Bitcoin Developers\r\n Distributed under the MIT/X11 software license, see the accompanying\r"
      },
      {
        "sha": "b458c7c221575637f5c31ea22cd27aac8ee21a5f",
        "filename": "doc/release-notes.md",
        "status": "modified",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/40809aed657502e9de158e2cfe2c659a316f2f90/doc/release-notes.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/40809aed657502e9de158e2cfe2c659a316f2f90/doc/release-notes.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/release-notes.md?ref=40809aed657502e9de158e2cfe2c659a316f2f90",
        "patch": "@@ -1,6 +1,17 @@\n (note: this is a temporary file, to be added-to by anybody, and deleted at\n release time)\n \n+0.8.3 changes\n+=============\n+\n+Fix a memory exhaustion attack that could crash low-memory nodes.\n+\n+Fix a regression that caused excessive writing of the peers.dat file.\n+\n+\n+0.8.2 changes\n+=============\n+\n Fee Policy changes\n ------------------\n "
      },
      {
        "sha": "b0afd79dc249af6dbe1a345a88b01351f96c0675",
        "filename": "share/setup.nsi",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/40809aed657502e9de158e2cfe2c659a316f2f90/share/setup.nsi",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/40809aed657502e9de158e2cfe2c659a316f2f90/share/setup.nsi",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/share/setup.nsi?ref=40809aed657502e9de158e2cfe2c659a316f2f90",
        "patch": "@@ -5,7 +5,7 @@ SetCompressor /SOLID lzma\n \r\n # General Symbol Definitions\r\n !define REGKEY \"SOFTWARE\\$(^Name)\"\r\n-!define VERSION 0.8.2\r\n+!define VERSION 0.8.3\r\n !define COMPANY \"Bitcoin project\"\r\n !define URL http://www.bitcoin.org/\r\n \r\n@@ -45,13 +45,13 @@ Var StartMenuGroup\n !insertmacro MUI_LANGUAGE English\r\n \r\n # Installer attributes\r\n-OutFile bitcoin-0.8.2-win32-setup.exe\r\n+OutFile bitcoin-0.8.3-win32-setup.exe\r\n InstallDir $PROGRAMFILES\\Bitcoin\r\n CRCCheck on\r\n XPStyle on\r\n BrandingText \" \"\r\n ShowInstDetails show\r\n-VIProductVersion 0.8.2.2\r\n+VIProductVersion 0.8.3.0\r\n VIAddVersionKey ProductName Bitcoin\r\n VIAddVersionKey ProductVersion \"${VERSION}\"\r\n VIAddVersionKey CompanyName \"${COMPANY}\"\r"
      },
      {
        "sha": "9be31b33596ed95974054a7f3ce19e9f9f711d2f",
        "filename": "src/clientversion.h",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/40809aed657502e9de158e2cfe2c659a316f2f90/src/clientversion.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/40809aed657502e9de158e2cfe2c659a316f2f90/src/clientversion.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/clientversion.h?ref=40809aed657502e9de158e2cfe2c659a316f2f90",
        "patch": "@@ -8,8 +8,8 @@\n // These need to be macros, as version.cpp's and bitcoin-qt.rc's voodoo requires it\n #define CLIENT_VERSION_MAJOR       0\n #define CLIENT_VERSION_MINOR       8\n-#define CLIENT_VERSION_REVISION    2\n-#define CLIENT_VERSION_BUILD       2\n+#define CLIENT_VERSION_REVISION    3\n+#define CLIENT_VERSION_BUILD       0\n \n // Set to true for release, false for prerelease or test build\n #define CLIENT_VERSION_IS_RELEASE  true"
      }
    ]
  },
  {
    "sha": "6929f2a45fad8949b71340b79af4a49dec00d3f7",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo2OTI5ZjJhNDVmYWQ4OTQ5YjcxMzQwYjc5YWY0YTQ5ZGVjMDBkM2Y3",
    "commit": {
      "author": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T01:20:04Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T01:20:04Z"
      },
      "message": "Bump version numbers to prep for 0.8.4 release",
      "tree": {
        "sha": "dd761aeb58990554fd214ffdfdfe2cdfc04d639a",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/dd761aeb58990554fd214ffdfdfe2cdfc04d639a"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/6929f2a45fad8949b71340b79af4a49dec00d3f7",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6929f2a45fad8949b71340b79af4a49dec00d3f7",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/6929f2a45fad8949b71340b79af4a49dec00d3f7",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6929f2a45fad8949b71340b79af4a49dec00d3f7/comments",
    "author": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "40809aed657502e9de158e2cfe2c659a316f2f90",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/40809aed657502e9de158e2cfe2c659a316f2f90",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/40809aed657502e9de158e2cfe2c659a316f2f90"
      }
    ],
    "stats": {
      "total": 103,
      "additions": 9,
      "deletions": 94
    },
    "files": [
      {
        "sha": "c1e4b0b8828ce32cfb4bcf02aeb221f2fe05d358",
        "filename": "bitcoin-qt.pro",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/6929f2a45fad8949b71340b79af4a49dec00d3f7/bitcoin-qt.pro",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/6929f2a45fad8949b71340b79af4a49dec00d3f7/bitcoin-qt.pro",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/bitcoin-qt.pro?ref=6929f2a45fad8949b71340b79af4a49dec00d3f7",
        "patch": "@@ -1,7 +1,7 @@\n TEMPLATE = app\n TARGET = bitcoin-qt\n macx:TARGET = \"Bitcoin-Qt\"\n-VERSION = 0.8.3\n+VERSION = 0.8.4\n INCLUDEPATH += src src/json src/qt\n QT += network\n DEFINES += QT_GUI BOOST_THREAD_USE_LIB BOOST_SPIRIT_THREADSAFE"
      },
      {
        "sha": "68febf647d912387857f81877218fc0317ce1db8",
        "filename": "contrib/verifysfbinaries/verify.sh",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/6929f2a45fad8949b71340b79af4a49dec00d3f7/contrib/verifysfbinaries/verify.sh",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/6929f2a45fad8949b71340b79af4a49dec00d3f7/contrib/verifysfbinaries/verify.sh",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/contrib/verifysfbinaries/verify.sh?ref=6929f2a45fad8949b71340b79af4a49dec00d3f7",
        "patch": "@@ -18,7 +18,7 @@ WORKINGDIR=\"/tmp/bitcoin\"\n TMPFILE=\"hashes.tmp\"\n \n #this URL is used if a version number is not specified as an argument to the script\n-SIGNATUREFILE=\"http://downloads.sourceforge.net/project/bitcoin/Bitcoin/bitcoin-0.8.3/SHA256SUMS.asc\"\n+SIGNATUREFILE=\"http://downloads.sourceforge.net/project/bitcoin/Bitcoin/bitcoin-0.8.4/SHA256SUMS.asc\"\n \n SIGNATUREFILENAME=\"SHA256SUMS.asc\"\n RCSUBDIR=\"test/\""
      },
      {
        "sha": "f63296759ee67b9da8c121798fe00b7a82c1f88d",
        "filename": "doc/README.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/6929f2a45fad8949b71340b79af4a49dec00d3f7/doc/README.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/6929f2a45fad8949b71340b79af4a49dec00d3f7/doc/README.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/README.md?ref=6929f2a45fad8949b71340b79af4a49dec00d3f7",
        "patch": "@@ -1,4 +1,4 @@\n-Bitcoin 0.8.3 BETA\n+Bitcoin 0.8.4 BETA\n ====================\n \n Copyright (c) 2009-2013 Bitcoin Developers"
      },
      {
        "sha": "46f6c1aed8679391662b112e25e499fd4c073045",
        "filename": "doc/README_windows.txt",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/6929f2a45fad8949b71340b79af4a49dec00d3f7/doc/README_windows.txt",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/6929f2a45fad8949b71340b79af4a49dec00d3f7/doc/README_windows.txt",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/README_windows.txt?ref=6929f2a45fad8949b71340b79af4a49dec00d3f7",
        "patch": "@@ -1,4 +1,4 @@\n-Bitcoin 0.8.3 BETA\r\n+Bitcoin 0.8.4 BETA\r\n \r\n Copyright (c) 2009-2013 Bitcoin Developers\r\n Distributed under the MIT/X11 software license, see the accompanying\r"
      },
      {
        "sha": "33fc351860820181bc7c7402c54d0eb5e65610e4",
        "filename": "doc/release-notes.md",
        "status": "modified",
        "additions": 1,
        "deletions": 86,
        "changes": 87,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/6929f2a45fad8949b71340b79af4a49dec00d3f7/doc/release-notes.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/6929f2a45fad8949b71340b79af4a49dec00d3f7/doc/release-notes.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/release-notes.md?ref=6929f2a45fad8949b71340b79af4a49dec00d3f7",
        "patch": "@@ -1,91 +1,6 @@\n (note: this is a temporary file, to be added-to by anybody, and deleted at\n release time)\n \n-0.8.3 changes\n+0.8.4 changes\n =============\n \n-Fix a memory exhaustion attack that could crash low-memory nodes.\n-\n-Fix a regression that caused excessive writing of the peers.dat file.\n-\n-\n-0.8.2 changes\n-=============\n-\n-Fee Policy changes\n-------------------\n-\n-The default fee for low-priority transactions is lowered from 0.0005 BTC \n-(for each 1,000 bytes in the transaction; an average transaction is\n-about 500 bytes) to 0.0001 BTC.\n-\n-Payments (transaction outputs) of 0.543 times the minimum relay fee\n-(0.00005430 BTC) are now considered 'non-standard', because storing them\n-costs the network more than they are worth and spending them will usually\n-cost their owner more in transaction fees than they are worth.\n-\n-Non-standard transactions are not relayed across the network, are not included\n-in blocks by most miners, and will not show up in your wallet until they are\n-included in a block.\n-\n-The default fee policy can be overridden using the -mintxfee and -minrelaytxfee\n-command-line options, but note that we intend to replace the hard-coded fees\n-with code that automatically calculates and suggests appropriate fees in the\n-0.9 release and note that if you set a fee policy significantly different from\n-the rest of the network your transactions may never confirm.\n-\n-Bitcoin-Qt changes\n-------------------\n-\n-- New icon and splash screen\n-- Improve reporting of synchronization process\n-- Remove hardcoded fee recommendations\n-- Improve metadata of executable on MacOSX and Windows\n-- Move export button to individual tabs instead of toolbar\n-- Add \"send coins\" command to context menu in address book\n-- Add \"copy txid\" command to copy transaction IDs from transaction overview\n-- Save & restore window size and position when showing & hiding window\n-- New translations: Arabic (ar), Bosnian (bs), Catalan (ca), Welsh (cy), Esperanto (eo), Interlingua (la), Latvian (lv) and many improvements to current translations\n-\n-MacOSX:\n-\n-- OSX support for click-to-pay (bitcoin:) links\n-- Fix GUI disappearing problem on MacOSX (issue #1522)\n-\n-Linux/Unix:\n-\n-- Copy addresses to middle-mouse-button clipboard\n-\n-\n-Command-line options\n---------------------\n-\n-* `-walletnotify` will call a command on receiving transactions that affect the wallet.\n-* `-alertnotify` will call a command on receiving an alert from the network.\n-* `-par` now takes a negative number, to leave a certain amount of cores free.\n-\n-JSON-RPC API changes\n---------------------\n-\n-* `listunspent` now lists account and address infromation.\n-* `getinfo` now also returns the time adjustment estimated from your peers.\n-* `getpeerinfo` now returns bytessent, bytesrecv and syncnode.\n-* `gettxoutsetinfo` returns statistics about the unspent transaction output database.\n-* `gettxout` returns information about a specific unspent transaction output.\n-\n-\n-Networking changes\n-------------------\n-\n-* Significant changes to the networking code, reducing latency and memory consumption.\n-* Avoid initial block download stalling.\n-* Remove IRC seeding support.\n-* Performance tweaks.\n-* Added testnet DNS seeds.\n-\n-Wallet compatibility/rescuing\n------------------------------\n-\n-* Cases where wallets cannot be opened in another version/installation should be reduced.\n-* `-salvagewallet` now works for encrypted wallets.\n-"
      },
      {
        "sha": "a4ad1299392d7e74c89df194c2be338438c7b956",
        "filename": "share/setup.nsi",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/6929f2a45fad8949b71340b79af4a49dec00d3f7/share/setup.nsi",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/6929f2a45fad8949b71340b79af4a49dec00d3f7/share/setup.nsi",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/share/setup.nsi?ref=6929f2a45fad8949b71340b79af4a49dec00d3f7",
        "patch": "@@ -5,7 +5,7 @@ SetCompressor /SOLID lzma\n \r\n # General Symbol Definitions\r\n !define REGKEY \"SOFTWARE\\$(^Name)\"\r\n-!define VERSION 0.8.3\r\n+!define VERSION 0.8.4\r\n !define COMPANY \"Bitcoin project\"\r\n !define URL http://www.bitcoin.org/\r\n \r\n@@ -45,13 +45,13 @@ Var StartMenuGroup\n !insertmacro MUI_LANGUAGE English\r\n \r\n # Installer attributes\r\n-OutFile bitcoin-0.8.3-win32-setup.exe\r\n+OutFile bitcoin-0.8.4-win32-setup.exe\r\n InstallDir $PROGRAMFILES\\Bitcoin\r\n CRCCheck on\r\n XPStyle on\r\n BrandingText \" \"\r\n ShowInstDetails show\r\n-VIProductVersion 0.8.3.0\r\n+VIProductVersion 0.8.4.0\r\n VIAddVersionKey ProductName Bitcoin\r\n VIAddVersionKey ProductVersion \"${VERSION}\"\r\n VIAddVersionKey CompanyName \"${COMPANY}\"\r"
      },
      {
        "sha": "07cac5cd6b7269378b70d42eb9a1d3f1253883b7",
        "filename": "src/clientversion.h",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/6929f2a45fad8949b71340b79af4a49dec00d3f7/src/clientversion.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/6929f2a45fad8949b71340b79af4a49dec00d3f7/src/clientversion.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/clientversion.h?ref=6929f2a45fad8949b71340b79af4a49dec00d3f7",
        "patch": "@@ -8,7 +8,7 @@\n // These need to be macros, as version.cpp's and bitcoin-qt.rc's voodoo requires it\n #define CLIENT_VERSION_MAJOR       0\n #define CLIENT_VERSION_MINOR       8\n-#define CLIENT_VERSION_REVISION    3\n+#define CLIENT_VERSION_REVISION    4\n #define CLIENT_VERSION_BUILD       0\n \n // Set to true for release, false for prerelease or test build"
      }
    ]
  },
  {
    "sha": "6f315b4016b61672cc5490b2618b933809403d19",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo2ZjMxNWI0MDE2YjYxNjcyY2M1NDkwYjI2MThiOTMzODA5NDAzZDE5",
    "commit": {
      "author": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-06-27T00:31:34Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T02:18:39Z"
      },
      "message": "Fix Gnome bitcoin: URI handler",
      "tree": {
        "sha": "8ae0584002ab277325a55eca21542ed5311299ed",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/8ae0584002ab277325a55eca21542ed5311299ed"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/6f315b4016b61672cc5490b2618b933809403d19",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6f315b4016b61672cc5490b2618b933809403d19",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/6f315b4016b61672cc5490b2618b933809403d19",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6f315b4016b61672cc5490b2618b933809403d19/comments",
    "author": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "6929f2a45fad8949b71340b79af4a49dec00d3f7",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6929f2a45fad8949b71340b79af4a49dec00d3f7",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/6929f2a45fad8949b71340b79af4a49dec00d3f7"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 1,
      "deletions": 1
    },
    "files": [
      {
        "sha": "b2a2cef622ed7cd73da65cd139b7b53a7c067323",
        "filename": "contrib/debian/bitcoin-qt.desktop",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/6f315b4016b61672cc5490b2618b933809403d19/contrib/debian/bitcoin-qt.desktop",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/6f315b4016b61672cc5490b2618b933809403d19/contrib/debian/bitcoin-qt.desktop",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/contrib/debian/bitcoin-qt.desktop?ref=6f315b4016b61672cc5490b2618b933809403d19",
        "patch": "@@ -4,7 +4,7 @@ Name=Bitcoin\n Comment=Bitcoin P2P Cryptocurrency\n Comment[fr]=Bitcoin, monnaie virtuelle cryptographique pair \u00e0 pair\n Comment[tr]=Bitcoin, e\u015ften e\u015fe kriptografik sanal para birimi\n-Exec=/usr/bin/bitcoin-qt\n+Exec=/usr/bin/bitcoin-qt %u\n Terminal=false\n Type=Application\n Icon=/usr/share/pixmaps/bitcoin128.png"
      }
    ]
  },
  {
    "sha": "38863afbcc6ddb8a247210ac1d7c5d9717265339",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzozODg2M2FmYmNjNmRkYjhhMjQ3MjEwYWMxZDdjNWQ5NzE3MjY1MzM5",
    "commit": {
      "author": {
        "name": "Matt Corallo",
        "email": "git@bluematt.me",
        "date": "2013-07-23T15:51:28Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T02:18:50Z"
      },
      "message": "Fix multi-block reorg transaction resurrection",
      "tree": {
        "sha": "62a096e6280aec976dd955c44009aa7ff3f7ebe6",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/62a096e6280aec976dd955c44009aa7ff3f7ebe6"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/38863afbcc6ddb8a247210ac1d7c5d9717265339",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/38863afbcc6ddb8a247210ac1d7c5d9717265339",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/38863afbcc6ddb8a247210ac1d7c5d9717265339",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/38863afbcc6ddb8a247210ac1d7c5d9717265339/comments",
    "author": {
      "login": "TheBlueMatt",
      "id": 649246,
      "node_id": "MDQ6VXNlcjY0OTI0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/TheBlueMatt",
      "html_url": "https://github.com/TheBlueMatt",
      "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
      "following_url": "https://api.github.com/users/TheBlueMatt/following{/other_user}",
      "gists_url": "https://api.github.com/users/TheBlueMatt/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/TheBlueMatt/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
      "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
      "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
      "events_url": "https://api.github.com/users/TheBlueMatt/events{/privacy}",
      "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "6f315b4016b61672cc5490b2618b933809403d19",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6f315b4016b61672cc5490b2618b933809403d19",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/6f315b4016b61672cc5490b2618b933809403d19"
      }
    ],
    "stats": {
      "total": 6,
      "additions": 3,
      "deletions": 3
    },
    "files": [
      {
        "sha": "effac19fb5fe61bca326613bd063f6f0c0ef66e6",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/38863afbcc6ddb8a247210ac1d7c5d9717265339/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/38863afbcc6ddb8a247210ac1d7c5d9717265339/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=38863afbcc6ddb8a247210ac1d7c5d9717265339",
        "patch": "@@ -1778,7 +1778,7 @@ bool SetBestChain(CValidationState &state, CBlockIndex* pindexNew)\n     }\n \n     // Disconnect shorter branch\n-    vector<CTransaction> vResurrect;\n+    list<CTransaction> vResurrect;\n     BOOST_FOREACH(CBlockIndex* pindex, vDisconnect) {\n         CBlock block;\n         if (!block.ReadFromDisk(pindex))\n@@ -1792,9 +1792,9 @@ bool SetBestChain(CValidationState &state, CBlockIndex* pindexNew)\n         // Queue memory transactions to resurrect.\n         // We only do this for blocks after the last checkpoint (reorganisation before that\n         // point should only happen with -reindex/-loadblock, or a misbehaving peer.\n-        BOOST_FOREACH(const CTransaction& tx, block.vtx)\n+        BOOST_REVERSE_FOREACH(const CTransaction& tx, block.vtx)\n             if (!tx.IsCoinBase() && pindex->nHeight > Checkpoints::GetTotalBlocksEstimate())\n-                vResurrect.push_back(tx);\n+                vResurrect.push_front(tx);\n     }\n \n     // Connect longer branch"
      }
    ]
  },
  {
    "sha": "cdb3441b5cd2c1bae49fae671dc4a496f7c96322",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpjZGIzNDQxYjVjZDJjMWJhZTQ5ZmFlNjcxZGM0YTQ5NmY3Yzk2MzIy",
    "commit": {
      "author": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-08T09:58:57Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T02:19:40Z"
      },
      "message": "Make RPC password resistant to timing attacks\n\nFixes issue#2838; this is a tweaked version of pull#2845 that\nshould not leak the length of the password and is more generic,\nin case we run into other situations where we need\ntiming-attack-resistant comparisons.",
      "tree": {
        "sha": "920b43f3e70c3801375c10ab728070a8eaaa320e",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/920b43f3e70c3801375c10ab728070a8eaaa320e"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/cdb3441b5cd2c1bae49fae671dc4a496f7c96322",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/cdb3441b5cd2c1bae49fae671dc4a496f7c96322",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/cdb3441b5cd2c1bae49fae671dc4a496f7c96322",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/cdb3441b5cd2c1bae49fae671dc4a496f7c96322/comments",
    "author": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "38863afbcc6ddb8a247210ac1d7c5d9717265339",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/38863afbcc6ddb8a247210ac1d7c5d9717265339",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/38863afbcc6ddb8a247210ac1d7c5d9717265339"
      }
    ],
    "stats": {
      "total": 28,
      "additions": 27,
      "deletions": 1
    },
    "files": [
      {
        "sha": "31452fa1e73393288ce0f5c3b75be5ca618ef9f1",
        "filename": "src/bitcoinrpc.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cdb3441b5cd2c1bae49fae671dc4a496f7c96322/src/bitcoinrpc.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cdb3441b5cd2c1bae49fae671dc4a496f7c96322/src/bitcoinrpc.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/bitcoinrpc.cpp?ref=cdb3441b5cd2c1bae49fae671dc4a496f7c96322",
        "patch": "@@ -479,7 +479,7 @@ bool HTTPAuthorized(map<string, string>& mapHeaders)\n         return false;\n     string strUserPass64 = strAuth.substr(6); boost::trim(strUserPass64);\n     string strUserPass = DecodeBase64(strUserPass64);\n-    return strUserPass == strRPCUserColonPass;\n+    return TimingResistantEqual(strUserPass, strRPCUserColonPass);\n }\n \n //"
      },
      {
        "sha": "9ca0eef7aa1e3d859e6ceeffc28411bc22e3fa03",
        "filename": "src/test/util_tests.cpp",
        "status": "modified",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cdb3441b5cd2c1bae49fae671dc4a496f7c96322/src/test/util_tests.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cdb3441b5cd2c1bae49fae671dc4a496f7c96322/src/test/util_tests.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/test/util_tests.cpp?ref=cdb3441b5cd2c1bae49fae671dc4a496f7c96322",
        "patch": "@@ -323,4 +323,15 @@ BOOST_AUTO_TEST_CASE(util_seed_insecure_rand)\n     }\n }\n \n+BOOST_AUTO_TEST_CASE(util_TimingResistantEqual)\n+{\n+    BOOST_CHECK(TimingResistantEqual(std::string(\"\"), std::string(\"\")));\n+    BOOST_CHECK(!TimingResistantEqual(std::string(\"abc\"), std::string(\"\")));\n+    BOOST_CHECK(!TimingResistantEqual(std::string(\"\"), std::string(\"abc\")));\n+    BOOST_CHECK(!TimingResistantEqual(std::string(\"a\"), std::string(\"aa\")));\n+    BOOST_CHECK(!TimingResistantEqual(std::string(\"aa\"), std::string(\"a\")));\n+    BOOST_CHECK(TimingResistantEqual(std::string(\"abc\"), std::string(\"abc\")));\n+    BOOST_CHECK(!TimingResistantEqual(std::string(\"abc\"), std::string(\"aba\")));\n+}\n+\n BOOST_AUTO_TEST_SUITE_END()"
      },
      {
        "sha": "9b7e2573da2d86ac127e5763e08942e4afc94fa3",
        "filename": "src/util.h",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cdb3441b5cd2c1bae49fae671dc4a496f7c96322/src/util.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cdb3441b5cd2c1bae49fae671dc4a496f7c96322/src/util.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/util.h?ref=cdb3441b5cd2c1bae49fae671dc4a496f7c96322",
        "patch": "@@ -433,6 +433,21 @@ static inline uint32_t insecure_rand(void)\n  */\n void seed_insecure_rand(bool fDeterministic=false);\n \n+/**\n+ * Timing-attack-resistant comparison.\n+ * Takes time proportional to length\n+ * of first argument.\n+ */\n+template <typename T>\n+bool TimingResistantEqual(const T& a, const T& b)\n+{\n+    if (b.size() == 0) return a.size() == 0;\n+    size_t accumulator = a.size() ^ b.size();\n+    for (size_t i = 0; i < a.size(); i++)\n+        accumulator |= a[i] ^ b[i%b.size()];\n+    return accumulator == 0;\n+}\n+\n /** Median filter over a stream of values.\n  * Returns the median of the last N numbers\n  */"
      }
    ]
  },
  {
    "sha": "08dd92060bc2e79ef62b383a8a71a07ce1e2699d",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzowOGRkOTIwNjBiYzJlNzllZjYyYjM4M2E4YTcxYTA3Y2UxZTI2OTlk",
    "commit": {
      "author": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T04:46:01Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T04:46:01Z"
      },
      "message": "Revert \"Truncate oversize 'tx' messages before relaying/storing.\"\n\nThis reverts commit 7cc960f8f57e7fe90ee7aa0ccd3e3c6c89ec5a25.",
      "tree": {
        "sha": "36f8ed532eaf96a5bb47e1a500e905ec29e1c393",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/36f8ed532eaf96a5bb47e1a500e905ec29e1c393"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/08dd92060bc2e79ef62b383a8a71a07ce1e2699d",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/08dd92060bc2e79ef62b383a8a71a07ce1e2699d",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/08dd92060bc2e79ef62b383a8a71a07ce1e2699d",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/08dd92060bc2e79ef62b383a8a71a07ce1e2699d/comments",
    "author": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "cdb3441b5cd2c1bae49fae671dc4a496f7c96322",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/cdb3441b5cd2c1bae49fae671dc4a496f7c96322",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/cdb3441b5cd2c1bae49fae671dc4a496f7c96322"
      }
    ],
    "stats": {
      "total": 10,
      "additions": 0,
      "deletions": 10
    },
    "files": [
      {
        "sha": "769f2dc41227dde22c430416c625b7b0943d787d",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 0,
        "deletions": 10,
        "changes": 10,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/08dd92060bc2e79ef62b383a8a71a07ce1e2699d/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/08dd92060bc2e79ef62b383a8a71a07ce1e2699d/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=08dd92060bc2e79ef62b383a8a71a07ce1e2699d",
        "patch": "@@ -3508,16 +3508,6 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n         CInv inv(MSG_TX, tx.GetHash());\n         pfrom->AddInventoryKnown(inv);\n \n-        // Truncate messages to the size of the tx in them\n-        unsigned int nSize = ::GetSerializeSize(tx, SER_NETWORK, PROTOCOL_VERSION);\n-        unsigned int oldSize = vMsg.size();\n-        if (nSize < oldSize) {\n-            vMsg.resize(nSize);\n-            printf(\"truncating oversized TX %s (%u -> %u)\\n\",\n-                   tx.GetHash().ToString().c_str(),\n-                   oldSize, nSize);\n-        }\n-\n         bool fMissingInputs = false;\n         CValidationState state;\n         if (tx.AcceptToMemoryPool(state, true, true, &fMissingInputs))"
      }
    ]
  },
  {
    "sha": "21696c12f3bdc3e24f5f6101644f0040a0f5f912",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzoyMTY5NmMxMmYzYmRjM2UyNGY1ZjYxMDE2NDRmMDA0MGEwZjVmOTEy",
    "commit": {
      "author": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-02T05:14:44Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T04:52:38Z"
      },
      "message": "Simplify storage of orphan transactions\n\nOrphan transactions were stored as a CDataStream pointer;\nthis changes the mapOrphanTransactions data structures to\nstore orphans as a CTransaction.\n\nThis also fixes CVE-2013-4627 by always re-serializing\ntransactions before relaying them.",
      "tree": {
        "sha": "c0f9ddba7073065f40fee7b5d96f07b0d92ecc70",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/c0f9ddba7073065f40fee7b5d96f07b0d92ecc70"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/21696c12f3bdc3e24f5f6101644f0040a0f5f912",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/21696c12f3bdc3e24f5f6101644f0040a0f5f912",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/21696c12f3bdc3e24f5f6101644f0040a0f5f912",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/21696c12f3bdc3e24f5f6101644f0040a0f5f912/comments",
    "author": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "08dd92060bc2e79ef62b383a8a71a07ce1e2699d",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/08dd92060bc2e79ef62b383a8a71a07ce1e2699d",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/08dd92060bc2e79ef62b383a8a71a07ce1e2699d"
      }
    ],
    "stats": {
      "total": 93,
      "additions": 36,
      "deletions": 57
    },
    "files": [
      {
        "sha": "73672eed69ed4c24be30386001951f4f098272eb",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 26,
        "deletions": 36,
        "changes": 62,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/21696c12f3bdc3e24f5f6101644f0040a0f5f912/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/21696c12f3bdc3e24f5f6101644f0040a0f5f912/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=21696c12f3bdc3e24f5f6101644f0040a0f5f912",
        "patch": "@@ -58,8 +58,8 @@ CMedianFilter<int> cPeerBlockCounts(8, 0); // Amount of blocks that other nodes\n map<uint256, CBlock*> mapOrphanBlocks;\n multimap<uint256, CBlock*> mapOrphanBlocksByPrev;\n \n-map<uint256, CDataStream*> mapOrphanTransactions;\n-map<uint256, map<uint256, CDataStream*> > mapOrphanTransactionsByPrev;\n+map<uint256, CTransaction> mapOrphanTransactions;\n+map<uint256, set<uint256> > mapOrphanTransactionsByPrev;\n \n // Constant stuff for coinbase transactions we create:\n CScript COINBASE_FLAGS;\n@@ -283,33 +283,29 @@ CBlockTreeDB *pblocktree = NULL;\n // mapOrphanTransactions\n //\n \n-bool AddOrphanTx(const CDataStream& vMsg)\n+bool AddOrphanTx(const CTransaction& tx)\n {\n-    CTransaction tx;\n-    CDataStream(vMsg) >> tx;\n     uint256 hash = tx.GetHash();\n     if (mapOrphanTransactions.count(hash))\n         return false;\n \n-    CDataStream* pvMsg = new CDataStream(vMsg);\n-\n     // Ignore big transactions, to avoid a\n     // send-big-orphans memory exhaustion attack. If a peer has a legitimate\n     // large transaction with a missing parent then we assume\n     // it will rebroadcast it later, after the parent transaction(s)\n     // have been mined or received.\n     // 10,000 orphans, each of which is at most 5,000 bytes big is\n     // at most 500 megabytes of orphans:\n-    if (pvMsg->size() > 5000)\n+    unsigned int sz = tx.GetSerializeSize(SER_NETWORK, CTransaction::CURRENT_VERSION);\n+    if (sz > 5000)\n     {\n-        printf(\"ignoring large orphan tx (size: %\"PRIszu\", hash: %s)\\n\", pvMsg->size(), hash.ToString().c_str());\n-        delete pvMsg;\n+        printf(\"ignoring large orphan tx (size: %u, hash: %s)\\n\", sz, hash.ToString().c_str());\n         return false;\n     }\n \n-    mapOrphanTransactions[hash] = pvMsg;\n+    mapOrphanTransactions[hash] = tx;\n     BOOST_FOREACH(const CTxIn& txin, tx.vin)\n-        mapOrphanTransactionsByPrev[txin.prevout.hash].insert(make_pair(hash, pvMsg));\n+        mapOrphanTransactionsByPrev[txin.prevout.hash].insert(hash);\n \n     printf(\"stored orphan tx %s (mapsz %\"PRIszu\")\\n\", hash.ToString().c_str(),\n         mapOrphanTransactions.size());\n@@ -320,16 +316,13 @@ void static EraseOrphanTx(uint256 hash)\n {\n     if (!mapOrphanTransactions.count(hash))\n         return;\n-    const CDataStream* pvMsg = mapOrphanTransactions[hash];\n-    CTransaction tx;\n-    CDataStream(*pvMsg) >> tx;\n+    const CTransaction& tx = mapOrphanTransactions[hash];\n     BOOST_FOREACH(const CTxIn& txin, tx.vin)\n     {\n         mapOrphanTransactionsByPrev[txin.prevout.hash].erase(hash);\n         if (mapOrphanTransactionsByPrev[txin.prevout.hash].empty())\n             mapOrphanTransactionsByPrev.erase(txin.prevout.hash);\n     }\n-    delete pvMsg;\n     mapOrphanTransactions.erase(hash);\n }\n \n@@ -340,7 +333,7 @@ unsigned int LimitOrphanTxSize(unsigned int nMaxOrphans)\n     {\n         // Evict a random orphan:\n         uint256 randomhash = GetRandHash();\n-        map<uint256, CDataStream*>::iterator it = mapOrphanTransactions.lower_bound(randomhash);\n+        map<uint256, CTransaction>::iterator it = mapOrphanTransactions.lower_bound(randomhash);\n         if (it == mapOrphanTransactions.end())\n             it = mapOrphanTransactions.begin();\n         EraseOrphanTx(it->first);\n@@ -824,7 +817,7 @@ bool CTransaction::AcceptToMemoryPool(CValidationState &state, bool fCheckInputs\n     }\n }\n \n-bool CTxMemPool::addUnchecked(const uint256& hash, CTransaction &tx)\n+bool CTxMemPool::addUnchecked(const uint256& hash, const CTransaction &tx)\n {\n     // Add to memory pool without checking anything.  Don't call this directly,\n     // call CTxMemPool::accept to properly check the transaction first.\n@@ -3512,7 +3505,7 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n         CValidationState state;\n         if (tx.AcceptToMemoryPool(state, true, true, &fMissingInputs))\n         {\n-            RelayTransaction(tx, inv.hash, vMsg);\n+            RelayTransaction(tx, inv.hash);\n             mapAlreadyAskedFor.erase(inv);\n             vWorkQueue.push_back(inv.hash);\n             vEraseQueue.push_back(inv.hash);\n@@ -3521,31 +3514,31 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n             for (unsigned int i = 0; i < vWorkQueue.size(); i++)\n             {\n                 uint256 hashPrev = vWorkQueue[i];\n-                for (map<uint256, CDataStream*>::iterator mi = mapOrphanTransactionsByPrev[hashPrev].begin();\n+                for (set<uint256>::iterator mi = mapOrphanTransactionsByPrev[hashPrev].begin();\n                      mi != mapOrphanTransactionsByPrev[hashPrev].end();\n                      ++mi)\n                 {\n-                    const CDataStream& vMsg = *((*mi).second);\n-                    CTransaction tx;\n-                    CDataStream(vMsg) >> tx;\n-                    CInv inv(MSG_TX, tx.GetHash());\n+                    const uint256& orphanHash = *mi;\n+                    const CTransaction& orphanTx = mapOrphanTransactions[orphanHash];\n                     bool fMissingInputs2 = false;\n-                    // Use a dummy CValidationState so someone can't setup nodes to counter-DoS based on orphan resolution (that is, feeding people an invalid transaction based on LegitTxX in order to get anyone relaying LegitTxX banned)\n+                    // Use a dummy CValidationState so someone can't setup nodes to counter-DoS based on orphan\n+                    // resolution (that is, feeding people an invalid transaction based on LegitTxX in order to get\n+                    // anyone relaying LegitTxX banned)\n                     CValidationState stateDummy;\n \n                     if (tx.AcceptToMemoryPool(stateDummy, true, true, &fMissingInputs2))\n                     {\n-                        printf(\"   accepted orphan tx %s\\n\", inv.hash.ToString().c_str());\n-                        RelayTransaction(tx, inv.hash, vMsg);\n-                        mapAlreadyAskedFor.erase(inv);\n-                        vWorkQueue.push_back(inv.hash);\n-                        vEraseQueue.push_back(inv.hash);\n+                        printf(\"   accepted orphan tx %s\\n\", orphanHash.ToString().c_str());\n+                        RelayTransaction(orphanTx, orphanHash);\n+                        mapAlreadyAskedFor.erase(CInv(MSG_TX, orphanHash));\n+                        vWorkQueue.push_back(orphanHash);\n+                        vEraseQueue.push_back(orphanHash);\n                     }\n                     else if (!fMissingInputs2)\n                     {\n                         // invalid or too-little-fee orphan\n-                        vEraseQueue.push_back(inv.hash);\n-                        printf(\"   removed orphan tx %s\\n\", inv.hash.ToString().c_str());\n+                        vEraseQueue.push_back(orphanHash);\n+                        printf(\"   removed orphan tx %s\\n\", orphanHash.ToString().c_str());\n                     }\n                 }\n             }\n@@ -3555,7 +3548,7 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n         }\n         else if (fMissingInputs)\n         {\n-            AddOrphanTx(vMsg);\n+            AddOrphanTx(tx);\n \n             // DoS prevention: do not allow mapOrphanTransactions to grow unbounded\n             unsigned int nEvicted = LimitOrphanTxSize(MAX_ORPHAN_TRANSACTIONS);\n@@ -4741,9 +4734,6 @@ class CMainCleanup\n         mapOrphanBlocks.clear();\n \n         // orphan transactions\n-        std::map<uint256, CDataStream*>::iterator it3 = mapOrphanTransactions.begin();\n-        for (; it3 != mapOrphanTransactions.end(); it3++)\n-            delete (*it3).second;\n         mapOrphanTransactions.clear();\n     }\n } instance_of_cmaincleanup;"
      },
      {
        "sha": "d5cd4cf0a6b2081fea8a8afe4d03bd7d86c12a60",
        "filename": "src/main.h",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/21696c12f3bdc3e24f5f6101644f0040a0f5f912/src/main.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/21696c12f3bdc3e24f5f6101644f0040a0f5f912/src/main.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.h?ref=21696c12f3bdc3e24f5f6101644f0040a0f5f912",
        "patch": "@@ -2067,7 +2067,7 @@ class CTxMemPool\n     std::map<COutPoint, CInPoint> mapNextTx;\n \n     bool accept(CValidationState &state, CTransaction &tx, bool fCheckInputs, bool fLimitFree, bool* pfMissingInputs);\n-    bool addUnchecked(const uint256& hash, CTransaction &tx);\n+    bool addUnchecked(const uint256& hash, const CTransaction &tx);\n     bool remove(const CTransaction &tx, bool fRecursive = false);\n     bool removeConflicts(const CTransaction &tx);\n     void clear();"
      },
      {
        "sha": "2ee64755d3a7a6228a304af044f6fec2256128e9",
        "filename": "src/test/DoS_tests.cpp",
        "status": "modified",
        "additions": 9,
        "deletions": 20,
        "changes": 29,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/21696c12f3bdc3e24f5f6101644f0040a0f5f912/src/test/DoS_tests.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/21696c12f3bdc3e24f5f6101644f0040a0f5f912/src/test/DoS_tests.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/test/DoS_tests.cpp?ref=21696c12f3bdc3e24f5f6101644f0040a0f5f912",
        "patch": "@@ -16,10 +16,10 @@\n #include <stdint.h>\n \n // Tests this internal-to-main.cpp method:\n-extern bool AddOrphanTx(const CDataStream& vMsg);\n+extern bool AddOrphanTx(const CTransaction& tx);\n extern unsigned int LimitOrphanTxSize(unsigned int nMaxOrphans);\n-extern std::map<uint256, CDataStream*> mapOrphanTransactions;\n-extern std::map<uint256, std::map<uint256, CDataStream*> > mapOrphanTransactionsByPrev;\n+extern std::map<uint256, CTransaction> mapOrphanTransactions;\n+extern std::map<uint256, std::set<uint256> > mapOrphanTransactionsByPrev;\n \n CService ip(uint32_t i)\n {\n@@ -133,14 +133,11 @@ BOOST_AUTO_TEST_CASE(DoS_checknbits)\n \n CTransaction RandomOrphan()\n {\n-    std::map<uint256, CDataStream*>::iterator it;\n+    std::map<uint256, CTransaction>::iterator it;\n     it = mapOrphanTransactions.lower_bound(GetRandHash());\n     if (it == mapOrphanTransactions.end())\n         it = mapOrphanTransactions.begin();\n-    const CDataStream* pvMsg = it->second;\n-    CTransaction tx;\n-    CDataStream(*pvMsg) >> tx;\n-    return tx;\n+    return it->second;\n }\n \n BOOST_AUTO_TEST_CASE(DoS_mapOrphans)\n@@ -162,9 +159,7 @@ BOOST_AUTO_TEST_CASE(DoS_mapOrphans)\n         tx.vout[0].nValue = 1*CENT;\n         tx.vout[0].scriptPubKey.SetDestination(key.GetPubKey().GetID());\n \n-        CDataStream ds(SER_DISK, CLIENT_VERSION);\n-        ds << tx;\n-        AddOrphanTx(ds);\n+        AddOrphanTx(tx);\n     }\n \n     // ... and 50 that depend on other orphans:\n@@ -181,9 +176,7 @@ BOOST_AUTO_TEST_CASE(DoS_mapOrphans)\n         tx.vout[0].scriptPubKey.SetDestination(key.GetPubKey().GetID());\n         SignSignature(keystore, txPrev, tx, 0);\n \n-        CDataStream ds(SER_DISK, CLIENT_VERSION);\n-        ds << tx;\n-        AddOrphanTx(ds);\n+        AddOrphanTx(tx);\n     }\n \n     // This really-big orphan should be ignored:\n@@ -207,9 +200,7 @@ BOOST_AUTO_TEST_CASE(DoS_mapOrphans)\n         for (unsigned int j = 1; j < tx.vin.size(); j++)\n             tx.vin[j].scriptSig = tx.vin[0].scriptSig;\n \n-        CDataStream ds(SER_DISK, CLIENT_VERSION);\n-        ds << tx;\n-        BOOST_CHECK(!AddOrphanTx(ds));\n+        BOOST_CHECK(!AddOrphanTx(tx));\n     }\n \n     // Test LimitOrphanTxSize() function:\n@@ -246,9 +237,7 @@ BOOST_AUTO_TEST_CASE(DoS_checkSig)\n         tx.vout[0].nValue = 1*CENT;\n         tx.vout[0].scriptPubKey.SetDestination(key.GetPubKey().GetID());\n \n-        CDataStream ds(SER_DISK, CLIENT_VERSION);\n-        ds << tx;\n-        AddOrphanTx(ds);\n+        AddOrphanTx(tx);\n     }\n \n     // Create a transaction that depends on orphans:"
      }
    ]
  },
  {
    "sha": "20b611770f6900209f125f7cd4bf45fdb1fe4370",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzoyMGI2MTE3NzBmNjkwMDIwOWYxMjVmN2NkNGJmNDVmZGIxZmU0Mzcw",
    "commit": {
      "author": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T07:30:33Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T07:36:50Z"
      },
      "message": "Checkpoint at block 250,000",
      "tree": {
        "sha": "7aebb0db074618121e953e64cc4efaafdf075e9c",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/7aebb0db074618121e953e64cc4efaafdf075e9c"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/20b611770f6900209f125f7cd4bf45fdb1fe4370",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/20b611770f6900209f125f7cd4bf45fdb1fe4370",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/20b611770f6900209f125f7cd4bf45fdb1fe4370",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/20b611770f6900209f125f7cd4bf45fdb1fe4370/comments",
    "author": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "21696c12f3bdc3e24f5f6101644f0040a0f5f912",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/21696c12f3bdc3e24f5f6101644f0040a0f5f912",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/21696c12f3bdc3e24f5f6101644f0040a0f5f912"
      }
    ],
    "stats": {
      "total": 5,
      "additions": 3,
      "deletions": 2
    },
    "files": [
      {
        "sha": "911b84f8d6740cf8359d32b16ca82497aca7cd5d",
        "filename": "src/checkpoints.cpp",
        "status": "modified",
        "additions": 3,
        "deletions": 2,
        "changes": 5,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/checkpoints.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/checkpoints.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/checkpoints.cpp?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -45,11 +45,12 @@ namespace Checkpoints\n         (210000, uint256(\"0x000000000000048b95347e83192f69cf0366076336c639f9b7228e9ba171342e\"))\n         (216116, uint256(\"0x00000000000001b4f4b433e81ee46494af945cf96014816a4e2370f11b23df4e\"))\n         (225430, uint256(\"0x00000000000001c108384350f74090433e7fcf79a606b8e797f065b130575932\"))\n+        (250000, uint256(\"0x000000000000003887df1f29024b06fc2200b55f8af8f35453d7be294df2d214\"))\n         ;\n     static const CCheckpointData data = {\n         &mapCheckpoints,\n-        1363044259, // * UNIX timestamp of last checkpoint block\n-        14264869,   // * total number of transactions between genesis and last checkpoint\n+        1375533383, // * UNIX timestamp of last checkpoint block\n+        21491097,   // * total number of transactions between genesis and last checkpoint\n                     //   (the tx=... number in the SetBestChain debug.log lines)\n         60000.0     // * estimated number of transactions per day after checkpoint\n     };"
      }
    ]
  },
  {
    "sha": "5c739c574fdc9197b317221305e14e7701a452b3",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo1YzczOWM1NzRmZGM5MTk3YjMxNzIyMTMwNWUxNGU3NzAxYTQ1MmIz",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "sipa@ulyssis.org",
        "date": "2013-08-20T12:03:29Z"
      },
      "committer": {
        "name": "Pieter Wuille",
        "email": "sipa@ulyssis.org",
        "date": "2013-08-20T12:03:29Z"
      },
      "message": "Remove inline LevelDB to prepare for git-subtree version",
      "tree": {
        "sha": "a033f705990fffccaf8b9d2d58d045f8f9c50a5d",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/a033f705990fffccaf8b9d2d58d045f8f9c50a5d"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/5c739c574fdc9197b317221305e14e7701a452b3",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5c739c574fdc9197b317221305e14e7701a452b3",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/5c739c574fdc9197b317221305e14e7701a452b3",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5c739c574fdc9197b317221305e14e7701a452b3/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/20b611770f6900209f125f7cd4bf45fdb1fe4370"
      }
    ],
    "stats": {
      "total": 27582,
      "additions": 0,
      "deletions": 27582
    },
    "files": [
      {
        "sha": "55ba072e0e9de524f2a1f977378de28b78f75e5f",
        "filename": "src/leveldb/.gitignore",
        "status": "removed",
        "additions": 0,
        "deletions": 12,
        "changes": 12,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/.gitignore",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/.gitignore",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/.gitignore?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,12 +0,0 @@\n-build_config.mk\n-*.a\n-*.o\n-*.dylib*\n-*.so\n-*.so.*\n-*_test\n-db_bench\n-Release\n-Debug\n-Benchmark\n-vs2010.*"
      },
      {
        "sha": "27a9407e52fdc517f3ab28741e0426c3180d444e",
        "filename": "src/leveldb/AUTHORS",
        "status": "removed",
        "additions": 0,
        "deletions": 8,
        "changes": 8,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/AUTHORS",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/AUTHORS",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/AUTHORS?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,8 +0,0 @@\n-# Names should be added to this file like so:\n-# Name or Organization <email address>\n-\n-Google Inc.\n-\n-# Initial version authors:\n-Jeffrey Dean <jeff@google.com>\n-Sanjay Ghemawat <sanjay@google.com>"
      },
      {
        "sha": "8e80208cd72b3225c87d9111c4d7cab13af1c2ac",
        "filename": "src/leveldb/LICENSE",
        "status": "removed",
        "additions": 0,
        "deletions": 27,
        "changes": 27,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/LICENSE",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/LICENSE",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/LICENSE?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,27 +0,0 @@\n-Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-\n-Redistribution and use in source and binary forms, with or without\n-modification, are permitted provided that the following conditions are\n-met:\n-\n-   * Redistributions of source code must retain the above copyright\n-notice, this list of conditions and the following disclaimer.\n-   * Redistributions in binary form must reproduce the above\n-copyright notice, this list of conditions and the following disclaimer\n-in the documentation and/or other materials provided with the\n-distribution.\n-   * Neither the name of Google Inc. nor the names of its\n-contributors may be used to endorse or promote products derived from\n-this software without specific prior written permission.\n-\n-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n-\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n-LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n-A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n-OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n-SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n-LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n-DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n-THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n-(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
      },
      {
        "sha": "42c4952fec048deabc07a6ffff2b1b951a4fe52c",
        "filename": "src/leveldb/Makefile",
        "status": "removed",
        "additions": 0,
        "deletions": 202,
        "changes": 202,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/Makefile",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/Makefile",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/Makefile?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,202 +0,0 @@\n-# Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-# Use of this source code is governed by a BSD-style license that can be\n-# found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#-----------------------------------------------\n-# Uncomment exactly one of the lines labelled (A), (B), and (C) below\n-# to switch between compilation modes.\n-\n-OPT ?= -O2 -DNDEBUG       # (A) Production use (optimized mode)\n-# OPT ?= -g2              # (B) Debug mode, w/ full line-level debugging symbols\n-# OPT ?= -O2 -g2 -DNDEBUG # (C) Profiling mode: opt, but w/debugging symbols\n-#-----------------------------------------------\n-\n-# detect what platform we're building on\n-$(shell CC=$(CC) CXX=$(CXX) TARGET_OS=$(TARGET_OS) \\\n-    ./build_detect_platform build_config.mk ./)\n-# this file is generated by the previous line to set build flags and sources\n-include build_config.mk\n-\n-CFLAGS += -I. -I./include $(PLATFORM_CCFLAGS) $(OPT)\n-CXXFLAGS += -I. -I./include $(PLATFORM_CXXFLAGS) $(OPT)\n-\n-LDFLAGS += $(PLATFORM_LDFLAGS)\n-LIBS += $(PLATFORM_LIBS)\n-\n-LIBOBJECTS = $(SOURCES:.cc=.o)\n-MEMENVOBJECTS = $(MEMENV_SOURCES:.cc=.o)\n-\n-TESTUTIL = ./util/testutil.o\n-TESTHARNESS = ./util/testharness.o $(TESTUTIL)\n-\n-TESTS = \\\n-\tarena_test \\\n-\tbloom_test \\\n-\tc_test \\\n-\tcache_test \\\n-\tcoding_test \\\n-\tcorruption_test \\\n-\tcrc32c_test \\\n-\tdb_test \\\n-\tdbformat_test \\\n-\tenv_test \\\n-\tfilename_test \\\n-\tfilter_block_test \\\n-\tlog_test \\\n-\tmemenv_test \\\n-\tskiplist_test \\\n-\ttable_test \\\n-\tversion_edit_test \\\n-\tversion_set_test \\\n-\twrite_batch_test\n-\n-PROGRAMS = db_bench leveldbutil $(TESTS)\n-BENCHMARKS = db_bench_sqlite3 db_bench_tree_db\n-\n-LIBRARY = libleveldb.a\n-MEMENVLIBRARY = libmemenv.a\n-\n-default: all\n-\n-# Should we build shared libraries?\n-ifneq ($(PLATFORM_SHARED_EXT),)\n-\n-ifneq ($(PLATFORM_SHARED_VERSIONED),true)\n-SHARED1 = libleveldb.$(PLATFORM_SHARED_EXT)\n-SHARED2 = $(SHARED1)\n-SHARED3 = $(SHARED1)\n-SHARED = $(SHARED1)\n-else\n-# Update db.h if you change these.\n-SHARED_MAJOR = 1\n-SHARED_MINOR = 9\n-SHARED1 = libleveldb.$(PLATFORM_SHARED_EXT)\n-SHARED2 = $(SHARED1).$(SHARED_MAJOR)\n-SHARED3 = $(SHARED1).$(SHARED_MAJOR).$(SHARED_MINOR)\n-SHARED = $(SHARED1) $(SHARED2) $(SHARED3)\n-$(SHARED1): $(SHARED3)\n-\tln -fs $(SHARED3) $(SHARED1)\n-$(SHARED2): $(SHARED3)\n-\tln -fs $(SHARED3) $(SHARED2)\n-endif\n-\n-$(SHARED3):\n-\t$(CXX) $(LDFLAGS) $(PLATFORM_SHARED_LDFLAGS)$(SHARED2) $(CXXFLAGS) $(PLATFORM_SHARED_CFLAGS) $(SOURCES) -o $(SHARED3) $(LIBS)\n-\n-endif  # PLATFORM_SHARED_EXT\n-\n-all: $(SHARED) $(LIBRARY)\n-\n-check: all $(PROGRAMS) $(TESTS)\n-\tfor t in $(TESTS); do echo \"***** Running $$t\"; ./$$t || exit 1; done\n-\n-clean:\n-\t-rm -f $(PROGRAMS) $(BENCHMARKS) $(LIBRARY) $(SHARED) $(MEMENVLIBRARY) */*.o */*/*.o ios-x86/*/*.o ios-arm/*/*.o build_config.mk\n-\t-rm -rf ios-x86/* ios-arm/*\n-\n-$(LIBRARY): $(LIBOBJECTS)\n-\trm -f $@\n-\t$(AR) -rs $@ $(LIBOBJECTS)\n-\n-db_bench: db/db_bench.o $(LIBOBJECTS) $(TESTUTIL)\n-\t$(CXX) $(LDFLAGS) db/db_bench.o $(LIBOBJECTS) $(TESTUTIL) -o $@ $(LIBS)\n-\n-db_bench_sqlite3: doc/bench/db_bench_sqlite3.o $(LIBOBJECTS) $(TESTUTIL)\n-\t$(CXX) $(LDFLAGS) doc/bench/db_bench_sqlite3.o $(LIBOBJECTS) $(TESTUTIL) -o $@ -lsqlite3 $(LIBS)\n-\n-db_bench_tree_db: doc/bench/db_bench_tree_db.o $(LIBOBJECTS) $(TESTUTIL)\n-\t$(CXX) $(LDFLAGS) doc/bench/db_bench_tree_db.o $(LIBOBJECTS) $(TESTUTIL) -o $@ -lkyotocabinet $(LIBS)\n-\n-leveldbutil: db/leveldb_main.o $(LIBOBJECTS)\n-\t$(CXX) $(LDFLAGS) db/leveldb_main.o $(LIBOBJECTS) -o $@ $(LIBS)\n-\n-arena_test: util/arena_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) util/arena_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-bloom_test: util/bloom_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) util/bloom_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-c_test: db/c_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) db/c_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-cache_test: util/cache_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) util/cache_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-coding_test: util/coding_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) util/coding_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-corruption_test: db/corruption_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) db/corruption_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-crc32c_test: util/crc32c_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) util/crc32c_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-db_test: db/db_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) db/db_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-dbformat_test: db/dbformat_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) db/dbformat_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-env_test: util/env_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) util/env_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-filename_test: db/filename_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) db/filename_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-filter_block_test: table/filter_block_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) table/filter_block_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-log_test: db/log_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) db/log_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-table_test: table/table_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) table/table_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-skiplist_test: db/skiplist_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) db/skiplist_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-version_edit_test: db/version_edit_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) db/version_edit_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-version_set_test: db/version_set_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) db/version_set_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-write_batch_test: db/write_batch_test.o $(LIBOBJECTS) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) db/write_batch_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-$(MEMENVLIBRARY) : $(MEMENVOBJECTS)\n-\trm -f $@\n-\t$(AR) -rs $@ $(MEMENVOBJECTS)\n-\n-memenv_test : helpers/memenv/memenv_test.o $(MEMENVLIBRARY) $(LIBRARY) $(TESTHARNESS)\n-\t$(CXX) $(LDFLAGS) helpers/memenv/memenv_test.o $(MEMENVLIBRARY) $(LIBRARY) $(TESTHARNESS) -o $@ $(LIBS)\n-\n-ifeq ($(PLATFORM), IOS)\n-# For iOS, create universal object files to be used on both the simulator and\n-# a device.\n-PLATFORMSROOT=/Applications/Xcode.app/Contents/Developer/Platforms\n-SIMULATORROOT=$(PLATFORMSROOT)/iPhoneSimulator.platform/Developer\n-DEVICEROOT=$(PLATFORMSROOT)/iPhoneOS.platform/Developer\n-IOSVERSION=$(shell defaults read $(PLATFORMSROOT)/iPhoneOS.platform/version CFBundleShortVersionString)\n-\n-.cc.o:\n-\tmkdir -p ios-x86/$(dir $@)\n-\t$(CXX) $(CXXFLAGS) -isysroot $(SIMULATORROOT)/SDKs/iPhoneSimulator$(IOSVERSION).sdk -arch i686 -c $< -o ios-x86/$@\n-\tmkdir -p ios-arm/$(dir $@)\n-\t$(DEVICEROOT)/usr/bin/$(CXX) $(CXXFLAGS) -isysroot $(DEVICEROOT)/SDKs/iPhoneOS$(IOSVERSION).sdk -arch armv6 -arch armv7 -c $< -o ios-arm/$@\n-\tlipo ios-x86/$@ ios-arm/$@ -create -output $@\n-\n-.c.o:\n-\tmkdir -p ios-x86/$(dir $@)\n-\t$(CC) $(CFLAGS) -isysroot $(SIMULATORROOT)/SDKs/iPhoneSimulator$(IOSVERSION).sdk -arch i686 -c $< -o ios-x86/$@\n-\tmkdir -p ios-arm/$(dir $@)\n-\t$(DEVICEROOT)/usr/bin/$(CC) $(CFLAGS) -isysroot $(DEVICEROOT)/SDKs/iPhoneOS$(IOSVERSION).sdk -arch armv6 -arch armv7 -c $< -o ios-arm/$@\n-\tlipo ios-x86/$@ ios-arm/$@ -create -output $@\n-\n-else\n-.cc.o:\n-\t$(CXX) $(CXXFLAGS) -c $< -o $@\n-\n-.c.o:\n-\t$(CC) $(CFLAGS) -c $< -o $@\n-endif"
      },
      {
        "sha": "3fd99242d7bbcfeffecfd01c0870f89382766baf",
        "filename": "src/leveldb/NEWS",
        "status": "removed",
        "additions": 0,
        "deletions": 17,
        "changes": 17,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/NEWS",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/NEWS",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/NEWS?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,17 +0,0 @@\n-Release 1.2 2011-05-16\n-----------------------\n-\n-Fixes for larger databases (tested up to one billion 100-byte entries,\n-i.e., ~100GB).\n-\n-(1) Place hard limit on number of level-0 files.  This fixes errors\n-of the form \"too many open files\".\n-\n-(2) Fixed memtable management.  Before the fix, a heavy write burst\n-could cause unbounded memory usage.\n-\n-A fix for a logging bug where the reader would incorrectly complain\n-about corruption.\n-\n-Allow public access to WriteBatch contents so that users can easily\n-wrap a DB."
      },
      {
        "sha": "3618adeeedbea04a14e00d5a1ef33dd4f0a7be06",
        "filename": "src/leveldb/README",
        "status": "removed",
        "additions": 0,
        "deletions": 51,
        "changes": 51,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/README",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/README",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/README?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,51 +0,0 @@\n-leveldb: A key-value store\n-Authors: Sanjay Ghemawat (sanjay@google.com) and Jeff Dean (jeff@google.com)\n-\n-The code under this directory implements a system for maintaining a\n-persistent key/value store.\n-\n-See doc/index.html for more explanation.\n-See doc/impl.html for a brief overview of the implementation.\n-\n-The public interface is in include/*.h.  Callers should not include or\n-rely on the details of any other header files in this package.  Those\n-internal APIs may be changed without warning.\n-\n-Guide to header files:\n-\n-include/db.h\n-    Main interface to the DB: Start here\n-\n-include/options.h\n-    Control over the behavior of an entire database, and also\n-    control over the behavior of individual reads and writes.\n-\n-include/comparator.h\n-    Abstraction for user-specified comparison function.  If you want\n-    just bytewise comparison of keys, you can use the default comparator,\n-    but clients can write their own comparator implementations if they\n-    want custom ordering (e.g. to handle different character\n-    encodings, etc.)\n-\n-include/iterator.h\n-    Interface for iterating over data. You can get an iterator\n-    from a DB object.\n-\n-include/write_batch.h\n-    Interface for atomically applying multiple updates to a database.\n-\n-include/slice.h\n-    A simple module for maintaining a pointer and a length into some\n-    other byte array.\n-\n-include/status.h\n-    Status is returned from many of the public interfaces and is used\n-    to report success and various kinds of errors.\n-\n-include/env.h\n-    Abstraction of the OS environment.  A posix implementation of\n-    this interface is in util/env_posix.cc\n-\n-include/table.h\n-include/table_builder.h\n-    Lower-level modules that most clients probably won't use directly"
      },
      {
        "sha": "e603c07137f1ea198a3de9ca3bbb369e1c069f61",
        "filename": "src/leveldb/TODO",
        "status": "removed",
        "additions": 0,
        "deletions": 14,
        "changes": 14,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/TODO",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/TODO",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/TODO?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,14 +0,0 @@\n-ss\n-- Stats\n-\n-db\n-- Maybe implement DB::BulkDeleteForRange(start_key, end_key)\n-  that would blow away files whose ranges are entirely contained\n-  within [start_key..end_key]?  For Chrome, deletion of obsolete\n-  object stores, etc. can be done in the background anyway, so\n-  probably not that important.\n-- There have been requests for MultiGet.\n-\n-After a range is completely deleted, what gets rid of the\n-corresponding files if we do no future changes to that range.  Make\n-the conditions for triggering compactions fire in more situations?"
      },
      {
        "sha": "5b76c2448fe1e475b97d12235bdff53b857557cf",
        "filename": "src/leveldb/WINDOWS.md",
        "status": "removed",
        "additions": 0,
        "deletions": 39,
        "changes": 39,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/WINDOWS.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/WINDOWS.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/WINDOWS.md?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,39 +0,0 @@\n-# Building LevelDB On Windows\n-\n-## Prereqs \n-\n-Install the [Windows Software Development Kit version 7.1](http://www.microsoft.com/downloads/dlx/en-us/listdetailsview.aspx?FamilyID=6b6c21d2-2006-4afa-9702-529fa782d63b).\n-\n-Download and extract the [Snappy source distribution](http://snappy.googlecode.com/files/snappy-1.0.5.tar.gz)\n-\n-1. Open the \"Windows SDK 7.1 Command Prompt\" :\n-   Start Menu -> \"Microsoft Windows SDK v7.1\" > \"Windows SDK 7.1 Command Prompt\"\n-2. Change the directory to the leveldb project\n-\n-## Building the Static lib \n-\n-* 32 bit Version \n-\n-        setenv /x86\n-        msbuild.exe /p:Configuration=Release /p:Platform=Win32 /p:Snappy=..\\snappy-1.0.5\n-\n-* 64 bit Version \n-\n-        setenv /x64\n-        msbuild.exe /p:Configuration=Release /p:Platform=x64 /p:Snappy=..\\snappy-1.0.5\n-\n-\n-## Building and Running the Benchmark app\n-\n-* 32 bit Version \n-\n-\t    setenv /x86\n-\t    msbuild.exe /p:Configuration=Benchmark /p:Platform=Win32 /p:Snappy=..\\snappy-1.0.5\n-\t\tBenchmark\\leveldb.exe\n-\n-* 64 bit Version \n-\n-\t    setenv /x64\n-\t    msbuild.exe /p:Configuration=Benchmark /p:Platform=x64 /p:Snappy=..\\snappy-1.0.5\n-\t    x64\\Benchmark\\leveldb.exe\n-"
      },
      {
        "sha": "609cb512248dcc14c1a402338a3baa9a49df5bb6",
        "filename": "src/leveldb/build_detect_platform",
        "status": "removed",
        "additions": 0,
        "deletions": 200,
        "changes": 200,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/build_detect_platform",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/build_detect_platform",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/build_detect_platform?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,200 +0,0 @@\n-#!/bin/sh\n-#\n-# Detects OS we're compiling on and outputs a file specified by the first\n-# argument, which in turn gets read while processing Makefile.\n-#\n-# The output will set the following variables:\n-#   CC                          C Compiler path\n-#   CXX                         C++ Compiler path\n-#   PLATFORM_LDFLAGS            Linker flags\n-#   PLATFORM_LIBS               Libraries flags\n-#   PLATFORM_SHARED_EXT         Extension for shared libraries\n-#   PLATFORM_SHARED_LDFLAGS     Flags for building shared library\n-#                               This flag is embedded just before the name\n-#                               of the shared library without intervening spaces\n-#   PLATFORM_SHARED_CFLAGS      Flags for compiling objects for shared library\n-#   PLATFORM_CCFLAGS            C compiler flags\n-#   PLATFORM_CXXFLAGS           C++ compiler flags.  Will contain:\n-#   PLATFORM_SHARED_VERSIONED   Set to 'true' if platform supports versioned\n-#                               shared libraries, empty otherwise.\n-#\n-# The PLATFORM_CCFLAGS and PLATFORM_CXXFLAGS might include the following:\n-#\n-#       -DLEVELDB_CSTDATOMIC_PRESENT if <cstdatomic> is present\n-#       -DLEVELDB_PLATFORM_POSIX     for Posix-based platforms\n-#       -DSNAPPY                     if the Snappy library is present\n-#\n-\n-OUTPUT=$1\n-PREFIX=$2\n-if test -z \"$OUTPUT\" || test -z \"$PREFIX\"; then\n-  echo \"usage: $0 <output-filename> <directory_prefix>\" >&2\n-  exit 1\n-fi\n-\n-# Delete existing output, if it exists\n-rm -f $OUTPUT\n-touch $OUTPUT\n-\n-if test -z \"$CC\"; then\n-    CC=cc\n-fi\n-\n-if test -z \"$CXX\"; then\n-    CXX=g++\n-fi\n-\n-# Detect OS\n-if test -z \"$TARGET_OS\"; then\n-    TARGET_OS=`uname -s`\n-fi\n-\n-COMMON_FLAGS=\n-CROSS_COMPILE=\n-PLATFORM_CCFLAGS=\n-PLATFORM_CXXFLAGS=\n-PLATFORM_LDFLAGS=\n-PLATFORM_LIBS=\n-PLATFORM_SHARED_EXT=\"so\"\n-PLATFORM_SHARED_LDFLAGS=\"-shared -Wl,-soname -Wl,\"\n-PLATFORM_SHARED_CFLAGS=\"-fPIC\"\n-PLATFORM_SHARED_VERSIONED=true\n-\n-MEMCMP_FLAG=\n-if [ \"$CXX\" = \"g++\" ]; then\n-    # Use libc's memcmp instead of GCC's memcmp.  This results in ~40%\n-    # performance improvement on readrandom under gcc 4.4.3 on Linux/x86.\n-    MEMCMP_FLAG=\"-fno-builtin-memcmp\"\n-fi\n-\n-case \"$TARGET_OS\" in\n-    Darwin)\n-        PLATFORM=OS_MACOSX\n-        COMMON_FLAGS=\"$MEMCMP_FLAG -DOS_MACOSX\"\n-        PLATFORM_SHARED_EXT=dylib\n-        [ -z \"$INSTALL_PATH\" ] && INSTALL_PATH=`pwd`\n-        PLATFORM_SHARED_LDFLAGS=\"-dynamiclib -install_name $INSTALL_PATH/\"\n-        PORT_FILE=port/port_posix.cc\n-        ;;\n-    Linux)\n-        PLATFORM=OS_LINUX\n-        COMMON_FLAGS=\"$MEMCMP_FLAG -pthread -DOS_LINUX\"\n-        PLATFORM_LDFLAGS=\"-pthread\"\n-        PORT_FILE=port/port_posix.cc\n-        ;;\n-    SunOS)\n-        PLATFORM=OS_SOLARIS\n-        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_SOLARIS\"\n-        PLATFORM_LIBS=\"-lpthread -lrt\"\n-        PORT_FILE=port/port_posix.cc\n-        ;;\n-    FreeBSD)\n-        PLATFORM=OS_FREEBSD\n-        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_FREEBSD\"\n-        PLATFORM_LIBS=\"-lpthread\"\n-        PORT_FILE=port/port_posix.cc\n-        ;;\n-    NetBSD)\n-        PLATFORM=OS_NETBSD\n-        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_NETBSD\"\n-        PLATFORM_LIBS=\"-lpthread -lgcc_s\"\n-        PORT_FILE=port/port_posix.cc\n-        ;;\n-    OpenBSD)\n-        PLATFORM=OS_OPENBSD\n-        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_OPENBSD\"\n-        PLATFORM_LDFLAGS=\"-pthread\"\n-        PORT_FILE=port/port_posix.cc\n-        ;;\n-    DragonFly)\n-        PLATFORM=OS_DRAGONFLYBSD\n-        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_DRAGONFLYBSD\"\n-        PLATFORM_LIBS=\"-lpthread\"\n-        PORT_FILE=port/port_posix.cc\n-        ;;\n-    OS_ANDROID_CROSSCOMPILE)\n-        PLATFORM=OS_ANDROID\n-        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_ANDROID -DLEVELDB_PLATFORM_POSIX\"\n-        PLATFORM_LDFLAGS=\"\"  # All pthread features are in the Android C library\n-        PORT_FILE=port/port_posix.cc\n-        CROSS_COMPILE=true\n-        ;;\n-    HP-UX)\n-        PLATFORM=OS_HPUX\n-        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_HPUX\"\n-        PLATFORM_LDFLAGS=\"-pthread\"\n-        PORT_FILE=port/port_posix.cc\n-        # man ld: +h internal_name\n-        PLATFORM_SHARED_LDFLAGS=\"-shared -Wl,+h -Wl,\"\n-        ;;\n-    OS_WINDOWS_CROSSCOMPILE | NATIVE_WINDOWS)\n-        PLATFORM=OS_WINDOWS\n-        COMMON_FLAGS=\"-fno-builtin-memcmp -D_REENTRANT -DOS_WINDOWS -DLEVELDB_PLATFORM_WINDOWS -DWINVER=0x0500 -D__USE_MINGW_ANSI_STDIO=1\"\n-        PLATFORM_SOURCES=\"util/env_win.cc\"\n-        PLATFORM_LIBS=\"-lshlwapi\"\n-        PORT_FILE=port/port_win.cc\n-        CROSS_COMPILE=true\n-        ;;\n-    *)\n-        echo \"Unknown platform!\" >&2\n-        exit 1\n-esac\n-\n-# We want to make a list of all cc files within util, db, table, and helpers\n-# except for the test and benchmark files. By default, find will output a list\n-# of all files matching either rule, so we need to append -print to make the\n-# prune take effect.\n-DIRS=\"$PREFIX/db $PREFIX/util $PREFIX/table\"\n-\n-set -f # temporarily disable globbing so that our patterns aren't expanded\n-PRUNE_TEST=\"-name *test*.cc -prune\"\n-PRUNE_BENCH=\"-name *_bench.cc -prune\"\n-PRUNE_TOOL=\"-name leveldb_main.cc -prune\"\n-PORTABLE_FILES=`find $DIRS $PRUNE_TEST -o $PRUNE_BENCH -o $PRUNE_TOOL -o -name '*.cc' -print | sort | sed \"s,^$PREFIX/,,\" | tr \"\\n\" \" \"`\n-\n-set +f # re-enable globbing\n-\n-# The sources consist of the portable files, plus the platform-specific port\n-# file.\n-echo \"SOURCES=$PORTABLE_FILES $PORT_FILE\" >> $OUTPUT\n-echo \"MEMENV_SOURCES=helpers/memenv/memenv.cc\" >> $OUTPUT\n-\n-if [ \"$CROSS_COMPILE\" = \"true\" ]; then\n-    # Cross-compiling; do not try any compilation tests.\n-    true\n-else\n-    # If -std=c++0x works, use <cstdatomic>.  Otherwise use port_posix.h.\n-    $CXX $CXXFLAGS -std=c++0x -x c++ - -o /dev/null 2>/dev/null  <<EOF\n-      #include <cstdatomic>\n-      int main() {}\n-EOF\n-    if [ \"$?\" = 0 ]; then\n-        COMMON_FLAGS=\"$COMMON_FLAGS -DLEVELDB_PLATFORM_POSIX -DLEVELDB_CSTDATOMIC_PRESENT\"\n-        PLATFORM_CXXFLAGS=\"-std=c++0x\"\n-    else\n-        COMMON_FLAGS=\"$COMMON_FLAGS -DLEVELDB_PLATFORM_POSIX\"\n-    fi\n-\n-    # Test whether tcmalloc is available\n-    $CXX $CXXFLAGS -x c++ - -o /dev/null -ltcmalloc 2>/dev/null  <<EOF\n-      int main() {}\n-EOF\n-    if [ \"$?\" = 0 ]; then\n-        PLATFORM_LIBS=\"$PLATFORM_LIBS -ltcmalloc\"\n-    fi\n-fi\n-\n-PLATFORM_CCFLAGS=\"$PLATFORM_CCFLAGS $COMMON_FLAGS\"\n-PLATFORM_CXXFLAGS=\"$PLATFORM_CXXFLAGS $COMMON_FLAGS\"\n-\n-echo \"CC=$CC\" >> $OUTPUT\n-echo \"CXX=$CXX\" >> $OUTPUT\n-echo \"PLATFORM=$PLATFORM\" >> $OUTPUT\n-echo \"PLATFORM_LDFLAGS=$PLATFORM_LDFLAGS\" >> $OUTPUT\n-echo \"PLATFORM_LIBS=$PLATFORM_LIBS\" >> $OUTPUT\n-echo \"PLATFORM_CCFLAGS=$PLATFORM_CCFLAGS\" >> $OUTPUT\n-echo \"PLATFORM_CXXFLAGS=$PLATFORM_CXXFLAGS\" >> $OUTPUT\n-echo \"PLATFORM_SHARED_CFLAGS=$PLATFORM_SHARED_CFLAGS\" >> $OUTPUT\n-echo \"PLATFORM_SHARED_EXT=$PLATFORM_SHARED_EXT\" >> $OUTPUT\n-echo \"PLATFORM_SHARED_LDFLAGS=$PLATFORM_SHARED_LDFLAGS\" >> $OUTPUT\n-echo \"PLATFORM_SHARED_VERSIONED=$PLATFORM_SHARED_VERSIONED\" >> $OUTPUT"
      },
      {
        "sha": "f4198821973c94f2e28148f79a10ca2c48d8d1e6",
        "filename": "src/leveldb/db/builder.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 88,
        "changes": 88,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/builder.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/builder.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/builder.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,88 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/builder.h\"\n-\n-#include \"db/filename.h\"\n-#include \"db/dbformat.h\"\n-#include \"db/table_cache.h\"\n-#include \"db/version_edit.h\"\n-#include \"leveldb/db.h\"\n-#include \"leveldb/env.h\"\n-#include \"leveldb/iterator.h\"\n-\n-namespace leveldb {\n-\n-Status BuildTable(const std::string& dbname,\n-                  Env* env,\n-                  const Options& options,\n-                  TableCache* table_cache,\n-                  Iterator* iter,\n-                  FileMetaData* meta) {\n-  Status s;\n-  meta->file_size = 0;\n-  iter->SeekToFirst();\n-\n-  std::string fname = TableFileName(dbname, meta->number);\n-  if (iter->Valid()) {\n-    WritableFile* file;\n-    s = env->NewWritableFile(fname, &file);\n-    if (!s.ok()) {\n-      return s;\n-    }\n-\n-    TableBuilder* builder = new TableBuilder(options, file);\n-    meta->smallest.DecodeFrom(iter->key());\n-    for (; iter->Valid(); iter->Next()) {\n-      Slice key = iter->key();\n-      meta->largest.DecodeFrom(key);\n-      builder->Add(key, iter->value());\n-    }\n-\n-    // Finish and check for builder errors\n-    if (s.ok()) {\n-      s = builder->Finish();\n-      if (s.ok()) {\n-        meta->file_size = builder->FileSize();\n-        assert(meta->file_size > 0);\n-      }\n-    } else {\n-      builder->Abandon();\n-    }\n-    delete builder;\n-\n-    // Finish and check for file errors\n-    if (s.ok()) {\n-      s = file->Sync();\n-    }\n-    if (s.ok()) {\n-      s = file->Close();\n-    }\n-    delete file;\n-    file = NULL;\n-\n-    if (s.ok()) {\n-      // Verify that the table is usable\n-      Iterator* it = table_cache->NewIterator(ReadOptions(),\n-                                              meta->number,\n-                                              meta->file_size);\n-      s = it->status();\n-      delete it;\n-    }\n-  }\n-\n-  // Check for input iterator errors\n-  if (!iter->status().ok()) {\n-    s = iter->status();\n-  }\n-\n-  if (s.ok() && meta->file_size > 0) {\n-    // Keep it\n-  } else {\n-    env->DeleteFile(fname);\n-  }\n-  return s;\n-}\n-\n-}  // namespace leveldb"
      },
      {
        "sha": "62431fcf44f4545490291d8ec1ab098c2fc2ba88",
        "filename": "src/leveldb/db/builder.h",
        "status": "removed",
        "additions": 0,
        "deletions": 34,
        "changes": 34,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/builder.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/builder.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/builder.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,34 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_DB_BUILDER_H_\n-#define STORAGE_LEVELDB_DB_BUILDER_H_\n-\n-#include \"leveldb/status.h\"\n-\n-namespace leveldb {\n-\n-struct Options;\n-struct FileMetaData;\n-\n-class Env;\n-class Iterator;\n-class TableCache;\n-class VersionEdit;\n-\n-// Build a Table file from the contents of *iter.  The generated file\n-// will be named according to meta->number.  On success, the rest of\n-// *meta will be filled with metadata about the generated table.\n-// If no data is present in *iter, meta->file_size will be set to\n-// zero, and no Table file will be produced.\n-extern Status BuildTable(const std::string& dbname,\n-                         Env* env,\n-                         const Options& options,\n-                         TableCache* table_cache,\n-                         Iterator* iter,\n-                         FileMetaData* meta);\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_DB_BUILDER_H_"
      },
      {
        "sha": "08ff0ad90ac00d63f05d5d71fb89f9f701894058",
        "filename": "src/leveldb/db/c.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 595,
        "changes": 595,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/c.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/c.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/c.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,595 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"leveldb/c.h\"\n-\n-#include <stdlib.h>\n-#include <unistd.h>\n-#include \"leveldb/cache.h\"\n-#include \"leveldb/comparator.h\"\n-#include \"leveldb/db.h\"\n-#include \"leveldb/env.h\"\n-#include \"leveldb/filter_policy.h\"\n-#include \"leveldb/iterator.h\"\n-#include \"leveldb/options.h\"\n-#include \"leveldb/status.h\"\n-#include \"leveldb/write_batch.h\"\n-\n-using leveldb::Cache;\n-using leveldb::Comparator;\n-using leveldb::CompressionType;\n-using leveldb::DB;\n-using leveldb::Env;\n-using leveldb::FileLock;\n-using leveldb::FilterPolicy;\n-using leveldb::Iterator;\n-using leveldb::kMajorVersion;\n-using leveldb::kMinorVersion;\n-using leveldb::Logger;\n-using leveldb::NewBloomFilterPolicy;\n-using leveldb::NewLRUCache;\n-using leveldb::Options;\n-using leveldb::RandomAccessFile;\n-using leveldb::Range;\n-using leveldb::ReadOptions;\n-using leveldb::SequentialFile;\n-using leveldb::Slice;\n-using leveldb::Snapshot;\n-using leveldb::Status;\n-using leveldb::WritableFile;\n-using leveldb::WriteBatch;\n-using leveldb::WriteOptions;\n-\n-extern \"C\" {\n-\n-struct leveldb_t              { DB*               rep; };\n-struct leveldb_iterator_t     { Iterator*         rep; };\n-struct leveldb_writebatch_t   { WriteBatch        rep; };\n-struct leveldb_snapshot_t     { const Snapshot*   rep; };\n-struct leveldb_readoptions_t  { ReadOptions       rep; };\n-struct leveldb_writeoptions_t { WriteOptions      rep; };\n-struct leveldb_options_t      { Options           rep; };\n-struct leveldb_cache_t        { Cache*            rep; };\n-struct leveldb_seqfile_t      { SequentialFile*   rep; };\n-struct leveldb_randomfile_t   { RandomAccessFile* rep; };\n-struct leveldb_writablefile_t { WritableFile*     rep; };\n-struct leveldb_logger_t       { Logger*           rep; };\n-struct leveldb_filelock_t     { FileLock*         rep; };\n-\n-struct leveldb_comparator_t : public Comparator {\n-  void* state_;\n-  void (*destructor_)(void*);\n-  int (*compare_)(\n-      void*,\n-      const char* a, size_t alen,\n-      const char* b, size_t blen);\n-  const char* (*name_)(void*);\n-\n-  virtual ~leveldb_comparator_t() {\n-    (*destructor_)(state_);\n-  }\n-\n-  virtual int Compare(const Slice& a, const Slice& b) const {\n-    return (*compare_)(state_, a.data(), a.size(), b.data(), b.size());\n-  }\n-\n-  virtual const char* Name() const {\n-    return (*name_)(state_);\n-  }\n-\n-  // No-ops since the C binding does not support key shortening methods.\n-  virtual void FindShortestSeparator(std::string*, const Slice&) const { }\n-  virtual void FindShortSuccessor(std::string* key) const { }\n-};\n-\n-struct leveldb_filterpolicy_t : public FilterPolicy {\n-  void* state_;\n-  void (*destructor_)(void*);\n-  const char* (*name_)(void*);\n-  char* (*create_)(\n-      void*,\n-      const char* const* key_array, const size_t* key_length_array,\n-      int num_keys,\n-      size_t* filter_length);\n-  unsigned char (*key_match_)(\n-      void*,\n-      const char* key, size_t length,\n-      const char* filter, size_t filter_length);\n-\n-  virtual ~leveldb_filterpolicy_t() {\n-    (*destructor_)(state_);\n-  }\n-\n-  virtual const char* Name() const {\n-    return (*name_)(state_);\n-  }\n-\n-  virtual void CreateFilter(const Slice* keys, int n, std::string* dst) const {\n-    std::vector<const char*> key_pointers(n);\n-    std::vector<size_t> key_sizes(n);\n-    for (int i = 0; i < n; i++) {\n-      key_pointers[i] = keys[i].data();\n-      key_sizes[i] = keys[i].size();\n-    }\n-    size_t len;\n-    char* filter = (*create_)(state_, &key_pointers[0], &key_sizes[0], n, &len);\n-    dst->append(filter, len);\n-    free(filter);\n-  }\n-\n-  virtual bool KeyMayMatch(const Slice& key, const Slice& filter) const {\n-    return (*key_match_)(state_, key.data(), key.size(),\n-                         filter.data(), filter.size());\n-  }\n-};\n-\n-struct leveldb_env_t {\n-  Env* rep;\n-  bool is_default;\n-};\n-\n-static bool SaveError(char** errptr, const Status& s) {\n-  assert(errptr != NULL);\n-  if (s.ok()) {\n-    return false;\n-  } else if (*errptr == NULL) {\n-    *errptr = strdup(s.ToString().c_str());\n-  } else {\n-    // TODO(sanjay): Merge with existing error?\n-    free(*errptr);\n-    *errptr = strdup(s.ToString().c_str());\n-  }\n-  return true;\n-}\n-\n-static char* CopyString(const std::string& str) {\n-  char* result = reinterpret_cast<char*>(malloc(sizeof(char) * str.size()));\n-  memcpy(result, str.data(), sizeof(char) * str.size());\n-  return result;\n-}\n-\n-leveldb_t* leveldb_open(\n-    const leveldb_options_t* options,\n-    const char* name,\n-    char** errptr) {\n-  DB* db;\n-  if (SaveError(errptr, DB::Open(options->rep, std::string(name), &db))) {\n-    return NULL;\n-  }\n-  leveldb_t* result = new leveldb_t;\n-  result->rep = db;\n-  return result;\n-}\n-\n-void leveldb_close(leveldb_t* db) {\n-  delete db->rep;\n-  delete db;\n-}\n-\n-void leveldb_put(\n-    leveldb_t* db,\n-    const leveldb_writeoptions_t* options,\n-    const char* key, size_t keylen,\n-    const char* val, size_t vallen,\n-    char** errptr) {\n-  SaveError(errptr,\n-            db->rep->Put(options->rep, Slice(key, keylen), Slice(val, vallen)));\n-}\n-\n-void leveldb_delete(\n-    leveldb_t* db,\n-    const leveldb_writeoptions_t* options,\n-    const char* key, size_t keylen,\n-    char** errptr) {\n-  SaveError(errptr, db->rep->Delete(options->rep, Slice(key, keylen)));\n-}\n-\n-\n-void leveldb_write(\n-    leveldb_t* db,\n-    const leveldb_writeoptions_t* options,\n-    leveldb_writebatch_t* batch,\n-    char** errptr) {\n-  SaveError(errptr, db->rep->Write(options->rep, &batch->rep));\n-}\n-\n-char* leveldb_get(\n-    leveldb_t* db,\n-    const leveldb_readoptions_t* options,\n-    const char* key, size_t keylen,\n-    size_t* vallen,\n-    char** errptr) {\n-  char* result = NULL;\n-  std::string tmp;\n-  Status s = db->rep->Get(options->rep, Slice(key, keylen), &tmp);\n-  if (s.ok()) {\n-    *vallen = tmp.size();\n-    result = CopyString(tmp);\n-  } else {\n-    *vallen = 0;\n-    if (!s.IsNotFound()) {\n-      SaveError(errptr, s);\n-    }\n-  }\n-  return result;\n-}\n-\n-leveldb_iterator_t* leveldb_create_iterator(\n-    leveldb_t* db,\n-    const leveldb_readoptions_t* options) {\n-  leveldb_iterator_t* result = new leveldb_iterator_t;\n-  result->rep = db->rep->NewIterator(options->rep);\n-  return result;\n-}\n-\n-const leveldb_snapshot_t* leveldb_create_snapshot(\n-    leveldb_t* db) {\n-  leveldb_snapshot_t* result = new leveldb_snapshot_t;\n-  result->rep = db->rep->GetSnapshot();\n-  return result;\n-}\n-\n-void leveldb_release_snapshot(\n-    leveldb_t* db,\n-    const leveldb_snapshot_t* snapshot) {\n-  db->rep->ReleaseSnapshot(snapshot->rep);\n-  delete snapshot;\n-}\n-\n-char* leveldb_property_value(\n-    leveldb_t* db,\n-    const char* propname) {\n-  std::string tmp;\n-  if (db->rep->GetProperty(Slice(propname), &tmp)) {\n-    // We use strdup() since we expect human readable output.\n-    return strdup(tmp.c_str());\n-  } else {\n-    return NULL;\n-  }\n-}\n-\n-void leveldb_approximate_sizes(\n-    leveldb_t* db,\n-    int num_ranges,\n-    const char* const* range_start_key, const size_t* range_start_key_len,\n-    const char* const* range_limit_key, const size_t* range_limit_key_len,\n-    uint64_t* sizes) {\n-  Range* ranges = new Range[num_ranges];\n-  for (int i = 0; i < num_ranges; i++) {\n-    ranges[i].start = Slice(range_start_key[i], range_start_key_len[i]);\n-    ranges[i].limit = Slice(range_limit_key[i], range_limit_key_len[i]);\n-  }\n-  db->rep->GetApproximateSizes(ranges, num_ranges, sizes);\n-  delete[] ranges;\n-}\n-\n-void leveldb_compact_range(\n-    leveldb_t* db,\n-    const char* start_key, size_t start_key_len,\n-    const char* limit_key, size_t limit_key_len) {\n-  Slice a, b;\n-  db->rep->CompactRange(\n-      // Pass NULL Slice if corresponding \"const char*\" is NULL\n-      (start_key ? (a = Slice(start_key, start_key_len), &a) : NULL),\n-      (limit_key ? (b = Slice(limit_key, limit_key_len), &b) : NULL));\n-}\n-\n-void leveldb_destroy_db(\n-    const leveldb_options_t* options,\n-    const char* name,\n-    char** errptr) {\n-  SaveError(errptr, DestroyDB(name, options->rep));\n-}\n-\n-void leveldb_repair_db(\n-    const leveldb_options_t* options,\n-    const char* name,\n-    char** errptr) {\n-  SaveError(errptr, RepairDB(name, options->rep));\n-}\n-\n-void leveldb_iter_destroy(leveldb_iterator_t* iter) {\n-  delete iter->rep;\n-  delete iter;\n-}\n-\n-unsigned char leveldb_iter_valid(const leveldb_iterator_t* iter) {\n-  return iter->rep->Valid();\n-}\n-\n-void leveldb_iter_seek_to_first(leveldb_iterator_t* iter) {\n-  iter->rep->SeekToFirst();\n-}\n-\n-void leveldb_iter_seek_to_last(leveldb_iterator_t* iter) {\n-  iter->rep->SeekToLast();\n-}\n-\n-void leveldb_iter_seek(leveldb_iterator_t* iter, const char* k, size_t klen) {\n-  iter->rep->Seek(Slice(k, klen));\n-}\n-\n-void leveldb_iter_next(leveldb_iterator_t* iter) {\n-  iter->rep->Next();\n-}\n-\n-void leveldb_iter_prev(leveldb_iterator_t* iter) {\n-  iter->rep->Prev();\n-}\n-\n-const char* leveldb_iter_key(const leveldb_iterator_t* iter, size_t* klen) {\n-  Slice s = iter->rep->key();\n-  *klen = s.size();\n-  return s.data();\n-}\n-\n-const char* leveldb_iter_value(const leveldb_iterator_t* iter, size_t* vlen) {\n-  Slice s = iter->rep->value();\n-  *vlen = s.size();\n-  return s.data();\n-}\n-\n-void leveldb_iter_get_error(const leveldb_iterator_t* iter, char** errptr) {\n-  SaveError(errptr, iter->rep->status());\n-}\n-\n-leveldb_writebatch_t* leveldb_writebatch_create() {\n-  return new leveldb_writebatch_t;\n-}\n-\n-void leveldb_writebatch_destroy(leveldb_writebatch_t* b) {\n-  delete b;\n-}\n-\n-void leveldb_writebatch_clear(leveldb_writebatch_t* b) {\n-  b->rep.Clear();\n-}\n-\n-void leveldb_writebatch_put(\n-    leveldb_writebatch_t* b,\n-    const char* key, size_t klen,\n-    const char* val, size_t vlen) {\n-  b->rep.Put(Slice(key, klen), Slice(val, vlen));\n-}\n-\n-void leveldb_writebatch_delete(\n-    leveldb_writebatch_t* b,\n-    const char* key, size_t klen) {\n-  b->rep.Delete(Slice(key, klen));\n-}\n-\n-void leveldb_writebatch_iterate(\n-    leveldb_writebatch_t* b,\n-    void* state,\n-    void (*put)(void*, const char* k, size_t klen, const char* v, size_t vlen),\n-    void (*deleted)(void*, const char* k, size_t klen)) {\n-  class H : public WriteBatch::Handler {\n-   public:\n-    void* state_;\n-    void (*put_)(void*, const char* k, size_t klen, const char* v, size_t vlen);\n-    void (*deleted_)(void*, const char* k, size_t klen);\n-    virtual void Put(const Slice& key, const Slice& value) {\n-      (*put_)(state_, key.data(), key.size(), value.data(), value.size());\n-    }\n-    virtual void Delete(const Slice& key) {\n-      (*deleted_)(state_, key.data(), key.size());\n-    }\n-  };\n-  H handler;\n-  handler.state_ = state;\n-  handler.put_ = put;\n-  handler.deleted_ = deleted;\n-  b->rep.Iterate(&handler);\n-}\n-\n-leveldb_options_t* leveldb_options_create() {\n-  return new leveldb_options_t;\n-}\n-\n-void leveldb_options_destroy(leveldb_options_t* options) {\n-  delete options;\n-}\n-\n-void leveldb_options_set_comparator(\n-    leveldb_options_t* opt,\n-    leveldb_comparator_t* cmp) {\n-  opt->rep.comparator = cmp;\n-}\n-\n-void leveldb_options_set_filter_policy(\n-    leveldb_options_t* opt,\n-    leveldb_filterpolicy_t* policy) {\n-  opt->rep.filter_policy = policy;\n-}\n-\n-void leveldb_options_set_create_if_missing(\n-    leveldb_options_t* opt, unsigned char v) {\n-  opt->rep.create_if_missing = v;\n-}\n-\n-void leveldb_options_set_error_if_exists(\n-    leveldb_options_t* opt, unsigned char v) {\n-  opt->rep.error_if_exists = v;\n-}\n-\n-void leveldb_options_set_paranoid_checks(\n-    leveldb_options_t* opt, unsigned char v) {\n-  opt->rep.paranoid_checks = v;\n-}\n-\n-void leveldb_options_set_env(leveldb_options_t* opt, leveldb_env_t* env) {\n-  opt->rep.env = (env ? env->rep : NULL);\n-}\n-\n-void leveldb_options_set_info_log(leveldb_options_t* opt, leveldb_logger_t* l) {\n-  opt->rep.info_log = (l ? l->rep : NULL);\n-}\n-\n-void leveldb_options_set_write_buffer_size(leveldb_options_t* opt, size_t s) {\n-  opt->rep.write_buffer_size = s;\n-}\n-\n-void leveldb_options_set_max_open_files(leveldb_options_t* opt, int n) {\n-  opt->rep.max_open_files = n;\n-}\n-\n-void leveldb_options_set_cache(leveldb_options_t* opt, leveldb_cache_t* c) {\n-  opt->rep.block_cache = c->rep;\n-}\n-\n-void leveldb_options_set_block_size(leveldb_options_t* opt, size_t s) {\n-  opt->rep.block_size = s;\n-}\n-\n-void leveldb_options_set_block_restart_interval(leveldb_options_t* opt, int n) {\n-  opt->rep.block_restart_interval = n;\n-}\n-\n-void leveldb_options_set_compression(leveldb_options_t* opt, int t) {\n-  opt->rep.compression = static_cast<CompressionType>(t);\n-}\n-\n-leveldb_comparator_t* leveldb_comparator_create(\n-    void* state,\n-    void (*destructor)(void*),\n-    int (*compare)(\n-        void*,\n-        const char* a, size_t alen,\n-        const char* b, size_t blen),\n-    const char* (*name)(void*)) {\n-  leveldb_comparator_t* result = new leveldb_comparator_t;\n-  result->state_ = state;\n-  result->destructor_ = destructor;\n-  result->compare_ = compare;\n-  result->name_ = name;\n-  return result;\n-}\n-\n-void leveldb_comparator_destroy(leveldb_comparator_t* cmp) {\n-  delete cmp;\n-}\n-\n-leveldb_filterpolicy_t* leveldb_filterpolicy_create(\n-    void* state,\n-    void (*destructor)(void*),\n-    char* (*create_filter)(\n-        void*,\n-        const char* const* key_array, const size_t* key_length_array,\n-        int num_keys,\n-        size_t* filter_length),\n-    unsigned char (*key_may_match)(\n-        void*,\n-        const char* key, size_t length,\n-        const char* filter, size_t filter_length),\n-    const char* (*name)(void*)) {\n-  leveldb_filterpolicy_t* result = new leveldb_filterpolicy_t;\n-  result->state_ = state;\n-  result->destructor_ = destructor;\n-  result->create_ = create_filter;\n-  result->key_match_ = key_may_match;\n-  result->name_ = name;\n-  return result;\n-}\n-\n-void leveldb_filterpolicy_destroy(leveldb_filterpolicy_t* filter) {\n-  delete filter;\n-}\n-\n-leveldb_filterpolicy_t* leveldb_filterpolicy_create_bloom(int bits_per_key) {\n-  // Make a leveldb_filterpolicy_t, but override all of its methods so\n-  // they delegate to a NewBloomFilterPolicy() instead of user\n-  // supplied C functions.\n-  struct Wrapper : public leveldb_filterpolicy_t {\n-    const FilterPolicy* rep_;\n-    ~Wrapper() { delete rep_; }\n-    const char* Name() const { return rep_->Name(); }\n-    void CreateFilter(const Slice* keys, int n, std::string* dst) const {\n-      return rep_->CreateFilter(keys, n, dst);\n-    }\n-    bool KeyMayMatch(const Slice& key, const Slice& filter) const {\n-      return rep_->KeyMayMatch(key, filter);\n-    }\n-    static void DoNothing(void*) { }\n-  };\n-  Wrapper* wrapper = new Wrapper;\n-  wrapper->rep_ = NewBloomFilterPolicy(bits_per_key);\n-  wrapper->state_ = NULL;\n-  wrapper->destructor_ = &Wrapper::DoNothing;\n-  return wrapper;\n-}\n-\n-leveldb_readoptions_t* leveldb_readoptions_create() {\n-  return new leveldb_readoptions_t;\n-}\n-\n-void leveldb_readoptions_destroy(leveldb_readoptions_t* opt) {\n-  delete opt;\n-}\n-\n-void leveldb_readoptions_set_verify_checksums(\n-    leveldb_readoptions_t* opt,\n-    unsigned char v) {\n-  opt->rep.verify_checksums = v;\n-}\n-\n-void leveldb_readoptions_set_fill_cache(\n-    leveldb_readoptions_t* opt, unsigned char v) {\n-  opt->rep.fill_cache = v;\n-}\n-\n-void leveldb_readoptions_set_snapshot(\n-    leveldb_readoptions_t* opt,\n-    const leveldb_snapshot_t* snap) {\n-  opt->rep.snapshot = (snap ? snap->rep : NULL);\n-}\n-\n-leveldb_writeoptions_t* leveldb_writeoptions_create() {\n-  return new leveldb_writeoptions_t;\n-}\n-\n-void leveldb_writeoptions_destroy(leveldb_writeoptions_t* opt) {\n-  delete opt;\n-}\n-\n-void leveldb_writeoptions_set_sync(\n-    leveldb_writeoptions_t* opt, unsigned char v) {\n-  opt->rep.sync = v;\n-}\n-\n-leveldb_cache_t* leveldb_cache_create_lru(size_t capacity) {\n-  leveldb_cache_t* c = new leveldb_cache_t;\n-  c->rep = NewLRUCache(capacity);\n-  return c;\n-}\n-\n-void leveldb_cache_destroy(leveldb_cache_t* cache) {\n-  delete cache->rep;\n-  delete cache;\n-}\n-\n-leveldb_env_t* leveldb_create_default_env() {\n-  leveldb_env_t* result = new leveldb_env_t;\n-  result->rep = Env::Default();\n-  result->is_default = true;\n-  return result;\n-}\n-\n-void leveldb_env_destroy(leveldb_env_t* env) {\n-  if (!env->is_default) delete env->rep;\n-  delete env;\n-}\n-\n-void leveldb_free(void* ptr) {\n-  free(ptr);\n-}\n-\n-int leveldb_major_version() {\n-  return kMajorVersion;\n-}\n-\n-int leveldb_minor_version() {\n-  return kMinorVersion;\n-}\n-\n-}  // end extern \"C\""
      },
      {
        "sha": "7cd5ee02076ab96303a2b1f25dbabbe721caaba7",
        "filename": "src/leveldb/db/c_test.c",
        "status": "removed",
        "additions": 0,
        "deletions": 390,
        "changes": 390,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/c_test.c",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/c_test.c",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/c_test.c?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,390 +0,0 @@\n-/* Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-   Use of this source code is governed by a BSD-style license that can be\n-   found in the LICENSE file. See the AUTHORS file for names of contributors. */\n-\n-#include \"leveldb/c.h\"\n-\n-#include <stddef.h>\n-#include <stdio.h>\n-#include <stdlib.h>\n-#include <string.h>\n-#include <sys/types.h>\n-#include <unistd.h>\n-\n-const char* phase = \"\";\n-static char dbname[200];\n-\n-static void StartPhase(const char* name) {\n-  fprintf(stderr, \"=== Test %s\\n\", name);\n-  phase = name;\n-}\n-\n-static const char* GetTempDir(void) {\n-    const char* ret = getenv(\"TEST_TMPDIR\");\n-    if (ret == NULL || ret[0] == '\\0')\n-        ret = \"/tmp\";\n-    return ret;\n-}\n-\n-#define CheckNoError(err)                                               \\\n-  if ((err) != NULL) {                                                  \\\n-    fprintf(stderr, \"%s:%d: %s: %s\\n\", __FILE__, __LINE__, phase, (err)); \\\n-    abort();                                                            \\\n-  }\n-\n-#define CheckCondition(cond)                                            \\\n-  if (!(cond)) {                                                        \\\n-    fprintf(stderr, \"%s:%d: %s: %s\\n\", __FILE__, __LINE__, phase, #cond); \\\n-    abort();                                                            \\\n-  }\n-\n-static void CheckEqual(const char* expected, const char* v, size_t n) {\n-  if (expected == NULL && v == NULL) {\n-    // ok\n-  } else if (expected != NULL && v != NULL && n == strlen(expected) &&\n-             memcmp(expected, v, n) == 0) {\n-    // ok\n-    return;\n-  } else {\n-    fprintf(stderr, \"%s: expected '%s', got '%s'\\n\",\n-            phase,\n-            (expected ? expected : \"(null)\"),\n-            (v ? v : \"(null\"));\n-    abort();\n-  }\n-}\n-\n-static void Free(char** ptr) {\n-  if (*ptr) {\n-    free(*ptr);\n-    *ptr = NULL;\n-  }\n-}\n-\n-static void CheckGet(\n-    leveldb_t* db,\n-    const leveldb_readoptions_t* options,\n-    const char* key,\n-    const char* expected) {\n-  char* err = NULL;\n-  size_t val_len;\n-  char* val;\n-  val = leveldb_get(db, options, key, strlen(key), &val_len, &err);\n-  CheckNoError(err);\n-  CheckEqual(expected, val, val_len);\n-  Free(&val);\n-}\n-\n-static void CheckIter(leveldb_iterator_t* iter,\n-                      const char* key, const char* val) {\n-  size_t len;\n-  const char* str;\n-  str = leveldb_iter_key(iter, &len);\n-  CheckEqual(key, str, len);\n-  str = leveldb_iter_value(iter, &len);\n-  CheckEqual(val, str, len);\n-}\n-\n-// Callback from leveldb_writebatch_iterate()\n-static void CheckPut(void* ptr,\n-                     const char* k, size_t klen,\n-                     const char* v, size_t vlen) {\n-  int* state = (int*) ptr;\n-  CheckCondition(*state < 2);\n-  switch (*state) {\n-    case 0:\n-      CheckEqual(\"bar\", k, klen);\n-      CheckEqual(\"b\", v, vlen);\n-      break;\n-    case 1:\n-      CheckEqual(\"box\", k, klen);\n-      CheckEqual(\"c\", v, vlen);\n-      break;\n-  }\n-  (*state)++;\n-}\n-\n-// Callback from leveldb_writebatch_iterate()\n-static void CheckDel(void* ptr, const char* k, size_t klen) {\n-  int* state = (int*) ptr;\n-  CheckCondition(*state == 2);\n-  CheckEqual(\"bar\", k, klen);\n-  (*state)++;\n-}\n-\n-static void CmpDestroy(void* arg) { }\n-\n-static int CmpCompare(void* arg, const char* a, size_t alen,\n-                      const char* b, size_t blen) {\n-  int n = (alen < blen) ? alen : blen;\n-  int r = memcmp(a, b, n);\n-  if (r == 0) {\n-    if (alen < blen) r = -1;\n-    else if (alen > blen) r = +1;\n-  }\n-  return r;\n-}\n-\n-static const char* CmpName(void* arg) {\n-  return \"foo\";\n-}\n-\n-// Custom filter policy\n-static unsigned char fake_filter_result = 1;\n-static void FilterDestroy(void* arg) { }\n-static const char* FilterName(void* arg) {\n-  return \"TestFilter\";\n-}\n-static char* FilterCreate(\n-    void* arg,\n-    const char* const* key_array, const size_t* key_length_array,\n-    int num_keys,\n-    size_t* filter_length) {\n-  *filter_length = 4;\n-  char* result = malloc(4);\n-  memcpy(result, \"fake\", 4);\n-  return result;\n-}\n-unsigned char FilterKeyMatch(\n-    void* arg,\n-    const char* key, size_t length,\n-    const char* filter, size_t filter_length) {\n-  CheckCondition(filter_length == 4);\n-  CheckCondition(memcmp(filter, \"fake\", 4) == 0);\n-  return fake_filter_result;\n-}\n-\n-int main(int argc, char** argv) {\n-  leveldb_t* db;\n-  leveldb_comparator_t* cmp;\n-  leveldb_cache_t* cache;\n-  leveldb_env_t* env;\n-  leveldb_options_t* options;\n-  leveldb_readoptions_t* roptions;\n-  leveldb_writeoptions_t* woptions;\n-  char* err = NULL;\n-  int run = -1;\n-\n-  CheckCondition(leveldb_major_version() >= 1);\n-  CheckCondition(leveldb_minor_version() >= 1);\n-\n-  snprintf(dbname, sizeof(dbname),\n-           \"%s/leveldb_c_test-%d\",\n-           GetTempDir(),\n-           ((int) geteuid()));\n-\n-  StartPhase(\"create_objects\");\n-  cmp = leveldb_comparator_create(NULL, CmpDestroy, CmpCompare, CmpName);\n-  env = leveldb_create_default_env();\n-  cache = leveldb_cache_create_lru(100000);\n-\n-  options = leveldb_options_create();\n-  leveldb_options_set_comparator(options, cmp);\n-  leveldb_options_set_error_if_exists(options, 1);\n-  leveldb_options_set_cache(options, cache);\n-  leveldb_options_set_env(options, env);\n-  leveldb_options_set_info_log(options, NULL);\n-  leveldb_options_set_write_buffer_size(options, 100000);\n-  leveldb_options_set_paranoid_checks(options, 1);\n-  leveldb_options_set_max_open_files(options, 10);\n-  leveldb_options_set_block_size(options, 1024);\n-  leveldb_options_set_block_restart_interval(options, 8);\n-  leveldb_options_set_compression(options, leveldb_no_compression);\n-\n-  roptions = leveldb_readoptions_create();\n-  leveldb_readoptions_set_verify_checksums(roptions, 1);\n-  leveldb_readoptions_set_fill_cache(roptions, 0);\n-\n-  woptions = leveldb_writeoptions_create();\n-  leveldb_writeoptions_set_sync(woptions, 1);\n-\n-  StartPhase(\"destroy\");\n-  leveldb_destroy_db(options, dbname, &err);\n-  Free(&err);\n-\n-  StartPhase(\"open_error\");\n-  db = leveldb_open(options, dbname, &err);\n-  CheckCondition(err != NULL);\n-  Free(&err);\n-\n-  StartPhase(\"leveldb_free\");\n-  db = leveldb_open(options, dbname, &err);\n-  CheckCondition(err != NULL);\n-  leveldb_free(err);\n-  err = NULL;\n-\n-  StartPhase(\"open\");\n-  leveldb_options_set_create_if_missing(options, 1);\n-  db = leveldb_open(options, dbname, &err);\n-  CheckNoError(err);\n-  CheckGet(db, roptions, \"foo\", NULL);\n-\n-  StartPhase(\"put\");\n-  leveldb_put(db, woptions, \"foo\", 3, \"hello\", 5, &err);\n-  CheckNoError(err);\n-  CheckGet(db, roptions, \"foo\", \"hello\");\n-\n-  StartPhase(\"compactall\");\n-  leveldb_compact_range(db, NULL, 0, NULL, 0);\n-  CheckGet(db, roptions, \"foo\", \"hello\");\n-\n-  StartPhase(\"compactrange\");\n-  leveldb_compact_range(db, \"a\", 1, \"z\", 1);\n-  CheckGet(db, roptions, \"foo\", \"hello\");\n-\n-  StartPhase(\"writebatch\");\n-  {\n-    leveldb_writebatch_t* wb = leveldb_writebatch_create();\n-    leveldb_writebatch_put(wb, \"foo\", 3, \"a\", 1);\n-    leveldb_writebatch_clear(wb);\n-    leveldb_writebatch_put(wb, \"bar\", 3, \"b\", 1);\n-    leveldb_writebatch_put(wb, \"box\", 3, \"c\", 1);\n-    leveldb_writebatch_delete(wb, \"bar\", 3);\n-    leveldb_write(db, woptions, wb, &err);\n-    CheckNoError(err);\n-    CheckGet(db, roptions, \"foo\", \"hello\");\n-    CheckGet(db, roptions, \"bar\", NULL);\n-    CheckGet(db, roptions, \"box\", \"c\");\n-    int pos = 0;\n-    leveldb_writebatch_iterate(wb, &pos, CheckPut, CheckDel);\n-    CheckCondition(pos == 3);\n-    leveldb_writebatch_destroy(wb);\n-  }\n-\n-  StartPhase(\"iter\");\n-  {\n-    leveldb_iterator_t* iter = leveldb_create_iterator(db, roptions);\n-    CheckCondition(!leveldb_iter_valid(iter));\n-    leveldb_iter_seek_to_first(iter);\n-    CheckCondition(leveldb_iter_valid(iter));\n-    CheckIter(iter, \"box\", \"c\");\n-    leveldb_iter_next(iter);\n-    CheckIter(iter, \"foo\", \"hello\");\n-    leveldb_iter_prev(iter);\n-    CheckIter(iter, \"box\", \"c\");\n-    leveldb_iter_prev(iter);\n-    CheckCondition(!leveldb_iter_valid(iter));\n-    leveldb_iter_seek_to_last(iter);\n-    CheckIter(iter, \"foo\", \"hello\");\n-    leveldb_iter_seek(iter, \"b\", 1);\n-    CheckIter(iter, \"box\", \"c\");\n-    leveldb_iter_get_error(iter, &err);\n-    CheckNoError(err);\n-    leveldb_iter_destroy(iter);\n-  }\n-\n-  StartPhase(\"approximate_sizes\");\n-  {\n-    int i;\n-    int n = 20000;\n-    char keybuf[100];\n-    char valbuf[100];\n-    uint64_t sizes[2];\n-    const char* start[2] = { \"a\", \"k00000000000000010000\" };\n-    size_t start_len[2] = { 1, 21 };\n-    const char* limit[2] = { \"k00000000000000010000\", \"z\" };\n-    size_t limit_len[2] = { 21, 1 };\n-    leveldb_writeoptions_set_sync(woptions, 0);\n-    for (i = 0; i < n; i++) {\n-      snprintf(keybuf, sizeof(keybuf), \"k%020d\", i);\n-      snprintf(valbuf, sizeof(valbuf), \"v%020d\", i);\n-      leveldb_put(db, woptions, keybuf, strlen(keybuf), valbuf, strlen(valbuf),\n-                  &err);\n-      CheckNoError(err);\n-    }\n-    leveldb_approximate_sizes(db, 2, start, start_len, limit, limit_len, sizes);\n-    CheckCondition(sizes[0] > 0);\n-    CheckCondition(sizes[1] > 0);\n-  }\n-\n-  StartPhase(\"property\");\n-  {\n-    char* prop = leveldb_property_value(db, \"nosuchprop\");\n-    CheckCondition(prop == NULL);\n-    prop = leveldb_property_value(db, \"leveldb.stats\");\n-    CheckCondition(prop != NULL);\n-    Free(&prop);\n-  }\n-\n-  StartPhase(\"snapshot\");\n-  {\n-    const leveldb_snapshot_t* snap;\n-    snap = leveldb_create_snapshot(db);\n-    leveldb_delete(db, woptions, \"foo\", 3, &err);\n-    CheckNoError(err);\n-    leveldb_readoptions_set_snapshot(roptions, snap);\n-    CheckGet(db, roptions, \"foo\", \"hello\");\n-    leveldb_readoptions_set_snapshot(roptions, NULL);\n-    CheckGet(db, roptions, \"foo\", NULL);\n-    leveldb_release_snapshot(db, snap);\n-  }\n-\n-  StartPhase(\"repair\");\n-  {\n-    leveldb_close(db);\n-    leveldb_options_set_create_if_missing(options, 0);\n-    leveldb_options_set_error_if_exists(options, 0);\n-    leveldb_repair_db(options, dbname, &err);\n-    CheckNoError(err);\n-    db = leveldb_open(options, dbname, &err);\n-    CheckNoError(err);\n-    CheckGet(db, roptions, \"foo\", NULL);\n-    CheckGet(db, roptions, \"bar\", NULL);\n-    CheckGet(db, roptions, \"box\", \"c\");\n-    leveldb_options_set_create_if_missing(options, 1);\n-    leveldb_options_set_error_if_exists(options, 1);\n-  }\n-\n-  StartPhase(\"filter\");\n-  for (run = 0; run < 2; run++) {\n-    // First run uses custom filter, second run uses bloom filter\n-    CheckNoError(err);\n-    leveldb_filterpolicy_t* policy;\n-    if (run == 0) {\n-      policy = leveldb_filterpolicy_create(\n-          NULL, FilterDestroy, FilterCreate, FilterKeyMatch, FilterName);\n-    } else {\n-      policy = leveldb_filterpolicy_create_bloom(10);\n-    }\n-\n-    // Create new database\n-    leveldb_close(db);\n-    leveldb_destroy_db(options, dbname, &err);\n-    leveldb_options_set_filter_policy(options, policy);\n-    db = leveldb_open(options, dbname, &err);\n-    CheckNoError(err);\n-    leveldb_put(db, woptions, \"foo\", 3, \"foovalue\", 8, &err);\n-    CheckNoError(err);\n-    leveldb_put(db, woptions, \"bar\", 3, \"barvalue\", 8, &err);\n-    CheckNoError(err);\n-    leveldb_compact_range(db, NULL, 0, NULL, 0);\n-\n-    fake_filter_result = 1;\n-    CheckGet(db, roptions, \"foo\", \"foovalue\");\n-    CheckGet(db, roptions, \"bar\", \"barvalue\");\n-    if (phase == 0) {\n-      // Must not find value when custom filter returns false\n-      fake_filter_result = 0;\n-      CheckGet(db, roptions, \"foo\", NULL);\n-      CheckGet(db, roptions, \"bar\", NULL);\n-      fake_filter_result = 1;\n-\n-      CheckGet(db, roptions, \"foo\", \"foovalue\");\n-      CheckGet(db, roptions, \"bar\", \"barvalue\");\n-    }\n-    leveldb_options_set_filter_policy(options, NULL);\n-    leveldb_filterpolicy_destroy(policy);\n-  }\n-\n-  StartPhase(\"cleanup\");\n-  leveldb_close(db);\n-  leveldb_options_destroy(options);\n-  leveldb_readoptions_destroy(roptions);\n-  leveldb_writeoptions_destroy(woptions);\n-  leveldb_cache_destroy(cache);\n-  leveldb_comparator_destroy(cmp);\n-  leveldb_env_destroy(env);\n-\n-  fprintf(stderr, \"PASS\\n\");\n-  return 0;\n-}"
      },
      {
        "sha": "31b2d5f4166bc200122c1773b1a3e7fdcff7c640",
        "filename": "src/leveldb/db/corruption_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 359,
        "changes": 359,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/corruption_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/corruption_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/corruption_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,359 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"leveldb/db.h\"\n-\n-#include <errno.h>\n-#include <fcntl.h>\n-#include <sys/stat.h>\n-#include <sys/types.h>\n-#include \"leveldb/cache.h\"\n-#include \"leveldb/env.h\"\n-#include \"leveldb/table.h\"\n-#include \"leveldb/write_batch.h\"\n-#include \"db/db_impl.h\"\n-#include \"db/filename.h\"\n-#include \"db/log_format.h\"\n-#include \"db/version_set.h\"\n-#include \"util/logging.h\"\n-#include \"util/testharness.h\"\n-#include \"util/testutil.h\"\n-\n-namespace leveldb {\n-\n-static const int kValueSize = 1000;\n-\n-class CorruptionTest {\n- public:\n-  test::ErrorEnv env_;\n-  std::string dbname_;\n-  Cache* tiny_cache_;\n-  Options options_;\n-  DB* db_;\n-\n-  CorruptionTest() {\n-    tiny_cache_ = NewLRUCache(100);\n-    options_.env = &env_;\n-    dbname_ = test::TmpDir() + \"/db_test\";\n-    DestroyDB(dbname_, options_);\n-\n-    db_ = NULL;\n-    options_.create_if_missing = true;\n-    Reopen();\n-    options_.create_if_missing = false;\n-  }\n-\n-  ~CorruptionTest() {\n-     delete db_;\n-     DestroyDB(dbname_, Options());\n-     delete tiny_cache_;\n-  }\n-\n-  Status TryReopen(Options* options = NULL) {\n-    delete db_;\n-    db_ = NULL;\n-    Options opt = (options ? *options : options_);\n-    opt.env = &env_;\n-    opt.block_cache = tiny_cache_;\n-    return DB::Open(opt, dbname_, &db_);\n-  }\n-\n-  void Reopen(Options* options = NULL) {\n-    ASSERT_OK(TryReopen(options));\n-  }\n-\n-  void RepairDB() {\n-    delete db_;\n-    db_ = NULL;\n-    ASSERT_OK(::leveldb::RepairDB(dbname_, options_));\n-  }\n-\n-  void Build(int n) {\n-    std::string key_space, value_space;\n-    WriteBatch batch;\n-    for (int i = 0; i < n; i++) {\n-      //if ((i % 100) == 0) fprintf(stderr, \"@ %d of %d\\n\", i, n);\n-      Slice key = Key(i, &key_space);\n-      batch.Clear();\n-      batch.Put(key, Value(i, &value_space));\n-      ASSERT_OK(db_->Write(WriteOptions(), &batch));\n-    }\n-  }\n-\n-  void Check(int min_expected, int max_expected) {\n-    int next_expected = 0;\n-    int missed = 0;\n-    int bad_keys = 0;\n-    int bad_values = 0;\n-    int correct = 0;\n-    std::string value_space;\n-    Iterator* iter = db_->NewIterator(ReadOptions());\n-    for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n-      uint64_t key;\n-      Slice in(iter->key());\n-      if (!ConsumeDecimalNumber(&in, &key) ||\n-          !in.empty() ||\n-          key < next_expected) {\n-        bad_keys++;\n-        continue;\n-      }\n-      missed += (key - next_expected);\n-      next_expected = key + 1;\n-      if (iter->value() != Value(key, &value_space)) {\n-        bad_values++;\n-      } else {\n-        correct++;\n-      }\n-    }\n-    delete iter;\n-\n-    fprintf(stderr,\n-            \"expected=%d..%d; got=%d; bad_keys=%d; bad_values=%d; missed=%d\\n\",\n-            min_expected, max_expected, correct, bad_keys, bad_values, missed);\n-    ASSERT_LE(min_expected, correct);\n-    ASSERT_GE(max_expected, correct);\n-  }\n-\n-  void Corrupt(FileType filetype, int offset, int bytes_to_corrupt) {\n-    // Pick file to corrupt\n-    std::vector<std::string> filenames;\n-    ASSERT_OK(env_.GetChildren(dbname_, &filenames));\n-    uint64_t number;\n-    FileType type;\n-    std::string fname;\n-    int picked_number = -1;\n-    for (int i = 0; i < filenames.size(); i++) {\n-      if (ParseFileName(filenames[i], &number, &type) &&\n-          type == filetype &&\n-          int(number) > picked_number) {  // Pick latest file\n-        fname = dbname_ + \"/\" + filenames[i];\n-        picked_number = number;\n-      }\n-    }\n-    ASSERT_TRUE(!fname.empty()) << filetype;\n-\n-    struct stat sbuf;\n-    if (stat(fname.c_str(), &sbuf) != 0) {\n-      const char* msg = strerror(errno);\n-      ASSERT_TRUE(false) << fname << \": \" << msg;\n-    }\n-\n-    if (offset < 0) {\n-      // Relative to end of file; make it absolute\n-      if (-offset > sbuf.st_size) {\n-        offset = 0;\n-      } else {\n-        offset = sbuf.st_size + offset;\n-      }\n-    }\n-    if (offset > sbuf.st_size) {\n-      offset = sbuf.st_size;\n-    }\n-    if (offset + bytes_to_corrupt > sbuf.st_size) {\n-      bytes_to_corrupt = sbuf.st_size - offset;\n-    }\n-\n-    // Do it\n-    std::string contents;\n-    Status s = ReadFileToString(Env::Default(), fname, &contents);\n-    ASSERT_TRUE(s.ok()) << s.ToString();\n-    for (int i = 0; i < bytes_to_corrupt; i++) {\n-      contents[i + offset] ^= 0x80;\n-    }\n-    s = WriteStringToFile(Env::Default(), contents, fname);\n-    ASSERT_TRUE(s.ok()) << s.ToString();\n-  }\n-\n-  int Property(const std::string& name) {\n-    std::string property;\n-    int result;\n-    if (db_->GetProperty(name, &property) &&\n-        sscanf(property.c_str(), \"%d\", &result) == 1) {\n-      return result;\n-    } else {\n-      return -1;\n-    }\n-  }\n-\n-  // Return the ith key\n-  Slice Key(int i, std::string* storage) {\n-    char buf[100];\n-    snprintf(buf, sizeof(buf), \"%016d\", i);\n-    storage->assign(buf, strlen(buf));\n-    return Slice(*storage);\n-  }\n-\n-  // Return the value to associate with the specified key\n-  Slice Value(int k, std::string* storage) {\n-    Random r(k);\n-    return test::RandomString(&r, kValueSize, storage);\n-  }\n-};\n-\n-TEST(CorruptionTest, Recovery) {\n-  Build(100);\n-  Check(100, 100);\n-  Corrupt(kLogFile, 19, 1);      // WriteBatch tag for first record\n-  Corrupt(kLogFile, log::kBlockSize + 1000, 1);  // Somewhere in second block\n-  Reopen();\n-\n-  // The 64 records in the first two log blocks are completely lost.\n-  Check(36, 36);\n-}\n-\n-TEST(CorruptionTest, RecoverWriteError) {\n-  env_.writable_file_error_ = true;\n-  Status s = TryReopen();\n-  ASSERT_TRUE(!s.ok());\n-}\n-\n-TEST(CorruptionTest, NewFileErrorDuringWrite) {\n-  // Do enough writing to force minor compaction\n-  env_.writable_file_error_ = true;\n-  const int num = 3 + (Options().write_buffer_size / kValueSize);\n-  std::string value_storage;\n-  Status s;\n-  for (int i = 0; s.ok() && i < num; i++) {\n-    WriteBatch batch;\n-    batch.Put(\"a\", Value(100, &value_storage));\n-    s = db_->Write(WriteOptions(), &batch);\n-  }\n-  ASSERT_TRUE(!s.ok());\n-  ASSERT_GE(env_.num_writable_file_errors_, 1);\n-  env_.writable_file_error_ = false;\n-  Reopen();\n-}\n-\n-TEST(CorruptionTest, TableFile) {\n-  Build(100);\n-  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n-  dbi->TEST_CompactMemTable();\n-  dbi->TEST_CompactRange(0, NULL, NULL);\n-  dbi->TEST_CompactRange(1, NULL, NULL);\n-\n-  Corrupt(kTableFile, 100, 1);\n-  Check(99, 99);\n-}\n-\n-TEST(CorruptionTest, TableFileIndexData) {\n-  Build(10000);  // Enough to build multiple Tables\n-  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n-  dbi->TEST_CompactMemTable();\n-\n-  Corrupt(kTableFile, -2000, 500);\n-  Reopen();\n-  Check(5000, 9999);\n-}\n-\n-TEST(CorruptionTest, MissingDescriptor) {\n-  Build(1000);\n-  RepairDB();\n-  Reopen();\n-  Check(1000, 1000);\n-}\n-\n-TEST(CorruptionTest, SequenceNumberRecovery) {\n-  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v1\"));\n-  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v2\"));\n-  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v3\"));\n-  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v4\"));\n-  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v5\"));\n-  RepairDB();\n-  Reopen();\n-  std::string v;\n-  ASSERT_OK(db_->Get(ReadOptions(), \"foo\", &v));\n-  ASSERT_EQ(\"v5\", v);\n-  // Write something.  If sequence number was not recovered properly,\n-  // it will be hidden by an earlier write.\n-  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v6\"));\n-  ASSERT_OK(db_->Get(ReadOptions(), \"foo\", &v));\n-  ASSERT_EQ(\"v6\", v);\n-  Reopen();\n-  ASSERT_OK(db_->Get(ReadOptions(), \"foo\", &v));\n-  ASSERT_EQ(\"v6\", v);\n-}\n-\n-TEST(CorruptionTest, CorruptedDescriptor) {\n-  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"hello\"));\n-  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n-  dbi->TEST_CompactMemTable();\n-  dbi->TEST_CompactRange(0, NULL, NULL);\n-\n-  Corrupt(kDescriptorFile, 0, 1000);\n-  Status s = TryReopen();\n-  ASSERT_TRUE(!s.ok());\n-\n-  RepairDB();\n-  Reopen();\n-  std::string v;\n-  ASSERT_OK(db_->Get(ReadOptions(), \"foo\", &v));\n-  ASSERT_EQ(\"hello\", v);\n-}\n-\n-TEST(CorruptionTest, CompactionInputError) {\n-  Build(10);\n-  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n-  dbi->TEST_CompactMemTable();\n-  const int last = config::kMaxMemCompactLevel;\n-  ASSERT_EQ(1, Property(\"leveldb.num-files-at-level\" + NumberToString(last)));\n-\n-  Corrupt(kTableFile, 100, 1);\n-  Check(9, 9);\n-\n-  // Force compactions by writing lots of values\n-  Build(10000);\n-  Check(10000, 10000);\n-}\n-\n-TEST(CorruptionTest, CompactionInputErrorParanoid) {\n-  Options options;\n-  options.paranoid_checks = true;\n-  options.write_buffer_size = 1048576;\n-  Reopen(&options);\n-  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n-\n-  // Fill levels >= 1 so memtable compaction outputs to level 1\n-  for (int level = 1; level < config::kNumLevels; level++) {\n-    dbi->Put(WriteOptions(), \"\", \"begin\");\n-    dbi->Put(WriteOptions(), \"~\", \"end\");\n-    dbi->TEST_CompactMemTable();\n-  }\n-\n-  Build(10);\n-  dbi->TEST_CompactMemTable();\n-  ASSERT_EQ(1, Property(\"leveldb.num-files-at-level0\"));\n-\n-  Corrupt(kTableFile, 100, 1);\n-  Check(9, 9);\n-\n-  // Write must eventually fail because of corrupted table\n-  Status s;\n-  std::string tmp1, tmp2;\n-  for (int i = 0; i < 10000 && s.ok(); i++) {\n-    s = db_->Put(WriteOptions(), Key(i, &tmp1), Value(i, &tmp2));\n-  }\n-  ASSERT_TRUE(!s.ok()) << \"write did not fail in corrupted paranoid db\";\n-}\n-\n-TEST(CorruptionTest, UnrelatedKeys) {\n-  Build(10);\n-  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n-  dbi->TEST_CompactMemTable();\n-  Corrupt(kTableFile, 100, 1);\n-\n-  std::string tmp1, tmp2;\n-  ASSERT_OK(db_->Put(WriteOptions(), Key(1000, &tmp1), Value(1000, &tmp2)));\n-  std::string v;\n-  ASSERT_OK(db_->Get(ReadOptions(), Key(1000, &tmp1), &v));\n-  ASSERT_EQ(Value(1000, &tmp2).ToString(), v);\n-  dbi->TEST_CompactMemTable();\n-  ASSERT_OK(db_->Get(ReadOptions(), Key(1000, &tmp1), &v));\n-  ASSERT_EQ(Value(1000, &tmp2).ToString(), v);\n-}\n-\n-}  // namespace leveldb\n-\n-int main(int argc, char** argv) {\n-  return leveldb::test::RunAllTests();\n-}"
      },
      {
        "sha": "7abdf87587dfc8d4fa0287d8ad45d82b2142d7fc",
        "filename": "src/leveldb/db/db_bench.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 979,
        "changes": 979,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/db_bench.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/db_bench.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_bench.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,979 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include <sys/types.h>\n-#include <stdio.h>\n-#include <stdlib.h>\n-#include \"db/db_impl.h\"\n-#include \"db/version_set.h\"\n-#include \"leveldb/cache.h\"\n-#include \"leveldb/db.h\"\n-#include \"leveldb/env.h\"\n-#include \"leveldb/write_batch.h\"\n-#include \"port/port.h\"\n-#include \"util/crc32c.h\"\n-#include \"util/histogram.h\"\n-#include \"util/mutexlock.h\"\n-#include \"util/random.h\"\n-#include \"util/testutil.h\"\n-\n-// Comma-separated list of operations to run in the specified order\n-//   Actual benchmarks:\n-//      fillseq       -- write N values in sequential key order in async mode\n-//      fillrandom    -- write N values in random key order in async mode\n-//      overwrite     -- overwrite N values in random key order in async mode\n-//      fillsync      -- write N/100 values in random key order in sync mode\n-//      fill100K      -- write N/1000 100K values in random order in async mode\n-//      deleteseq     -- delete N keys in sequential order\n-//      deleterandom  -- delete N keys in random order\n-//      readseq       -- read N times sequentially\n-//      readreverse   -- read N times in reverse order\n-//      readrandom    -- read N times in random order\n-//      readmissing   -- read N missing keys in random order\n-//      readhot       -- read N times in random order from 1% section of DB\n-//      seekrandom    -- N random seeks\n-//      crc32c        -- repeated crc32c of 4K of data\n-//      acquireload   -- load N*1000 times\n-//   Meta operations:\n-//      compact     -- Compact the entire DB\n-//      stats       -- Print DB stats\n-//      sstables    -- Print sstable info\n-//      heapprofile -- Dump a heap profile (if supported by this port)\n-static const char* FLAGS_benchmarks =\n-    \"fillseq,\"\n-    \"fillsync,\"\n-    \"fillrandom,\"\n-    \"overwrite,\"\n-    \"readrandom,\"\n-    \"readrandom,\"  // Extra run to allow previous compactions to quiesce\n-    \"readseq,\"\n-    \"readreverse,\"\n-    \"compact,\"\n-    \"readrandom,\"\n-    \"readseq,\"\n-    \"readreverse,\"\n-    \"fill100K,\"\n-    \"crc32c,\"\n-    \"snappycomp,\"\n-    \"snappyuncomp,\"\n-    \"acquireload,\"\n-    ;\n-\n-// Number of key/values to place in database\n-static int FLAGS_num = 1000000;\n-\n-// Number of read operations to do.  If negative, do FLAGS_num reads.\n-static int FLAGS_reads = -1;\n-\n-// Number of concurrent threads to run.\n-static int FLAGS_threads = 1;\n-\n-// Size of each value\n-static int FLAGS_value_size = 100;\n-\n-// Arrange to generate values that shrink to this fraction of\n-// their original size after compression\n-static double FLAGS_compression_ratio = 0.5;\n-\n-// Print histogram of operation timings\n-static bool FLAGS_histogram = false;\n-\n-// Number of bytes to buffer in memtable before compacting\n-// (initialized to default value by \"main\")\n-static int FLAGS_write_buffer_size = 0;\n-\n-// Number of bytes to use as a cache of uncompressed data.\n-// Negative means use default settings.\n-static int FLAGS_cache_size = -1;\n-\n-// Maximum number of files to keep open at the same time (use default if == 0)\n-static int FLAGS_open_files = 0;\n-\n-// Bloom filter bits per key.\n-// Negative means use default settings.\n-static int FLAGS_bloom_bits = -1;\n-\n-// If true, do not destroy the existing database.  If you set this\n-// flag and also specify a benchmark that wants a fresh database, that\n-// benchmark will fail.\n-static bool FLAGS_use_existing_db = false;\n-\n-// Use the db with the following name.\n-static const char* FLAGS_db = NULL;\n-\n-namespace leveldb {\n-\n-namespace {\n-\n-// Helper for quickly generating random data.\n-class RandomGenerator {\n- private:\n-  std::string data_;\n-  int pos_;\n-\n- public:\n-  RandomGenerator() {\n-    // We use a limited amount of data over and over again and ensure\n-    // that it is larger than the compression window (32KB), and also\n-    // large enough to serve all typical value sizes we want to write.\n-    Random rnd(301);\n-    std::string piece;\n-    while (data_.size() < 1048576) {\n-      // Add a short fragment that is as compressible as specified\n-      // by FLAGS_compression_ratio.\n-      test::CompressibleString(&rnd, FLAGS_compression_ratio, 100, &piece);\n-      data_.append(piece);\n-    }\n-    pos_ = 0;\n-  }\n-\n-  Slice Generate(int len) {\n-    if (pos_ + len > data_.size()) {\n-      pos_ = 0;\n-      assert(len < data_.size());\n-    }\n-    pos_ += len;\n-    return Slice(data_.data() + pos_ - len, len);\n-  }\n-};\n-\n-static Slice TrimSpace(Slice s) {\n-  int start = 0;\n-  while (start < s.size() && isspace(s[start])) {\n-    start++;\n-  }\n-  int limit = s.size();\n-  while (limit > start && isspace(s[limit-1])) {\n-    limit--;\n-  }\n-  return Slice(s.data() + start, limit - start);\n-}\n-\n-static void AppendWithSpace(std::string* str, Slice msg) {\n-  if (msg.empty()) return;\n-  if (!str->empty()) {\n-    str->push_back(' ');\n-  }\n-  str->append(msg.data(), msg.size());\n-}\n-\n-class Stats {\n- private:\n-  double start_;\n-  double finish_;\n-  double seconds_;\n-  int done_;\n-  int next_report_;\n-  int64_t bytes_;\n-  double last_op_finish_;\n-  Histogram hist_;\n-  std::string message_;\n-\n- public:\n-  Stats() { Start(); }\n-\n-  void Start() {\n-    next_report_ = 100;\n-    last_op_finish_ = start_;\n-    hist_.Clear();\n-    done_ = 0;\n-    bytes_ = 0;\n-    seconds_ = 0;\n-    start_ = Env::Default()->NowMicros();\n-    finish_ = start_;\n-    message_.clear();\n-  }\n-\n-  void Merge(const Stats& other) {\n-    hist_.Merge(other.hist_);\n-    done_ += other.done_;\n-    bytes_ += other.bytes_;\n-    seconds_ += other.seconds_;\n-    if (other.start_ < start_) start_ = other.start_;\n-    if (other.finish_ > finish_) finish_ = other.finish_;\n-\n-    // Just keep the messages from one thread\n-    if (message_.empty()) message_ = other.message_;\n-  }\n-\n-  void Stop() {\n-    finish_ = Env::Default()->NowMicros();\n-    seconds_ = (finish_ - start_) * 1e-6;\n-  }\n-\n-  void AddMessage(Slice msg) {\n-    AppendWithSpace(&message_, msg);\n-  }\n-\n-  void FinishedSingleOp() {\n-    if (FLAGS_histogram) {\n-      double now = Env::Default()->NowMicros();\n-      double micros = now - last_op_finish_;\n-      hist_.Add(micros);\n-      if (micros > 20000) {\n-        fprintf(stderr, \"long op: %.1f micros%30s\\r\", micros, \"\");\n-        fflush(stderr);\n-      }\n-      last_op_finish_ = now;\n-    }\n-\n-    done_++;\n-    if (done_ >= next_report_) {\n-      if      (next_report_ < 1000)   next_report_ += 100;\n-      else if (next_report_ < 5000)   next_report_ += 500;\n-      else if (next_report_ < 10000)  next_report_ += 1000;\n-      else if (next_report_ < 50000)  next_report_ += 5000;\n-      else if (next_report_ < 100000) next_report_ += 10000;\n-      else if (next_report_ < 500000) next_report_ += 50000;\n-      else                            next_report_ += 100000;\n-      fprintf(stderr, \"... finished %d ops%30s\\r\", done_, \"\");\n-      fflush(stderr);\n-    }\n-  }\n-\n-  void AddBytes(int64_t n) {\n-    bytes_ += n;\n-  }\n-\n-  void Report(const Slice& name) {\n-    // Pretend at least one op was done in case we are running a benchmark\n-    // that does not call FinishedSingleOp().\n-    if (done_ < 1) done_ = 1;\n-\n-    std::string extra;\n-    if (bytes_ > 0) {\n-      // Rate is computed on actual elapsed time, not the sum of per-thread\n-      // elapsed times.\n-      double elapsed = (finish_ - start_) * 1e-6;\n-      char rate[100];\n-      snprintf(rate, sizeof(rate), \"%6.1f MB/s\",\n-               (bytes_ / 1048576.0) / elapsed);\n-      extra = rate;\n-    }\n-    AppendWithSpace(&extra, message_);\n-\n-    fprintf(stdout, \"%-12s : %11.3f micros/op;%s%s\\n\",\n-            name.ToString().c_str(),\n-            seconds_ * 1e6 / done_,\n-            (extra.empty() ? \"\" : \" \"),\n-            extra.c_str());\n-    if (FLAGS_histogram) {\n-      fprintf(stdout, \"Microseconds per op:\\n%s\\n\", hist_.ToString().c_str());\n-    }\n-    fflush(stdout);\n-  }\n-};\n-\n-// State shared by all concurrent executions of the same benchmark.\n-struct SharedState {\n-  port::Mutex mu;\n-  port::CondVar cv;\n-  int total;\n-\n-  // Each thread goes through the following states:\n-  //    (1) initializing\n-  //    (2) waiting for others to be initialized\n-  //    (3) running\n-  //    (4) done\n-\n-  int num_initialized;\n-  int num_done;\n-  bool start;\n-\n-  SharedState() : cv(&mu) { }\n-};\n-\n-// Per-thread state for concurrent executions of the same benchmark.\n-struct ThreadState {\n-  int tid;             // 0..n-1 when running in n threads\n-  Random rand;         // Has different seeds for different threads\n-  Stats stats;\n-  SharedState* shared;\n-\n-  ThreadState(int index)\n-      : tid(index),\n-        rand(1000 + index) {\n-  }\n-};\n-\n-}  // namespace\n-\n-class Benchmark {\n- private:\n-  Cache* cache_;\n-  const FilterPolicy* filter_policy_;\n-  DB* db_;\n-  int num_;\n-  int value_size_;\n-  int entries_per_batch_;\n-  WriteOptions write_options_;\n-  int reads_;\n-  int heap_counter_;\n-\n-  void PrintHeader() {\n-    const int kKeySize = 16;\n-    PrintEnvironment();\n-    fprintf(stdout, \"Keys:       %d bytes each\\n\", kKeySize);\n-    fprintf(stdout, \"Values:     %d bytes each (%d bytes after compression)\\n\",\n-            FLAGS_value_size,\n-            static_cast<int>(FLAGS_value_size * FLAGS_compression_ratio + 0.5));\n-    fprintf(stdout, \"Entries:    %d\\n\", num_);\n-    fprintf(stdout, \"RawSize:    %.1f MB (estimated)\\n\",\n-            ((static_cast<int64_t>(kKeySize + FLAGS_value_size) * num_)\n-             / 1048576.0));\n-    fprintf(stdout, \"FileSize:   %.1f MB (estimated)\\n\",\n-            (((kKeySize + FLAGS_value_size * FLAGS_compression_ratio) * num_)\n-             / 1048576.0));\n-    PrintWarnings();\n-    fprintf(stdout, \"------------------------------------------------\\n\");\n-  }\n-\n-  void PrintWarnings() {\n-#if defined(__GNUC__) && !defined(__OPTIMIZE__)\n-    fprintf(stdout,\n-            \"WARNING: Optimization is disabled: benchmarks unnecessarily slow\\n\"\n-            );\n-#endif\n-#ifndef NDEBUG\n-    fprintf(stdout,\n-            \"WARNING: Assertions are enabled; benchmarks unnecessarily slow\\n\");\n-#endif\n-\n-    // See if snappy is working by attempting to compress a compressible string\n-    const char text[] = \"yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\";\n-    std::string compressed;\n-    if (!port::Snappy_Compress(text, sizeof(text), &compressed)) {\n-      fprintf(stdout, \"WARNING: Snappy compression is not enabled\\n\");\n-    } else if (compressed.size() >= sizeof(text)) {\n-      fprintf(stdout, \"WARNING: Snappy compression is not effective\\n\");\n-    }\n-  }\n-\n-  void PrintEnvironment() {\n-    fprintf(stderr, \"LevelDB:    version %d.%d\\n\",\n-            kMajorVersion, kMinorVersion);\n-\n-#if defined(__linux)\n-    time_t now = time(NULL);\n-    fprintf(stderr, \"Date:       %s\", ctime(&now));  // ctime() adds newline\n-\n-    FILE* cpuinfo = fopen(\"/proc/cpuinfo\", \"r\");\n-    if (cpuinfo != NULL) {\n-      char line[1000];\n-      int num_cpus = 0;\n-      std::string cpu_type;\n-      std::string cache_size;\n-      while (fgets(line, sizeof(line), cpuinfo) != NULL) {\n-        const char* sep = strchr(line, ':');\n-        if (sep == NULL) {\n-          continue;\n-        }\n-        Slice key = TrimSpace(Slice(line, sep - 1 - line));\n-        Slice val = TrimSpace(Slice(sep + 1));\n-        if (key == \"model name\") {\n-          ++num_cpus;\n-          cpu_type = val.ToString();\n-        } else if (key == \"cache size\") {\n-          cache_size = val.ToString();\n-        }\n-      }\n-      fclose(cpuinfo);\n-      fprintf(stderr, \"CPU:        %d * %s\\n\", num_cpus, cpu_type.c_str());\n-      fprintf(stderr, \"CPUCache:   %s\\n\", cache_size.c_str());\n-    }\n-#endif\n-  }\n-\n- public:\n-  Benchmark()\n-  : cache_(FLAGS_cache_size >= 0 ? NewLRUCache(FLAGS_cache_size) : NULL),\n-    filter_policy_(FLAGS_bloom_bits >= 0\n-                   ? NewBloomFilterPolicy(FLAGS_bloom_bits)\n-                   : NULL),\n-    db_(NULL),\n-    num_(FLAGS_num),\n-    value_size_(FLAGS_value_size),\n-    entries_per_batch_(1),\n-    reads_(FLAGS_reads < 0 ? FLAGS_num : FLAGS_reads),\n-    heap_counter_(0) {\n-    std::vector<std::string> files;\n-    Env::Default()->GetChildren(FLAGS_db, &files);\n-    for (int i = 0; i < files.size(); i++) {\n-      if (Slice(files[i]).starts_with(\"heap-\")) {\n-        Env::Default()->DeleteFile(std::string(FLAGS_db) + \"/\" + files[i]);\n-      }\n-    }\n-    if (!FLAGS_use_existing_db) {\n-      DestroyDB(FLAGS_db, Options());\n-    }\n-  }\n-\n-  ~Benchmark() {\n-    delete db_;\n-    delete cache_;\n-    delete filter_policy_;\n-  }\n-\n-  void Run() {\n-    PrintHeader();\n-    Open();\n-\n-    const char* benchmarks = FLAGS_benchmarks;\n-    while (benchmarks != NULL) {\n-      const char* sep = strchr(benchmarks, ',');\n-      Slice name;\n-      if (sep == NULL) {\n-        name = benchmarks;\n-        benchmarks = NULL;\n-      } else {\n-        name = Slice(benchmarks, sep - benchmarks);\n-        benchmarks = sep + 1;\n-      }\n-\n-      // Reset parameters that may be overriddden bwlow\n-      num_ = FLAGS_num;\n-      reads_ = (FLAGS_reads < 0 ? FLAGS_num : FLAGS_reads);\n-      value_size_ = FLAGS_value_size;\n-      entries_per_batch_ = 1;\n-      write_options_ = WriteOptions();\n-\n-      void (Benchmark::*method)(ThreadState*) = NULL;\n-      bool fresh_db = false;\n-      int num_threads = FLAGS_threads;\n-\n-      if (name == Slice(\"fillseq\")) {\n-        fresh_db = true;\n-        method = &Benchmark::WriteSeq;\n-      } else if (name == Slice(\"fillbatch\")) {\n-        fresh_db = true;\n-        entries_per_batch_ = 1000;\n-        method = &Benchmark::WriteSeq;\n-      } else if (name == Slice(\"fillrandom\")) {\n-        fresh_db = true;\n-        method = &Benchmark::WriteRandom;\n-      } else if (name == Slice(\"overwrite\")) {\n-        fresh_db = false;\n-        method = &Benchmark::WriteRandom;\n-      } else if (name == Slice(\"fillsync\")) {\n-        fresh_db = true;\n-        num_ /= 1000;\n-        write_options_.sync = true;\n-        method = &Benchmark::WriteRandom;\n-      } else if (name == Slice(\"fill100K\")) {\n-        fresh_db = true;\n-        num_ /= 1000;\n-        value_size_ = 100 * 1000;\n-        method = &Benchmark::WriteRandom;\n-      } else if (name == Slice(\"readseq\")) {\n-        method = &Benchmark::ReadSequential;\n-      } else if (name == Slice(\"readreverse\")) {\n-        method = &Benchmark::ReadReverse;\n-      } else if (name == Slice(\"readrandom\")) {\n-        method = &Benchmark::ReadRandom;\n-      } else if (name == Slice(\"readmissing\")) {\n-        method = &Benchmark::ReadMissing;\n-      } else if (name == Slice(\"seekrandom\")) {\n-        method = &Benchmark::SeekRandom;\n-      } else if (name == Slice(\"readhot\")) {\n-        method = &Benchmark::ReadHot;\n-      } else if (name == Slice(\"readrandomsmall\")) {\n-        reads_ /= 1000;\n-        method = &Benchmark::ReadRandom;\n-      } else if (name == Slice(\"deleteseq\")) {\n-        method = &Benchmark::DeleteSeq;\n-      } else if (name == Slice(\"deleterandom\")) {\n-        method = &Benchmark::DeleteRandom;\n-      } else if (name == Slice(\"readwhilewriting\")) {\n-        num_threads++;  // Add extra thread for writing\n-        method = &Benchmark::ReadWhileWriting;\n-      } else if (name == Slice(\"compact\")) {\n-        method = &Benchmark::Compact;\n-      } else if (name == Slice(\"crc32c\")) {\n-        method = &Benchmark::Crc32c;\n-      } else if (name == Slice(\"acquireload\")) {\n-        method = &Benchmark::AcquireLoad;\n-      } else if (name == Slice(\"snappycomp\")) {\n-        method = &Benchmark::SnappyCompress;\n-      } else if (name == Slice(\"snappyuncomp\")) {\n-        method = &Benchmark::SnappyUncompress;\n-      } else if (name == Slice(\"heapprofile\")) {\n-        HeapProfile();\n-      } else if (name == Slice(\"stats\")) {\n-        PrintStats(\"leveldb.stats\");\n-      } else if (name == Slice(\"sstables\")) {\n-        PrintStats(\"leveldb.sstables\");\n-      } else {\n-        if (name != Slice()) {  // No error message for empty name\n-          fprintf(stderr, \"unknown benchmark '%s'\\n\", name.ToString().c_str());\n-        }\n-      }\n-\n-      if (fresh_db) {\n-        if (FLAGS_use_existing_db) {\n-          fprintf(stdout, \"%-12s : skipped (--use_existing_db is true)\\n\",\n-                  name.ToString().c_str());\n-          method = NULL;\n-        } else {\n-          delete db_;\n-          db_ = NULL;\n-          DestroyDB(FLAGS_db, Options());\n-          Open();\n-        }\n-      }\n-\n-      if (method != NULL) {\n-        RunBenchmark(num_threads, name, method);\n-      }\n-    }\n-  }\n-\n- private:\n-  struct ThreadArg {\n-    Benchmark* bm;\n-    SharedState* shared;\n-    ThreadState* thread;\n-    void (Benchmark::*method)(ThreadState*);\n-  };\n-\n-  static void ThreadBody(void* v) {\n-    ThreadArg* arg = reinterpret_cast<ThreadArg*>(v);\n-    SharedState* shared = arg->shared;\n-    ThreadState* thread = arg->thread;\n-    {\n-      MutexLock l(&shared->mu);\n-      shared->num_initialized++;\n-      if (shared->num_initialized >= shared->total) {\n-        shared->cv.SignalAll();\n-      }\n-      while (!shared->start) {\n-        shared->cv.Wait();\n-      }\n-    }\n-\n-    thread->stats.Start();\n-    (arg->bm->*(arg->method))(thread);\n-    thread->stats.Stop();\n-\n-    {\n-      MutexLock l(&shared->mu);\n-      shared->num_done++;\n-      if (shared->num_done >= shared->total) {\n-        shared->cv.SignalAll();\n-      }\n-    }\n-  }\n-\n-  void RunBenchmark(int n, Slice name,\n-                    void (Benchmark::*method)(ThreadState*)) {\n-    SharedState shared;\n-    shared.total = n;\n-    shared.num_initialized = 0;\n-    shared.num_done = 0;\n-    shared.start = false;\n-\n-    ThreadArg* arg = new ThreadArg[n];\n-    for (int i = 0; i < n; i++) {\n-      arg[i].bm = this;\n-      arg[i].method = method;\n-      arg[i].shared = &shared;\n-      arg[i].thread = new ThreadState(i);\n-      arg[i].thread->shared = &shared;\n-      Env::Default()->StartThread(ThreadBody, &arg[i]);\n-    }\n-\n-    shared.mu.Lock();\n-    while (shared.num_initialized < n) {\n-      shared.cv.Wait();\n-    }\n-\n-    shared.start = true;\n-    shared.cv.SignalAll();\n-    while (shared.num_done < n) {\n-      shared.cv.Wait();\n-    }\n-    shared.mu.Unlock();\n-\n-    for (int i = 1; i < n; i++) {\n-      arg[0].thread->stats.Merge(arg[i].thread->stats);\n-    }\n-    arg[0].thread->stats.Report(name);\n-\n-    for (int i = 0; i < n; i++) {\n-      delete arg[i].thread;\n-    }\n-    delete[] arg;\n-  }\n-\n-  void Crc32c(ThreadState* thread) {\n-    // Checksum about 500MB of data total\n-    const int size = 4096;\n-    const char* label = \"(4K per op)\";\n-    std::string data(size, 'x');\n-    int64_t bytes = 0;\n-    uint32_t crc = 0;\n-    while (bytes < 500 * 1048576) {\n-      crc = crc32c::Value(data.data(), size);\n-      thread->stats.FinishedSingleOp();\n-      bytes += size;\n-    }\n-    // Print so result is not dead\n-    fprintf(stderr, \"... crc=0x%x\\r\", static_cast<unsigned int>(crc));\n-\n-    thread->stats.AddBytes(bytes);\n-    thread->stats.AddMessage(label);\n-  }\n-\n-  void AcquireLoad(ThreadState* thread) {\n-    int dummy;\n-    port::AtomicPointer ap(&dummy);\n-    int count = 0;\n-    void *ptr = NULL;\n-    thread->stats.AddMessage(\"(each op is 1000 loads)\");\n-    while (count < 100000) {\n-      for (int i = 0; i < 1000; i++) {\n-        ptr = ap.Acquire_Load();\n-      }\n-      count++;\n-      thread->stats.FinishedSingleOp();\n-    }\n-    if (ptr == NULL) exit(1); // Disable unused variable warning.\n-  }\n-\n-  void SnappyCompress(ThreadState* thread) {\n-    RandomGenerator gen;\n-    Slice input = gen.Generate(Options().block_size);\n-    int64_t bytes = 0;\n-    int64_t produced = 0;\n-    bool ok = true;\n-    std::string compressed;\n-    while (ok && bytes < 1024 * 1048576) {  // Compress 1G\n-      ok = port::Snappy_Compress(input.data(), input.size(), &compressed);\n-      produced += compressed.size();\n-      bytes += input.size();\n-      thread->stats.FinishedSingleOp();\n-    }\n-\n-    if (!ok) {\n-      thread->stats.AddMessage(\"(snappy failure)\");\n-    } else {\n-      char buf[100];\n-      snprintf(buf, sizeof(buf), \"(output: %.1f%%)\",\n-               (produced * 100.0) / bytes);\n-      thread->stats.AddMessage(buf);\n-      thread->stats.AddBytes(bytes);\n-    }\n-  }\n-\n-  void SnappyUncompress(ThreadState* thread) {\n-    RandomGenerator gen;\n-    Slice input = gen.Generate(Options().block_size);\n-    std::string compressed;\n-    bool ok = port::Snappy_Compress(input.data(), input.size(), &compressed);\n-    int64_t bytes = 0;\n-    char* uncompressed = new char[input.size()];\n-    while (ok && bytes < 1024 * 1048576) {  // Compress 1G\n-      ok =  port::Snappy_Uncompress(compressed.data(), compressed.size(),\n-                                    uncompressed);\n-      bytes += input.size();\n-      thread->stats.FinishedSingleOp();\n-    }\n-    delete[] uncompressed;\n-\n-    if (!ok) {\n-      thread->stats.AddMessage(\"(snappy failure)\");\n-    } else {\n-      thread->stats.AddBytes(bytes);\n-    }\n-  }\n-\n-  void Open() {\n-    assert(db_ == NULL);\n-    Options options;\n-    options.create_if_missing = !FLAGS_use_existing_db;\n-    options.block_cache = cache_;\n-    options.write_buffer_size = FLAGS_write_buffer_size;\n-    options.max_open_files = FLAGS_open_files;\n-    options.filter_policy = filter_policy_;\n-    Status s = DB::Open(options, FLAGS_db, &db_);\n-    if (!s.ok()) {\n-      fprintf(stderr, \"open error: %s\\n\", s.ToString().c_str());\n-      exit(1);\n-    }\n-  }\n-\n-  void WriteSeq(ThreadState* thread) {\n-    DoWrite(thread, true);\n-  }\n-\n-  void WriteRandom(ThreadState* thread) {\n-    DoWrite(thread, false);\n-  }\n-\n-  void DoWrite(ThreadState* thread, bool seq) {\n-    if (num_ != FLAGS_num) {\n-      char msg[100];\n-      snprintf(msg, sizeof(msg), \"(%d ops)\", num_);\n-      thread->stats.AddMessage(msg);\n-    }\n-\n-    RandomGenerator gen;\n-    WriteBatch batch;\n-    Status s;\n-    int64_t bytes = 0;\n-    for (int i = 0; i < num_; i += entries_per_batch_) {\n-      batch.Clear();\n-      for (int j = 0; j < entries_per_batch_; j++) {\n-        const int k = seq ? i+j : (thread->rand.Next() % FLAGS_num);\n-        char key[100];\n-        snprintf(key, sizeof(key), \"%016d\", k);\n-        batch.Put(key, gen.Generate(value_size_));\n-        bytes += value_size_ + strlen(key);\n-        thread->stats.FinishedSingleOp();\n-      }\n-      s = db_->Write(write_options_, &batch);\n-      if (!s.ok()) {\n-        fprintf(stderr, \"put error: %s\\n\", s.ToString().c_str());\n-        exit(1);\n-      }\n-    }\n-    thread->stats.AddBytes(bytes);\n-  }\n-\n-  void ReadSequential(ThreadState* thread) {\n-    Iterator* iter = db_->NewIterator(ReadOptions());\n-    int i = 0;\n-    int64_t bytes = 0;\n-    for (iter->SeekToFirst(); i < reads_ && iter->Valid(); iter->Next()) {\n-      bytes += iter->key().size() + iter->value().size();\n-      thread->stats.FinishedSingleOp();\n-      ++i;\n-    }\n-    delete iter;\n-    thread->stats.AddBytes(bytes);\n-  }\n-\n-  void ReadReverse(ThreadState* thread) {\n-    Iterator* iter = db_->NewIterator(ReadOptions());\n-    int i = 0;\n-    int64_t bytes = 0;\n-    for (iter->SeekToLast(); i < reads_ && iter->Valid(); iter->Prev()) {\n-      bytes += iter->key().size() + iter->value().size();\n-      thread->stats.FinishedSingleOp();\n-      ++i;\n-    }\n-    delete iter;\n-    thread->stats.AddBytes(bytes);\n-  }\n-\n-  void ReadRandom(ThreadState* thread) {\n-    ReadOptions options;\n-    std::string value;\n-    int found = 0;\n-    for (int i = 0; i < reads_; i++) {\n-      char key[100];\n-      const int k = thread->rand.Next() % FLAGS_num;\n-      snprintf(key, sizeof(key), \"%016d\", k);\n-      if (db_->Get(options, key, &value).ok()) {\n-        found++;\n-      }\n-      thread->stats.FinishedSingleOp();\n-    }\n-    char msg[100];\n-    snprintf(msg, sizeof(msg), \"(%d of %d found)\", found, num_);\n-    thread->stats.AddMessage(msg);\n-  }\n-\n-  void ReadMissing(ThreadState* thread) {\n-    ReadOptions options;\n-    std::string value;\n-    for (int i = 0; i < reads_; i++) {\n-      char key[100];\n-      const int k = thread->rand.Next() % FLAGS_num;\n-      snprintf(key, sizeof(key), \"%016d.\", k);\n-      db_->Get(options, key, &value);\n-      thread->stats.FinishedSingleOp();\n-    }\n-  }\n-\n-  void ReadHot(ThreadState* thread) {\n-    ReadOptions options;\n-    std::string value;\n-    const int range = (FLAGS_num + 99) / 100;\n-    for (int i = 0; i < reads_; i++) {\n-      char key[100];\n-      const int k = thread->rand.Next() % range;\n-      snprintf(key, sizeof(key), \"%016d\", k);\n-      db_->Get(options, key, &value);\n-      thread->stats.FinishedSingleOp();\n-    }\n-  }\n-\n-  void SeekRandom(ThreadState* thread) {\n-    ReadOptions options;\n-    std::string value;\n-    int found = 0;\n-    for (int i = 0; i < reads_; i++) {\n-      Iterator* iter = db_->NewIterator(options);\n-      char key[100];\n-      const int k = thread->rand.Next() % FLAGS_num;\n-      snprintf(key, sizeof(key), \"%016d\", k);\n-      iter->Seek(key);\n-      if (iter->Valid() && iter->key() == key) found++;\n-      delete iter;\n-      thread->stats.FinishedSingleOp();\n-    }\n-    char msg[100];\n-    snprintf(msg, sizeof(msg), \"(%d of %d found)\", found, num_);\n-    thread->stats.AddMessage(msg);\n-  }\n-\n-  void DoDelete(ThreadState* thread, bool seq) {\n-    RandomGenerator gen;\n-    WriteBatch batch;\n-    Status s;\n-    for (int i = 0; i < num_; i += entries_per_batch_) {\n-      batch.Clear();\n-      for (int j = 0; j < entries_per_batch_; j++) {\n-        const int k = seq ? i+j : (thread->rand.Next() % FLAGS_num);\n-        char key[100];\n-        snprintf(key, sizeof(key), \"%016d\", k);\n-        batch.Delete(key);\n-        thread->stats.FinishedSingleOp();\n-      }\n-      s = db_->Write(write_options_, &batch);\n-      if (!s.ok()) {\n-        fprintf(stderr, \"del error: %s\\n\", s.ToString().c_str());\n-        exit(1);\n-      }\n-    }\n-  }\n-\n-  void DeleteSeq(ThreadState* thread) {\n-    DoDelete(thread, true);\n-  }\n-\n-  void DeleteRandom(ThreadState* thread) {\n-    DoDelete(thread, false);\n-  }\n-\n-  void ReadWhileWriting(ThreadState* thread) {\n-    if (thread->tid > 0) {\n-      ReadRandom(thread);\n-    } else {\n-      // Special thread that keeps writing until other threads are done.\n-      RandomGenerator gen;\n-      while (true) {\n-        {\n-          MutexLock l(&thread->shared->mu);\n-          if (thread->shared->num_done + 1 >= thread->shared->num_initialized) {\n-            // Other threads have finished\n-            break;\n-          }\n-        }\n-\n-        const int k = thread->rand.Next() % FLAGS_num;\n-        char key[100];\n-        snprintf(key, sizeof(key), \"%016d\", k);\n-        Status s = db_->Put(write_options_, key, gen.Generate(value_size_));\n-        if (!s.ok()) {\n-          fprintf(stderr, \"put error: %s\\n\", s.ToString().c_str());\n-          exit(1);\n-        }\n-      }\n-\n-      // Do not count any of the preceding work/delay in stats.\n-      thread->stats.Start();\n-    }\n-  }\n-\n-  void Compact(ThreadState* thread) {\n-    db_->CompactRange(NULL, NULL);\n-  }\n-\n-  void PrintStats(const char* key) {\n-    std::string stats;\n-    if (!db_->GetProperty(key, &stats)) {\n-      stats = \"(failed)\";\n-    }\n-    fprintf(stdout, \"\\n%s\\n\", stats.c_str());\n-  }\n-\n-  static void WriteToFile(void* arg, const char* buf, int n) {\n-    reinterpret_cast<WritableFile*>(arg)->Append(Slice(buf, n));\n-  }\n-\n-  void HeapProfile() {\n-    char fname[100];\n-    snprintf(fname, sizeof(fname), \"%s/heap-%04d\", FLAGS_db, ++heap_counter_);\n-    WritableFile* file;\n-    Status s = Env::Default()->NewWritableFile(fname, &file);\n-    if (!s.ok()) {\n-      fprintf(stderr, \"%s\\n\", s.ToString().c_str());\n-      return;\n-    }\n-    bool ok = port::GetHeapProfile(WriteToFile, file);\n-    delete file;\n-    if (!ok) {\n-      fprintf(stderr, \"heap profiling not supported\\n\");\n-      Env::Default()->DeleteFile(fname);\n-    }\n-  }\n-};\n-\n-}  // namespace leveldb\n-\n-int main(int argc, char** argv) {\n-  FLAGS_write_buffer_size = leveldb::Options().write_buffer_size;\n-  FLAGS_open_files = leveldb::Options().max_open_files;\n-  std::string default_db_path;\n-\n-  for (int i = 1; i < argc; i++) {\n-    double d;\n-    int n;\n-    char junk;\n-    if (leveldb::Slice(argv[i]).starts_with(\"--benchmarks=\")) {\n-      FLAGS_benchmarks = argv[i] + strlen(\"--benchmarks=\");\n-    } else if (sscanf(argv[i], \"--compression_ratio=%lf%c\", &d, &junk) == 1) {\n-      FLAGS_compression_ratio = d;\n-    } else if (sscanf(argv[i], \"--histogram=%d%c\", &n, &junk) == 1 &&\n-               (n == 0 || n == 1)) {\n-      FLAGS_histogram = n;\n-    } else if (sscanf(argv[i], \"--use_existing_db=%d%c\", &n, &junk) == 1 &&\n-               (n == 0 || n == 1)) {\n-      FLAGS_use_existing_db = n;\n-    } else if (sscanf(argv[i], \"--num=%d%c\", &n, &junk) == 1) {\n-      FLAGS_num = n;\n-    } else if (sscanf(argv[i], \"--reads=%d%c\", &n, &junk) == 1) {\n-      FLAGS_reads = n;\n-    } else if (sscanf(argv[i], \"--threads=%d%c\", &n, &junk) == 1) {\n-      FLAGS_threads = n;\n-    } else if (sscanf(argv[i], \"--value_size=%d%c\", &n, &junk) == 1) {\n-      FLAGS_value_size = n;\n-    } else if (sscanf(argv[i], \"--write_buffer_size=%d%c\", &n, &junk) == 1) {\n-      FLAGS_write_buffer_size = n;\n-    } else if (sscanf(argv[i], \"--cache_size=%d%c\", &n, &junk) == 1) {\n-      FLAGS_cache_size = n;\n-    } else if (sscanf(argv[i], \"--bloom_bits=%d%c\", &n, &junk) == 1) {\n-      FLAGS_bloom_bits = n;\n-    } else if (sscanf(argv[i], \"--open_files=%d%c\", &n, &junk) == 1) {\n-      FLAGS_open_files = n;\n-    } else if (strncmp(argv[i], \"--db=\", 5) == 0) {\n-      FLAGS_db = argv[i] + 5;\n-    } else {\n-      fprintf(stderr, \"Invalid flag '%s'\\n\", argv[i]);\n-      exit(1);\n-    }\n-  }\n-\n-  // Choose a location for the test database if none given with --db=<path>\n-  if (FLAGS_db == NULL) {\n-      leveldb::Env::Default()->GetTestDirectory(&default_db_path);\n-      default_db_path += \"/dbbench\";\n-      FLAGS_db = default_db_path.c_str();\n-  }\n-\n-  leveldb::Benchmark benchmark;\n-  benchmark.Run();\n-  return 0;\n-}"
      },
      {
        "sha": "c9de169f29f6cad112f8427006bb7ba0391cb29e",
        "filename": "src/leveldb/db/db_impl.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 1467,
        "changes": 1467,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/db_impl.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/db_impl.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_impl.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,1467 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/db_impl.h\"\n-\n-#include <algorithm>\n-#include <set>\n-#include <string>\n-#include <stdint.h>\n-#include <stdio.h>\n-#include <vector>\n-#include \"db/builder.h\"\n-#include \"db/db_iter.h\"\n-#include \"db/dbformat.h\"\n-#include \"db/filename.h\"\n-#include \"db/log_reader.h\"\n-#include \"db/log_writer.h\"\n-#include \"db/memtable.h\"\n-#include \"db/table_cache.h\"\n-#include \"db/version_set.h\"\n-#include \"db/write_batch_internal.h\"\n-#include \"leveldb/db.h\"\n-#include \"leveldb/env.h\"\n-#include \"leveldb/status.h\"\n-#include \"leveldb/table.h\"\n-#include \"leveldb/table_builder.h\"\n-#include \"port/port.h\"\n-#include \"table/block.h\"\n-#include \"table/merger.h\"\n-#include \"table/two_level_iterator.h\"\n-#include \"util/coding.h\"\n-#include \"util/logging.h\"\n-#include \"util/mutexlock.h\"\n-\n-namespace leveldb {\n-\n-// Information kept for every waiting writer\n-struct DBImpl::Writer {\n-  Status status;\n-  WriteBatch* batch;\n-  bool sync;\n-  bool done;\n-  port::CondVar cv;\n-\n-  explicit Writer(port::Mutex* mu) : cv(mu) { }\n-};\n-\n-struct DBImpl::CompactionState {\n-  Compaction* const compaction;\n-\n-  // Sequence numbers < smallest_snapshot are not significant since we\n-  // will never have to service a snapshot below smallest_snapshot.\n-  // Therefore if we have seen a sequence number S <= smallest_snapshot,\n-  // we can drop all entries for the same key with sequence numbers < S.\n-  SequenceNumber smallest_snapshot;\n-\n-  // Files produced by compaction\n-  struct Output {\n-    uint64_t number;\n-    uint64_t file_size;\n-    InternalKey smallest, largest;\n-  };\n-  std::vector<Output> outputs;\n-\n-  // State kept for output being generated\n-  WritableFile* outfile;\n-  TableBuilder* builder;\n-\n-  uint64_t total_bytes;\n-\n-  Output* current_output() { return &outputs[outputs.size()-1]; }\n-\n-  explicit CompactionState(Compaction* c)\n-      : compaction(c),\n-        outfile(NULL),\n-        builder(NULL),\n-        total_bytes(0) {\n-  }\n-};\n-\n-// Fix user-supplied options to be reasonable\n-template <class T,class V>\n-static void ClipToRange(T* ptr, V minvalue, V maxvalue) {\n-  if (static_cast<V>(*ptr) > maxvalue) *ptr = maxvalue;\n-  if (static_cast<V>(*ptr) < minvalue) *ptr = minvalue;\n-}\n-Options SanitizeOptions(const std::string& dbname,\n-                        const InternalKeyComparator* icmp,\n-                        const InternalFilterPolicy* ipolicy,\n-                        const Options& src) {\n-  Options result = src;\n-  result.comparator = icmp;\n-  result.filter_policy = (src.filter_policy != NULL) ? ipolicy : NULL;\n-  ClipToRange(&result.max_open_files,            20,     50000);\n-  ClipToRange(&result.write_buffer_size,         64<<10, 1<<30);\n-  ClipToRange(&result.block_size,                1<<10,  4<<20);\n-  if (result.info_log == NULL) {\n-    // Open a log file in the same directory as the db\n-    src.env->CreateDir(dbname);  // In case it does not exist\n-    src.env->RenameFile(InfoLogFileName(dbname), OldInfoLogFileName(dbname));\n-    Status s = src.env->NewLogger(InfoLogFileName(dbname), &result.info_log);\n-    if (!s.ok()) {\n-      // No place suitable for logging\n-      result.info_log = NULL;\n-    }\n-  }\n-  if (result.block_cache == NULL) {\n-    result.block_cache = NewLRUCache(8 << 20);\n-  }\n-  return result;\n-}\n-\n-DBImpl::DBImpl(const Options& options, const std::string& dbname)\n-    : env_(options.env),\n-      internal_comparator_(options.comparator),\n-      internal_filter_policy_(options.filter_policy),\n-      options_(SanitizeOptions(\n-          dbname, &internal_comparator_, &internal_filter_policy_, options)),\n-      owns_info_log_(options_.info_log != options.info_log),\n-      owns_cache_(options_.block_cache != options.block_cache),\n-      dbname_(dbname),\n-      db_lock_(NULL),\n-      shutting_down_(NULL),\n-      bg_cv_(&mutex_),\n-      mem_(new MemTable(internal_comparator_)),\n-      imm_(NULL),\n-      logfile_(NULL),\n-      logfile_number_(0),\n-      log_(NULL),\n-      tmp_batch_(new WriteBatch),\n-      bg_compaction_scheduled_(false),\n-      manual_compaction_(NULL) {\n-  mem_->Ref();\n-  has_imm_.Release_Store(NULL);\n-\n-  // Reserve ten files or so for other uses and give the rest to TableCache.\n-  const int table_cache_size = options.max_open_files - 10;\n-  table_cache_ = new TableCache(dbname_, &options_, table_cache_size);\n-\n-  versions_ = new VersionSet(dbname_, &options_, table_cache_,\n-                             &internal_comparator_);\n-}\n-\n-DBImpl::~DBImpl() {\n-  // Wait for background work to finish\n-  mutex_.Lock();\n-  shutting_down_.Release_Store(this);  // Any non-NULL value is ok\n-  while (bg_compaction_scheduled_) {\n-    bg_cv_.Wait();\n-  }\n-  mutex_.Unlock();\n-\n-  if (db_lock_ != NULL) {\n-    env_->UnlockFile(db_lock_);\n-  }\n-\n-  delete versions_;\n-  if (mem_ != NULL) mem_->Unref();\n-  if (imm_ != NULL) imm_->Unref();\n-  delete tmp_batch_;\n-  delete log_;\n-  delete logfile_;\n-  delete table_cache_;\n-\n-  if (owns_info_log_) {\n-    delete options_.info_log;\n-  }\n-  if (owns_cache_) {\n-    delete options_.block_cache;\n-  }\n-}\n-\n-Status DBImpl::NewDB() {\n-  VersionEdit new_db;\n-  new_db.SetComparatorName(user_comparator()->Name());\n-  new_db.SetLogNumber(0);\n-  new_db.SetNextFile(2);\n-  new_db.SetLastSequence(0);\n-\n-  const std::string manifest = DescriptorFileName(dbname_, 1);\n-  WritableFile* file;\n-  Status s = env_->NewWritableFile(manifest, &file);\n-  if (!s.ok()) {\n-    return s;\n-  }\n-  {\n-    log::Writer log(file);\n-    std::string record;\n-    new_db.EncodeTo(&record);\n-    s = log.AddRecord(record);\n-    if (s.ok()) {\n-      s = file->Close();\n-    }\n-  }\n-  delete file;\n-  if (s.ok()) {\n-    // Make \"CURRENT\" file that points to the new manifest file.\n-    s = SetCurrentFile(env_, dbname_, 1);\n-  } else {\n-    env_->DeleteFile(manifest);\n-  }\n-  return s;\n-}\n-\n-void DBImpl::MaybeIgnoreError(Status* s) const {\n-  if (s->ok() || options_.paranoid_checks) {\n-    // No change needed\n-  } else {\n-    Log(options_.info_log, \"Ignoring error %s\", s->ToString().c_str());\n-    *s = Status::OK();\n-  }\n-}\n-\n-void DBImpl::DeleteObsoleteFiles() {\n-  // Make a set of all of the live files\n-  std::set<uint64_t> live = pending_outputs_;\n-  versions_->AddLiveFiles(&live);\n-\n-  std::vector<std::string> filenames;\n-  env_->GetChildren(dbname_, &filenames); // Ignoring errors on purpose\n-  uint64_t number;\n-  FileType type;\n-  for (size_t i = 0; i < filenames.size(); i++) {\n-    if (ParseFileName(filenames[i], &number, &type)) {\n-      bool keep = true;\n-      switch (type) {\n-        case kLogFile:\n-          keep = ((number >= versions_->LogNumber()) ||\n-                  (number == versions_->PrevLogNumber()));\n-          break;\n-        case kDescriptorFile:\n-          // Keep my manifest file, and any newer incarnations'\n-          // (in case there is a race that allows other incarnations)\n-          keep = (number >= versions_->ManifestFileNumber());\n-          break;\n-        case kTableFile:\n-          keep = (live.find(number) != live.end());\n-          break;\n-        case kTempFile:\n-          // Any temp files that are currently being written to must\n-          // be recorded in pending_outputs_, which is inserted into \"live\"\n-          keep = (live.find(number) != live.end());\n-          break;\n-        case kCurrentFile:\n-        case kDBLockFile:\n-        case kInfoLogFile:\n-          keep = true;\n-          break;\n-      }\n-\n-      if (!keep) {\n-        if (type == kTableFile) {\n-          table_cache_->Evict(number);\n-        }\n-        Log(options_.info_log, \"Delete type=%d #%lld\\n\",\n-            int(type),\n-            static_cast<unsigned long long>(number));\n-        env_->DeleteFile(dbname_ + \"/\" + filenames[i]);\n-      }\n-    }\n-  }\n-}\n-\n-Status DBImpl::Recover(VersionEdit* edit) {\n-  mutex_.AssertHeld();\n-\n-  // Ignore error from CreateDir since the creation of the DB is\n-  // committed only when the descriptor is created, and this directory\n-  // may already exist from a previous failed creation attempt.\n-  env_->CreateDir(dbname_);\n-  assert(db_lock_ == NULL);\n-  Status s = env_->LockFile(LockFileName(dbname_), &db_lock_);\n-  if (!s.ok()) {\n-    return s;\n-  }\n-\n-  if (!env_->FileExists(CurrentFileName(dbname_))) {\n-    if (options_.create_if_missing) {\n-      s = NewDB();\n-      if (!s.ok()) {\n-        return s;\n-      }\n-    } else {\n-      return Status::InvalidArgument(\n-          dbname_, \"does not exist (create_if_missing is false)\");\n-    }\n-  } else {\n-    if (options_.error_if_exists) {\n-      return Status::InvalidArgument(\n-          dbname_, \"exists (error_if_exists is true)\");\n-    }\n-  }\n-\n-  s = versions_->Recover();\n-  if (s.ok()) {\n-    SequenceNumber max_sequence(0);\n-\n-    // Recover from all newer log files than the ones named in the\n-    // descriptor (new log files may have been added by the previous\n-    // incarnation without registering them in the descriptor).\n-    //\n-    // Note that PrevLogNumber() is no longer used, but we pay\n-    // attention to it in case we are recovering a database\n-    // produced by an older version of leveldb.\n-    const uint64_t min_log = versions_->LogNumber();\n-    const uint64_t prev_log = versions_->PrevLogNumber();\n-    std::vector<std::string> filenames;\n-    s = env_->GetChildren(dbname_, &filenames);\n-    if (!s.ok()) {\n-      return s;\n-    }\n-    uint64_t number;\n-    FileType type;\n-    std::vector<uint64_t> logs;\n-    for (size_t i = 0; i < filenames.size(); i++) {\n-      if (ParseFileName(filenames[i], &number, &type)\n-          && type == kLogFile\n-          && ((number >= min_log) || (number == prev_log))) {\n-        logs.push_back(number);\n-      }\n-    }\n-\n-    // Recover in the order in which the logs were generated\n-    std::sort(logs.begin(), logs.end());\n-    for (size_t i = 0; i < logs.size(); i++) {\n-      s = RecoverLogFile(logs[i], edit, &max_sequence);\n-\n-      // The previous incarnation may not have written any MANIFEST\n-      // records after allocating this log number.  So we manually\n-      // update the file number allocation counter in VersionSet.\n-      versions_->MarkFileNumberUsed(logs[i]);\n-    }\n-\n-    if (s.ok()) {\n-      if (versions_->LastSequence() < max_sequence) {\n-        versions_->SetLastSequence(max_sequence);\n-      }\n-    }\n-  }\n-\n-  return s;\n-}\n-\n-Status DBImpl::RecoverLogFile(uint64_t log_number,\n-                              VersionEdit* edit,\n-                              SequenceNumber* max_sequence) {\n-  struct LogReporter : public log::Reader::Reporter {\n-    Env* env;\n-    Logger* info_log;\n-    const char* fname;\n-    Status* status;  // NULL if options_.paranoid_checks==false\n-    virtual void Corruption(size_t bytes, const Status& s) {\n-      Log(info_log, \"%s%s: dropping %d bytes; %s\",\n-          (this->status == NULL ? \"(ignoring error) \" : \"\"),\n-          fname, static_cast<int>(bytes), s.ToString().c_str());\n-      if (this->status != NULL && this->status->ok()) *this->status = s;\n-    }\n-  };\n-\n-  mutex_.AssertHeld();\n-\n-  // Open the log file\n-  std::string fname = LogFileName(dbname_, log_number);\n-  SequentialFile* file;\n-  Status status = env_->NewSequentialFile(fname, &file);\n-  if (!status.ok()) {\n-    MaybeIgnoreError(&status);\n-    return status;\n-  }\n-\n-  // Create the log reader.\n-  LogReporter reporter;\n-  reporter.env = env_;\n-  reporter.info_log = options_.info_log;\n-  reporter.fname = fname.c_str();\n-  reporter.status = (options_.paranoid_checks ? &status : NULL);\n-  // We intentially make log::Reader do checksumming even if\n-  // paranoid_checks==false so that corruptions cause entire commits\n-  // to be skipped instead of propagating bad information (like overly\n-  // large sequence numbers).\n-  log::Reader reader(file, &reporter, true/*checksum*/,\n-                     0/*initial_offset*/);\n-  Log(options_.info_log, \"Recovering log #%llu\",\n-      (unsigned long long) log_number);\n-\n-  // Read all the records and add to a memtable\n-  std::string scratch;\n-  Slice record;\n-  WriteBatch batch;\n-  MemTable* mem = NULL;\n-  while (reader.ReadRecord(&record, &scratch) &&\n-         status.ok()) {\n-    if (record.size() < 12) {\n-      reporter.Corruption(\n-          record.size(), Status::Corruption(\"log record too small\"));\n-      continue;\n-    }\n-    WriteBatchInternal::SetContents(&batch, record);\n-\n-    if (mem == NULL) {\n-      mem = new MemTable(internal_comparator_);\n-      mem->Ref();\n-    }\n-    status = WriteBatchInternal::InsertInto(&batch, mem);\n-    MaybeIgnoreError(&status);\n-    if (!status.ok()) {\n-      break;\n-    }\n-    const SequenceNumber last_seq =\n-        WriteBatchInternal::Sequence(&batch) +\n-        WriteBatchInternal::Count(&batch) - 1;\n-    if (last_seq > *max_sequence) {\n-      *max_sequence = last_seq;\n-    }\n-\n-    if (mem->ApproximateMemoryUsage() > options_.write_buffer_size) {\n-      status = WriteLevel0Table(mem, edit, NULL);\n-      if (!status.ok()) {\n-        // Reflect errors immediately so that conditions like full\n-        // file-systems cause the DB::Open() to fail.\n-        break;\n-      }\n-      mem->Unref();\n-      mem = NULL;\n-    }\n-  }\n-\n-  if (status.ok() && mem != NULL) {\n-    status = WriteLevel0Table(mem, edit, NULL);\n-    // Reflect errors immediately so that conditions like full\n-    // file-systems cause the DB::Open() to fail.\n-  }\n-\n-  if (mem != NULL) mem->Unref();\n-  delete file;\n-  return status;\n-}\n-\n-Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit,\n-                                Version* base) {\n-  mutex_.AssertHeld();\n-  const uint64_t start_micros = env_->NowMicros();\n-  FileMetaData meta;\n-  meta.number = versions_->NewFileNumber();\n-  pending_outputs_.insert(meta.number);\n-  Iterator* iter = mem->NewIterator();\n-  Log(options_.info_log, \"Level-0 table #%llu: started\",\n-      (unsigned long long) meta.number);\n-\n-  Status s;\n-  {\n-    mutex_.Unlock();\n-    s = BuildTable(dbname_, env_, options_, table_cache_, iter, &meta);\n-    mutex_.Lock();\n-  }\n-\n-  Log(options_.info_log, \"Level-0 table #%llu: %lld bytes %s\",\n-      (unsigned long long) meta.number,\n-      (unsigned long long) meta.file_size,\n-      s.ToString().c_str());\n-  delete iter;\n-  pending_outputs_.erase(meta.number);\n-\n-\n-  // Note that if file_size is zero, the file has been deleted and\n-  // should not be added to the manifest.\n-  int level = 0;\n-  if (s.ok() && meta.file_size > 0) {\n-    const Slice min_user_key = meta.smallest.user_key();\n-    const Slice max_user_key = meta.largest.user_key();\n-    if (base != NULL) {\n-      level = base->PickLevelForMemTableOutput(min_user_key, max_user_key);\n-    }\n-    edit->AddFile(level, meta.number, meta.file_size,\n-                  meta.smallest, meta.largest);\n-  }\n-\n-  CompactionStats stats;\n-  stats.micros = env_->NowMicros() - start_micros;\n-  stats.bytes_written = meta.file_size;\n-  stats_[level].Add(stats);\n-  return s;\n-}\n-\n-Status DBImpl::CompactMemTable() {\n-  mutex_.AssertHeld();\n-  assert(imm_ != NULL);\n-\n-  // Save the contents of the memtable as a new Table\n-  VersionEdit edit;\n-  Version* base = versions_->current();\n-  base->Ref();\n-  Status s = WriteLevel0Table(imm_, &edit, base);\n-  base->Unref();\n-\n-  if (s.ok() && shutting_down_.Acquire_Load()) {\n-    s = Status::IOError(\"Deleting DB during memtable compaction\");\n-  }\n-\n-  // Replace immutable memtable with the generated Table\n-  if (s.ok()) {\n-    edit.SetPrevLogNumber(0);\n-    edit.SetLogNumber(logfile_number_);  // Earlier logs no longer needed\n-    s = versions_->LogAndApply(&edit, &mutex_);\n-  }\n-\n-  if (s.ok()) {\n-    // Commit to the new state\n-    imm_->Unref();\n-    imm_ = NULL;\n-    has_imm_.Release_Store(NULL);\n-    DeleteObsoleteFiles();\n-  }\n-\n-  return s;\n-}\n-\n-void DBImpl::CompactRange(const Slice* begin, const Slice* end) {\n-  int max_level_with_files = 1;\n-  {\n-    MutexLock l(&mutex_);\n-    Version* base = versions_->current();\n-    for (int level = 1; level < config::kNumLevels; level++) {\n-      if (base->OverlapInLevel(level, begin, end)) {\n-        max_level_with_files = level;\n-      }\n-    }\n-  }\n-  TEST_CompactMemTable(); // TODO(sanjay): Skip if memtable does not overlap\n-  for (int level = 0; level < max_level_with_files; level++) {\n-    TEST_CompactRange(level, begin, end);\n-  }\n-}\n-\n-void DBImpl::TEST_CompactRange(int level, const Slice* begin,const Slice* end) {\n-  assert(level >= 0);\n-  assert(level + 1 < config::kNumLevels);\n-\n-  InternalKey begin_storage, end_storage;\n-\n-  ManualCompaction manual;\n-  manual.level = level;\n-  manual.done = false;\n-  if (begin == NULL) {\n-    manual.begin = NULL;\n-  } else {\n-    begin_storage = InternalKey(*begin, kMaxSequenceNumber, kValueTypeForSeek);\n-    manual.begin = &begin_storage;\n-  }\n-  if (end == NULL) {\n-    manual.end = NULL;\n-  } else {\n-    end_storage = InternalKey(*end, 0, static_cast<ValueType>(0));\n-    manual.end = &end_storage;\n-  }\n-\n-  MutexLock l(&mutex_);\n-  while (!manual.done) {\n-    while (manual_compaction_ != NULL) {\n-      bg_cv_.Wait();\n-    }\n-    manual_compaction_ = &manual;\n-    MaybeScheduleCompaction();\n-    while (manual_compaction_ == &manual) {\n-      bg_cv_.Wait();\n-    }\n-  }\n-}\n-\n-Status DBImpl::TEST_CompactMemTable() {\n-  // NULL batch means just wait for earlier writes to be done\n-  Status s = Write(WriteOptions(), NULL);\n-  if (s.ok()) {\n-    // Wait until the compaction completes\n-    MutexLock l(&mutex_);\n-    while (imm_ != NULL && bg_error_.ok()) {\n-      bg_cv_.Wait();\n-    }\n-    if (imm_ != NULL) {\n-      s = bg_error_;\n-    }\n-  }\n-  return s;\n-}\n-\n-void DBImpl::MaybeScheduleCompaction() {\n-  mutex_.AssertHeld();\n-  if (bg_compaction_scheduled_) {\n-    // Already scheduled\n-  } else if (shutting_down_.Acquire_Load()) {\n-    // DB is being deleted; no more background compactions\n-  } else if (imm_ == NULL &&\n-             manual_compaction_ == NULL &&\n-             !versions_->NeedsCompaction()) {\n-    // No work to be done\n-  } else {\n-    bg_compaction_scheduled_ = true;\n-    env_->Schedule(&DBImpl::BGWork, this);\n-  }\n-}\n-\n-void DBImpl::BGWork(void* db) {\n-  reinterpret_cast<DBImpl*>(db)->BackgroundCall();\n-}\n-\n-void DBImpl::BackgroundCall() {\n-  MutexLock l(&mutex_);\n-  assert(bg_compaction_scheduled_);\n-  if (!shutting_down_.Acquire_Load()) {\n-    Status s = BackgroundCompaction();\n-    if (s.ok()) {\n-      // Success\n-    } else if (shutting_down_.Acquire_Load()) {\n-      // Error most likely due to shutdown; do not wait\n-    } else {\n-      // Wait a little bit before retrying background compaction in\n-      // case this is an environmental problem and we do not want to\n-      // chew up resources for failed compactions for the duration of\n-      // the problem.\n-      bg_cv_.SignalAll();  // In case a waiter can proceed despite the error\n-      Log(options_.info_log, \"Waiting after background compaction error: %s\",\n-          s.ToString().c_str());\n-      mutex_.Unlock();\n-      env_->SleepForMicroseconds(1000000);\n-      mutex_.Lock();\n-    }\n-  }\n-\n-  bg_compaction_scheduled_ = false;\n-\n-  // Previous compaction may have produced too many files in a level,\n-  // so reschedule another compaction if needed.\n-  MaybeScheduleCompaction();\n-  bg_cv_.SignalAll();\n-}\n-\n-Status DBImpl::BackgroundCompaction() {\n-  mutex_.AssertHeld();\n-\n-  if (imm_ != NULL) {\n-    return CompactMemTable();\n-  }\n-\n-  Compaction* c;\n-  bool is_manual = (manual_compaction_ != NULL);\n-  InternalKey manual_end;\n-  if (is_manual) {\n-    ManualCompaction* m = manual_compaction_;\n-    c = versions_->CompactRange(m->level, m->begin, m->end);\n-    m->done = (c == NULL);\n-    if (c != NULL) {\n-      manual_end = c->input(0, c->num_input_files(0) - 1)->largest;\n-    }\n-    Log(options_.info_log,\n-        \"Manual compaction at level-%d from %s .. %s; will stop at %s\\n\",\n-        m->level,\n-        (m->begin ? m->begin->DebugString().c_str() : \"(begin)\"),\n-        (m->end ? m->end->DebugString().c_str() : \"(end)\"),\n-        (m->done ? \"(end)\" : manual_end.DebugString().c_str()));\n-  } else {\n-    c = versions_->PickCompaction();\n-  }\n-\n-  Status status;\n-  if (c == NULL) {\n-    // Nothing to do\n-  } else if (!is_manual && c->IsTrivialMove()) {\n-    // Move file to next level\n-    assert(c->num_input_files(0) == 1);\n-    FileMetaData* f = c->input(0, 0);\n-    c->edit()->DeleteFile(c->level(), f->number);\n-    c->edit()->AddFile(c->level() + 1, f->number, f->file_size,\n-                       f->smallest, f->largest);\n-    status = versions_->LogAndApply(c->edit(), &mutex_);\n-    VersionSet::LevelSummaryStorage tmp;\n-    Log(options_.info_log, \"Moved #%lld to level-%d %lld bytes %s: %s\\n\",\n-        static_cast<unsigned long long>(f->number),\n-        c->level() + 1,\n-        static_cast<unsigned long long>(f->file_size),\n-        status.ToString().c_str(),\n-        versions_->LevelSummary(&tmp));\n-  } else {\n-    CompactionState* compact = new CompactionState(c);\n-    status = DoCompactionWork(compact);\n-    CleanupCompaction(compact);\n-    c->ReleaseInputs();\n-    DeleteObsoleteFiles();\n-  }\n-  delete c;\n-\n-  if (status.ok()) {\n-    // Done\n-  } else if (shutting_down_.Acquire_Load()) {\n-    // Ignore compaction errors found during shutting down\n-  } else {\n-    Log(options_.info_log,\n-        \"Compaction error: %s\", status.ToString().c_str());\n-    if (options_.paranoid_checks && bg_error_.ok()) {\n-      bg_error_ = status;\n-    }\n-  }\n-\n-  if (is_manual) {\n-    ManualCompaction* m = manual_compaction_;\n-    if (!status.ok()) {\n-      m->done = true;\n-    }\n-    if (!m->done) {\n-      // We only compacted part of the requested range.  Update *m\n-      // to the range that is left to be compacted.\n-      m->tmp_storage = manual_end;\n-      m->begin = &m->tmp_storage;\n-    }\n-    manual_compaction_ = NULL;\n-  }\n-  return status;\n-}\n-\n-void DBImpl::CleanupCompaction(CompactionState* compact) {\n-  mutex_.AssertHeld();\n-  if (compact->builder != NULL) {\n-    // May happen if we get a shutdown call in the middle of compaction\n-    compact->builder->Abandon();\n-    delete compact->builder;\n-  } else {\n-    assert(compact->outfile == NULL);\n-  }\n-  delete compact->outfile;\n-  for (size_t i = 0; i < compact->outputs.size(); i++) {\n-    const CompactionState::Output& out = compact->outputs[i];\n-    pending_outputs_.erase(out.number);\n-  }\n-  delete compact;\n-}\n-\n-Status DBImpl::OpenCompactionOutputFile(CompactionState* compact) {\n-  assert(compact != NULL);\n-  assert(compact->builder == NULL);\n-  uint64_t file_number;\n-  {\n-    mutex_.Lock();\n-    file_number = versions_->NewFileNumber();\n-    pending_outputs_.insert(file_number);\n-    CompactionState::Output out;\n-    out.number = file_number;\n-    out.smallest.Clear();\n-    out.largest.Clear();\n-    compact->outputs.push_back(out);\n-    mutex_.Unlock();\n-  }\n-\n-  // Make the output file\n-  std::string fname = TableFileName(dbname_, file_number);\n-  Status s = env_->NewWritableFile(fname, &compact->outfile);\n-  if (s.ok()) {\n-    compact->builder = new TableBuilder(options_, compact->outfile);\n-  }\n-  return s;\n-}\n-\n-Status DBImpl::FinishCompactionOutputFile(CompactionState* compact,\n-                                          Iterator* input) {\n-  assert(compact != NULL);\n-  assert(compact->outfile != NULL);\n-  assert(compact->builder != NULL);\n-\n-  const uint64_t output_number = compact->current_output()->number;\n-  assert(output_number != 0);\n-\n-  // Check for iterator errors\n-  Status s = input->status();\n-  const uint64_t current_entries = compact->builder->NumEntries();\n-  if (s.ok()) {\n-    s = compact->builder->Finish();\n-  } else {\n-    compact->builder->Abandon();\n-  }\n-  const uint64_t current_bytes = compact->builder->FileSize();\n-  compact->current_output()->file_size = current_bytes;\n-  compact->total_bytes += current_bytes;\n-  delete compact->builder;\n-  compact->builder = NULL;\n-\n-  // Finish and check for file errors\n-  if (s.ok()) {\n-    s = compact->outfile->Sync();\n-  }\n-  if (s.ok()) {\n-    s = compact->outfile->Close();\n-  }\n-  delete compact->outfile;\n-  compact->outfile = NULL;\n-\n-  if (s.ok() && current_entries > 0) {\n-    // Verify that the table is usable\n-    Iterator* iter = table_cache_->NewIterator(ReadOptions(),\n-                                               output_number,\n-                                               current_bytes);\n-    s = iter->status();\n-    delete iter;\n-    if (s.ok()) {\n-      Log(options_.info_log,\n-          \"Generated table #%llu: %lld keys, %lld bytes\",\n-          (unsigned long long) output_number,\n-          (unsigned long long) current_entries,\n-          (unsigned long long) current_bytes);\n-    }\n-  }\n-  return s;\n-}\n-\n-\n-Status DBImpl::InstallCompactionResults(CompactionState* compact) {\n-  mutex_.AssertHeld();\n-  Log(options_.info_log,  \"Compacted %d@%d + %d@%d files => %lld bytes\",\n-      compact->compaction->num_input_files(0),\n-      compact->compaction->level(),\n-      compact->compaction->num_input_files(1),\n-      compact->compaction->level() + 1,\n-      static_cast<long long>(compact->total_bytes));\n-\n-  // Add compaction outputs\n-  compact->compaction->AddInputDeletions(compact->compaction->edit());\n-  const int level = compact->compaction->level();\n-  for (size_t i = 0; i < compact->outputs.size(); i++) {\n-    const CompactionState::Output& out = compact->outputs[i];\n-    compact->compaction->edit()->AddFile(\n-        level + 1,\n-        out.number, out.file_size, out.smallest, out.largest);\n-  }\n-  return versions_->LogAndApply(compact->compaction->edit(), &mutex_);\n-}\n-\n-Status DBImpl::DoCompactionWork(CompactionState* compact) {\n-  const uint64_t start_micros = env_->NowMicros();\n-  int64_t imm_micros = 0;  // Micros spent doing imm_ compactions\n-\n-  Log(options_.info_log,  \"Compacting %d@%d + %d@%d files\",\n-      compact->compaction->num_input_files(0),\n-      compact->compaction->level(),\n-      compact->compaction->num_input_files(1),\n-      compact->compaction->level() + 1);\n-\n-  assert(versions_->NumLevelFiles(compact->compaction->level()) > 0);\n-  assert(compact->builder == NULL);\n-  assert(compact->outfile == NULL);\n-  if (snapshots_.empty()) {\n-    compact->smallest_snapshot = versions_->LastSequence();\n-  } else {\n-    compact->smallest_snapshot = snapshots_.oldest()->number_;\n-  }\n-\n-  // Release mutex while we're actually doing the compaction work\n-  mutex_.Unlock();\n-\n-  Iterator* input = versions_->MakeInputIterator(compact->compaction);\n-  input->SeekToFirst();\n-  Status status;\n-  ParsedInternalKey ikey;\n-  std::string current_user_key;\n-  bool has_current_user_key = false;\n-  SequenceNumber last_sequence_for_key = kMaxSequenceNumber;\n-  for (; input->Valid() && !shutting_down_.Acquire_Load(); ) {\n-    // Prioritize immutable compaction work\n-    if (has_imm_.NoBarrier_Load() != NULL) {\n-      const uint64_t imm_start = env_->NowMicros();\n-      mutex_.Lock();\n-      if (imm_ != NULL) {\n-        CompactMemTable();\n-        bg_cv_.SignalAll();  // Wakeup MakeRoomForWrite() if necessary\n-      }\n-      mutex_.Unlock();\n-      imm_micros += (env_->NowMicros() - imm_start);\n-    }\n-\n-    Slice key = input->key();\n-    if (compact->compaction->ShouldStopBefore(key) &&\n-        compact->builder != NULL) {\n-      status = FinishCompactionOutputFile(compact, input);\n-      if (!status.ok()) {\n-        break;\n-      }\n-    }\n-\n-    // Handle key/value, add to state, etc.\n-    bool drop = false;\n-    if (!ParseInternalKey(key, &ikey)) {\n-      // Do not hide error keys\n-      current_user_key.clear();\n-      has_current_user_key = false;\n-      last_sequence_for_key = kMaxSequenceNumber;\n-    } else {\n-      if (!has_current_user_key ||\n-          user_comparator()->Compare(ikey.user_key,\n-                                     Slice(current_user_key)) != 0) {\n-        // First occurrence of this user key\n-        current_user_key.assign(ikey.user_key.data(), ikey.user_key.size());\n-        has_current_user_key = true;\n-        last_sequence_for_key = kMaxSequenceNumber;\n-      }\n-\n-      if (last_sequence_for_key <= compact->smallest_snapshot) {\n-        // Hidden by an newer entry for same user key\n-        drop = true;    // (A)\n-      } else if (ikey.type == kTypeDeletion &&\n-                 ikey.sequence <= compact->smallest_snapshot &&\n-                 compact->compaction->IsBaseLevelForKey(ikey.user_key)) {\n-        // For this user key:\n-        // (1) there is no data in higher levels\n-        // (2) data in lower levels will have larger sequence numbers\n-        // (3) data in layers that are being compacted here and have\n-        //     smaller sequence numbers will be dropped in the next\n-        //     few iterations of this loop (by rule (A) above).\n-        // Therefore this deletion marker is obsolete and can be dropped.\n-        drop = true;\n-      }\n-\n-      last_sequence_for_key = ikey.sequence;\n-    }\n-#if 0\n-    Log(options_.info_log,\n-        \"  Compact: %s, seq %d, type: %d %d, drop: %d, is_base: %d, \"\n-        \"%d smallest_snapshot: %d\",\n-        ikey.user_key.ToString().c_str(),\n-        (int)ikey.sequence, ikey.type, kTypeValue, drop,\n-        compact->compaction->IsBaseLevelForKey(ikey.user_key),\n-        (int)last_sequence_for_key, (int)compact->smallest_snapshot);\n-#endif\n-\n-    if (!drop) {\n-      // Open output file if necessary\n-      if (compact->builder == NULL) {\n-        status = OpenCompactionOutputFile(compact);\n-        if (!status.ok()) {\n-          break;\n-        }\n-      }\n-      if (compact->builder->NumEntries() == 0) {\n-        compact->current_output()->smallest.DecodeFrom(key);\n-      }\n-      compact->current_output()->largest.DecodeFrom(key);\n-      compact->builder->Add(key, input->value());\n-\n-      // Close output file if it is big enough\n-      if (compact->builder->FileSize() >=\n-          compact->compaction->MaxOutputFileSize()) {\n-        status = FinishCompactionOutputFile(compact, input);\n-        if (!status.ok()) {\n-          break;\n-        }\n-      }\n-    }\n-\n-    input->Next();\n-  }\n-\n-  if (status.ok() && shutting_down_.Acquire_Load()) {\n-    status = Status::IOError(\"Deleting DB during compaction\");\n-  }\n-  if (status.ok() && compact->builder != NULL) {\n-    status = FinishCompactionOutputFile(compact, input);\n-  }\n-  if (status.ok()) {\n-    status = input->status();\n-  }\n-  delete input;\n-  input = NULL;\n-\n-  CompactionStats stats;\n-  stats.micros = env_->NowMicros() - start_micros - imm_micros;\n-  for (int which = 0; which < 2; which++) {\n-    for (int i = 0; i < compact->compaction->num_input_files(which); i++) {\n-      stats.bytes_read += compact->compaction->input(which, i)->file_size;\n-    }\n-  }\n-  for (size_t i = 0; i < compact->outputs.size(); i++) {\n-    stats.bytes_written += compact->outputs[i].file_size;\n-  }\n-\n-  mutex_.Lock();\n-  stats_[compact->compaction->level() + 1].Add(stats);\n-\n-  if (status.ok()) {\n-    status = InstallCompactionResults(compact);\n-  }\n-  VersionSet::LevelSummaryStorage tmp;\n-  Log(options_.info_log,\n-      \"compacted to: %s\", versions_->LevelSummary(&tmp));\n-  return status;\n-}\n-\n-namespace {\n-struct IterState {\n-  port::Mutex* mu;\n-  Version* version;\n-  MemTable* mem;\n-  MemTable* imm;\n-};\n-\n-static void CleanupIteratorState(void* arg1, void* arg2) {\n-  IterState* state = reinterpret_cast<IterState*>(arg1);\n-  state->mu->Lock();\n-  state->mem->Unref();\n-  if (state->imm != NULL) state->imm->Unref();\n-  state->version->Unref();\n-  state->mu->Unlock();\n-  delete state;\n-}\n-}  // namespace\n-\n-Iterator* DBImpl::NewInternalIterator(const ReadOptions& options,\n-                                      SequenceNumber* latest_snapshot) {\n-  IterState* cleanup = new IterState;\n-  mutex_.Lock();\n-  *latest_snapshot = versions_->LastSequence();\n-\n-  // Collect together all needed child iterators\n-  std::vector<Iterator*> list;\n-  list.push_back(mem_->NewIterator());\n-  mem_->Ref();\n-  if (imm_ != NULL) {\n-    list.push_back(imm_->NewIterator());\n-    imm_->Ref();\n-  }\n-  versions_->current()->AddIterators(options, &list);\n-  Iterator* internal_iter =\n-      NewMergingIterator(&internal_comparator_, &list[0], list.size());\n-  versions_->current()->Ref();\n-\n-  cleanup->mu = &mutex_;\n-  cleanup->mem = mem_;\n-  cleanup->imm = imm_;\n-  cleanup->version = versions_->current();\n-  internal_iter->RegisterCleanup(CleanupIteratorState, cleanup, NULL);\n-\n-  mutex_.Unlock();\n-  return internal_iter;\n-}\n-\n-Iterator* DBImpl::TEST_NewInternalIterator() {\n-  SequenceNumber ignored;\n-  return NewInternalIterator(ReadOptions(), &ignored);\n-}\n-\n-int64_t DBImpl::TEST_MaxNextLevelOverlappingBytes() {\n-  MutexLock l(&mutex_);\n-  return versions_->MaxNextLevelOverlappingBytes();\n-}\n-\n-Status DBImpl::Get(const ReadOptions& options,\n-                   const Slice& key,\n-                   std::string* value) {\n-  Status s;\n-  MutexLock l(&mutex_);\n-  SequenceNumber snapshot;\n-  if (options.snapshot != NULL) {\n-    snapshot = reinterpret_cast<const SnapshotImpl*>(options.snapshot)->number_;\n-  } else {\n-    snapshot = versions_->LastSequence();\n-  }\n-\n-  MemTable* mem = mem_;\n-  MemTable* imm = imm_;\n-  Version* current = versions_->current();\n-  mem->Ref();\n-  if (imm != NULL) imm->Ref();\n-  current->Ref();\n-\n-  bool have_stat_update = false;\n-  Version::GetStats stats;\n-\n-  // Unlock while reading from files and memtables\n-  {\n-    mutex_.Unlock();\n-    // First look in the memtable, then in the immutable memtable (if any).\n-    LookupKey lkey(key, snapshot);\n-    if (mem->Get(lkey, value, &s)) {\n-      // Done\n-    } else if (imm != NULL && imm->Get(lkey, value, &s)) {\n-      // Done\n-    } else {\n-      s = current->Get(options, lkey, value, &stats);\n-      have_stat_update = true;\n-    }\n-    mutex_.Lock();\n-  }\n-\n-  if (have_stat_update && current->UpdateStats(stats)) {\n-    MaybeScheduleCompaction();\n-  }\n-  mem->Unref();\n-  if (imm != NULL) imm->Unref();\n-  current->Unref();\n-  return s;\n-}\n-\n-Iterator* DBImpl::NewIterator(const ReadOptions& options) {\n-  SequenceNumber latest_snapshot;\n-  Iterator* internal_iter = NewInternalIterator(options, &latest_snapshot);\n-  return NewDBIterator(\n-      &dbname_, env_, user_comparator(), internal_iter,\n-      (options.snapshot != NULL\n-       ? reinterpret_cast<const SnapshotImpl*>(options.snapshot)->number_\n-       : latest_snapshot));\n-}\n-\n-const Snapshot* DBImpl::GetSnapshot() {\n-  MutexLock l(&mutex_);\n-  return snapshots_.New(versions_->LastSequence());\n-}\n-\n-void DBImpl::ReleaseSnapshot(const Snapshot* s) {\n-  MutexLock l(&mutex_);\n-  snapshots_.Delete(reinterpret_cast<const SnapshotImpl*>(s));\n-}\n-\n-// Convenience methods\n-Status DBImpl::Put(const WriteOptions& o, const Slice& key, const Slice& val) {\n-  return DB::Put(o, key, val);\n-}\n-\n-Status DBImpl::Delete(const WriteOptions& options, const Slice& key) {\n-  return DB::Delete(options, key);\n-}\n-\n-Status DBImpl::Write(const WriteOptions& options, WriteBatch* my_batch) {\n-  Writer w(&mutex_);\n-  w.batch = my_batch;\n-  w.sync = options.sync;\n-  w.done = false;\n-\n-  MutexLock l(&mutex_);\n-  writers_.push_back(&w);\n-  while (!w.done && &w != writers_.front()) {\n-    w.cv.Wait();\n-  }\n-  if (w.done) {\n-    return w.status;\n-  }\n-\n-  // May temporarily unlock and wait.\n-  Status status = MakeRoomForWrite(my_batch == NULL);\n-  uint64_t last_sequence = versions_->LastSequence();\n-  Writer* last_writer = &w;\n-  if (status.ok() && my_batch != NULL) {  // NULL batch is for compactions\n-    WriteBatch* updates = BuildBatchGroup(&last_writer);\n-    WriteBatchInternal::SetSequence(updates, last_sequence + 1);\n-    last_sequence += WriteBatchInternal::Count(updates);\n-\n-    // Add to log and apply to memtable.  We can release the lock\n-    // during this phase since &w is currently responsible for logging\n-    // and protects against concurrent loggers and concurrent writes\n-    // into mem_.\n-    {\n-      mutex_.Unlock();\n-      status = log_->AddRecord(WriteBatchInternal::Contents(updates));\n-      if (status.ok() && options.sync) {\n-        status = logfile_->Sync();\n-      }\n-      if (status.ok()) {\n-        status = WriteBatchInternal::InsertInto(updates, mem_);\n-      }\n-      mutex_.Lock();\n-    }\n-    if (updates == tmp_batch_) tmp_batch_->Clear();\n-\n-    versions_->SetLastSequence(last_sequence);\n-  }\n-\n-  while (true) {\n-    Writer* ready = writers_.front();\n-    writers_.pop_front();\n-    if (ready != &w) {\n-      ready->status = status;\n-      ready->done = true;\n-      ready->cv.Signal();\n-    }\n-    if (ready == last_writer) break;\n-  }\n-\n-  // Notify new head of write queue\n-  if (!writers_.empty()) {\n-    writers_.front()->cv.Signal();\n-  }\n-\n-  return status;\n-}\n-\n-// REQUIRES: Writer list must be non-empty\n-// REQUIRES: First writer must have a non-NULL batch\n-WriteBatch* DBImpl::BuildBatchGroup(Writer** last_writer) {\n-  assert(!writers_.empty());\n-  Writer* first = writers_.front();\n-  WriteBatch* result = first->batch;\n-  assert(result != NULL);\n-\n-  size_t size = WriteBatchInternal::ByteSize(first->batch);\n-\n-  // Allow the group to grow up to a maximum size, but if the\n-  // original write is small, limit the growth so we do not slow\n-  // down the small write too much.\n-  size_t max_size = 1 << 20;\n-  if (size <= (128<<10)) {\n-    max_size = size + (128<<10);\n-  }\n-\n-  *last_writer = first;\n-  std::deque<Writer*>::iterator iter = writers_.begin();\n-  ++iter;  // Advance past \"first\"\n-  for (; iter != writers_.end(); ++iter) {\n-    Writer* w = *iter;\n-    if (w->sync && !first->sync) {\n-      // Do not include a sync write into a batch handled by a non-sync write.\n-      break;\n-    }\n-\n-    if (w->batch != NULL) {\n-      size += WriteBatchInternal::ByteSize(w->batch);\n-      if (size > max_size) {\n-        // Do not make batch too big\n-        break;\n-      }\n-\n-      // Append to *reuslt\n-      if (result == first->batch) {\n-        // Switch to temporary batch instead of disturbing caller's batch\n-        result = tmp_batch_;\n-        assert(WriteBatchInternal::Count(result) == 0);\n-        WriteBatchInternal::Append(result, first->batch);\n-      }\n-      WriteBatchInternal::Append(result, w->batch);\n-    }\n-    *last_writer = w;\n-  }\n-  return result;\n-}\n-\n-// REQUIRES: mutex_ is held\n-// REQUIRES: this thread is currently at the front of the writer queue\n-Status DBImpl::MakeRoomForWrite(bool force) {\n-  mutex_.AssertHeld();\n-  assert(!writers_.empty());\n-  bool allow_delay = !force;\n-  Status s;\n-  while (true) {\n-    if (!bg_error_.ok()) {\n-      // Yield previous error\n-      s = bg_error_;\n-      break;\n-    } else if (\n-        allow_delay &&\n-        versions_->NumLevelFiles(0) >= config::kL0_SlowdownWritesTrigger) {\n-      // We are getting close to hitting a hard limit on the number of\n-      // L0 files.  Rather than delaying a single write by several\n-      // seconds when we hit the hard limit, start delaying each\n-      // individual write by 1ms to reduce latency variance.  Also,\n-      // this delay hands over some CPU to the compaction thread in\n-      // case it is sharing the same core as the writer.\n-      mutex_.Unlock();\n-      env_->SleepForMicroseconds(1000);\n-      allow_delay = false;  // Do not delay a single write more than once\n-      mutex_.Lock();\n-    } else if (!force &&\n-               (mem_->ApproximateMemoryUsage() <= options_.write_buffer_size)) {\n-      // There is room in current memtable\n-      break;\n-    } else if (imm_ != NULL) {\n-      // We have filled up the current memtable, but the previous\n-      // one is still being compacted, so we wait.\n-      bg_cv_.Wait();\n-    } else if (versions_->NumLevelFiles(0) >= config::kL0_StopWritesTrigger) {\n-      // There are too many level-0 files.\n-      Log(options_.info_log, \"waiting...\\n\");\n-      bg_cv_.Wait();\n-    } else {\n-      // Attempt to switch to a new memtable and trigger compaction of old\n-      assert(versions_->PrevLogNumber() == 0);\n-      uint64_t new_log_number = versions_->NewFileNumber();\n-      WritableFile* lfile = NULL;\n-      s = env_->NewWritableFile(LogFileName(dbname_, new_log_number), &lfile);\n-      if (!s.ok()) {\n-        // Avoid chewing through file number space in a tight loop.\n-        versions_->ReuseFileNumber(new_log_number);\n-        break;\n-      }\n-      delete log_;\n-      delete logfile_;\n-      logfile_ = lfile;\n-      logfile_number_ = new_log_number;\n-      log_ = new log::Writer(lfile);\n-      imm_ = mem_;\n-      has_imm_.Release_Store(imm_);\n-      mem_ = new MemTable(internal_comparator_);\n-      mem_->Ref();\n-      force = false;   // Do not force another compaction if have room\n-      MaybeScheduleCompaction();\n-    }\n-  }\n-  return s;\n-}\n-\n-bool DBImpl::GetProperty(const Slice& property, std::string* value) {\n-  value->clear();\n-\n-  MutexLock l(&mutex_);\n-  Slice in = property;\n-  Slice prefix(\"leveldb.\");\n-  if (!in.starts_with(prefix)) return false;\n-  in.remove_prefix(prefix.size());\n-\n-  if (in.starts_with(\"num-files-at-level\")) {\n-    in.remove_prefix(strlen(\"num-files-at-level\"));\n-    uint64_t level;\n-    bool ok = ConsumeDecimalNumber(&in, &level) && in.empty();\n-    if (!ok || level >= config::kNumLevels) {\n-      return false;\n-    } else {\n-      char buf[100];\n-      snprintf(buf, sizeof(buf), \"%d\",\n-               versions_->NumLevelFiles(static_cast<int>(level)));\n-      *value = buf;\n-      return true;\n-    }\n-  } else if (in == \"stats\") {\n-    char buf[200];\n-    snprintf(buf, sizeof(buf),\n-             \"                               Compactions\\n\"\n-             \"Level  Files Size(MB) Time(sec) Read(MB) Write(MB)\\n\"\n-             \"--------------------------------------------------\\n\"\n-             );\n-    value->append(buf);\n-    for (int level = 0; level < config::kNumLevels; level++) {\n-      int files = versions_->NumLevelFiles(level);\n-      if (stats_[level].micros > 0 || files > 0) {\n-        snprintf(\n-            buf, sizeof(buf),\n-            \"%3d %8d %8.0f %9.0f %8.0f %9.0f\\n\",\n-            level,\n-            files,\n-            versions_->NumLevelBytes(level) / 1048576.0,\n-            stats_[level].micros / 1e6,\n-            stats_[level].bytes_read / 1048576.0,\n-            stats_[level].bytes_written / 1048576.0);\n-        value->append(buf);\n-      }\n-    }\n-    return true;\n-  } else if (in == \"sstables\") {\n-    *value = versions_->current()->DebugString();\n-    return true;\n-  }\n-\n-  return false;\n-}\n-\n-void DBImpl::GetApproximateSizes(\n-    const Range* range, int n,\n-    uint64_t* sizes) {\n-  // TODO(opt): better implementation\n-  Version* v;\n-  {\n-    MutexLock l(&mutex_);\n-    versions_->current()->Ref();\n-    v = versions_->current();\n-  }\n-\n-  for (int i = 0; i < n; i++) {\n-    // Convert user_key into a corresponding internal key.\n-    InternalKey k1(range[i].start, kMaxSequenceNumber, kValueTypeForSeek);\n-    InternalKey k2(range[i].limit, kMaxSequenceNumber, kValueTypeForSeek);\n-    uint64_t start = versions_->ApproximateOffsetOf(v, k1);\n-    uint64_t limit = versions_->ApproximateOffsetOf(v, k2);\n-    sizes[i] = (limit >= start ? limit - start : 0);\n-  }\n-\n-  {\n-    MutexLock l(&mutex_);\n-    v->Unref();\n-  }\n-}\n-\n-// Default implementations of convenience methods that subclasses of DB\n-// can call if they wish\n-Status DB::Put(const WriteOptions& opt, const Slice& key, const Slice& value) {\n-  WriteBatch batch;\n-  batch.Put(key, value);\n-  return Write(opt, &batch);\n-}\n-\n-Status DB::Delete(const WriteOptions& opt, const Slice& key) {\n-  WriteBatch batch;\n-  batch.Delete(key);\n-  return Write(opt, &batch);\n-}\n-\n-DB::~DB() { }\n-\n-Status DB::Open(const Options& options, const std::string& dbname,\n-                DB** dbptr) {\n-  *dbptr = NULL;\n-\n-  DBImpl* impl = new DBImpl(options, dbname);\n-  impl->mutex_.Lock();\n-  VersionEdit edit;\n-  Status s = impl->Recover(&edit); // Handles create_if_missing, error_if_exists\n-  if (s.ok()) {\n-    uint64_t new_log_number = impl->versions_->NewFileNumber();\n-    WritableFile* lfile;\n-    s = options.env->NewWritableFile(LogFileName(dbname, new_log_number),\n-                                     &lfile);\n-    if (s.ok()) {\n-      edit.SetLogNumber(new_log_number);\n-      impl->logfile_ = lfile;\n-      impl->logfile_number_ = new_log_number;\n-      impl->log_ = new log::Writer(lfile);\n-      s = impl->versions_->LogAndApply(&edit, &impl->mutex_);\n-    }\n-    if (s.ok()) {\n-      impl->DeleteObsoleteFiles();\n-      impl->MaybeScheduleCompaction();\n-    }\n-  }\n-  impl->mutex_.Unlock();\n-  if (s.ok()) {\n-    *dbptr = impl;\n-  } else {\n-    delete impl;\n-  }\n-  return s;\n-}\n-\n-Snapshot::~Snapshot() {\n-}\n-\n-Status DestroyDB(const std::string& dbname, const Options& options) {\n-  Env* env = options.env;\n-  std::vector<std::string> filenames;\n-  // Ignore error in case directory does not exist\n-  env->GetChildren(dbname, &filenames);\n-  if (filenames.empty()) {\n-    return Status::OK();\n-  }\n-\n-  FileLock* lock;\n-  const std::string lockname = LockFileName(dbname);\n-  Status result = env->LockFile(lockname, &lock);\n-  if (result.ok()) {\n-    uint64_t number;\n-    FileType type;\n-    for (size_t i = 0; i < filenames.size(); i++) {\n-      if (ParseFileName(filenames[i], &number, &type) &&\n-          type != kDBLockFile) {  // Lock file will be deleted at end\n-        Status del = env->DeleteFile(dbname + \"/\" + filenames[i]);\n-        if (result.ok() && !del.ok()) {\n-          result = del;\n-        }\n-      }\n-    }\n-    env->UnlockFile(lock);  // Ignore error since state is already gone\n-    env->DeleteFile(lockname);\n-    env->DeleteDir(dbname);  // Ignore error in case dir contains other files\n-  }\n-  return result;\n-}\n-\n-}  // namespace leveldb"
      },
      {
        "sha": "bd29dd80554be1358f73039e293fd44d1e437827",
        "filename": "src/leveldb/db/db_impl.h",
        "status": "removed",
        "additions": 0,
        "deletions": 202,
        "changes": 202,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/db_impl.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/db_impl.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_impl.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,202 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_DB_DB_IMPL_H_\n-#define STORAGE_LEVELDB_DB_DB_IMPL_H_\n-\n-#include <deque>\n-#include <set>\n-#include \"db/dbformat.h\"\n-#include \"db/log_writer.h\"\n-#include \"db/snapshot.h\"\n-#include \"leveldb/db.h\"\n-#include \"leveldb/env.h\"\n-#include \"port/port.h\"\n-#include \"port/thread_annotations.h\"\n-\n-namespace leveldb {\n-\n-class MemTable;\n-class TableCache;\n-class Version;\n-class VersionEdit;\n-class VersionSet;\n-\n-class DBImpl : public DB {\n- public:\n-  DBImpl(const Options& options, const std::string& dbname);\n-  virtual ~DBImpl();\n-\n-  // Implementations of the DB interface\n-  virtual Status Put(const WriteOptions&, const Slice& key, const Slice& value);\n-  virtual Status Delete(const WriteOptions&, const Slice& key);\n-  virtual Status Write(const WriteOptions& options, WriteBatch* updates);\n-  virtual Status Get(const ReadOptions& options,\n-                     const Slice& key,\n-                     std::string* value);\n-  virtual Iterator* NewIterator(const ReadOptions&);\n-  virtual const Snapshot* GetSnapshot();\n-  virtual void ReleaseSnapshot(const Snapshot* snapshot);\n-  virtual bool GetProperty(const Slice& property, std::string* value);\n-  virtual void GetApproximateSizes(const Range* range, int n, uint64_t* sizes);\n-  virtual void CompactRange(const Slice* begin, const Slice* end);\n-\n-  // Extra methods (for testing) that are not in the public DB interface\n-\n-  // Compact any files in the named level that overlap [*begin,*end]\n-  void TEST_CompactRange(int level, const Slice* begin, const Slice* end);\n-\n-  // Force current memtable contents to be compacted.\n-  Status TEST_CompactMemTable();\n-\n-  // Return an internal iterator over the current state of the database.\n-  // The keys of this iterator are internal keys (see format.h).\n-  // The returned iterator should be deleted when no longer needed.\n-  Iterator* TEST_NewInternalIterator();\n-\n-  // Return the maximum overlapping data (in bytes) at next level for any\n-  // file at a level >= 1.\n-  int64_t TEST_MaxNextLevelOverlappingBytes();\n-\n- private:\n-  friend class DB;\n-  struct CompactionState;\n-  struct Writer;\n-\n-  Iterator* NewInternalIterator(const ReadOptions&,\n-                                SequenceNumber* latest_snapshot);\n-\n-  Status NewDB();\n-\n-  // Recover the descriptor from persistent storage.  May do a significant\n-  // amount of work to recover recently logged updates.  Any changes to\n-  // be made to the descriptor are added to *edit.\n-  Status Recover(VersionEdit* edit) EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n-\n-  void MaybeIgnoreError(Status* s) const;\n-\n-  // Delete any unneeded files and stale in-memory entries.\n-  void DeleteObsoleteFiles();\n-\n-  // Compact the in-memory write buffer to disk.  Switches to a new\n-  // log-file/memtable and writes a new descriptor iff successful.\n-  Status CompactMemTable()\n-      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n-\n-  Status RecoverLogFile(uint64_t log_number,\n-                        VersionEdit* edit,\n-                        SequenceNumber* max_sequence)\n-      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n-\n-  Status WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base)\n-      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n-\n-  Status MakeRoomForWrite(bool force /* compact even if there is room? */)\n-      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n-  WriteBatch* BuildBatchGroup(Writer** last_writer);\n-\n-  void MaybeScheduleCompaction() EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n-  static void BGWork(void* db);\n-  void BackgroundCall();\n-  Status BackgroundCompaction() EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n-  void CleanupCompaction(CompactionState* compact)\n-      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n-  Status DoCompactionWork(CompactionState* compact)\n-      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n-\n-  Status OpenCompactionOutputFile(CompactionState* compact);\n-  Status FinishCompactionOutputFile(CompactionState* compact, Iterator* input);\n-  Status InstallCompactionResults(CompactionState* compact)\n-      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n-\n-  // Constant after construction\n-  Env* const env_;\n-  const InternalKeyComparator internal_comparator_;\n-  const InternalFilterPolicy internal_filter_policy_;\n-  const Options options_;  // options_.comparator == &internal_comparator_\n-  bool owns_info_log_;\n-  bool owns_cache_;\n-  const std::string dbname_;\n-\n-  // table_cache_ provides its own synchronization\n-  TableCache* table_cache_;\n-\n-  // Lock over the persistent DB state.  Non-NULL iff successfully acquired.\n-  FileLock* db_lock_;\n-\n-  // State below is protected by mutex_\n-  port::Mutex mutex_;\n-  port::AtomicPointer shutting_down_;\n-  port::CondVar bg_cv_;          // Signalled when background work finishes\n-  MemTable* mem_;\n-  MemTable* imm_;                // Memtable being compacted\n-  port::AtomicPointer has_imm_;  // So bg thread can detect non-NULL imm_\n-  WritableFile* logfile_;\n-  uint64_t logfile_number_;\n-  log::Writer* log_;\n-\n-  // Queue of writers.\n-  std::deque<Writer*> writers_;\n-  WriteBatch* tmp_batch_;\n-\n-  SnapshotList snapshots_;\n-\n-  // Set of table files to protect from deletion because they are\n-  // part of ongoing compactions.\n-  std::set<uint64_t> pending_outputs_;\n-\n-  // Has a background compaction been scheduled or is running?\n-  bool bg_compaction_scheduled_;\n-\n-  // Information for a manual compaction\n-  struct ManualCompaction {\n-    int level;\n-    bool done;\n-    const InternalKey* begin;   // NULL means beginning of key range\n-    const InternalKey* end;     // NULL means end of key range\n-    InternalKey tmp_storage;    // Used to keep track of compaction progress\n-  };\n-  ManualCompaction* manual_compaction_;\n-\n-  VersionSet* versions_;\n-\n-  // Have we encountered a background error in paranoid mode?\n-  Status bg_error_;\n-\n-  // Per level compaction stats.  stats_[level] stores the stats for\n-  // compactions that produced data for the specified \"level\".\n-  struct CompactionStats {\n-    int64_t micros;\n-    int64_t bytes_read;\n-    int64_t bytes_written;\n-\n-    CompactionStats() : micros(0), bytes_read(0), bytes_written(0) { }\n-\n-    void Add(const CompactionStats& c) {\n-      this->micros += c.micros;\n-      this->bytes_read += c.bytes_read;\n-      this->bytes_written += c.bytes_written;\n-    }\n-  };\n-  CompactionStats stats_[config::kNumLevels];\n-\n-  // No copying allowed\n-  DBImpl(const DBImpl&);\n-  void operator=(const DBImpl&);\n-\n-  const Comparator* user_comparator() const {\n-    return internal_comparator_.user_comparator();\n-  }\n-};\n-\n-// Sanitize db options.  The caller should delete result.info_log if\n-// it is not equal to src.info_log.\n-extern Options SanitizeOptions(const std::string& db,\n-                               const InternalKeyComparator* icmp,\n-                               const InternalFilterPolicy* ipolicy,\n-                               const Options& src);\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_DB_DB_IMPL_H_"
      },
      {
        "sha": "87dca2ded46453dec5e04cef652d2545086af485",
        "filename": "src/leveldb/db/db_iter.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 299,
        "changes": 299,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/db_iter.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/db_iter.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_iter.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,299 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/db_iter.h\"\n-\n-#include \"db/filename.h\"\n-#include \"db/dbformat.h\"\n-#include \"leveldb/env.h\"\n-#include \"leveldb/iterator.h\"\n-#include \"port/port.h\"\n-#include \"util/logging.h\"\n-#include \"util/mutexlock.h\"\n-\n-namespace leveldb {\n-\n-#if 0\n-static void DumpInternalIter(Iterator* iter) {\n-  for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n-    ParsedInternalKey k;\n-    if (!ParseInternalKey(iter->key(), &k)) {\n-      fprintf(stderr, \"Corrupt '%s'\\n\", EscapeString(iter->key()).c_str());\n-    } else {\n-      fprintf(stderr, \"@ '%s'\\n\", k.DebugString().c_str());\n-    }\n-  }\n-}\n-#endif\n-\n-namespace {\n-\n-// Memtables and sstables that make the DB representation contain\n-// (userkey,seq,type) => uservalue entries.  DBIter\n-// combines multiple entries for the same userkey found in the DB\n-// representation into a single entry while accounting for sequence\n-// numbers, deletion markers, overwrites, etc.\n-class DBIter: public Iterator {\n- public:\n-  // Which direction is the iterator currently moving?\n-  // (1) When moving forward, the internal iterator is positioned at\n-  //     the exact entry that yields this->key(), this->value()\n-  // (2) When moving backwards, the internal iterator is positioned\n-  //     just before all entries whose user key == this->key().\n-  enum Direction {\n-    kForward,\n-    kReverse\n-  };\n-\n-  DBIter(const std::string* dbname, Env* env,\n-         const Comparator* cmp, Iterator* iter, SequenceNumber s)\n-      : dbname_(dbname),\n-        env_(env),\n-        user_comparator_(cmp),\n-        iter_(iter),\n-        sequence_(s),\n-        direction_(kForward),\n-        valid_(false) {\n-  }\n-  virtual ~DBIter() {\n-    delete iter_;\n-  }\n-  virtual bool Valid() const { return valid_; }\n-  virtual Slice key() const {\n-    assert(valid_);\n-    return (direction_ == kForward) ? ExtractUserKey(iter_->key()) : saved_key_;\n-  }\n-  virtual Slice value() const {\n-    assert(valid_);\n-    return (direction_ == kForward) ? iter_->value() : saved_value_;\n-  }\n-  virtual Status status() const {\n-    if (status_.ok()) {\n-      return iter_->status();\n-    } else {\n-      return status_;\n-    }\n-  }\n-\n-  virtual void Next();\n-  virtual void Prev();\n-  virtual void Seek(const Slice& target);\n-  virtual void SeekToFirst();\n-  virtual void SeekToLast();\n-\n- private:\n-  void FindNextUserEntry(bool skipping, std::string* skip);\n-  void FindPrevUserEntry();\n-  bool ParseKey(ParsedInternalKey* key);\n-\n-  inline void SaveKey(const Slice& k, std::string* dst) {\n-    dst->assign(k.data(), k.size());\n-  }\n-\n-  inline void ClearSavedValue() {\n-    if (saved_value_.capacity() > 1048576) {\n-      std::string empty;\n-      swap(empty, saved_value_);\n-    } else {\n-      saved_value_.clear();\n-    }\n-  }\n-\n-  const std::string* const dbname_;\n-  Env* const env_;\n-  const Comparator* const user_comparator_;\n-  Iterator* const iter_;\n-  SequenceNumber const sequence_;\n-\n-  Status status_;\n-  std::string saved_key_;     // == current key when direction_==kReverse\n-  std::string saved_value_;   // == current raw value when direction_==kReverse\n-  Direction direction_;\n-  bool valid_;\n-\n-  // No copying allowed\n-  DBIter(const DBIter&);\n-  void operator=(const DBIter&);\n-};\n-\n-inline bool DBIter::ParseKey(ParsedInternalKey* ikey) {\n-  if (!ParseInternalKey(iter_->key(), ikey)) {\n-    status_ = Status::Corruption(\"corrupted internal key in DBIter\");\n-    return false;\n-  } else {\n-    return true;\n-  }\n-}\n-\n-void DBIter::Next() {\n-  assert(valid_);\n-\n-  if (direction_ == kReverse) {  // Switch directions?\n-    direction_ = kForward;\n-    // iter_ is pointing just before the entries for this->key(),\n-    // so advance into the range of entries for this->key() and then\n-    // use the normal skipping code below.\n-    if (!iter_->Valid()) {\n-      iter_->SeekToFirst();\n-    } else {\n-      iter_->Next();\n-    }\n-    if (!iter_->Valid()) {\n-      valid_ = false;\n-      saved_key_.clear();\n-      return;\n-    }\n-  }\n-\n-  // Temporarily use saved_key_ as storage for key to skip.\n-  std::string* skip = &saved_key_;\n-  SaveKey(ExtractUserKey(iter_->key()), skip);\n-  FindNextUserEntry(true, skip);\n-}\n-\n-void DBIter::FindNextUserEntry(bool skipping, std::string* skip) {\n-  // Loop until we hit an acceptable entry to yield\n-  assert(iter_->Valid());\n-  assert(direction_ == kForward);\n-  do {\n-    ParsedInternalKey ikey;\n-    if (ParseKey(&ikey) && ikey.sequence <= sequence_) {\n-      switch (ikey.type) {\n-        case kTypeDeletion:\n-          // Arrange to skip all upcoming entries for this key since\n-          // they are hidden by this deletion.\n-          SaveKey(ikey.user_key, skip);\n-          skipping = true;\n-          break;\n-        case kTypeValue:\n-          if (skipping &&\n-              user_comparator_->Compare(ikey.user_key, *skip) <= 0) {\n-            // Entry hidden\n-          } else {\n-            valid_ = true;\n-            saved_key_.clear();\n-            return;\n-          }\n-          break;\n-      }\n-    }\n-    iter_->Next();\n-  } while (iter_->Valid());\n-  saved_key_.clear();\n-  valid_ = false;\n-}\n-\n-void DBIter::Prev() {\n-  assert(valid_);\n-\n-  if (direction_ == kForward) {  // Switch directions?\n-    // iter_ is pointing at the current entry.  Scan backwards until\n-    // the key changes so we can use the normal reverse scanning code.\n-    assert(iter_->Valid());  // Otherwise valid_ would have been false\n-    SaveKey(ExtractUserKey(iter_->key()), &saved_key_);\n-    while (true) {\n-      iter_->Prev();\n-      if (!iter_->Valid()) {\n-        valid_ = false;\n-        saved_key_.clear();\n-        ClearSavedValue();\n-        return;\n-      }\n-      if (user_comparator_->Compare(ExtractUserKey(iter_->key()),\n-                                    saved_key_) < 0) {\n-        break;\n-      }\n-    }\n-    direction_ = kReverse;\n-  }\n-\n-  FindPrevUserEntry();\n-}\n-\n-void DBIter::FindPrevUserEntry() {\n-  assert(direction_ == kReverse);\n-\n-  ValueType value_type = kTypeDeletion;\n-  if (iter_->Valid()) {\n-    do {\n-      ParsedInternalKey ikey;\n-      if (ParseKey(&ikey) && ikey.sequence <= sequence_) {\n-        if ((value_type != kTypeDeletion) &&\n-            user_comparator_->Compare(ikey.user_key, saved_key_) < 0) {\n-          // We encountered a non-deleted value in entries for previous keys,\n-          break;\n-        }\n-        value_type = ikey.type;\n-        if (value_type == kTypeDeletion) {\n-          saved_key_.clear();\n-          ClearSavedValue();\n-        } else {\n-          Slice raw_value = iter_->value();\n-          if (saved_value_.capacity() > raw_value.size() + 1048576) {\n-            std::string empty;\n-            swap(empty, saved_value_);\n-          }\n-          SaveKey(ExtractUserKey(iter_->key()), &saved_key_);\n-          saved_value_.assign(raw_value.data(), raw_value.size());\n-        }\n-      }\n-      iter_->Prev();\n-    } while (iter_->Valid());\n-  }\n-\n-  if (value_type == kTypeDeletion) {\n-    // End\n-    valid_ = false;\n-    saved_key_.clear();\n-    ClearSavedValue();\n-    direction_ = kForward;\n-  } else {\n-    valid_ = true;\n-  }\n-}\n-\n-void DBIter::Seek(const Slice& target) {\n-  direction_ = kForward;\n-  ClearSavedValue();\n-  saved_key_.clear();\n-  AppendInternalKey(\n-      &saved_key_, ParsedInternalKey(target, sequence_, kValueTypeForSeek));\n-  iter_->Seek(saved_key_);\n-  if (iter_->Valid()) {\n-    FindNextUserEntry(false, &saved_key_ /* temporary storage */);\n-  } else {\n-    valid_ = false;\n-  }\n-}\n-\n-void DBIter::SeekToFirst() {\n-  direction_ = kForward;\n-  ClearSavedValue();\n-  iter_->SeekToFirst();\n-  if (iter_->Valid()) {\n-    FindNextUserEntry(false, &saved_key_ /* temporary storage */);\n-  } else {\n-    valid_ = false;\n-  }\n-}\n-\n-void DBIter::SeekToLast() {\n-  direction_ = kReverse;\n-  ClearSavedValue();\n-  iter_->SeekToLast();\n-  FindPrevUserEntry();\n-}\n-\n-}  // anonymous namespace\n-\n-Iterator* NewDBIterator(\n-    const std::string* dbname,\n-    Env* env,\n-    const Comparator* user_key_comparator,\n-    Iterator* internal_iter,\n-    const SequenceNumber& sequence) {\n-  return new DBIter(dbname, env, user_key_comparator, internal_iter, sequence);\n-}\n-\n-}  // namespace leveldb"
      },
      {
        "sha": "d9e1b174ab8726eeca58197bb2c36f73099d13e5",
        "filename": "src/leveldb/db/db_iter.h",
        "status": "removed",
        "additions": 0,
        "deletions": 26,
        "changes": 26,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/db_iter.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/db_iter.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_iter.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,26 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_DB_DB_ITER_H_\n-#define STORAGE_LEVELDB_DB_DB_ITER_H_\n-\n-#include <stdint.h>\n-#include \"leveldb/db.h\"\n-#include \"db/dbformat.h\"\n-\n-namespace leveldb {\n-\n-// Return a new iterator that converts internal keys (yielded by\n-// \"*internal_iter\") that were live at the specified \"sequence\" number\n-// into appropriate user keys.\n-extern Iterator* NewDBIterator(\n-    const std::string* dbname,\n-    Env* env,\n-    const Comparator* user_key_comparator,\n-    Iterator* internal_iter,\n-    const SequenceNumber& sequence);\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_DB_DB_ITER_H_"
      },
      {
        "sha": "684ea3bdbc8ecc80d51cfecbfbe8be1d78711e6a",
        "filename": "src/leveldb/db/db_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 2027,
        "changes": 2027,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/db_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/db_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,2027 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"leveldb/db.h\"\n-#include \"leveldb/filter_policy.h\"\n-#include \"db/db_impl.h\"\n-#include \"db/filename.h\"\n-#include \"db/version_set.h\"\n-#include \"db/write_batch_internal.h\"\n-#include \"leveldb/cache.h\"\n-#include \"leveldb/env.h\"\n-#include \"leveldb/table.h\"\n-#include \"util/hash.h\"\n-#include \"util/logging.h\"\n-#include \"util/mutexlock.h\"\n-#include \"util/testharness.h\"\n-#include \"util/testutil.h\"\n-\n-namespace leveldb {\n-\n-static std::string RandomString(Random* rnd, int len) {\n-  std::string r;\n-  test::RandomString(rnd, len, &r);\n-  return r;\n-}\n-\n-namespace {\n-class AtomicCounter {\n- private:\n-  port::Mutex mu_;\n-  int count_;\n- public:\n-  AtomicCounter() : count_(0) { }\n-  void Increment() {\n-    MutexLock l(&mu_);\n-    count_++;\n-  }\n-  int Read() {\n-    MutexLock l(&mu_);\n-    return count_;\n-  }\n-  void Reset() {\n-    MutexLock l(&mu_);\n-    count_ = 0;\n-  }\n-};\n-}\n-\n-// Special Env used to delay background operations\n-class SpecialEnv : public EnvWrapper {\n- public:\n-  // sstable Sync() calls are blocked while this pointer is non-NULL.\n-  port::AtomicPointer delay_sstable_sync_;\n-\n-  // Simulate no-space errors while this pointer is non-NULL.\n-  port::AtomicPointer no_space_;\n-\n-  // Simulate non-writable file system while this pointer is non-NULL\n-  port::AtomicPointer non_writable_;\n-\n-  // Force sync of manifest files to fail while this pointer is non-NULL\n-  port::AtomicPointer manifest_sync_error_;\n-\n-  // Force write to manifest files to fail while this pointer is non-NULL\n-  port::AtomicPointer manifest_write_error_;\n-\n-  bool count_random_reads_;\n-  AtomicCounter random_read_counter_;\n-\n-  AtomicCounter sleep_counter_;\n-\n-  explicit SpecialEnv(Env* base) : EnvWrapper(base) {\n-    delay_sstable_sync_.Release_Store(NULL);\n-    no_space_.Release_Store(NULL);\n-    non_writable_.Release_Store(NULL);\n-    count_random_reads_ = false;\n-    manifest_sync_error_.Release_Store(NULL);\n-    manifest_write_error_.Release_Store(NULL);\n-  }\n-\n-  Status NewWritableFile(const std::string& f, WritableFile** r) {\n-    class SSTableFile : public WritableFile {\n-     private:\n-      SpecialEnv* env_;\n-      WritableFile* base_;\n-\n-     public:\n-      SSTableFile(SpecialEnv* env, WritableFile* base)\n-          : env_(env),\n-            base_(base) {\n-      }\n-      ~SSTableFile() { delete base_; }\n-      Status Append(const Slice& data) {\n-        if (env_->no_space_.Acquire_Load() != NULL) {\n-          // Drop writes on the floor\n-          return Status::OK();\n-        } else {\n-          return base_->Append(data);\n-        }\n-      }\n-      Status Close() { return base_->Close(); }\n-      Status Flush() { return base_->Flush(); }\n-      Status Sync() {\n-        while (env_->delay_sstable_sync_.Acquire_Load() != NULL) {\n-          env_->SleepForMicroseconds(100000);\n-        }\n-        return base_->Sync();\n-      }\n-    };\n-    class ManifestFile : public WritableFile {\n-     private:\n-      SpecialEnv* env_;\n-      WritableFile* base_;\n-     public:\n-      ManifestFile(SpecialEnv* env, WritableFile* b) : env_(env), base_(b) { }\n-      ~ManifestFile() { delete base_; }\n-      Status Append(const Slice& data) {\n-        if (env_->manifest_write_error_.Acquire_Load() != NULL) {\n-          return Status::IOError(\"simulated writer error\");\n-        } else {\n-          return base_->Append(data);\n-        }\n-      }\n-      Status Close() { return base_->Close(); }\n-      Status Flush() { return base_->Flush(); }\n-      Status Sync() {\n-        if (env_->manifest_sync_error_.Acquire_Load() != NULL) {\n-          return Status::IOError(\"simulated sync error\");\n-        } else {\n-          return base_->Sync();\n-        }\n-      }\n-    };\n-\n-    if (non_writable_.Acquire_Load() != NULL) {\n-      return Status::IOError(\"simulated write error\");\n-    }\n-\n-    Status s = target()->NewWritableFile(f, r);\n-    if (s.ok()) {\n-      if (strstr(f.c_str(), \".sst\") != NULL) {\n-        *r = new SSTableFile(this, *r);\n-      } else if (strstr(f.c_str(), \"MANIFEST\") != NULL) {\n-        *r = new ManifestFile(this, *r);\n-      }\n-    }\n-    return s;\n-  }\n-\n-  Status NewRandomAccessFile(const std::string& f, RandomAccessFile** r) {\n-    class CountingFile : public RandomAccessFile {\n-     private:\n-      RandomAccessFile* target_;\n-      AtomicCounter* counter_;\n-     public:\n-      CountingFile(RandomAccessFile* target, AtomicCounter* counter)\n-          : target_(target), counter_(counter) {\n-      }\n-      virtual ~CountingFile() { delete target_; }\n-      virtual Status Read(uint64_t offset, size_t n, Slice* result,\n-                          char* scratch) const {\n-        counter_->Increment();\n-        return target_->Read(offset, n, result, scratch);\n-      }\n-    };\n-\n-    Status s = target()->NewRandomAccessFile(f, r);\n-    if (s.ok() && count_random_reads_) {\n-      *r = new CountingFile(*r, &random_read_counter_);\n-    }\n-    return s;\n-  }\n-\n-  virtual void SleepForMicroseconds(int micros) {\n-    sleep_counter_.Increment();\n-    target()->SleepForMicroseconds(micros);\n-  }\n-};\n-\n-class DBTest {\n- private:\n-  const FilterPolicy* filter_policy_;\n-\n-  // Sequence of option configurations to try\n-  enum OptionConfig {\n-    kDefault,\n-    kFilter,\n-    kUncompressed,\n-    kEnd\n-  };\n-  int option_config_;\n-\n- public:\n-  std::string dbname_;\n-  SpecialEnv* env_;\n-  DB* db_;\n-\n-  Options last_options_;\n-\n-  DBTest() : option_config_(kDefault),\n-             env_(new SpecialEnv(Env::Default())) {\n-    filter_policy_ = NewBloomFilterPolicy(10);\n-    dbname_ = test::TmpDir() + \"/db_test\";\n-    DestroyDB(dbname_, Options());\n-    db_ = NULL;\n-    Reopen();\n-  }\n-\n-  ~DBTest() {\n-    delete db_;\n-    DestroyDB(dbname_, Options());\n-    delete env_;\n-    delete filter_policy_;\n-  }\n-\n-  // Switch to a fresh database with the next option configuration to\n-  // test.  Return false if there are no more configurations to test.\n-  bool ChangeOptions() {\n-    option_config_++;\n-    if (option_config_ >= kEnd) {\n-      return false;\n-    } else {\n-      DestroyAndReopen();\n-      return true;\n-    }\n-  }\n-\n-  // Return the current option configuration.\n-  Options CurrentOptions() {\n-    Options options;\n-    switch (option_config_) {\n-      case kFilter:\n-        options.filter_policy = filter_policy_;\n-        break;\n-      case kUncompressed:\n-        options.compression = kNoCompression;\n-        break;\n-      default:\n-        break;\n-    }\n-    return options;\n-  }\n-\n-  DBImpl* dbfull() {\n-    return reinterpret_cast<DBImpl*>(db_);\n-  }\n-\n-  void Reopen(Options* options = NULL) {\n-    ASSERT_OK(TryReopen(options));\n-  }\n-\n-  void Close() {\n-    delete db_;\n-    db_ = NULL;\n-  }\n-\n-  void DestroyAndReopen(Options* options = NULL) {\n-    delete db_;\n-    db_ = NULL;\n-    DestroyDB(dbname_, Options());\n-    ASSERT_OK(TryReopen(options));\n-  }\n-\n-  Status TryReopen(Options* options) {\n-    delete db_;\n-    db_ = NULL;\n-    Options opts;\n-    if (options != NULL) {\n-      opts = *options;\n-    } else {\n-      opts = CurrentOptions();\n-      opts.create_if_missing = true;\n-    }\n-    last_options_ = opts;\n-\n-    return DB::Open(opts, dbname_, &db_);\n-  }\n-\n-  Status Put(const std::string& k, const std::string& v) {\n-    return db_->Put(WriteOptions(), k, v);\n-  }\n-\n-  Status Delete(const std::string& k) {\n-    return db_->Delete(WriteOptions(), k);\n-  }\n-\n-  std::string Get(const std::string& k, const Snapshot* snapshot = NULL) {\n-    ReadOptions options;\n-    options.snapshot = snapshot;\n-    std::string result;\n-    Status s = db_->Get(options, k, &result);\n-    if (s.IsNotFound()) {\n-      result = \"NOT_FOUND\";\n-    } else if (!s.ok()) {\n-      result = s.ToString();\n-    }\n-    return result;\n-  }\n-\n-  // Return a string that contains all key,value pairs in order,\n-  // formatted like \"(k1->v1)(k2->v2)\".\n-  std::string Contents() {\n-    std::vector<std::string> forward;\n-    std::string result;\n-    Iterator* iter = db_->NewIterator(ReadOptions());\n-    for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n-      std::string s = IterStatus(iter);\n-      result.push_back('(');\n-      result.append(s);\n-      result.push_back(')');\n-      forward.push_back(s);\n-    }\n-\n-    // Check reverse iteration results are the reverse of forward results\n-    int matched = 0;\n-    for (iter->SeekToLast(); iter->Valid(); iter->Prev()) {\n-      ASSERT_LT(matched, forward.size());\n-      ASSERT_EQ(IterStatus(iter), forward[forward.size() - matched - 1]);\n-      matched++;\n-    }\n-    ASSERT_EQ(matched, forward.size());\n-\n-    delete iter;\n-    return result;\n-  }\n-\n-  std::string AllEntriesFor(const Slice& user_key) {\n-    Iterator* iter = dbfull()->TEST_NewInternalIterator();\n-    InternalKey target(user_key, kMaxSequenceNumber, kTypeValue);\n-    iter->Seek(target.Encode());\n-    std::string result;\n-    if (!iter->status().ok()) {\n-      result = iter->status().ToString();\n-    } else {\n-      result = \"[ \";\n-      bool first = true;\n-      while (iter->Valid()) {\n-        ParsedInternalKey ikey;\n-        if (!ParseInternalKey(iter->key(), &ikey)) {\n-          result += \"CORRUPTED\";\n-        } else {\n-          if (last_options_.comparator->Compare(ikey.user_key, user_key) != 0) {\n-            break;\n-          }\n-          if (!first) {\n-            result += \", \";\n-          }\n-          first = false;\n-          switch (ikey.type) {\n-            case kTypeValue:\n-              result += iter->value().ToString();\n-              break;\n-            case kTypeDeletion:\n-              result += \"DEL\";\n-              break;\n-          }\n-        }\n-        iter->Next();\n-      }\n-      if (!first) {\n-        result += \" \";\n-      }\n-      result += \"]\";\n-    }\n-    delete iter;\n-    return result;\n-  }\n-\n-  int NumTableFilesAtLevel(int level) {\n-    std::string property;\n-    ASSERT_TRUE(\n-        db_->GetProperty(\"leveldb.num-files-at-level\" + NumberToString(level),\n-                         &property));\n-    return atoi(property.c_str());\n-  }\n-\n-  int TotalTableFiles() {\n-    int result = 0;\n-    for (int level = 0; level < config::kNumLevels; level++) {\n-      result += NumTableFilesAtLevel(level);\n-    }\n-    return result;\n-  }\n-\n-  // Return spread of files per level\n-  std::string FilesPerLevel() {\n-    std::string result;\n-    int last_non_zero_offset = 0;\n-    for (int level = 0; level < config::kNumLevels; level++) {\n-      int f = NumTableFilesAtLevel(level);\n-      char buf[100];\n-      snprintf(buf, sizeof(buf), \"%s%d\", (level ? \",\" : \"\"), f);\n-      result += buf;\n-      if (f > 0) {\n-        last_non_zero_offset = result.size();\n-      }\n-    }\n-    result.resize(last_non_zero_offset);\n-    return result;\n-  }\n-\n-  int CountFiles() {\n-    std::vector<std::string> files;\n-    env_->GetChildren(dbname_, &files);\n-    return static_cast<int>(files.size());\n-  }\n-\n-  uint64_t Size(const Slice& start, const Slice& limit) {\n-    Range r(start, limit);\n-    uint64_t size;\n-    db_->GetApproximateSizes(&r, 1, &size);\n-    return size;\n-  }\n-\n-  void Compact(const Slice& start, const Slice& limit) {\n-    db_->CompactRange(&start, &limit);\n-  }\n-\n-  // Do n memtable compactions, each of which produces an sstable\n-  // covering the range [small,large].\n-  void MakeTables(int n, const std::string& small, const std::string& large) {\n-    for (int i = 0; i < n; i++) {\n-      Put(small, \"begin\");\n-      Put(large, \"end\");\n-      dbfull()->TEST_CompactMemTable();\n-    }\n-  }\n-\n-  // Prevent pushing of new sstables into deeper levels by adding\n-  // tables that cover a specified range to all levels.\n-  void FillLevels(const std::string& smallest, const std::string& largest) {\n-    MakeTables(config::kNumLevels, smallest, largest);\n-  }\n-\n-  void DumpFileCounts(const char* label) {\n-    fprintf(stderr, \"---\\n%s:\\n\", label);\n-    fprintf(stderr, \"maxoverlap: %lld\\n\",\n-            static_cast<long long>(\n-                dbfull()->TEST_MaxNextLevelOverlappingBytes()));\n-    for (int level = 0; level < config::kNumLevels; level++) {\n-      int num = NumTableFilesAtLevel(level);\n-      if (num > 0) {\n-        fprintf(stderr, \"  level %3d : %d files\\n\", level, num);\n-      }\n-    }\n-  }\n-\n-  std::string DumpSSTableList() {\n-    std::string property;\n-    db_->GetProperty(\"leveldb.sstables\", &property);\n-    return property;\n-  }\n-\n-  std::string IterStatus(Iterator* iter) {\n-    std::string result;\n-    if (iter->Valid()) {\n-      result = iter->key().ToString() + \"->\" + iter->value().ToString();\n-    } else {\n-      result = \"(invalid)\";\n-    }\n-    return result;\n-  }\n-};\n-\n-TEST(DBTest, Empty) {\n-  do {\n-    ASSERT_TRUE(db_ != NULL);\n-    ASSERT_EQ(\"NOT_FOUND\", Get(\"foo\"));\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, ReadWrite) {\n-  do {\n-    ASSERT_OK(Put(\"foo\", \"v1\"));\n-    ASSERT_EQ(\"v1\", Get(\"foo\"));\n-    ASSERT_OK(Put(\"bar\", \"v2\"));\n-    ASSERT_OK(Put(\"foo\", \"v3\"));\n-    ASSERT_EQ(\"v3\", Get(\"foo\"));\n-    ASSERT_EQ(\"v2\", Get(\"bar\"));\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, PutDeleteGet) {\n-  do {\n-    ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v1\"));\n-    ASSERT_EQ(\"v1\", Get(\"foo\"));\n-    ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v2\"));\n-    ASSERT_EQ(\"v2\", Get(\"foo\"));\n-    ASSERT_OK(db_->Delete(WriteOptions(), \"foo\"));\n-    ASSERT_EQ(\"NOT_FOUND\", Get(\"foo\"));\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, GetFromImmutableLayer) {\n-  do {\n-    Options options = CurrentOptions();\n-    options.env = env_;\n-    options.write_buffer_size = 100000;  // Small write buffer\n-    Reopen(&options);\n-\n-    ASSERT_OK(Put(\"foo\", \"v1\"));\n-    ASSERT_EQ(\"v1\", Get(\"foo\"));\n-\n-    env_->delay_sstable_sync_.Release_Store(env_);   // Block sync calls\n-    Put(\"k1\", std::string(100000, 'x'));             // Fill memtable\n-    Put(\"k2\", std::string(100000, 'y'));             // Trigger compaction\n-    ASSERT_EQ(\"v1\", Get(\"foo\"));\n-    env_->delay_sstable_sync_.Release_Store(NULL);   // Release sync calls\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, GetFromVersions) {\n-  do {\n-    ASSERT_OK(Put(\"foo\", \"v1\"));\n-    dbfull()->TEST_CompactMemTable();\n-    ASSERT_EQ(\"v1\", Get(\"foo\"));\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, GetSnapshot) {\n-  do {\n-    // Try with both a short key and a long key\n-    for (int i = 0; i < 2; i++) {\n-      std::string key = (i == 0) ? std::string(\"foo\") : std::string(200, 'x');\n-      ASSERT_OK(Put(key, \"v1\"));\n-      const Snapshot* s1 = db_->GetSnapshot();\n-      ASSERT_OK(Put(key, \"v2\"));\n-      ASSERT_EQ(\"v2\", Get(key));\n-      ASSERT_EQ(\"v1\", Get(key, s1));\n-      dbfull()->TEST_CompactMemTable();\n-      ASSERT_EQ(\"v2\", Get(key));\n-      ASSERT_EQ(\"v1\", Get(key, s1));\n-      db_->ReleaseSnapshot(s1);\n-    }\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, GetLevel0Ordering) {\n-  do {\n-    // Check that we process level-0 files in correct order.  The code\n-    // below generates two level-0 files where the earlier one comes\n-    // before the later one in the level-0 file list since the earlier\n-    // one has a smaller \"smallest\" key.\n-    ASSERT_OK(Put(\"bar\", \"b\"));\n-    ASSERT_OK(Put(\"foo\", \"v1\"));\n-    dbfull()->TEST_CompactMemTable();\n-    ASSERT_OK(Put(\"foo\", \"v2\"));\n-    dbfull()->TEST_CompactMemTable();\n-    ASSERT_EQ(\"v2\", Get(\"foo\"));\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, GetOrderedByLevels) {\n-  do {\n-    ASSERT_OK(Put(\"foo\", \"v1\"));\n-    Compact(\"a\", \"z\");\n-    ASSERT_EQ(\"v1\", Get(\"foo\"));\n-    ASSERT_OK(Put(\"foo\", \"v2\"));\n-    ASSERT_EQ(\"v2\", Get(\"foo\"));\n-    dbfull()->TEST_CompactMemTable();\n-    ASSERT_EQ(\"v2\", Get(\"foo\"));\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, GetPicksCorrectFile) {\n-  do {\n-    // Arrange to have multiple files in a non-level-0 level.\n-    ASSERT_OK(Put(\"a\", \"va\"));\n-    Compact(\"a\", \"b\");\n-    ASSERT_OK(Put(\"x\", \"vx\"));\n-    Compact(\"x\", \"y\");\n-    ASSERT_OK(Put(\"f\", \"vf\"));\n-    Compact(\"f\", \"g\");\n-    ASSERT_EQ(\"va\", Get(\"a\"));\n-    ASSERT_EQ(\"vf\", Get(\"f\"));\n-    ASSERT_EQ(\"vx\", Get(\"x\"));\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, GetEncountersEmptyLevel) {\n-  do {\n-    // Arrange for the following to happen:\n-    //   * sstable A in level 0\n-    //   * nothing in level 1\n-    //   * sstable B in level 2\n-    // Then do enough Get() calls to arrange for an automatic compaction\n-    // of sstable A.  A bug would cause the compaction to be marked as\n-    // occuring at level 1 (instead of the correct level 0).\n-\n-    // Step 1: First place sstables in levels 0 and 2\n-    int compaction_count = 0;\n-    while (NumTableFilesAtLevel(0) == 0 ||\n-           NumTableFilesAtLevel(2) == 0) {\n-      ASSERT_LE(compaction_count, 100) << \"could not fill levels 0 and 2\";\n-      compaction_count++;\n-      Put(\"a\", \"begin\");\n-      Put(\"z\", \"end\");\n-      dbfull()->TEST_CompactMemTable();\n-    }\n-\n-    // Step 2: clear level 1 if necessary.\n-    dbfull()->TEST_CompactRange(1, NULL, NULL);\n-    ASSERT_EQ(NumTableFilesAtLevel(0), 1);\n-    ASSERT_EQ(NumTableFilesAtLevel(1), 0);\n-    ASSERT_EQ(NumTableFilesAtLevel(2), 1);\n-\n-    // Step 3: read a bunch of times\n-    for (int i = 0; i < 1000; i++) {\n-      ASSERT_EQ(\"NOT_FOUND\", Get(\"missing\"));\n-    }\n-\n-    // Step 4: Wait for compaction to finish\n-    env_->SleepForMicroseconds(1000000);\n-\n-    ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, IterEmpty) {\n-  Iterator* iter = db_->NewIterator(ReadOptions());\n-\n-  iter->SeekToFirst();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  iter->SeekToLast();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  iter->Seek(\"foo\");\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  delete iter;\n-}\n-\n-TEST(DBTest, IterSingle) {\n-  ASSERT_OK(Put(\"a\", \"va\"));\n-  Iterator* iter = db_->NewIterator(ReadOptions());\n-\n-  iter->SeekToFirst();\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-  iter->SeekToFirst();\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  iter->SeekToLast();\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-  iter->SeekToLast();\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  iter->Seek(\"\");\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  iter->Seek(\"a\");\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  iter->Seek(\"b\");\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  delete iter;\n-}\n-\n-TEST(DBTest, IterMulti) {\n-  ASSERT_OK(Put(\"a\", \"va\"));\n-  ASSERT_OK(Put(\"b\", \"vb\"));\n-  ASSERT_OK(Put(\"c\", \"vc\"));\n-  Iterator* iter = db_->NewIterator(ReadOptions());\n-\n-  iter->SeekToFirst();\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-  iter->SeekToFirst();\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  iter->SeekToLast();\n-  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-  iter->SeekToLast();\n-  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  iter->Seek(\"\");\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Seek(\"a\");\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Seek(\"ax\");\n-  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n-  iter->Seek(\"b\");\n-  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n-  iter->Seek(\"z\");\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  // Switch from reverse to forward\n-  iter->SeekToLast();\n-  iter->Prev();\n-  iter->Prev();\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n-\n-  // Switch from forward to reverse\n-  iter->SeekToFirst();\n-  iter->Next();\n-  iter->Next();\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n-\n-  // Make sure iter stays at snapshot\n-  ASSERT_OK(Put(\"a\",  \"va2\"));\n-  ASSERT_OK(Put(\"a2\", \"va3\"));\n-  ASSERT_OK(Put(\"b\",  \"vb2\"));\n-  ASSERT_OK(Put(\"c\",  \"vc2\"));\n-  ASSERT_OK(Delete(\"b\"));\n-  iter->SeekToFirst();\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-  iter->SeekToLast();\n-  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  delete iter;\n-}\n-\n-TEST(DBTest, IterSmallAndLargeMix) {\n-  ASSERT_OK(Put(\"a\", \"va\"));\n-  ASSERT_OK(Put(\"b\", std::string(100000, 'b')));\n-  ASSERT_OK(Put(\"c\", \"vc\"));\n-  ASSERT_OK(Put(\"d\", std::string(100000, 'd')));\n-  ASSERT_OK(Put(\"e\", std::string(100000, 'e')));\n-\n-  Iterator* iter = db_->NewIterator(ReadOptions());\n-\n-  iter->SeekToFirst();\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"b->\" + std::string(100000, 'b'));\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"d->\" + std::string(100000, 'd'));\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"e->\" + std::string(100000, 'e'));\n-  iter->Next();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  iter->SeekToLast();\n-  ASSERT_EQ(IterStatus(iter), \"e->\" + std::string(100000, 'e'));\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"d->\" + std::string(100000, 'd'));\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"b->\" + std::string(100000, 'b'));\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"a->va\");\n-  iter->Prev();\n-  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n-\n-  delete iter;\n-}\n-\n-TEST(DBTest, IterMultiWithDelete) {\n-  do {\n-    ASSERT_OK(Put(\"a\", \"va\"));\n-    ASSERT_OK(Put(\"b\", \"vb\"));\n-    ASSERT_OK(Put(\"c\", \"vc\"));\n-    ASSERT_OK(Delete(\"b\"));\n-    ASSERT_EQ(\"NOT_FOUND\", Get(\"b\"));\n-\n-    Iterator* iter = db_->NewIterator(ReadOptions());\n-    iter->Seek(\"c\");\n-    ASSERT_EQ(IterStatus(iter), \"c->vc\");\n-    iter->Prev();\n-    ASSERT_EQ(IterStatus(iter), \"a->va\");\n-    delete iter;\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, Recover) {\n-  do {\n-    ASSERT_OK(Put(\"foo\", \"v1\"));\n-    ASSERT_OK(Put(\"baz\", \"v5\"));\n-\n-    Reopen();\n-    ASSERT_EQ(\"v1\", Get(\"foo\"));\n-\n-    ASSERT_EQ(\"v1\", Get(\"foo\"));\n-    ASSERT_EQ(\"v5\", Get(\"baz\"));\n-    ASSERT_OK(Put(\"bar\", \"v2\"));\n-    ASSERT_OK(Put(\"foo\", \"v3\"));\n-\n-    Reopen();\n-    ASSERT_EQ(\"v3\", Get(\"foo\"));\n-    ASSERT_OK(Put(\"foo\", \"v4\"));\n-    ASSERT_EQ(\"v4\", Get(\"foo\"));\n-    ASSERT_EQ(\"v2\", Get(\"bar\"));\n-    ASSERT_EQ(\"v5\", Get(\"baz\"));\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, RecoveryWithEmptyLog) {\n-  do {\n-    ASSERT_OK(Put(\"foo\", \"v1\"));\n-    ASSERT_OK(Put(\"foo\", \"v2\"));\n-    Reopen();\n-    Reopen();\n-    ASSERT_OK(Put(\"foo\", \"v3\"));\n-    Reopen();\n-    ASSERT_EQ(\"v3\", Get(\"foo\"));\n-  } while (ChangeOptions());\n-}\n-\n-// Check that writes done during a memtable compaction are recovered\n-// if the database is shutdown during the memtable compaction.\n-TEST(DBTest, RecoverDuringMemtableCompaction) {\n-  do {\n-    Options options = CurrentOptions();\n-    options.env = env_;\n-    options.write_buffer_size = 1000000;\n-    Reopen(&options);\n-\n-    // Trigger a long memtable compaction and reopen the database during it\n-    ASSERT_OK(Put(\"foo\", \"v1\"));                         // Goes to 1st log file\n-    ASSERT_OK(Put(\"big1\", std::string(10000000, 'x')));  // Fills memtable\n-    ASSERT_OK(Put(\"big2\", std::string(1000, 'y')));      // Triggers compaction\n-    ASSERT_OK(Put(\"bar\", \"v2\"));                         // Goes to new log file\n-\n-    Reopen(&options);\n-    ASSERT_EQ(\"v1\", Get(\"foo\"));\n-    ASSERT_EQ(\"v2\", Get(\"bar\"));\n-    ASSERT_EQ(std::string(10000000, 'x'), Get(\"big1\"));\n-    ASSERT_EQ(std::string(1000, 'y'), Get(\"big2\"));\n-  } while (ChangeOptions());\n-}\n-\n-static std::string Key(int i) {\n-  char buf[100];\n-  snprintf(buf, sizeof(buf), \"key%06d\", i);\n-  return std::string(buf);\n-}\n-\n-TEST(DBTest, MinorCompactionsHappen) {\n-  Options options = CurrentOptions();\n-  options.write_buffer_size = 10000;\n-  Reopen(&options);\n-\n-  const int N = 500;\n-\n-  int starting_num_tables = TotalTableFiles();\n-  for (int i = 0; i < N; i++) {\n-    ASSERT_OK(Put(Key(i), Key(i) + std::string(1000, 'v')));\n-  }\n-  int ending_num_tables = TotalTableFiles();\n-  ASSERT_GT(ending_num_tables, starting_num_tables);\n-\n-  for (int i = 0; i < N; i++) {\n-    ASSERT_EQ(Key(i) + std::string(1000, 'v'), Get(Key(i)));\n-  }\n-\n-  Reopen();\n-\n-  for (int i = 0; i < N; i++) {\n-    ASSERT_EQ(Key(i) + std::string(1000, 'v'), Get(Key(i)));\n-  }\n-}\n-\n-TEST(DBTest, RecoverWithLargeLog) {\n-  {\n-    Options options = CurrentOptions();\n-    Reopen(&options);\n-    ASSERT_OK(Put(\"big1\", std::string(200000, '1')));\n-    ASSERT_OK(Put(\"big2\", std::string(200000, '2')));\n-    ASSERT_OK(Put(\"small3\", std::string(10, '3')));\n-    ASSERT_OK(Put(\"small4\", std::string(10, '4')));\n-    ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n-  }\n-\n-  // Make sure that if we re-open with a small write buffer size that\n-  // we flush table files in the middle of a large log file.\n-  Options options = CurrentOptions();\n-  options.write_buffer_size = 100000;\n-  Reopen(&options);\n-  ASSERT_EQ(NumTableFilesAtLevel(0), 3);\n-  ASSERT_EQ(std::string(200000, '1'), Get(\"big1\"));\n-  ASSERT_EQ(std::string(200000, '2'), Get(\"big2\"));\n-  ASSERT_EQ(std::string(10, '3'), Get(\"small3\"));\n-  ASSERT_EQ(std::string(10, '4'), Get(\"small4\"));\n-  ASSERT_GT(NumTableFilesAtLevel(0), 1);\n-}\n-\n-TEST(DBTest, CompactionsGenerateMultipleFiles) {\n-  Options options = CurrentOptions();\n-  options.write_buffer_size = 100000000;        // Large write buffer\n-  Reopen(&options);\n-\n-  Random rnd(301);\n-\n-  // Write 8MB (80 values, each 100K)\n-  ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n-  std::vector<std::string> values;\n-  for (int i = 0; i < 80; i++) {\n-    values.push_back(RandomString(&rnd, 100000));\n-    ASSERT_OK(Put(Key(i), values[i]));\n-  }\n-\n-  // Reopening moves updates to level-0\n-  Reopen(&options);\n-  dbfull()->TEST_CompactRange(0, NULL, NULL);\n-\n-  ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n-  ASSERT_GT(NumTableFilesAtLevel(1), 1);\n-  for (int i = 0; i < 80; i++) {\n-    ASSERT_EQ(Get(Key(i)), values[i]);\n-  }\n-}\n-\n-TEST(DBTest, RepeatedWritesToSameKey) {\n-  Options options = CurrentOptions();\n-  options.env = env_;\n-  options.write_buffer_size = 100000;  // Small write buffer\n-  Reopen(&options);\n-\n-  // We must have at most one file per level except for level-0,\n-  // which may have up to kL0_StopWritesTrigger files.\n-  const int kMaxFiles = config::kNumLevels + config::kL0_StopWritesTrigger;\n-\n-  Random rnd(301);\n-  std::string value = RandomString(&rnd, 2 * options.write_buffer_size);\n-  for (int i = 0; i < 5 * kMaxFiles; i++) {\n-    Put(\"key\", value);\n-    ASSERT_LE(TotalTableFiles(), kMaxFiles);\n-    fprintf(stderr, \"after %d: %d files\\n\", int(i+1), TotalTableFiles());\n-  }\n-}\n-\n-TEST(DBTest, SparseMerge) {\n-  Options options = CurrentOptions();\n-  options.compression = kNoCompression;\n-  Reopen(&options);\n-\n-  FillLevels(\"A\", \"Z\");\n-\n-  // Suppose there is:\n-  //    small amount of data with prefix A\n-  //    large amount of data with prefix B\n-  //    small amount of data with prefix C\n-  // and that recent updates have made small changes to all three prefixes.\n-  // Check that we do not do a compaction that merges all of B in one shot.\n-  const std::string value(1000, 'x');\n-  Put(\"A\", \"va\");\n-  // Write approximately 100MB of \"B\" values\n-  for (int i = 0; i < 100000; i++) {\n-    char key[100];\n-    snprintf(key, sizeof(key), \"B%010d\", i);\n-    Put(key, value);\n-  }\n-  Put(\"C\", \"vc\");\n-  dbfull()->TEST_CompactMemTable();\n-  dbfull()->TEST_CompactRange(0, NULL, NULL);\n-\n-  // Make sparse update\n-  Put(\"A\",    \"va2\");\n-  Put(\"B100\", \"bvalue2\");\n-  Put(\"C\",    \"vc2\");\n-  dbfull()->TEST_CompactMemTable();\n-\n-  // Compactions should not cause us to create a situation where\n-  // a file overlaps too much data at the next level.\n-  ASSERT_LE(dbfull()->TEST_MaxNextLevelOverlappingBytes(), 20*1048576);\n-  dbfull()->TEST_CompactRange(0, NULL, NULL);\n-  ASSERT_LE(dbfull()->TEST_MaxNextLevelOverlappingBytes(), 20*1048576);\n-  dbfull()->TEST_CompactRange(1, NULL, NULL);\n-  ASSERT_LE(dbfull()->TEST_MaxNextLevelOverlappingBytes(), 20*1048576);\n-}\n-\n-static bool Between(uint64_t val, uint64_t low, uint64_t high) {\n-  bool result = (val >= low) && (val <= high);\n-  if (!result) {\n-    fprintf(stderr, \"Value %llu is not in range [%llu, %llu]\\n\",\n-            (unsigned long long)(val),\n-            (unsigned long long)(low),\n-            (unsigned long long)(high));\n-  }\n-  return result;\n-}\n-\n-TEST(DBTest, ApproximateSizes) {\n-  do {\n-    Options options = CurrentOptions();\n-    options.write_buffer_size = 100000000;        // Large write buffer\n-    options.compression = kNoCompression;\n-    DestroyAndReopen();\n-\n-    ASSERT_TRUE(Between(Size(\"\", \"xyz\"), 0, 0));\n-    Reopen(&options);\n-    ASSERT_TRUE(Between(Size(\"\", \"xyz\"), 0, 0));\n-\n-    // Write 8MB (80 values, each 100K)\n-    ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n-    const int N = 80;\n-    static const int S1 = 100000;\n-    static const int S2 = 105000;  // Allow some expansion from metadata\n-    Random rnd(301);\n-    for (int i = 0; i < N; i++) {\n-      ASSERT_OK(Put(Key(i), RandomString(&rnd, S1)));\n-    }\n-\n-    // 0 because GetApproximateSizes() does not account for memtable space\n-    ASSERT_TRUE(Between(Size(\"\", Key(50)), 0, 0));\n-\n-    // Check sizes across recovery by reopening a few times\n-    for (int run = 0; run < 3; run++) {\n-      Reopen(&options);\n-\n-      for (int compact_start = 0; compact_start < N; compact_start += 10) {\n-        for (int i = 0; i < N; i += 10) {\n-          ASSERT_TRUE(Between(Size(\"\", Key(i)), S1*i, S2*i));\n-          ASSERT_TRUE(Between(Size(\"\", Key(i)+\".suffix\"), S1*(i+1), S2*(i+1)));\n-          ASSERT_TRUE(Between(Size(Key(i), Key(i+10)), S1*10, S2*10));\n-        }\n-        ASSERT_TRUE(Between(Size(\"\", Key(50)), S1*50, S2*50));\n-        ASSERT_TRUE(Between(Size(\"\", Key(50)+\".suffix\"), S1*50, S2*50));\n-\n-        std::string cstart_str = Key(compact_start);\n-        std::string cend_str = Key(compact_start + 9);\n-        Slice cstart = cstart_str;\n-        Slice cend = cend_str;\n-        dbfull()->TEST_CompactRange(0, &cstart, &cend);\n-      }\n-\n-      ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n-      ASSERT_GT(NumTableFilesAtLevel(1), 0);\n-    }\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, ApproximateSizes_MixOfSmallAndLarge) {\n-  do {\n-    Options options = CurrentOptions();\n-    options.compression = kNoCompression;\n-    Reopen();\n-\n-    Random rnd(301);\n-    std::string big1 = RandomString(&rnd, 100000);\n-    ASSERT_OK(Put(Key(0), RandomString(&rnd, 10000)));\n-    ASSERT_OK(Put(Key(1), RandomString(&rnd, 10000)));\n-    ASSERT_OK(Put(Key(2), big1));\n-    ASSERT_OK(Put(Key(3), RandomString(&rnd, 10000)));\n-    ASSERT_OK(Put(Key(4), big1));\n-    ASSERT_OK(Put(Key(5), RandomString(&rnd, 10000)));\n-    ASSERT_OK(Put(Key(6), RandomString(&rnd, 300000)));\n-    ASSERT_OK(Put(Key(7), RandomString(&rnd, 10000)));\n-\n-    // Check sizes across recovery by reopening a few times\n-    for (int run = 0; run < 3; run++) {\n-      Reopen(&options);\n-\n-      ASSERT_TRUE(Between(Size(\"\", Key(0)), 0, 0));\n-      ASSERT_TRUE(Between(Size(\"\", Key(1)), 10000, 11000));\n-      ASSERT_TRUE(Between(Size(\"\", Key(2)), 20000, 21000));\n-      ASSERT_TRUE(Between(Size(\"\", Key(3)), 120000, 121000));\n-      ASSERT_TRUE(Between(Size(\"\", Key(4)), 130000, 131000));\n-      ASSERT_TRUE(Between(Size(\"\", Key(5)), 230000, 231000));\n-      ASSERT_TRUE(Between(Size(\"\", Key(6)), 240000, 241000));\n-      ASSERT_TRUE(Between(Size(\"\", Key(7)), 540000, 541000));\n-      ASSERT_TRUE(Between(Size(\"\", Key(8)), 550000, 560000));\n-\n-      ASSERT_TRUE(Between(Size(Key(3), Key(5)), 110000, 111000));\n-\n-      dbfull()->TEST_CompactRange(0, NULL, NULL);\n-    }\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, IteratorPinsRef) {\n-  Put(\"foo\", \"hello\");\n-\n-  // Get iterator that will yield the current contents of the DB.\n-  Iterator* iter = db_->NewIterator(ReadOptions());\n-\n-  // Write to force compactions\n-  Put(\"foo\", \"newvalue1\");\n-  for (int i = 0; i < 100; i++) {\n-    ASSERT_OK(Put(Key(i), Key(i) + std::string(100000, 'v'))); // 100K values\n-  }\n-  Put(\"foo\", \"newvalue2\");\n-\n-  iter->SeekToFirst();\n-  ASSERT_TRUE(iter->Valid());\n-  ASSERT_EQ(\"foo\", iter->key().ToString());\n-  ASSERT_EQ(\"hello\", iter->value().ToString());\n-  iter->Next();\n-  ASSERT_TRUE(!iter->Valid());\n-  delete iter;\n-}\n-\n-TEST(DBTest, Snapshot) {\n-  do {\n-    Put(\"foo\", \"v1\");\n-    const Snapshot* s1 = db_->GetSnapshot();\n-    Put(\"foo\", \"v2\");\n-    const Snapshot* s2 = db_->GetSnapshot();\n-    Put(\"foo\", \"v3\");\n-    const Snapshot* s3 = db_->GetSnapshot();\n-\n-    Put(\"foo\", \"v4\");\n-    ASSERT_EQ(\"v1\", Get(\"foo\", s1));\n-    ASSERT_EQ(\"v2\", Get(\"foo\", s2));\n-    ASSERT_EQ(\"v3\", Get(\"foo\", s3));\n-    ASSERT_EQ(\"v4\", Get(\"foo\"));\n-\n-    db_->ReleaseSnapshot(s3);\n-    ASSERT_EQ(\"v1\", Get(\"foo\", s1));\n-    ASSERT_EQ(\"v2\", Get(\"foo\", s2));\n-    ASSERT_EQ(\"v4\", Get(\"foo\"));\n-\n-    db_->ReleaseSnapshot(s1);\n-    ASSERT_EQ(\"v2\", Get(\"foo\", s2));\n-    ASSERT_EQ(\"v4\", Get(\"foo\"));\n-\n-    db_->ReleaseSnapshot(s2);\n-    ASSERT_EQ(\"v4\", Get(\"foo\"));\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, HiddenValuesAreRemoved) {\n-  do {\n-    Random rnd(301);\n-    FillLevels(\"a\", \"z\");\n-\n-    std::string big = RandomString(&rnd, 50000);\n-    Put(\"foo\", big);\n-    Put(\"pastfoo\", \"v\");\n-    const Snapshot* snapshot = db_->GetSnapshot();\n-    Put(\"foo\", \"tiny\");\n-    Put(\"pastfoo2\", \"v2\");        // Advance sequence number one more\n-\n-    ASSERT_OK(dbfull()->TEST_CompactMemTable());\n-    ASSERT_GT(NumTableFilesAtLevel(0), 0);\n-\n-    ASSERT_EQ(big, Get(\"foo\", snapshot));\n-    ASSERT_TRUE(Between(Size(\"\", \"pastfoo\"), 50000, 60000));\n-    db_->ReleaseSnapshot(snapshot);\n-    ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ tiny, \" + big + \" ]\");\n-    Slice x(\"x\");\n-    dbfull()->TEST_CompactRange(0, NULL, &x);\n-    ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ tiny ]\");\n-    ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n-    ASSERT_GE(NumTableFilesAtLevel(1), 1);\n-    dbfull()->TEST_CompactRange(1, NULL, &x);\n-    ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ tiny ]\");\n-\n-    ASSERT_TRUE(Between(Size(\"\", \"pastfoo\"), 0, 1000));\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, DeletionMarkers1) {\n-  Put(\"foo\", \"v1\");\n-  ASSERT_OK(dbfull()->TEST_CompactMemTable());\n-  const int last = config::kMaxMemCompactLevel;\n-  ASSERT_EQ(NumTableFilesAtLevel(last), 1);   // foo => v1 is now in last level\n-\n-  // Place a table at level last-1 to prevent merging with preceding mutation\n-  Put(\"a\", \"begin\");\n-  Put(\"z\", \"end\");\n-  dbfull()->TEST_CompactMemTable();\n-  ASSERT_EQ(NumTableFilesAtLevel(last), 1);\n-  ASSERT_EQ(NumTableFilesAtLevel(last-1), 1);\n-\n-  Delete(\"foo\");\n-  Put(\"foo\", \"v2\");\n-  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ v2, DEL, v1 ]\");\n-  ASSERT_OK(dbfull()->TEST_CompactMemTable());  // Moves to level last-2\n-  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ v2, DEL, v1 ]\");\n-  Slice z(\"z\");\n-  dbfull()->TEST_CompactRange(last-2, NULL, &z);\n-  // DEL eliminated, but v1 remains because we aren't compacting that level\n-  // (DEL can be eliminated because v2 hides v1).\n-  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ v2, v1 ]\");\n-  dbfull()->TEST_CompactRange(last-1, NULL, NULL);\n-  // Merging last-1 w/ last, so we are the base level for \"foo\", so\n-  // DEL is removed.  (as is v1).\n-  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ v2 ]\");\n-}\n-\n-TEST(DBTest, DeletionMarkers2) {\n-  Put(\"foo\", \"v1\");\n-  ASSERT_OK(dbfull()->TEST_CompactMemTable());\n-  const int last = config::kMaxMemCompactLevel;\n-  ASSERT_EQ(NumTableFilesAtLevel(last), 1);   // foo => v1 is now in last level\n-\n-  // Place a table at level last-1 to prevent merging with preceding mutation\n-  Put(\"a\", \"begin\");\n-  Put(\"z\", \"end\");\n-  dbfull()->TEST_CompactMemTable();\n-  ASSERT_EQ(NumTableFilesAtLevel(last), 1);\n-  ASSERT_EQ(NumTableFilesAtLevel(last-1), 1);\n-\n-  Delete(\"foo\");\n-  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ DEL, v1 ]\");\n-  ASSERT_OK(dbfull()->TEST_CompactMemTable());  // Moves to level last-2\n-  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ DEL, v1 ]\");\n-  dbfull()->TEST_CompactRange(last-2, NULL, NULL);\n-  // DEL kept: \"last\" file overlaps\n-  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ DEL, v1 ]\");\n-  dbfull()->TEST_CompactRange(last-1, NULL, NULL);\n-  // Merging last-1 w/ last, so we are the base level for \"foo\", so\n-  // DEL is removed.  (as is v1).\n-  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ ]\");\n-}\n-\n-TEST(DBTest, OverlapInLevel0) {\n-  do {\n-    ASSERT_EQ(config::kMaxMemCompactLevel, 2) << \"Fix test to match config\";\n-\n-    // Fill levels 1 and 2 to disable the pushing of new memtables to levels > 0.\n-    ASSERT_OK(Put(\"100\", \"v100\"));\n-    ASSERT_OK(Put(\"999\", \"v999\"));\n-    dbfull()->TEST_CompactMemTable();\n-    ASSERT_OK(Delete(\"100\"));\n-    ASSERT_OK(Delete(\"999\"));\n-    dbfull()->TEST_CompactMemTable();\n-    ASSERT_EQ(\"0,1,1\", FilesPerLevel());\n-\n-    // Make files spanning the following ranges in level-0:\n-    //  files[0]  200 .. 900\n-    //  files[1]  300 .. 500\n-    // Note that files are sorted by smallest key.\n-    ASSERT_OK(Put(\"300\", \"v300\"));\n-    ASSERT_OK(Put(\"500\", \"v500\"));\n-    dbfull()->TEST_CompactMemTable();\n-    ASSERT_OK(Put(\"200\", \"v200\"));\n-    ASSERT_OK(Put(\"600\", \"v600\"));\n-    ASSERT_OK(Put(\"900\", \"v900\"));\n-    dbfull()->TEST_CompactMemTable();\n-    ASSERT_EQ(\"2,1,1\", FilesPerLevel());\n-\n-    // Compact away the placeholder files we created initially\n-    dbfull()->TEST_CompactRange(1, NULL, NULL);\n-    dbfull()->TEST_CompactRange(2, NULL, NULL);\n-    ASSERT_EQ(\"2\", FilesPerLevel());\n-\n-    // Do a memtable compaction.  Before bug-fix, the compaction would\n-    // not detect the overlap with level-0 files and would incorrectly place\n-    // the deletion in a deeper level.\n-    ASSERT_OK(Delete(\"600\"));\n-    dbfull()->TEST_CompactMemTable();\n-    ASSERT_EQ(\"3\", FilesPerLevel());\n-    ASSERT_EQ(\"NOT_FOUND\", Get(\"600\"));\n-  } while (ChangeOptions());\n-}\n-\n-TEST(DBTest, L0_CompactionBug_Issue44_a) {\n-  Reopen();\n-  ASSERT_OK(Put(\"b\", \"v\"));\n-  Reopen();\n-  ASSERT_OK(Delete(\"b\"));\n-  ASSERT_OK(Delete(\"a\"));\n-  Reopen();\n-  ASSERT_OK(Delete(\"a\"));\n-  Reopen();\n-  ASSERT_OK(Put(\"a\", \"v\"));\n-  Reopen();\n-  Reopen();\n-  ASSERT_EQ(\"(a->v)\", Contents());\n-  env_->SleepForMicroseconds(1000000);  // Wait for compaction to finish\n-  ASSERT_EQ(\"(a->v)\", Contents());\n-}\n-\n-TEST(DBTest, L0_CompactionBug_Issue44_b) {\n-  Reopen();\n-  Put(\"\",\"\");\n-  Reopen();\n-  Delete(\"e\");\n-  Put(\"\",\"\");\n-  Reopen();\n-  Put(\"c\", \"cv\");\n-  Reopen();\n-  Put(\"\",\"\");\n-  Reopen();\n-  Put(\"\",\"\");\n-  env_->SleepForMicroseconds(1000000);  // Wait for compaction to finish\n-  Reopen();\n-  Put(\"d\",\"dv\");\n-  Reopen();\n-  Put(\"\",\"\");\n-  Reopen();\n-  Delete(\"d\");\n-  Delete(\"b\");\n-  Reopen();\n-  ASSERT_EQ(\"(->)(c->cv)\", Contents());\n-  env_->SleepForMicroseconds(1000000);  // Wait for compaction to finish\n-  ASSERT_EQ(\"(->)(c->cv)\", Contents());\n-}\n-\n-TEST(DBTest, ComparatorCheck) {\n-  class NewComparator : public Comparator {\n-   public:\n-    virtual const char* Name() const { return \"leveldb.NewComparator\"; }\n-    virtual int Compare(const Slice& a, const Slice& b) const {\n-      return BytewiseComparator()->Compare(a, b);\n-    }\n-    virtual void FindShortestSeparator(std::string* s, const Slice& l) const {\n-      BytewiseComparator()->FindShortestSeparator(s, l);\n-    }\n-    virtual void FindShortSuccessor(std::string* key) const {\n-      BytewiseComparator()->FindShortSuccessor(key);\n-    }\n-  };\n-  NewComparator cmp;\n-  Options new_options = CurrentOptions();\n-  new_options.comparator = &cmp;\n-  Status s = TryReopen(&new_options);\n-  ASSERT_TRUE(!s.ok());\n-  ASSERT_TRUE(s.ToString().find(\"comparator\") != std::string::npos)\n-      << s.ToString();\n-}\n-\n-TEST(DBTest, CustomComparator) {\n-  class NumberComparator : public Comparator {\n-   public:\n-    virtual const char* Name() const { return \"test.NumberComparator\"; }\n-    virtual int Compare(const Slice& a, const Slice& b) const {\n-      return ToNumber(a) - ToNumber(b);\n-    }\n-    virtual void FindShortestSeparator(std::string* s, const Slice& l) const {\n-      ToNumber(*s);     // Check format\n-      ToNumber(l);      // Check format\n-    }\n-    virtual void FindShortSuccessor(std::string* key) const {\n-      ToNumber(*key);   // Check format\n-    }\n-   private:\n-    static int ToNumber(const Slice& x) {\n-      // Check that there are no extra characters.\n-      ASSERT_TRUE(x.size() >= 2 && x[0] == '[' && x[x.size()-1] == ']')\n-          << EscapeString(x);\n-      int val;\n-      char ignored;\n-      ASSERT_TRUE(sscanf(x.ToString().c_str(), \"[%i]%c\", &val, &ignored) == 1)\n-          << EscapeString(x);\n-      return val;\n-    }\n-  };\n-  NumberComparator cmp;\n-  Options new_options = CurrentOptions();\n-  new_options.create_if_missing = true;\n-  new_options.comparator = &cmp;\n-  new_options.filter_policy = NULL;     // Cannot use bloom filters\n-  new_options.write_buffer_size = 1000;  // Compact more often\n-  DestroyAndReopen(&new_options);\n-  ASSERT_OK(Put(\"[10]\", \"ten\"));\n-  ASSERT_OK(Put(\"[0x14]\", \"twenty\"));\n-  for (int i = 0; i < 2; i++) {\n-    ASSERT_EQ(\"ten\", Get(\"[10]\"));\n-    ASSERT_EQ(\"ten\", Get(\"[0xa]\"));\n-    ASSERT_EQ(\"twenty\", Get(\"[20]\"));\n-    ASSERT_EQ(\"twenty\", Get(\"[0x14]\"));\n-    ASSERT_EQ(\"NOT_FOUND\", Get(\"[15]\"));\n-    ASSERT_EQ(\"NOT_FOUND\", Get(\"[0xf]\"));\n-    Compact(\"[0]\", \"[9999]\");\n-  }\n-\n-  for (int run = 0; run < 2; run++) {\n-    for (int i = 0; i < 1000; i++) {\n-      char buf[100];\n-      snprintf(buf, sizeof(buf), \"[%d]\", i*10);\n-      ASSERT_OK(Put(buf, buf));\n-    }\n-    Compact(\"[0]\", \"[1000000]\");\n-  }\n-}\n-\n-TEST(DBTest, ManualCompaction) {\n-  ASSERT_EQ(config::kMaxMemCompactLevel, 2)\n-      << \"Need to update this test to match kMaxMemCompactLevel\";\n-\n-  MakeTables(3, \"p\", \"q\");\n-  ASSERT_EQ(\"1,1,1\", FilesPerLevel());\n-\n-  // Compaction range falls before files\n-  Compact(\"\", \"c\");\n-  ASSERT_EQ(\"1,1,1\", FilesPerLevel());\n-\n-  // Compaction range falls after files\n-  Compact(\"r\", \"z\");\n-  ASSERT_EQ(\"1,1,1\", FilesPerLevel());\n-\n-  // Compaction range overlaps files\n-  Compact(\"p1\", \"p9\");\n-  ASSERT_EQ(\"0,0,1\", FilesPerLevel());\n-\n-  // Populate a different range\n-  MakeTables(3, \"c\", \"e\");\n-  ASSERT_EQ(\"1,1,2\", FilesPerLevel());\n-\n-  // Compact just the new range\n-  Compact(\"b\", \"f\");\n-  ASSERT_EQ(\"0,0,2\", FilesPerLevel());\n-\n-  // Compact all\n-  MakeTables(1, \"a\", \"z\");\n-  ASSERT_EQ(\"0,1,2\", FilesPerLevel());\n-  db_->CompactRange(NULL, NULL);\n-  ASSERT_EQ(\"0,0,1\", FilesPerLevel());\n-}\n-\n-TEST(DBTest, DBOpen_Options) {\n-  std::string dbname = test::TmpDir() + \"/db_options_test\";\n-  DestroyDB(dbname, Options());\n-\n-  // Does not exist, and create_if_missing == false: error\n-  DB* db = NULL;\n-  Options opts;\n-  opts.create_if_missing = false;\n-  Status s = DB::Open(opts, dbname, &db);\n-  ASSERT_TRUE(strstr(s.ToString().c_str(), \"does not exist\") != NULL);\n-  ASSERT_TRUE(db == NULL);\n-\n-  // Does not exist, and create_if_missing == true: OK\n-  opts.create_if_missing = true;\n-  s = DB::Open(opts, dbname, &db);\n-  ASSERT_OK(s);\n-  ASSERT_TRUE(db != NULL);\n-\n-  delete db;\n-  db = NULL;\n-\n-  // Does exist, and error_if_exists == true: error\n-  opts.create_if_missing = false;\n-  opts.error_if_exists = true;\n-  s = DB::Open(opts, dbname, &db);\n-  ASSERT_TRUE(strstr(s.ToString().c_str(), \"exists\") != NULL);\n-  ASSERT_TRUE(db == NULL);\n-\n-  // Does exist, and error_if_exists == false: OK\n-  opts.create_if_missing = true;\n-  opts.error_if_exists = false;\n-  s = DB::Open(opts, dbname, &db);\n-  ASSERT_OK(s);\n-  ASSERT_TRUE(db != NULL);\n-\n-  delete db;\n-  db = NULL;\n-}\n-\n-TEST(DBTest, Locking) {\n-  DB* db2 = NULL;\n-  Status s = DB::Open(CurrentOptions(), dbname_, &db2);\n-  ASSERT_TRUE(!s.ok()) << \"Locking did not prevent re-opening db\";\n-}\n-\n-// Check that number of files does not grow when we are out of space\n-TEST(DBTest, NoSpace) {\n-  Options options = CurrentOptions();\n-  options.env = env_;\n-  Reopen(&options);\n-\n-  ASSERT_OK(Put(\"foo\", \"v1\"));\n-  ASSERT_EQ(\"v1\", Get(\"foo\"));\n-  Compact(\"a\", \"z\");\n-  const int num_files = CountFiles();\n-  env_->no_space_.Release_Store(env_);   // Force out-of-space errors\n-  env_->sleep_counter_.Reset();\n-  for (int i = 0; i < 5; i++) {\n-    for (int level = 0; level < config::kNumLevels-1; level++) {\n-      dbfull()->TEST_CompactRange(level, NULL, NULL);\n-    }\n-  }\n-  env_->no_space_.Release_Store(NULL);\n-  ASSERT_LT(CountFiles(), num_files + 3);\n-\n-  // Check that compaction attempts slept after errors\n-  ASSERT_GE(env_->sleep_counter_.Read(), 5);\n-}\n-\n-TEST(DBTest, NonWritableFileSystem) {\n-  Options options = CurrentOptions();\n-  options.write_buffer_size = 1000;\n-  options.env = env_;\n-  Reopen(&options);\n-  ASSERT_OK(Put(\"foo\", \"v1\"));\n-  env_->non_writable_.Release_Store(env_);  // Force errors for new files\n-  std::string big(100000, 'x');\n-  int errors = 0;\n-  for (int i = 0; i < 20; i++) {\n-    fprintf(stderr, \"iter %d; errors %d\\n\", i, errors);\n-    if (!Put(\"foo\", big).ok()) {\n-      errors++;\n-      env_->SleepForMicroseconds(100000);\n-    }\n-  }\n-  ASSERT_GT(errors, 0);\n-  env_->non_writable_.Release_Store(NULL);\n-}\n-\n-TEST(DBTest, ManifestWriteError) {\n-  // Test for the following problem:\n-  // (a) Compaction produces file F\n-  // (b) Log record containing F is written to MANIFEST file, but Sync() fails\n-  // (c) GC deletes F\n-  // (d) After reopening DB, reads fail since deleted F is named in log record\n-\n-  // We iterate twice.  In the second iteration, everything is the\n-  // same except the log record never makes it to the MANIFEST file.\n-  for (int iter = 0; iter < 2; iter++) {\n-    port::AtomicPointer* error_type = (iter == 0)\n-        ? &env_->manifest_sync_error_\n-        : &env_->manifest_write_error_;\n-\n-    // Insert foo=>bar mapping\n-    Options options = CurrentOptions();\n-    options.env = env_;\n-    options.create_if_missing = true;\n-    options.error_if_exists = false;\n-    DestroyAndReopen(&options);\n-    ASSERT_OK(Put(\"foo\", \"bar\"));\n-    ASSERT_EQ(\"bar\", Get(\"foo\"));\n-\n-    // Memtable compaction (will succeed)\n-    dbfull()->TEST_CompactMemTable();\n-    ASSERT_EQ(\"bar\", Get(\"foo\"));\n-    const int last = config::kMaxMemCompactLevel;\n-    ASSERT_EQ(NumTableFilesAtLevel(last), 1);   // foo=>bar is now in last level\n-\n-    // Merging compaction (will fail)\n-    error_type->Release_Store(env_);\n-    dbfull()->TEST_CompactRange(last, NULL, NULL);  // Should fail\n-    ASSERT_EQ(\"bar\", Get(\"foo\"));\n-\n-    // Recovery: should not lose data\n-    error_type->Release_Store(NULL);\n-    Reopen(&options);\n-    ASSERT_EQ(\"bar\", Get(\"foo\"));\n-  }\n-}\n-\n-TEST(DBTest, FilesDeletedAfterCompaction) {\n-  ASSERT_OK(Put(\"foo\", \"v2\"));\n-  Compact(\"a\", \"z\");\n-  const int num_files = CountFiles();\n-  for (int i = 0; i < 10; i++) {\n-    ASSERT_OK(Put(\"foo\", \"v2\"));\n-    Compact(\"a\", \"z\");\n-  }\n-  ASSERT_EQ(CountFiles(), num_files);\n-}\n-\n-TEST(DBTest, BloomFilter) {\n-  env_->count_random_reads_ = true;\n-  Options options = CurrentOptions();\n-  options.env = env_;\n-  options.block_cache = NewLRUCache(0);  // Prevent cache hits\n-  options.filter_policy = NewBloomFilterPolicy(10);\n-  Reopen(&options);\n-\n-  // Populate multiple layers\n-  const int N = 10000;\n-  for (int i = 0; i < N; i++) {\n-    ASSERT_OK(Put(Key(i), Key(i)));\n-  }\n-  Compact(\"a\", \"z\");\n-  for (int i = 0; i < N; i += 100) {\n-    ASSERT_OK(Put(Key(i), Key(i)));\n-  }\n-  dbfull()->TEST_CompactMemTable();\n-\n-  // Prevent auto compactions triggered by seeks\n-  env_->delay_sstable_sync_.Release_Store(env_);\n-\n-  // Lookup present keys.  Should rarely read from small sstable.\n-  env_->random_read_counter_.Reset();\n-  for (int i = 0; i < N; i++) {\n-    ASSERT_EQ(Key(i), Get(Key(i)));\n-  }\n-  int reads = env_->random_read_counter_.Read();\n-  fprintf(stderr, \"%d present => %d reads\\n\", N, reads);\n-  ASSERT_GE(reads, N);\n-  ASSERT_LE(reads, N + 2*N/100);\n-\n-  // Lookup present keys.  Should rarely read from either sstable.\n-  env_->random_read_counter_.Reset();\n-  for (int i = 0; i < N; i++) {\n-    ASSERT_EQ(\"NOT_FOUND\", Get(Key(i) + \".missing\"));\n-  }\n-  reads = env_->random_read_counter_.Read();\n-  fprintf(stderr, \"%d missing => %d reads\\n\", N, reads);\n-  ASSERT_LE(reads, 3*N/100);\n-\n-  env_->delay_sstable_sync_.Release_Store(NULL);\n-  Close();\n-  delete options.block_cache;\n-  delete options.filter_policy;\n-}\n-\n-// Multi-threaded test:\n-namespace {\n-\n-static const int kNumThreads = 4;\n-static const int kTestSeconds = 10;\n-static const int kNumKeys = 1000;\n-\n-struct MTState {\n-  DBTest* test;\n-  port::AtomicPointer stop;\n-  port::AtomicPointer counter[kNumThreads];\n-  port::AtomicPointer thread_done[kNumThreads];\n-};\n-\n-struct MTThread {\n-  MTState* state;\n-  int id;\n-};\n-\n-static void MTThreadBody(void* arg) {\n-  MTThread* t = reinterpret_cast<MTThread*>(arg);\n-  int id = t->id;\n-  DB* db = t->state->test->db_;\n-  uintptr_t counter = 0;\n-  fprintf(stderr, \"... starting thread %d\\n\", id);\n-  Random rnd(1000 + id);\n-  std::string value;\n-  char valbuf[1500];\n-  while (t->state->stop.Acquire_Load() == NULL) {\n-    t->state->counter[id].Release_Store(reinterpret_cast<void*>(counter));\n-\n-    int key = rnd.Uniform(kNumKeys);\n-    char keybuf[20];\n-    snprintf(keybuf, sizeof(keybuf), \"%016d\", key);\n-\n-    if (rnd.OneIn(2)) {\n-      // Write values of the form <key, my id, counter>.\n-      // We add some padding for force compactions.\n-      snprintf(valbuf, sizeof(valbuf), \"%d.%d.%-1000d\",\n-               key, id, static_cast<int>(counter));\n-      ASSERT_OK(db->Put(WriteOptions(), Slice(keybuf), Slice(valbuf)));\n-    } else {\n-      // Read a value and verify that it matches the pattern written above.\n-      Status s = db->Get(ReadOptions(), Slice(keybuf), &value);\n-      if (s.IsNotFound()) {\n-        // Key has not yet been written\n-      } else {\n-        // Check that the writer thread counter is >= the counter in the value\n-        ASSERT_OK(s);\n-        int k, w, c;\n-        ASSERT_EQ(3, sscanf(value.c_str(), \"%d.%d.%d\", &k, &w, &c)) << value;\n-        ASSERT_EQ(k, key);\n-        ASSERT_GE(w, 0);\n-        ASSERT_LT(w, kNumThreads);\n-        ASSERT_LE(c, reinterpret_cast<uintptr_t>(\n-            t->state->counter[w].Acquire_Load()));\n-      }\n-    }\n-    counter++;\n-  }\n-  t->state->thread_done[id].Release_Store(t);\n-  fprintf(stderr, \"... stopping thread %d after %d ops\\n\", id, int(counter));\n-}\n-\n-}  // namespace\n-\n-TEST(DBTest, MultiThreaded) {\n-  do {\n-    // Initialize state\n-    MTState mt;\n-    mt.test = this;\n-    mt.stop.Release_Store(0);\n-    for (int id = 0; id < kNumThreads; id++) {\n-      mt.counter[id].Release_Store(0);\n-      mt.thread_done[id].Release_Store(0);\n-    }\n-\n-    // Start threads\n-    MTThread thread[kNumThreads];\n-    for (int id = 0; id < kNumThreads; id++) {\n-      thread[id].state = &mt;\n-      thread[id].id = id;\n-      env_->StartThread(MTThreadBody, &thread[id]);\n-    }\n-\n-    // Let them run for a while\n-    env_->SleepForMicroseconds(kTestSeconds * 1000000);\n-\n-    // Stop the threads and wait for them to finish\n-    mt.stop.Release_Store(&mt);\n-    for (int id = 0; id < kNumThreads; id++) {\n-      while (mt.thread_done[id].Acquire_Load() == NULL) {\n-        env_->SleepForMicroseconds(100000);\n-      }\n-    }\n-  } while (ChangeOptions());\n-}\n-\n-namespace {\n-typedef std::map<std::string, std::string> KVMap;\n-}\n-\n-class ModelDB: public DB {\n- public:\n-  class ModelSnapshot : public Snapshot {\n-   public:\n-    KVMap map_;\n-  };\n-\n-  explicit ModelDB(const Options& options): options_(options) { }\n-  ~ModelDB() { }\n-  virtual Status Put(const WriteOptions& o, const Slice& k, const Slice& v) {\n-    return DB::Put(o, k, v);\n-  }\n-  virtual Status Delete(const WriteOptions& o, const Slice& key) {\n-    return DB::Delete(o, key);\n-  }\n-  virtual Status Get(const ReadOptions& options,\n-                     const Slice& key, std::string* value) {\n-    assert(false);      // Not implemented\n-    return Status::NotFound(key);\n-  }\n-  virtual Iterator* NewIterator(const ReadOptions& options) {\n-    if (options.snapshot == NULL) {\n-      KVMap* saved = new KVMap;\n-      *saved = map_;\n-      return new ModelIter(saved, true);\n-    } else {\n-      const KVMap* snapshot_state =\n-          &(reinterpret_cast<const ModelSnapshot*>(options.snapshot)->map_);\n-      return new ModelIter(snapshot_state, false);\n-    }\n-  }\n-  virtual const Snapshot* GetSnapshot() {\n-    ModelSnapshot* snapshot = new ModelSnapshot;\n-    snapshot->map_ = map_;\n-    return snapshot;\n-  }\n-\n-  virtual void ReleaseSnapshot(const Snapshot* snapshot) {\n-    delete reinterpret_cast<const ModelSnapshot*>(snapshot);\n-  }\n-  virtual Status Write(const WriteOptions& options, WriteBatch* batch) {\n-    class Handler : public WriteBatch::Handler {\n-     public:\n-      KVMap* map_;\n-      virtual void Put(const Slice& key, const Slice& value) {\n-        (*map_)[key.ToString()] = value.ToString();\n-      }\n-      virtual void Delete(const Slice& key) {\n-        map_->erase(key.ToString());\n-      }\n-    };\n-    Handler handler;\n-    handler.map_ = &map_;\n-    return batch->Iterate(&handler);\n-  }\n-\n-  virtual bool GetProperty(const Slice& property, std::string* value) {\n-    return false;\n-  }\n-  virtual void GetApproximateSizes(const Range* r, int n, uint64_t* sizes) {\n-    for (int i = 0; i < n; i++) {\n-      sizes[i] = 0;\n-    }\n-  }\n-  virtual void CompactRange(const Slice* start, const Slice* end) {\n-  }\n-\n- private:\n-  class ModelIter: public Iterator {\n-   public:\n-    ModelIter(const KVMap* map, bool owned)\n-        : map_(map), owned_(owned), iter_(map_->end()) {\n-    }\n-    ~ModelIter() {\n-      if (owned_) delete map_;\n-    }\n-    virtual bool Valid() const { return iter_ != map_->end(); }\n-    virtual void SeekToFirst() { iter_ = map_->begin(); }\n-    virtual void SeekToLast() {\n-      if (map_->empty()) {\n-        iter_ = map_->end();\n-      } else {\n-        iter_ = map_->find(map_->rbegin()->first);\n-      }\n-    }\n-    virtual void Seek(const Slice& k) {\n-      iter_ = map_->lower_bound(k.ToString());\n-    }\n-    virtual void Next() { ++iter_; }\n-    virtual void Prev() { --iter_; }\n-    virtual Slice key() const { return iter_->first; }\n-    virtual Slice value() const { return iter_->second; }\n-    virtual Status status() const { return Status::OK(); }\n-   private:\n-    const KVMap* const map_;\n-    const bool owned_;  // Do we own map_\n-    KVMap::const_iterator iter_;\n-  };\n-  const Options options_;\n-  KVMap map_;\n-};\n-\n-static std::string RandomKey(Random* rnd) {\n-  int len = (rnd->OneIn(3)\n-             ? 1                // Short sometimes to encourage collisions\n-             : (rnd->OneIn(100) ? rnd->Skewed(10) : rnd->Uniform(10)));\n-  return test::RandomKey(rnd, len);\n-}\n-\n-static bool CompareIterators(int step,\n-                             DB* model,\n-                             DB* db,\n-                             const Snapshot* model_snap,\n-                             const Snapshot* db_snap) {\n-  ReadOptions options;\n-  options.snapshot = model_snap;\n-  Iterator* miter = model->NewIterator(options);\n-  options.snapshot = db_snap;\n-  Iterator* dbiter = db->NewIterator(options);\n-  bool ok = true;\n-  int count = 0;\n-  for (miter->SeekToFirst(), dbiter->SeekToFirst();\n-       ok && miter->Valid() && dbiter->Valid();\n-       miter->Next(), dbiter->Next()) {\n-    count++;\n-    if (miter->key().compare(dbiter->key()) != 0) {\n-      fprintf(stderr, \"step %d: Key mismatch: '%s' vs. '%s'\\n\",\n-              step,\n-              EscapeString(miter->key()).c_str(),\n-              EscapeString(dbiter->key()).c_str());\n-      ok = false;\n-      break;\n-    }\n-\n-    if (miter->value().compare(dbiter->value()) != 0) {\n-      fprintf(stderr, \"step %d: Value mismatch for key '%s': '%s' vs. '%s'\\n\",\n-              step,\n-              EscapeString(miter->key()).c_str(),\n-              EscapeString(miter->value()).c_str(),\n-              EscapeString(miter->value()).c_str());\n-      ok = false;\n-    }\n-  }\n-\n-  if (ok) {\n-    if (miter->Valid() != dbiter->Valid()) {\n-      fprintf(stderr, \"step %d: Mismatch at end of iterators: %d vs. %d\\n\",\n-              step, miter->Valid(), dbiter->Valid());\n-      ok = false;\n-    }\n-  }\n-  fprintf(stderr, \"%d entries compared: ok=%d\\n\", count, ok);\n-  delete miter;\n-  delete dbiter;\n-  return ok;\n-}\n-\n-TEST(DBTest, Randomized) {\n-  Random rnd(test::RandomSeed());\n-  do {\n-    ModelDB model(CurrentOptions());\n-    const int N = 10000;\n-    const Snapshot* model_snap = NULL;\n-    const Snapshot* db_snap = NULL;\n-    std::string k, v;\n-    for (int step = 0; step < N; step++) {\n-      if (step % 100 == 0) {\n-        fprintf(stderr, \"Step %d of %d\\n\", step, N);\n-      }\n-      // TODO(sanjay): Test Get() works\n-      int p = rnd.Uniform(100);\n-      if (p < 45) {                               // Put\n-        k = RandomKey(&rnd);\n-        v = RandomString(&rnd,\n-                         rnd.OneIn(20)\n-                         ? 100 + rnd.Uniform(100)\n-                         : rnd.Uniform(8));\n-        ASSERT_OK(model.Put(WriteOptions(), k, v));\n-        ASSERT_OK(db_->Put(WriteOptions(), k, v));\n-\n-      } else if (p < 90) {                        // Delete\n-        k = RandomKey(&rnd);\n-        ASSERT_OK(model.Delete(WriteOptions(), k));\n-        ASSERT_OK(db_->Delete(WriteOptions(), k));\n-\n-\n-      } else {                                    // Multi-element batch\n-        WriteBatch b;\n-        const int num = rnd.Uniform(8);\n-        for (int i = 0; i < num; i++) {\n-          if (i == 0 || !rnd.OneIn(10)) {\n-            k = RandomKey(&rnd);\n-          } else {\n-            // Periodically re-use the same key from the previous iter, so\n-            // we have multiple entries in the write batch for the same key\n-          }\n-          if (rnd.OneIn(2)) {\n-            v = RandomString(&rnd, rnd.Uniform(10));\n-            b.Put(k, v);\n-          } else {\n-            b.Delete(k);\n-          }\n-        }\n-        ASSERT_OK(model.Write(WriteOptions(), &b));\n-        ASSERT_OK(db_->Write(WriteOptions(), &b));\n-      }\n-\n-      if ((step % 100) == 0) {\n-        ASSERT_TRUE(CompareIterators(step, &model, db_, NULL, NULL));\n-        ASSERT_TRUE(CompareIterators(step, &model, db_, model_snap, db_snap));\n-        // Save a snapshot from each DB this time that we'll use next\n-        // time we compare things, to make sure the current state is\n-        // preserved with the snapshot\n-        if (model_snap != NULL) model.ReleaseSnapshot(model_snap);\n-        if (db_snap != NULL) db_->ReleaseSnapshot(db_snap);\n-\n-        Reopen();\n-        ASSERT_TRUE(CompareIterators(step, &model, db_, NULL, NULL));\n-\n-        model_snap = model.GetSnapshot();\n-        db_snap = db_->GetSnapshot();\n-      }\n-    }\n-    if (model_snap != NULL) model.ReleaseSnapshot(model_snap);\n-    if (db_snap != NULL) db_->ReleaseSnapshot(db_snap);\n-  } while (ChangeOptions());\n-}\n-\n-std::string MakeKey(unsigned int num) {\n-  char buf[30];\n-  snprintf(buf, sizeof(buf), \"%016u\", num);\n-  return std::string(buf);\n-}\n-\n-void BM_LogAndApply(int iters, int num_base_files) {\n-  std::string dbname = test::TmpDir() + \"/leveldb_test_benchmark\";\n-  DestroyDB(dbname, Options());\n-\n-  DB* db = NULL;\n-  Options opts;\n-  opts.create_if_missing = true;\n-  Status s = DB::Open(opts, dbname, &db);\n-  ASSERT_OK(s);\n-  ASSERT_TRUE(db != NULL);\n-\n-  delete db;\n-  db = NULL;\n-\n-  Env* env = Env::Default();\n-\n-  port::Mutex mu;\n-  MutexLock l(&mu);\n-\n-  InternalKeyComparator cmp(BytewiseComparator());\n-  Options options;\n-  VersionSet vset(dbname, &options, NULL, &cmp);\n-  ASSERT_OK(vset.Recover());\n-  VersionEdit vbase;\n-  uint64_t fnum = 1;\n-  for (int i = 0; i < num_base_files; i++) {\n-    InternalKey start(MakeKey(2*fnum), 1, kTypeValue);\n-    InternalKey limit(MakeKey(2*fnum+1), 1, kTypeDeletion);\n-    vbase.AddFile(2, fnum++, 1 /* file size */, start, limit);\n-  }\n-  ASSERT_OK(vset.LogAndApply(&vbase, &mu));\n-\n-  uint64_t start_micros = env->NowMicros();\n-\n-  for (int i = 0; i < iters; i++) {\n-    VersionEdit vedit;\n-    vedit.DeleteFile(2, fnum);\n-    InternalKey start(MakeKey(2*fnum), 1, kTypeValue);\n-    InternalKey limit(MakeKey(2*fnum+1), 1, kTypeDeletion);\n-    vedit.AddFile(2, fnum++, 1 /* file size */, start, limit);\n-    vset.LogAndApply(&vedit, &mu);\n-  }\n-  uint64_t stop_micros = env->NowMicros();\n-  unsigned int us = stop_micros - start_micros;\n-  char buf[16];\n-  snprintf(buf, sizeof(buf), \"%d\", num_base_files);\n-  fprintf(stderr,\n-          \"BM_LogAndApply/%-6s   %8d iters : %9u us (%7.0f us / iter)\\n\",\n-          buf, iters, us, ((float)us) / iters);\n-}\n-\n-}  // namespace leveldb\n-\n-int main(int argc, char** argv) {\n-  if (argc > 1 && std::string(argv[1]) == \"--benchmark\") {\n-    leveldb::BM_LogAndApply(1000, 1);\n-    leveldb::BM_LogAndApply(1000, 100);\n-    leveldb::BM_LogAndApply(1000, 10000);\n-    leveldb::BM_LogAndApply(100, 100000);\n-    return 0;\n-  }\n-\n-  return leveldb::test::RunAllTests();\n-}"
      },
      {
        "sha": "28e11b398d03bf855cc232030e406e90fa35c544",
        "filename": "src/leveldb/db/dbformat.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 140,
        "changes": 140,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/dbformat.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/dbformat.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/dbformat.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,140 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include <stdio.h>\n-#include \"db/dbformat.h\"\n-#include \"port/port.h\"\n-#include \"util/coding.h\"\n-\n-namespace leveldb {\n-\n-static uint64_t PackSequenceAndType(uint64_t seq, ValueType t) {\n-  assert(seq <= kMaxSequenceNumber);\n-  assert(t <= kValueTypeForSeek);\n-  return (seq << 8) | t;\n-}\n-\n-void AppendInternalKey(std::string* result, const ParsedInternalKey& key) {\n-  result->append(key.user_key.data(), key.user_key.size());\n-  PutFixed64(result, PackSequenceAndType(key.sequence, key.type));\n-}\n-\n-std::string ParsedInternalKey::DebugString() const {\n-  char buf[50];\n-  snprintf(buf, sizeof(buf), \"' @ %llu : %d\",\n-           (unsigned long long) sequence,\n-           int(type));\n-  std::string result = \"'\";\n-  result += user_key.ToString();\n-  result += buf;\n-  return result;\n-}\n-\n-std::string InternalKey::DebugString() const {\n-  std::string result;\n-  ParsedInternalKey parsed;\n-  if (ParseInternalKey(rep_, &parsed)) {\n-    result = parsed.DebugString();\n-  } else {\n-    result = \"(bad)\";\n-    result.append(EscapeString(rep_));\n-  }\n-  return result;\n-}\n-\n-const char* InternalKeyComparator::Name() const {\n-  return \"leveldb.InternalKeyComparator\";\n-}\n-\n-int InternalKeyComparator::Compare(const Slice& akey, const Slice& bkey) const {\n-  // Order by:\n-  //    increasing user key (according to user-supplied comparator)\n-  //    decreasing sequence number\n-  //    decreasing type (though sequence# should be enough to disambiguate)\n-  int r = user_comparator_->Compare(ExtractUserKey(akey), ExtractUserKey(bkey));\n-  if (r == 0) {\n-    const uint64_t anum = DecodeFixed64(akey.data() + akey.size() - 8);\n-    const uint64_t bnum = DecodeFixed64(bkey.data() + bkey.size() - 8);\n-    if (anum > bnum) {\n-      r = -1;\n-    } else if (anum < bnum) {\n-      r = +1;\n-    }\n-  }\n-  return r;\n-}\n-\n-void InternalKeyComparator::FindShortestSeparator(\n-      std::string* start,\n-      const Slice& limit) const {\n-  // Attempt to shorten the user portion of the key\n-  Slice user_start = ExtractUserKey(*start);\n-  Slice user_limit = ExtractUserKey(limit);\n-  std::string tmp(user_start.data(), user_start.size());\n-  user_comparator_->FindShortestSeparator(&tmp, user_limit);\n-  if (tmp.size() < user_start.size() &&\n-      user_comparator_->Compare(user_start, tmp) < 0) {\n-    // User key has become shorter physically, but larger logically.\n-    // Tack on the earliest possible number to the shortened user key.\n-    PutFixed64(&tmp, PackSequenceAndType(kMaxSequenceNumber,kValueTypeForSeek));\n-    assert(this->Compare(*start, tmp) < 0);\n-    assert(this->Compare(tmp, limit) < 0);\n-    start->swap(tmp);\n-  }\n-}\n-\n-void InternalKeyComparator::FindShortSuccessor(std::string* key) const {\n-  Slice user_key = ExtractUserKey(*key);\n-  std::string tmp(user_key.data(), user_key.size());\n-  user_comparator_->FindShortSuccessor(&tmp);\n-  if (tmp.size() < user_key.size() &&\n-      user_comparator_->Compare(user_key, tmp) < 0) {\n-    // User key has become shorter physically, but larger logically.\n-    // Tack on the earliest possible number to the shortened user key.\n-    PutFixed64(&tmp, PackSequenceAndType(kMaxSequenceNumber,kValueTypeForSeek));\n-    assert(this->Compare(*key, tmp) < 0);\n-    key->swap(tmp);\n-  }\n-}\n-\n-const char* InternalFilterPolicy::Name() const {\n-  return user_policy_->Name();\n-}\n-\n-void InternalFilterPolicy::CreateFilter(const Slice* keys, int n,\n-                                        std::string* dst) const {\n-  // We rely on the fact that the code in table.cc does not mind us\n-  // adjusting keys[].\n-  Slice* mkey = const_cast<Slice*>(keys);\n-  for (int i = 0; i < n; i++) {\n-    mkey[i] = ExtractUserKey(keys[i]);\n-    // TODO(sanjay): Suppress dups?\n-  }\n-  user_policy_->CreateFilter(keys, n, dst);\n-}\n-\n-bool InternalFilterPolicy::KeyMayMatch(const Slice& key, const Slice& f) const {\n-  return user_policy_->KeyMayMatch(ExtractUserKey(key), f);\n-}\n-\n-LookupKey::LookupKey(const Slice& user_key, SequenceNumber s) {\n-  size_t usize = user_key.size();\n-  size_t needed = usize + 13;  // A conservative estimate\n-  char* dst;\n-  if (needed <= sizeof(space_)) {\n-    dst = space_;\n-  } else {\n-    dst = new char[needed];\n-  }\n-  start_ = dst;\n-  dst = EncodeVarint32(dst, usize + 8);\n-  kstart_ = dst;\n-  memcpy(dst, user_key.data(), usize);\n-  dst += usize;\n-  EncodeFixed64(dst, PackSequenceAndType(s, kValueTypeForSeek));\n-  dst += 8;\n-  end_ = dst;\n-}\n-\n-}  // namespace leveldb"
      },
      {
        "sha": "f7f64dafb6d5b3d676ecde9d0fc86ab2942b2494",
        "filename": "src/leveldb/db/dbformat.h",
        "status": "removed",
        "additions": 0,
        "deletions": 227,
        "changes": 227,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/dbformat.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/dbformat.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/dbformat.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,227 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_DB_FORMAT_H_\n-#define STORAGE_LEVELDB_DB_FORMAT_H_\n-\n-#include <stdio.h>\n-#include \"leveldb/comparator.h\"\n-#include \"leveldb/db.h\"\n-#include \"leveldb/filter_policy.h\"\n-#include \"leveldb/slice.h\"\n-#include \"leveldb/table_builder.h\"\n-#include \"util/coding.h\"\n-#include \"util/logging.h\"\n-\n-namespace leveldb {\n-\n-// Grouping of constants.  We may want to make some of these\n-// parameters set via options.\n-namespace config {\n-static const int kNumLevels = 7;\n-\n-// Level-0 compaction is started when we hit this many files.\n-static const int kL0_CompactionTrigger = 4;\n-\n-// Soft limit on number of level-0 files.  We slow down writes at this point.\n-static const int kL0_SlowdownWritesTrigger = 8;\n-\n-// Maximum number of level-0 files.  We stop writes at this point.\n-static const int kL0_StopWritesTrigger = 12;\n-\n-// Maximum level to which a new compacted memtable is pushed if it\n-// does not create overlap.  We try to push to level 2 to avoid the\n-// relatively expensive level 0=>1 compactions and to avoid some\n-// expensive manifest file operations.  We do not push all the way to\n-// the largest level since that can generate a lot of wasted disk\n-// space if the same key space is being repeatedly overwritten.\n-static const int kMaxMemCompactLevel = 2;\n-\n-}  // namespace config\n-\n-class InternalKey;\n-\n-// Value types encoded as the last component of internal keys.\n-// DO NOT CHANGE THESE ENUM VALUES: they are embedded in the on-disk\n-// data structures.\n-enum ValueType {\n-  kTypeDeletion = 0x0,\n-  kTypeValue = 0x1\n-};\n-// kValueTypeForSeek defines the ValueType that should be passed when\n-// constructing a ParsedInternalKey object for seeking to a particular\n-// sequence number (since we sort sequence numbers in decreasing order\n-// and the value type is embedded as the low 8 bits in the sequence\n-// number in internal keys, we need to use the highest-numbered\n-// ValueType, not the lowest).\n-static const ValueType kValueTypeForSeek = kTypeValue;\n-\n-typedef uint64_t SequenceNumber;\n-\n-// We leave eight bits empty at the bottom so a type and sequence#\n-// can be packed together into 64-bits.\n-static const SequenceNumber kMaxSequenceNumber =\n-    ((0x1ull << 56) - 1);\n-\n-struct ParsedInternalKey {\n-  Slice user_key;\n-  SequenceNumber sequence;\n-  ValueType type;\n-\n-  ParsedInternalKey() { }  // Intentionally left uninitialized (for speed)\n-  ParsedInternalKey(const Slice& u, const SequenceNumber& seq, ValueType t)\n-      : user_key(u), sequence(seq), type(t) { }\n-  std::string DebugString() const;\n-};\n-\n-// Return the length of the encoding of \"key\".\n-inline size_t InternalKeyEncodingLength(const ParsedInternalKey& key) {\n-  return key.user_key.size() + 8;\n-}\n-\n-// Append the serialization of \"key\" to *result.\n-extern void AppendInternalKey(std::string* result,\n-                              const ParsedInternalKey& key);\n-\n-// Attempt to parse an internal key from \"internal_key\".  On success,\n-// stores the parsed data in \"*result\", and returns true.\n-//\n-// On error, returns false, leaves \"*result\" in an undefined state.\n-extern bool ParseInternalKey(const Slice& internal_key,\n-                             ParsedInternalKey* result);\n-\n-// Returns the user key portion of an internal key.\n-inline Slice ExtractUserKey(const Slice& internal_key) {\n-  assert(internal_key.size() >= 8);\n-  return Slice(internal_key.data(), internal_key.size() - 8);\n-}\n-\n-inline ValueType ExtractValueType(const Slice& internal_key) {\n-  assert(internal_key.size() >= 8);\n-  const size_t n = internal_key.size();\n-  uint64_t num = DecodeFixed64(internal_key.data() + n - 8);\n-  unsigned char c = num & 0xff;\n-  return static_cast<ValueType>(c);\n-}\n-\n-// A comparator for internal keys that uses a specified comparator for\n-// the user key portion and breaks ties by decreasing sequence number.\n-class InternalKeyComparator : public Comparator {\n- private:\n-  const Comparator* user_comparator_;\n- public:\n-  explicit InternalKeyComparator(const Comparator* c) : user_comparator_(c) { }\n-  virtual const char* Name() const;\n-  virtual int Compare(const Slice& a, const Slice& b) const;\n-  virtual void FindShortestSeparator(\n-      std::string* start,\n-      const Slice& limit) const;\n-  virtual void FindShortSuccessor(std::string* key) const;\n-\n-  const Comparator* user_comparator() const { return user_comparator_; }\n-\n-  int Compare(const InternalKey& a, const InternalKey& b) const;\n-};\n-\n-// Filter policy wrapper that converts from internal keys to user keys\n-class InternalFilterPolicy : public FilterPolicy {\n- private:\n-  const FilterPolicy* const user_policy_;\n- public:\n-  explicit InternalFilterPolicy(const FilterPolicy* p) : user_policy_(p) { }\n-  virtual const char* Name() const;\n-  virtual void CreateFilter(const Slice* keys, int n, std::string* dst) const;\n-  virtual bool KeyMayMatch(const Slice& key, const Slice& filter) const;\n-};\n-\n-// Modules in this directory should keep internal keys wrapped inside\n-// the following class instead of plain strings so that we do not\n-// incorrectly use string comparisons instead of an InternalKeyComparator.\n-class InternalKey {\n- private:\n-  std::string rep_;\n- public:\n-  InternalKey() { }   // Leave rep_ as empty to indicate it is invalid\n-  InternalKey(const Slice& user_key, SequenceNumber s, ValueType t) {\n-    AppendInternalKey(&rep_, ParsedInternalKey(user_key, s, t));\n-  }\n-\n-  void DecodeFrom(const Slice& s) { rep_.assign(s.data(), s.size()); }\n-  Slice Encode() const {\n-    assert(!rep_.empty());\n-    return rep_;\n-  }\n-\n-  Slice user_key() const { return ExtractUserKey(rep_); }\n-\n-  void SetFrom(const ParsedInternalKey& p) {\n-    rep_.clear();\n-    AppendInternalKey(&rep_, p);\n-  }\n-\n-  void Clear() { rep_.clear(); }\n-\n-  std::string DebugString() const;\n-};\n-\n-inline int InternalKeyComparator::Compare(\n-    const InternalKey& a, const InternalKey& b) const {\n-  return Compare(a.Encode(), b.Encode());\n-}\n-\n-inline bool ParseInternalKey(const Slice& internal_key,\n-                             ParsedInternalKey* result) {\n-  const size_t n = internal_key.size();\n-  if (n < 8) return false;\n-  uint64_t num = DecodeFixed64(internal_key.data() + n - 8);\n-  unsigned char c = num & 0xff;\n-  result->sequence = num >> 8;\n-  result->type = static_cast<ValueType>(c);\n-  result->user_key = Slice(internal_key.data(), n - 8);\n-  return (c <= static_cast<unsigned char>(kTypeValue));\n-}\n-\n-// A helper class useful for DBImpl::Get()\n-class LookupKey {\n- public:\n-  // Initialize *this for looking up user_key at a snapshot with\n-  // the specified sequence number.\n-  LookupKey(const Slice& user_key, SequenceNumber sequence);\n-\n-  ~LookupKey();\n-\n-  // Return a key suitable for lookup in a MemTable.\n-  Slice memtable_key() const { return Slice(start_, end_ - start_); }\n-\n-  // Return an internal key (suitable for passing to an internal iterator)\n-  Slice internal_key() const { return Slice(kstart_, end_ - kstart_); }\n-\n-  // Return the user key\n-  Slice user_key() const { return Slice(kstart_, end_ - kstart_ - 8); }\n-\n- private:\n-  // We construct a char array of the form:\n-  //    klength  varint32               <-- start_\n-  //    userkey  char[klength]          <-- kstart_\n-  //    tag      uint64\n-  //                                    <-- end_\n-  // The array is a suitable MemTable key.\n-  // The suffix starting with \"userkey\" can be used as an InternalKey.\n-  const char* start_;\n-  const char* kstart_;\n-  const char* end_;\n-  char space_[200];      // Avoid allocation for short keys\n-\n-  // No copying allowed\n-  LookupKey(const LookupKey&);\n-  void operator=(const LookupKey&);\n-};\n-\n-inline LookupKey::~LookupKey() {\n-  if (start_ != space_) delete[] start_;\n-}\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_DB_FORMAT_H_"
      },
      {
        "sha": "5d82f5d313fad88ea4e1d079427bba13df667cfe",
        "filename": "src/leveldb/db/dbformat_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 112,
        "changes": 112,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/dbformat_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/dbformat_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/dbformat_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,112 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/dbformat.h\"\n-#include \"util/logging.h\"\n-#include \"util/testharness.h\"\n-\n-namespace leveldb {\n-\n-static std::string IKey(const std::string& user_key,\n-                        uint64_t seq,\n-                        ValueType vt) {\n-  std::string encoded;\n-  AppendInternalKey(&encoded, ParsedInternalKey(user_key, seq, vt));\n-  return encoded;\n-}\n-\n-static std::string Shorten(const std::string& s, const std::string& l) {\n-  std::string result = s;\n-  InternalKeyComparator(BytewiseComparator()).FindShortestSeparator(&result, l);\n-  return result;\n-}\n-\n-static std::string ShortSuccessor(const std::string& s) {\n-  std::string result = s;\n-  InternalKeyComparator(BytewiseComparator()).FindShortSuccessor(&result);\n-  return result;\n-}\n-\n-static void TestKey(const std::string& key,\n-                    uint64_t seq,\n-                    ValueType vt) {\n-  std::string encoded = IKey(key, seq, vt);\n-\n-  Slice in(encoded);\n-  ParsedInternalKey decoded(\"\", 0, kTypeValue);\n-\n-  ASSERT_TRUE(ParseInternalKey(in, &decoded));\n-  ASSERT_EQ(key, decoded.user_key.ToString());\n-  ASSERT_EQ(seq, decoded.sequence);\n-  ASSERT_EQ(vt, decoded.type);\n-\n-  ASSERT_TRUE(!ParseInternalKey(Slice(\"bar\"), &decoded));\n-}\n-\n-class FormatTest { };\n-\n-TEST(FormatTest, InternalKey_EncodeDecode) {\n-  const char* keys[] = { \"\", \"k\", \"hello\", \"longggggggggggggggggggggg\" };\n-  const uint64_t seq[] = {\n-    1, 2, 3,\n-    (1ull << 8) - 1, 1ull << 8, (1ull << 8) + 1,\n-    (1ull << 16) - 1, 1ull << 16, (1ull << 16) + 1,\n-    (1ull << 32) - 1, 1ull << 32, (1ull << 32) + 1\n-  };\n-  for (int k = 0; k < sizeof(keys) / sizeof(keys[0]); k++) {\n-    for (int s = 0; s < sizeof(seq) / sizeof(seq[0]); s++) {\n-      TestKey(keys[k], seq[s], kTypeValue);\n-      TestKey(\"hello\", 1, kTypeDeletion);\n-    }\n-  }\n-}\n-\n-TEST(FormatTest, InternalKeyShortSeparator) {\n-  // When user keys are same\n-  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n-            Shorten(IKey(\"foo\", 100, kTypeValue),\n-                    IKey(\"foo\", 99, kTypeValue)));\n-  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n-            Shorten(IKey(\"foo\", 100, kTypeValue),\n-                    IKey(\"foo\", 101, kTypeValue)));\n-  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n-            Shorten(IKey(\"foo\", 100, kTypeValue),\n-                    IKey(\"foo\", 100, kTypeValue)));\n-  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n-            Shorten(IKey(\"foo\", 100, kTypeValue),\n-                    IKey(\"foo\", 100, kTypeDeletion)));\n-\n-  // When user keys are misordered\n-  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n-            Shorten(IKey(\"foo\", 100, kTypeValue),\n-                    IKey(\"bar\", 99, kTypeValue)));\n-\n-  // When user keys are different, but correctly ordered\n-  ASSERT_EQ(IKey(\"g\", kMaxSequenceNumber, kValueTypeForSeek),\n-            Shorten(IKey(\"foo\", 100, kTypeValue),\n-                    IKey(\"hello\", 200, kTypeValue)));\n-\n-  // When start user key is prefix of limit user key\n-  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n-            Shorten(IKey(\"foo\", 100, kTypeValue),\n-                    IKey(\"foobar\", 200, kTypeValue)));\n-\n-  // When limit user key is prefix of start user key\n-  ASSERT_EQ(IKey(\"foobar\", 100, kTypeValue),\n-            Shorten(IKey(\"foobar\", 100, kTypeValue),\n-                    IKey(\"foo\", 200, kTypeValue)));\n-}\n-\n-TEST(FormatTest, InternalKeyShortestSuccessor) {\n-  ASSERT_EQ(IKey(\"g\", kMaxSequenceNumber, kValueTypeForSeek),\n-            ShortSuccessor(IKey(\"foo\", 100, kTypeValue)));\n-  ASSERT_EQ(IKey(\"\\xff\\xff\", 100, kTypeValue),\n-            ShortSuccessor(IKey(\"\\xff\\xff\", 100, kTypeValue)));\n-}\n-\n-}  // namespace leveldb\n-\n-int main(int argc, char** argv) {\n-  return leveldb::test::RunAllTests();\n-}"
      },
      {
        "sha": "3c4d49f64eb6dfbf9d3740cbfda3f6ad218a4f52",
        "filename": "src/leveldb/db/filename.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 139,
        "changes": 139,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/filename.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/filename.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/filename.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,139 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include <ctype.h>\n-#include <stdio.h>\n-#include \"db/filename.h\"\n-#include \"db/dbformat.h\"\n-#include \"leveldb/env.h\"\n-#include \"util/logging.h\"\n-\n-namespace leveldb {\n-\n-// A utility routine: write \"data\" to the named file and Sync() it.\n-extern Status WriteStringToFileSync(Env* env, const Slice& data,\n-                                    const std::string& fname);\n-\n-static std::string MakeFileName(const std::string& name, uint64_t number,\n-                                const char* suffix) {\n-  char buf[100];\n-  snprintf(buf, sizeof(buf), \"/%06llu.%s\",\n-           static_cast<unsigned long long>(number),\n-           suffix);\n-  return name + buf;\n-}\n-\n-std::string LogFileName(const std::string& name, uint64_t number) {\n-  assert(number > 0);\n-  return MakeFileName(name, number, \"log\");\n-}\n-\n-std::string TableFileName(const std::string& name, uint64_t number) {\n-  assert(number > 0);\n-  return MakeFileName(name, number, \"sst\");\n-}\n-\n-std::string DescriptorFileName(const std::string& dbname, uint64_t number) {\n-  assert(number > 0);\n-  char buf[100];\n-  snprintf(buf, sizeof(buf), \"/MANIFEST-%06llu\",\n-           static_cast<unsigned long long>(number));\n-  return dbname + buf;\n-}\n-\n-std::string CurrentFileName(const std::string& dbname) {\n-  return dbname + \"/CURRENT\";\n-}\n-\n-std::string LockFileName(const std::string& dbname) {\n-  return dbname + \"/LOCK\";\n-}\n-\n-std::string TempFileName(const std::string& dbname, uint64_t number) {\n-  assert(number > 0);\n-  return MakeFileName(dbname, number, \"dbtmp\");\n-}\n-\n-std::string InfoLogFileName(const std::string& dbname) {\n-  return dbname + \"/LOG\";\n-}\n-\n-// Return the name of the old info log file for \"dbname\".\n-std::string OldInfoLogFileName(const std::string& dbname) {\n-  return dbname + \"/LOG.old\";\n-}\n-\n-\n-// Owned filenames have the form:\n-//    dbname/CURRENT\n-//    dbname/LOCK\n-//    dbname/LOG\n-//    dbname/LOG.old\n-//    dbname/MANIFEST-[0-9]+\n-//    dbname/[0-9]+.(log|sst)\n-bool ParseFileName(const std::string& fname,\n-                   uint64_t* number,\n-                   FileType* type) {\n-  Slice rest(fname);\n-  if (rest == \"CURRENT\") {\n-    *number = 0;\n-    *type = kCurrentFile;\n-  } else if (rest == \"LOCK\") {\n-    *number = 0;\n-    *type = kDBLockFile;\n-  } else if (rest == \"LOG\" || rest == \"LOG.old\") {\n-    *number = 0;\n-    *type = kInfoLogFile;\n-  } else if (rest.starts_with(\"MANIFEST-\")) {\n-    rest.remove_prefix(strlen(\"MANIFEST-\"));\n-    uint64_t num;\n-    if (!ConsumeDecimalNumber(&rest, &num)) {\n-      return false;\n-    }\n-    if (!rest.empty()) {\n-      return false;\n-    }\n-    *type = kDescriptorFile;\n-    *number = num;\n-  } else {\n-    // Avoid strtoull() to keep filename format independent of the\n-    // current locale\n-    uint64_t num;\n-    if (!ConsumeDecimalNumber(&rest, &num)) {\n-      return false;\n-    }\n-    Slice suffix = rest;\n-    if (suffix == Slice(\".log\")) {\n-      *type = kLogFile;\n-    } else if (suffix == Slice(\".sst\")) {\n-      *type = kTableFile;\n-    } else if (suffix == Slice(\".dbtmp\")) {\n-      *type = kTempFile;\n-    } else {\n-      return false;\n-    }\n-    *number = num;\n-  }\n-  return true;\n-}\n-\n-Status SetCurrentFile(Env* env, const std::string& dbname,\n-                      uint64_t descriptor_number) {\n-  // Remove leading \"dbname/\" and add newline to manifest file name\n-  std::string manifest = DescriptorFileName(dbname, descriptor_number);\n-  Slice contents = manifest;\n-  assert(contents.starts_with(dbname + \"/\"));\n-  contents.remove_prefix(dbname.size() + 1);\n-  std::string tmp = TempFileName(dbname, descriptor_number);\n-  Status s = WriteStringToFileSync(env, contents.ToString() + \"\\n\", tmp);\n-  if (s.ok()) {\n-    s = env->RenameFile(tmp, CurrentFileName(dbname));\n-  }\n-  if (!s.ok()) {\n-    env->DeleteFile(tmp);\n-  }\n-  return s;\n-}\n-\n-}  // namespace leveldb"
      },
      {
        "sha": "d5d09b11468105324761761665bb8db72abf9540",
        "filename": "src/leveldb/db/filename.h",
        "status": "removed",
        "additions": 0,
        "deletions": 80,
        "changes": 80,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/filename.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/filename.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/filename.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,80 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// File names used by DB code\n-\n-#ifndef STORAGE_LEVELDB_DB_FILENAME_H_\n-#define STORAGE_LEVELDB_DB_FILENAME_H_\n-\n-#include <stdint.h>\n-#include <string>\n-#include \"leveldb/slice.h\"\n-#include \"leveldb/status.h\"\n-#include \"port/port.h\"\n-\n-namespace leveldb {\n-\n-class Env;\n-\n-enum FileType {\n-  kLogFile,\n-  kDBLockFile,\n-  kTableFile,\n-  kDescriptorFile,\n-  kCurrentFile,\n-  kTempFile,\n-  kInfoLogFile  // Either the current one, or an old one\n-};\n-\n-// Return the name of the log file with the specified number\n-// in the db named by \"dbname\".  The result will be prefixed with\n-// \"dbname\".\n-extern std::string LogFileName(const std::string& dbname, uint64_t number);\n-\n-// Return the name of the sstable with the specified number\n-// in the db named by \"dbname\".  The result will be prefixed with\n-// \"dbname\".\n-extern std::string TableFileName(const std::string& dbname, uint64_t number);\n-\n-// Return the name of the descriptor file for the db named by\n-// \"dbname\" and the specified incarnation number.  The result will be\n-// prefixed with \"dbname\".\n-extern std::string DescriptorFileName(const std::string& dbname,\n-                                      uint64_t number);\n-\n-// Return the name of the current file.  This file contains the name\n-// of the current manifest file.  The result will be prefixed with\n-// \"dbname\".\n-extern std::string CurrentFileName(const std::string& dbname);\n-\n-// Return the name of the lock file for the db named by\n-// \"dbname\".  The result will be prefixed with \"dbname\".\n-extern std::string LockFileName(const std::string& dbname);\n-\n-// Return the name of a temporary file owned by the db named \"dbname\".\n-// The result will be prefixed with \"dbname\".\n-extern std::string TempFileName(const std::string& dbname, uint64_t number);\n-\n-// Return the name of the info log file for \"dbname\".\n-extern std::string InfoLogFileName(const std::string& dbname);\n-\n-// Return the name of the old info log file for \"dbname\".\n-extern std::string OldInfoLogFileName(const std::string& dbname);\n-\n-// If filename is a leveldb file, store the type of the file in *type.\n-// The number encoded in the filename is stored in *number.  If the\n-// filename was successfully parsed, returns true.  Else return false.\n-extern bool ParseFileName(const std::string& filename,\n-                          uint64_t* number,\n-                          FileType* type);\n-\n-// Make the CURRENT file point to the descriptor file with the\n-// specified number.\n-extern Status SetCurrentFile(Env* env, const std::string& dbname,\n-                             uint64_t descriptor_number);\n-\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_DB_FILENAME_H_"
      },
      {
        "sha": "47353d6c9aaf699be82fc632cdb8f00f8dae50d0",
        "filename": "src/leveldb/db/filename_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 122,
        "changes": 122,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/filename_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/filename_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/filename_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,122 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/filename.h\"\n-\n-#include \"db/dbformat.h\"\n-#include \"port/port.h\"\n-#include \"util/logging.h\"\n-#include \"util/testharness.h\"\n-\n-namespace leveldb {\n-\n-class FileNameTest { };\n-\n-TEST(FileNameTest, Parse) {\n-  Slice db;\n-  FileType type;\n-  uint64_t number;\n-\n-  // Successful parses\n-  static struct {\n-    const char* fname;\n-    uint64_t number;\n-    FileType type;\n-  } cases[] = {\n-    { \"100.log\",            100,   kLogFile },\n-    { \"0.log\",              0,     kLogFile },\n-    { \"0.sst\",              0,     kTableFile },\n-    { \"CURRENT\",            0,     kCurrentFile },\n-    { \"LOCK\",               0,     kDBLockFile },\n-    { \"MANIFEST-2\",         2,     kDescriptorFile },\n-    { \"MANIFEST-7\",         7,     kDescriptorFile },\n-    { \"LOG\",                0,     kInfoLogFile },\n-    { \"LOG.old\",            0,     kInfoLogFile },\n-    { \"18446744073709551615.log\", 18446744073709551615ull, kLogFile },\n-  };\n-  for (int i = 0; i < sizeof(cases) / sizeof(cases[0]); i++) {\n-    std::string f = cases[i].fname;\n-    ASSERT_TRUE(ParseFileName(f, &number, &type)) << f;\n-    ASSERT_EQ(cases[i].type, type) << f;\n-    ASSERT_EQ(cases[i].number, number) << f;\n-  }\n-\n-  // Errors\n-  static const char* errors[] = {\n-    \"\",\n-    \"foo\",\n-    \"foo-dx-100.log\",\n-    \".log\",\n-    \"\",\n-    \"manifest\",\n-    \"CURREN\",\n-    \"CURRENTX\",\n-    \"MANIFES\",\n-    \"MANIFEST\",\n-    \"MANIFEST-\",\n-    \"XMANIFEST-3\",\n-    \"MANIFEST-3x\",\n-    \"LOC\",\n-    \"LOCKx\",\n-    \"LO\",\n-    \"LOGx\",\n-    \"18446744073709551616.log\",\n-    \"184467440737095516150.log\",\n-    \"100\",\n-    \"100.\",\n-    \"100.lop\"\n-  };\n-  for (int i = 0; i < sizeof(errors) / sizeof(errors[0]); i++) {\n-    std::string f = errors[i];\n-    ASSERT_TRUE(!ParseFileName(f, &number, &type)) << f;\n-  };\n-}\n-\n-TEST(FileNameTest, Construction) {\n-  uint64_t number;\n-  FileType type;\n-  std::string fname;\n-\n-  fname = CurrentFileName(\"foo\");\n-  ASSERT_EQ(\"foo/\", std::string(fname.data(), 4));\n-  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n-  ASSERT_EQ(0, number);\n-  ASSERT_EQ(kCurrentFile, type);\n-\n-  fname = LockFileName(\"foo\");\n-  ASSERT_EQ(\"foo/\", std::string(fname.data(), 4));\n-  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n-  ASSERT_EQ(0, number);\n-  ASSERT_EQ(kDBLockFile, type);\n-\n-  fname = LogFileName(\"foo\", 192);\n-  ASSERT_EQ(\"foo/\", std::string(fname.data(), 4));\n-  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n-  ASSERT_EQ(192, number);\n-  ASSERT_EQ(kLogFile, type);\n-\n-  fname = TableFileName(\"bar\", 200);\n-  ASSERT_EQ(\"bar/\", std::string(fname.data(), 4));\n-  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n-  ASSERT_EQ(200, number);\n-  ASSERT_EQ(kTableFile, type);\n-\n-  fname = DescriptorFileName(\"bar\", 100);\n-  ASSERT_EQ(\"bar/\", std::string(fname.data(), 4));\n-  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n-  ASSERT_EQ(100, number);\n-  ASSERT_EQ(kDescriptorFile, type);\n-\n-  fname = TempFileName(\"tmp\", 999);\n-  ASSERT_EQ(\"tmp/\", std::string(fname.data(), 4));\n-  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n-  ASSERT_EQ(999, number);\n-  ASSERT_EQ(kTempFile, type);\n-}\n-\n-}  // namespace leveldb\n-\n-int main(int argc, char** argv) {\n-  return leveldb::test::RunAllTests();\n-}"
      },
      {
        "sha": "995d76107a16efd9677ec6d2c125c04502fcec3d",
        "filename": "src/leveldb/db/leveldb_main.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 238,
        "changes": 238,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/leveldb_main.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/leveldb_main.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/leveldb_main.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,238 +0,0 @@\n-// Copyright (c) 2012 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include <stdio.h>\n-#include \"db/dbformat.h\"\n-#include \"db/filename.h\"\n-#include \"db/log_reader.h\"\n-#include \"db/version_edit.h\"\n-#include \"db/write_batch_internal.h\"\n-#include \"leveldb/env.h\"\n-#include \"leveldb/iterator.h\"\n-#include \"leveldb/options.h\"\n-#include \"leveldb/status.h\"\n-#include \"leveldb/table.h\"\n-#include \"leveldb/write_batch.h\"\n-#include \"util/logging.h\"\n-\n-namespace leveldb {\n-\n-namespace {\n-\n-bool GuessType(const std::string& fname, FileType* type) {\n-  size_t pos = fname.rfind('/');\n-  std::string basename;\n-  if (pos == std::string::npos) {\n-    basename = fname;\n-  } else {\n-    basename = std::string(fname.data() + pos + 1, fname.size() - pos - 1);\n-  }\n-  uint64_t ignored;\n-  return ParseFileName(basename, &ignored, type);\n-}\n-\n-// Notified when log reader encounters corruption.\n-class CorruptionReporter : public log::Reader::Reporter {\n- public:\n-  virtual void Corruption(size_t bytes, const Status& status) {\n-    printf(\"corruption: %d bytes; %s\\n\",\n-            static_cast<int>(bytes),\n-            status.ToString().c_str());\n-  }\n-};\n-\n-// Print contents of a log file. (*func)() is called on every record.\n-bool PrintLogContents(Env* env, const std::string& fname,\n-                      void (*func)(Slice)) {\n-  SequentialFile* file;\n-  Status s = env->NewSequentialFile(fname, &file);\n-  if (!s.ok()) {\n-    fprintf(stderr, \"%s\\n\", s.ToString().c_str());\n-    return false;\n-  }\n-  CorruptionReporter reporter;\n-  log::Reader reader(file, &reporter, true, 0);\n-  Slice record;\n-  std::string scratch;\n-  while (reader.ReadRecord(&record, &scratch)) {\n-    printf(\"--- offset %llu; \",\n-           static_cast<unsigned long long>(reader.LastRecordOffset()));\n-    (*func)(record);\n-  }\n-  delete file;\n-  return true;\n-}\n-\n-// Called on every item found in a WriteBatch.\n-class WriteBatchItemPrinter : public WriteBatch::Handler {\n- public:\n-  uint64_t offset_;\n-  uint64_t sequence_;\n-\n-  virtual void Put(const Slice& key, const Slice& value) {\n-    printf(\"  put '%s' '%s'\\n\",\n-           EscapeString(key).c_str(),\n-           EscapeString(value).c_str());\n-  }\n-  virtual void Delete(const Slice& key) {\n-    printf(\"  del '%s'\\n\",\n-           EscapeString(key).c_str());\n-  }\n-};\n-\n-\n-// Called on every log record (each one of which is a WriteBatch)\n-// found in a kLogFile.\n-static void WriteBatchPrinter(Slice record) {\n-  if (record.size() < 12) {\n-    printf(\"log record length %d is too small\\n\",\n-           static_cast<int>(record.size()));\n-    return;\n-  }\n-  WriteBatch batch;\n-  WriteBatchInternal::SetContents(&batch, record);\n-  printf(\"sequence %llu\\n\",\n-         static_cast<unsigned long long>(WriteBatchInternal::Sequence(&batch)));\n-  WriteBatchItemPrinter batch_item_printer;\n-  Status s = batch.Iterate(&batch_item_printer);\n-  if (!s.ok()) {\n-    printf(\"  error: %s\\n\", s.ToString().c_str());\n-  }\n-}\n-\n-bool DumpLog(Env* env, const std::string& fname) {\n-  return PrintLogContents(env, fname, WriteBatchPrinter);\n-}\n-\n-// Called on every log record (each one of which is a WriteBatch)\n-// found in a kDescriptorFile.\n-static void VersionEditPrinter(Slice record) {\n-  VersionEdit edit;\n-  Status s = edit.DecodeFrom(record);\n-  if (!s.ok()) {\n-    printf(\"%s\\n\", s.ToString().c_str());\n-    return;\n-  }\n-  printf(\"%s\", edit.DebugString().c_str());\n-}\n-\n-bool DumpDescriptor(Env* env, const std::string& fname) {\n-  return PrintLogContents(env, fname, VersionEditPrinter);\n-}\n-\n-bool DumpTable(Env* env, const std::string& fname) {\n-  uint64_t file_size;\n-  RandomAccessFile* file = NULL;\n-  Table* table = NULL;\n-  Status s = env->GetFileSize(fname, &file_size);\n-  if (s.ok()) {\n-    s = env->NewRandomAccessFile(fname, &file);\n-  }\n-  if (s.ok()) {\n-    // We use the default comparator, which may or may not match the\n-    // comparator used in this database. However this should not cause\n-    // problems since we only use Table operations that do not require\n-    // any comparisons.  In particular, we do not call Seek or Prev.\n-    s = Table::Open(Options(), file, file_size, &table);\n-  }\n-  if (!s.ok()) {\n-    fprintf(stderr, \"%s\\n\", s.ToString().c_str());\n-    delete table;\n-    delete file;\n-    return false;\n-  }\n-\n-  ReadOptions ro;\n-  ro.fill_cache = false;\n-  Iterator* iter = table->NewIterator(ro);\n-  for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n-    ParsedInternalKey key;\n-    if (!ParseInternalKey(iter->key(), &key)) {\n-      printf(\"badkey '%s' => '%s'\\n\",\n-             EscapeString(iter->key()).c_str(),\n-             EscapeString(iter->value()).c_str());\n-    } else {\n-      char kbuf[20];\n-      const char* type;\n-      if (key.type == kTypeDeletion) {\n-        type = \"del\";\n-      } else if (key.type == kTypeValue) {\n-        type = \"val\";\n-      } else {\n-        snprintf(kbuf, sizeof(kbuf), \"%d\", static_cast<int>(key.type));\n-        type = kbuf;\n-      }\n-      printf(\"'%s' @ %8llu : %s => '%s'\\n\",\n-             EscapeString(key.user_key).c_str(),\n-             static_cast<unsigned long long>(key.sequence),\n-             type,\n-             EscapeString(iter->value()).c_str());\n-    }\n-  }\n-  s = iter->status();\n-  if (!s.ok()) {\n-    printf(\"iterator error: %s\\n\", s.ToString().c_str());\n-  }\n-\n-  delete iter;\n-  delete table;\n-  delete file;\n-  return true;\n-}\n-\n-bool DumpFile(Env* env, const std::string& fname) {\n-  FileType ftype;\n-  if (!GuessType(fname, &ftype)) {\n-    fprintf(stderr, \"%s: unknown file type\\n\", fname.c_str());\n-    return false;\n-  }\n-  switch (ftype) {\n-    case kLogFile:         return DumpLog(env, fname);\n-    case kDescriptorFile:  return DumpDescriptor(env, fname);\n-    case kTableFile:       return DumpTable(env, fname);\n-\n-    default: {\n-      fprintf(stderr, \"%s: not a dump-able file type\\n\", fname.c_str());\n-      break;\n-    }\n-  }\n-  return false;\n-}\n-\n-bool HandleDumpCommand(Env* env, char** files, int num) {\n-  bool ok = true;\n-  for (int i = 0; i < num; i++) {\n-    ok &= DumpFile(env, files[i]);\n-  }\n-  return ok;\n-}\n-\n-}\n-}  // namespace leveldb\n-\n-static void Usage() {\n-  fprintf(\n-      stderr,\n-      \"Usage: leveldbutil command...\\n\"\n-      \"   dump files...         -- dump contents of specified files\\n\"\n-      );\n-}\n-\n-int main(int argc, char** argv) {\n-  leveldb::Env* env = leveldb::Env::Default();\n-  bool ok = true;\n-  if (argc < 2) {\n-    Usage();\n-    ok = false;\n-  } else {\n-    std::string command = argv[1];\n-    if (command == \"dump\") {\n-      ok = leveldb::HandleDumpCommand(env, argv+2, argc-2);\n-    } else {\n-      Usage();\n-      ok = false;\n-    }\n-  }\n-  return (ok ? 0 : 1);\n-}"
      },
      {
        "sha": "2690cb9789ee63e85260500a6736ec302d22f9b7",
        "filename": "src/leveldb/db/log_format.h",
        "status": "removed",
        "additions": 0,
        "deletions": 35,
        "changes": 35,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/log_format.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/log_format.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/log_format.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,35 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// Log format information shared by reader and writer.\n-// See ../doc/log_format.txt for more detail.\n-\n-#ifndef STORAGE_LEVELDB_DB_LOG_FORMAT_H_\n-#define STORAGE_LEVELDB_DB_LOG_FORMAT_H_\n-\n-namespace leveldb {\n-namespace log {\n-\n-enum RecordType {\n-  // Zero is reserved for preallocated files\n-  kZeroType = 0,\n-\n-  kFullType = 1,\n-\n-  // For fragments\n-  kFirstType = 2,\n-  kMiddleType = 3,\n-  kLastType = 4\n-};\n-static const int kMaxRecordType = kLastType;\n-\n-static const int kBlockSize = 32768;\n-\n-// Header is checksum (4 bytes), type (1 byte), length (2 bytes).\n-static const int kHeaderSize = 4 + 1 + 2;\n-\n-}  // namespace log\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_DB_LOG_FORMAT_H_"
      },
      {
        "sha": "b35f115aadac28055b16fce6f133bd5148ecec16",
        "filename": "src/leveldb/db/log_reader.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 259,
        "changes": 259,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/log_reader.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/log_reader.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/log_reader.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,259 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/log_reader.h\"\n-\n-#include <stdio.h>\n-#include \"leveldb/env.h\"\n-#include \"util/coding.h\"\n-#include \"util/crc32c.h\"\n-\n-namespace leveldb {\n-namespace log {\n-\n-Reader::Reporter::~Reporter() {\n-}\n-\n-Reader::Reader(SequentialFile* file, Reporter* reporter, bool checksum,\n-               uint64_t initial_offset)\n-    : file_(file),\n-      reporter_(reporter),\n-      checksum_(checksum),\n-      backing_store_(new char[kBlockSize]),\n-      buffer_(),\n-      eof_(false),\n-      last_record_offset_(0),\n-      end_of_buffer_offset_(0),\n-      initial_offset_(initial_offset) {\n-}\n-\n-Reader::~Reader() {\n-  delete[] backing_store_;\n-}\n-\n-bool Reader::SkipToInitialBlock() {\n-  size_t offset_in_block = initial_offset_ % kBlockSize;\n-  uint64_t block_start_location = initial_offset_ - offset_in_block;\n-\n-  // Don't search a block if we'd be in the trailer\n-  if (offset_in_block > kBlockSize - 6) {\n-    offset_in_block = 0;\n-    block_start_location += kBlockSize;\n-  }\n-\n-  end_of_buffer_offset_ = block_start_location;\n-\n-  // Skip to start of first block that can contain the initial record\n-  if (block_start_location > 0) {\n-    Status skip_status = file_->Skip(block_start_location);\n-    if (!skip_status.ok()) {\n-      ReportDrop(block_start_location, skip_status);\n-      return false;\n-    }\n-  }\n-\n-  return true;\n-}\n-\n-bool Reader::ReadRecord(Slice* record, std::string* scratch) {\n-  if (last_record_offset_ < initial_offset_) {\n-    if (!SkipToInitialBlock()) {\n-      return false;\n-    }\n-  }\n-\n-  scratch->clear();\n-  record->clear();\n-  bool in_fragmented_record = false;\n-  // Record offset of the logical record that we're reading\n-  // 0 is a dummy value to make compilers happy\n-  uint64_t prospective_record_offset = 0;\n-\n-  Slice fragment;\n-  while (true) {\n-    uint64_t physical_record_offset = end_of_buffer_offset_ - buffer_.size();\n-    const unsigned int record_type = ReadPhysicalRecord(&fragment);\n-    switch (record_type) {\n-      case kFullType:\n-        if (in_fragmented_record) {\n-          // Handle bug in earlier versions of log::Writer where\n-          // it could emit an empty kFirstType record at the tail end\n-          // of a block followed by a kFullType or kFirstType record\n-          // at the beginning of the next block.\n-          if (scratch->empty()) {\n-            in_fragmented_record = false;\n-          } else {\n-            ReportCorruption(scratch->size(), \"partial record without end(1)\");\n-          }\n-        }\n-        prospective_record_offset = physical_record_offset;\n-        scratch->clear();\n-        *record = fragment;\n-        last_record_offset_ = prospective_record_offset;\n-        return true;\n-\n-      case kFirstType:\n-        if (in_fragmented_record) {\n-          // Handle bug in earlier versions of log::Writer where\n-          // it could emit an empty kFirstType record at the tail end\n-          // of a block followed by a kFullType or kFirstType record\n-          // at the beginning of the next block.\n-          if (scratch->empty()) {\n-            in_fragmented_record = false;\n-          } else {\n-            ReportCorruption(scratch->size(), \"partial record without end(2)\");\n-          }\n-        }\n-        prospective_record_offset = physical_record_offset;\n-        scratch->assign(fragment.data(), fragment.size());\n-        in_fragmented_record = true;\n-        break;\n-\n-      case kMiddleType:\n-        if (!in_fragmented_record) {\n-          ReportCorruption(fragment.size(),\n-                           \"missing start of fragmented record(1)\");\n-        } else {\n-          scratch->append(fragment.data(), fragment.size());\n-        }\n-        break;\n-\n-      case kLastType:\n-        if (!in_fragmented_record) {\n-          ReportCorruption(fragment.size(),\n-                           \"missing start of fragmented record(2)\");\n-        } else {\n-          scratch->append(fragment.data(), fragment.size());\n-          *record = Slice(*scratch);\n-          last_record_offset_ = prospective_record_offset;\n-          return true;\n-        }\n-        break;\n-\n-      case kEof:\n-        if (in_fragmented_record) {\n-          ReportCorruption(scratch->size(), \"partial record without end(3)\");\n-          scratch->clear();\n-        }\n-        return false;\n-\n-      case kBadRecord:\n-        if (in_fragmented_record) {\n-          ReportCorruption(scratch->size(), \"error in middle of record\");\n-          in_fragmented_record = false;\n-          scratch->clear();\n-        }\n-        break;\n-\n-      default: {\n-        char buf[40];\n-        snprintf(buf, sizeof(buf), \"unknown record type %u\", record_type);\n-        ReportCorruption(\n-            (fragment.size() + (in_fragmented_record ? scratch->size() : 0)),\n-            buf);\n-        in_fragmented_record = false;\n-        scratch->clear();\n-        break;\n-      }\n-    }\n-  }\n-  return false;\n-}\n-\n-uint64_t Reader::LastRecordOffset() {\n-  return last_record_offset_;\n-}\n-\n-void Reader::ReportCorruption(size_t bytes, const char* reason) {\n-  ReportDrop(bytes, Status::Corruption(reason));\n-}\n-\n-void Reader::ReportDrop(size_t bytes, const Status& reason) {\n-  if (reporter_ != NULL &&\n-      end_of_buffer_offset_ - buffer_.size() - bytes >= initial_offset_) {\n-    reporter_->Corruption(bytes, reason);\n-  }\n-}\n-\n-unsigned int Reader::ReadPhysicalRecord(Slice* result) {\n-  while (true) {\n-    if (buffer_.size() < kHeaderSize) {\n-      if (!eof_) {\n-        // Last read was a full read, so this is a trailer to skip\n-        buffer_.clear();\n-        Status status = file_->Read(kBlockSize, &buffer_, backing_store_);\n-        end_of_buffer_offset_ += buffer_.size();\n-        if (!status.ok()) {\n-          buffer_.clear();\n-          ReportDrop(kBlockSize, status);\n-          eof_ = true;\n-          return kEof;\n-        } else if (buffer_.size() < kBlockSize) {\n-          eof_ = true;\n-        }\n-        continue;\n-      } else if (buffer_.size() == 0) {\n-        // End of file\n-        return kEof;\n-      } else {\n-        size_t drop_size = buffer_.size();\n-        buffer_.clear();\n-        ReportCorruption(drop_size, \"truncated record at end of file\");\n-        return kEof;\n-      }\n-    }\n-\n-    // Parse the header\n-    const char* header = buffer_.data();\n-    const uint32_t a = static_cast<uint32_t>(header[4]) & 0xff;\n-    const uint32_t b = static_cast<uint32_t>(header[5]) & 0xff;\n-    const unsigned int type = header[6];\n-    const uint32_t length = a | (b << 8);\n-    if (kHeaderSize + length > buffer_.size()) {\n-      size_t drop_size = buffer_.size();\n-      buffer_.clear();\n-      ReportCorruption(drop_size, \"bad record length\");\n-      return kBadRecord;\n-    }\n-\n-    if (type == kZeroType && length == 0) {\n-      // Skip zero length record without reporting any drops since\n-      // such records are produced by the mmap based writing code in\n-      // env_posix.cc that preallocates file regions.\n-      buffer_.clear();\n-      return kBadRecord;\n-    }\n-\n-    // Check crc\n-    if (checksum_) {\n-      uint32_t expected_crc = crc32c::Unmask(DecodeFixed32(header));\n-      uint32_t actual_crc = crc32c::Value(header + 6, 1 + length);\n-      if (actual_crc != expected_crc) {\n-        // Drop the rest of the buffer since \"length\" itself may have\n-        // been corrupted and if we trust it, we could find some\n-        // fragment of a real log record that just happens to look\n-        // like a valid log record.\n-        size_t drop_size = buffer_.size();\n-        buffer_.clear();\n-        ReportCorruption(drop_size, \"checksum mismatch\");\n-        return kBadRecord;\n-      }\n-    }\n-\n-    buffer_.remove_prefix(kHeaderSize + length);\n-\n-    // Skip physical record that started before initial_offset_\n-    if (end_of_buffer_offset_ - buffer_.size() - kHeaderSize - length <\n-        initial_offset_) {\n-      result->clear();\n-      return kBadRecord;\n-    }\n-\n-    *result = Slice(header + kHeaderSize, length);\n-    return type;\n-  }\n-}\n-\n-}  // namespace log\n-}  // namespace leveldb"
      },
      {
        "sha": "82d4bee68d0eea3a7fdd270e91b49545a536f45e",
        "filename": "src/leveldb/db/log_reader.h",
        "status": "removed",
        "additions": 0,
        "deletions": 108,
        "changes": 108,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/log_reader.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/log_reader.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/log_reader.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,108 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_DB_LOG_READER_H_\n-#define STORAGE_LEVELDB_DB_LOG_READER_H_\n-\n-#include <stdint.h>\n-\n-#include \"db/log_format.h\"\n-#include \"leveldb/slice.h\"\n-#include \"leveldb/status.h\"\n-\n-namespace leveldb {\n-\n-class SequentialFile;\n-\n-namespace log {\n-\n-class Reader {\n- public:\n-  // Interface for reporting errors.\n-  class Reporter {\n-   public:\n-    virtual ~Reporter();\n-\n-    // Some corruption was detected.  \"size\" is the approximate number\n-    // of bytes dropped due to the corruption.\n-    virtual void Corruption(size_t bytes, const Status& status) = 0;\n-  };\n-\n-  // Create a reader that will return log records from \"*file\".\n-  // \"*file\" must remain live while this Reader is in use.\n-  //\n-  // If \"reporter\" is non-NULL, it is notified whenever some data is\n-  // dropped due to a detected corruption.  \"*reporter\" must remain\n-  // live while this Reader is in use.\n-  //\n-  // If \"checksum\" is true, verify checksums if available.\n-  //\n-  // The Reader will start reading at the first record located at physical\n-  // position >= initial_offset within the file.\n-  Reader(SequentialFile* file, Reporter* reporter, bool checksum,\n-         uint64_t initial_offset);\n-\n-  ~Reader();\n-\n-  // Read the next record into *record.  Returns true if read\n-  // successfully, false if we hit end of the input.  May use\n-  // \"*scratch\" as temporary storage.  The contents filled in *record\n-  // will only be valid until the next mutating operation on this\n-  // reader or the next mutation to *scratch.\n-  bool ReadRecord(Slice* record, std::string* scratch);\n-\n-  // Returns the physical offset of the last record returned by ReadRecord.\n-  //\n-  // Undefined before the first call to ReadRecord.\n-  uint64_t LastRecordOffset();\n-\n- private:\n-  SequentialFile* const file_;\n-  Reporter* const reporter_;\n-  bool const checksum_;\n-  char* const backing_store_;\n-  Slice buffer_;\n-  bool eof_;   // Last Read() indicated EOF by returning < kBlockSize\n-\n-  // Offset of the last record returned by ReadRecord.\n-  uint64_t last_record_offset_;\n-  // Offset of the first location past the end of buffer_.\n-  uint64_t end_of_buffer_offset_;\n-\n-  // Offset at which to start looking for the first record to return\n-  uint64_t const initial_offset_;\n-\n-  // Extend record types with the following special values\n-  enum {\n-    kEof = kMaxRecordType + 1,\n-    // Returned whenever we find an invalid physical record.\n-    // Currently there are three situations in which this happens:\n-    // * The record has an invalid CRC (ReadPhysicalRecord reports a drop)\n-    // * The record is a 0-length record (No drop is reported)\n-    // * The record is below constructor's initial_offset (No drop is reported)\n-    kBadRecord = kMaxRecordType + 2\n-  };\n-\n-  // Skips all blocks that are completely before \"initial_offset_\".\n-  //\n-  // Returns true on success. Handles reporting.\n-  bool SkipToInitialBlock();\n-\n-  // Return type, or one of the preceding special values\n-  unsigned int ReadPhysicalRecord(Slice* result);\n-\n-  // Reports dropped bytes to the reporter.\n-  // buffer_ must be updated to remove the dropped bytes prior to invocation.\n-  void ReportCorruption(size_t bytes, const char* reason);\n-  void ReportDrop(size_t bytes, const Status& reason);\n-\n-  // No copying allowed\n-  Reader(const Reader&);\n-  void operator=(const Reader&);\n-};\n-\n-}  // namespace log\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_DB_LOG_READER_H_"
      },
      {
        "sha": "4c5cf875733c16175743224e642f0507a0da663f",
        "filename": "src/leveldb/db/log_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 500,
        "changes": 500,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/log_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/log_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/log_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,500 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/log_reader.h\"\n-#include \"db/log_writer.h\"\n-#include \"leveldb/env.h\"\n-#include \"util/coding.h\"\n-#include \"util/crc32c.h\"\n-#include \"util/random.h\"\n-#include \"util/testharness.h\"\n-\n-namespace leveldb {\n-namespace log {\n-\n-// Construct a string of the specified length made out of the supplied\n-// partial string.\n-static std::string BigString(const std::string& partial_string, size_t n) {\n-  std::string result;\n-  while (result.size() < n) {\n-    result.append(partial_string);\n-  }\n-  result.resize(n);\n-  return result;\n-}\n-\n-// Construct a string from a number\n-static std::string NumberString(int n) {\n-  char buf[50];\n-  snprintf(buf, sizeof(buf), \"%d.\", n);\n-  return std::string(buf);\n-}\n-\n-// Return a skewed potentially long string\n-static std::string RandomSkewedString(int i, Random* rnd) {\n-  return BigString(NumberString(i), rnd->Skewed(17));\n-}\n-\n-class LogTest {\n- private:\n-  class StringDest : public WritableFile {\n-   public:\n-    std::string contents_;\n-\n-    virtual Status Close() { return Status::OK(); }\n-    virtual Status Flush() { return Status::OK(); }\n-    virtual Status Sync() { return Status::OK(); }\n-    virtual Status Append(const Slice& slice) {\n-      contents_.append(slice.data(), slice.size());\n-      return Status::OK();\n-    }\n-  };\n-\n-  class StringSource : public SequentialFile {\n-   public:\n-    Slice contents_;\n-    bool force_error_;\n-    bool returned_partial_;\n-    StringSource() : force_error_(false), returned_partial_(false) { }\n-\n-    virtual Status Read(size_t n, Slice* result, char* scratch) {\n-      ASSERT_TRUE(!returned_partial_) << \"must not Read() after eof/error\";\n-\n-      if (force_error_) {\n-        force_error_ = false;\n-        returned_partial_ = true;\n-        return Status::Corruption(\"read error\");\n-      }\n-\n-      if (contents_.size() < n) {\n-        n = contents_.size();\n-        returned_partial_ = true;\n-      }\n-      *result = Slice(contents_.data(), n);\n-      contents_.remove_prefix(n);\n-      return Status::OK();\n-    }\n-\n-    virtual Status Skip(uint64_t n) {\n-      if (n > contents_.size()) {\n-        contents_.clear();\n-        return Status::NotFound(\"in-memory file skipepd past end\");\n-      }\n-\n-      contents_.remove_prefix(n);\n-\n-      return Status::OK();\n-    }\n-  };\n-\n-  class ReportCollector : public Reader::Reporter {\n-   public:\n-    size_t dropped_bytes_;\n-    std::string message_;\n-\n-    ReportCollector() : dropped_bytes_(0) { }\n-    virtual void Corruption(size_t bytes, const Status& status) {\n-      dropped_bytes_ += bytes;\n-      message_.append(status.ToString());\n-    }\n-  };\n-\n-  StringDest dest_;\n-  StringSource source_;\n-  ReportCollector report_;\n-  bool reading_;\n-  Writer writer_;\n-  Reader reader_;\n-\n-  // Record metadata for testing initial offset functionality\n-  static size_t initial_offset_record_sizes_[];\n-  static uint64_t initial_offset_last_record_offsets_[];\n-\n- public:\n-  LogTest() : reading_(false),\n-              writer_(&dest_),\n-              reader_(&source_, &report_, true/*checksum*/,\n-                      0/*initial_offset*/) {\n-  }\n-\n-  void Write(const std::string& msg) {\n-    ASSERT_TRUE(!reading_) << \"Write() after starting to read\";\n-    writer_.AddRecord(Slice(msg));\n-  }\n-\n-  size_t WrittenBytes() const {\n-    return dest_.contents_.size();\n-  }\n-\n-  std::string Read() {\n-    if (!reading_) {\n-      reading_ = true;\n-      source_.contents_ = Slice(dest_.contents_);\n-    }\n-    std::string scratch;\n-    Slice record;\n-    if (reader_.ReadRecord(&record, &scratch)) {\n-      return record.ToString();\n-    } else {\n-      return \"EOF\";\n-    }\n-  }\n-\n-  void IncrementByte(int offset, int delta) {\n-    dest_.contents_[offset] += delta;\n-  }\n-\n-  void SetByte(int offset, char new_byte) {\n-    dest_.contents_[offset] = new_byte;\n-  }\n-\n-  void ShrinkSize(int bytes) {\n-    dest_.contents_.resize(dest_.contents_.size() - bytes);\n-  }\n-\n-  void FixChecksum(int header_offset, int len) {\n-    // Compute crc of type/len/data\n-    uint32_t crc = crc32c::Value(&dest_.contents_[header_offset+6], 1 + len);\n-    crc = crc32c::Mask(crc);\n-    EncodeFixed32(&dest_.contents_[header_offset], crc);\n-  }\n-\n-  void ForceError() {\n-    source_.force_error_ = true;\n-  }\n-\n-  size_t DroppedBytes() const {\n-    return report_.dropped_bytes_;\n-  }\n-\n-  std::string ReportMessage() const {\n-    return report_.message_;\n-  }\n-\n-  // Returns OK iff recorded error message contains \"msg\"\n-  std::string MatchError(const std::string& msg) const {\n-    if (report_.message_.find(msg) == std::string::npos) {\n-      return report_.message_;\n-    } else {\n-      return \"OK\";\n-    }\n-  }\n-\n-  void WriteInitialOffsetLog() {\n-    for (int i = 0; i < 4; i++) {\n-      std::string record(initial_offset_record_sizes_[i],\n-                         static_cast<char>('a' + i));\n-      Write(record);\n-    }\n-  }\n-\n-  void CheckOffsetPastEndReturnsNoRecords(uint64_t offset_past_end) {\n-    WriteInitialOffsetLog();\n-    reading_ = true;\n-    source_.contents_ = Slice(dest_.contents_);\n-    Reader* offset_reader = new Reader(&source_, &report_, true/*checksum*/,\n-                                       WrittenBytes() + offset_past_end);\n-    Slice record;\n-    std::string scratch;\n-    ASSERT_TRUE(!offset_reader->ReadRecord(&record, &scratch));\n-    delete offset_reader;\n-  }\n-\n-  void CheckInitialOffsetRecord(uint64_t initial_offset,\n-                                int expected_record_offset) {\n-    WriteInitialOffsetLog();\n-    reading_ = true;\n-    source_.contents_ = Slice(dest_.contents_);\n-    Reader* offset_reader = new Reader(&source_, &report_, true/*checksum*/,\n-                                       initial_offset);\n-    Slice record;\n-    std::string scratch;\n-    ASSERT_TRUE(offset_reader->ReadRecord(&record, &scratch));\n-    ASSERT_EQ(initial_offset_record_sizes_[expected_record_offset],\n-              record.size());\n-    ASSERT_EQ(initial_offset_last_record_offsets_[expected_record_offset],\n-              offset_reader->LastRecordOffset());\n-    ASSERT_EQ((char)('a' + expected_record_offset), record.data()[0]);\n-    delete offset_reader;\n-  }\n-\n-};\n-\n-size_t LogTest::initial_offset_record_sizes_[] =\n-    {10000,  // Two sizable records in first block\n-     10000,\n-     2 * log::kBlockSize - 1000,  // Span three blocks\n-     1};\n-\n-uint64_t LogTest::initial_offset_last_record_offsets_[] =\n-    {0,\n-     kHeaderSize + 10000,\n-     2 * (kHeaderSize + 10000),\n-     2 * (kHeaderSize + 10000) +\n-         (2 * log::kBlockSize - 1000) + 3 * kHeaderSize};\n-\n-\n-TEST(LogTest, Empty) {\n-  ASSERT_EQ(\"EOF\", Read());\n-}\n-\n-TEST(LogTest, ReadWrite) {\n-  Write(\"foo\");\n-  Write(\"bar\");\n-  Write(\"\");\n-  Write(\"xxxx\");\n-  ASSERT_EQ(\"foo\", Read());\n-  ASSERT_EQ(\"bar\", Read());\n-  ASSERT_EQ(\"\", Read());\n-  ASSERT_EQ(\"xxxx\", Read());\n-  ASSERT_EQ(\"EOF\", Read());\n-  ASSERT_EQ(\"EOF\", Read());  // Make sure reads at eof work\n-}\n-\n-TEST(LogTest, ManyBlocks) {\n-  for (int i = 0; i < 100000; i++) {\n-    Write(NumberString(i));\n-  }\n-  for (int i = 0; i < 100000; i++) {\n-    ASSERT_EQ(NumberString(i), Read());\n-  }\n-  ASSERT_EQ(\"EOF\", Read());\n-}\n-\n-TEST(LogTest, Fragmentation) {\n-  Write(\"small\");\n-  Write(BigString(\"medium\", 50000));\n-  Write(BigString(\"large\", 100000));\n-  ASSERT_EQ(\"small\", Read());\n-  ASSERT_EQ(BigString(\"medium\", 50000), Read());\n-  ASSERT_EQ(BigString(\"large\", 100000), Read());\n-  ASSERT_EQ(\"EOF\", Read());\n-}\n-\n-TEST(LogTest, MarginalTrailer) {\n-  // Make a trailer that is exactly the same length as an empty record.\n-  const int n = kBlockSize - 2*kHeaderSize;\n-  Write(BigString(\"foo\", n));\n-  ASSERT_EQ(kBlockSize - kHeaderSize, WrittenBytes());\n-  Write(\"\");\n-  Write(\"bar\");\n-  ASSERT_EQ(BigString(\"foo\", n), Read());\n-  ASSERT_EQ(\"\", Read());\n-  ASSERT_EQ(\"bar\", Read());\n-  ASSERT_EQ(\"EOF\", Read());\n-}\n-\n-TEST(LogTest, MarginalTrailer2) {\n-  // Make a trailer that is exactly the same length as an empty record.\n-  const int n = kBlockSize - 2*kHeaderSize;\n-  Write(BigString(\"foo\", n));\n-  ASSERT_EQ(kBlockSize - kHeaderSize, WrittenBytes());\n-  Write(\"bar\");\n-  ASSERT_EQ(BigString(\"foo\", n), Read());\n-  ASSERT_EQ(\"bar\", Read());\n-  ASSERT_EQ(\"EOF\", Read());\n-  ASSERT_EQ(0, DroppedBytes());\n-  ASSERT_EQ(\"\", ReportMessage());\n-}\n-\n-TEST(LogTest, ShortTrailer) {\n-  const int n = kBlockSize - 2*kHeaderSize + 4;\n-  Write(BigString(\"foo\", n));\n-  ASSERT_EQ(kBlockSize - kHeaderSize + 4, WrittenBytes());\n-  Write(\"\");\n-  Write(\"bar\");\n-  ASSERT_EQ(BigString(\"foo\", n), Read());\n-  ASSERT_EQ(\"\", Read());\n-  ASSERT_EQ(\"bar\", Read());\n-  ASSERT_EQ(\"EOF\", Read());\n-}\n-\n-TEST(LogTest, AlignedEof) {\n-  const int n = kBlockSize - 2*kHeaderSize + 4;\n-  Write(BigString(\"foo\", n));\n-  ASSERT_EQ(kBlockSize - kHeaderSize + 4, WrittenBytes());\n-  ASSERT_EQ(BigString(\"foo\", n), Read());\n-  ASSERT_EQ(\"EOF\", Read());\n-}\n-\n-TEST(LogTest, RandomRead) {\n-  const int N = 500;\n-  Random write_rnd(301);\n-  for (int i = 0; i < N; i++) {\n-    Write(RandomSkewedString(i, &write_rnd));\n-  }\n-  Random read_rnd(301);\n-  for (int i = 0; i < N; i++) {\n-    ASSERT_EQ(RandomSkewedString(i, &read_rnd), Read());\n-  }\n-  ASSERT_EQ(\"EOF\", Read());\n-}\n-\n-// Tests of all the error paths in log_reader.cc follow:\n-\n-TEST(LogTest, ReadError) {\n-  Write(\"foo\");\n-  ForceError();\n-  ASSERT_EQ(\"EOF\", Read());\n-  ASSERT_EQ(kBlockSize, DroppedBytes());\n-  ASSERT_EQ(\"OK\", MatchError(\"read error\"));\n-}\n-\n-TEST(LogTest, BadRecordType) {\n-  Write(\"foo\");\n-  // Type is stored in header[6]\n-  IncrementByte(6, 100);\n-  FixChecksum(0, 3);\n-  ASSERT_EQ(\"EOF\", Read());\n-  ASSERT_EQ(3, DroppedBytes());\n-  ASSERT_EQ(\"OK\", MatchError(\"unknown record type\"));\n-}\n-\n-TEST(LogTest, TruncatedTrailingRecord) {\n-  Write(\"foo\");\n-  ShrinkSize(4);   // Drop all payload as well as a header byte\n-  ASSERT_EQ(\"EOF\", Read());\n-  ASSERT_EQ(kHeaderSize - 1, DroppedBytes());\n-  ASSERT_EQ(\"OK\", MatchError(\"truncated record at end of file\"));\n-}\n-\n-TEST(LogTest, BadLength) {\n-  Write(\"foo\");\n-  ShrinkSize(1);\n-  ASSERT_EQ(\"EOF\", Read());\n-  ASSERT_EQ(kHeaderSize + 2, DroppedBytes());\n-  ASSERT_EQ(\"OK\", MatchError(\"bad record length\"));\n-}\n-\n-TEST(LogTest, ChecksumMismatch) {\n-  Write(\"foo\");\n-  IncrementByte(0, 10);\n-  ASSERT_EQ(\"EOF\", Read());\n-  ASSERT_EQ(10, DroppedBytes());\n-  ASSERT_EQ(\"OK\", MatchError(\"checksum mismatch\"));\n-}\n-\n-TEST(LogTest, UnexpectedMiddleType) {\n-  Write(\"foo\");\n-  SetByte(6, kMiddleType);\n-  FixChecksum(0, 3);\n-  ASSERT_EQ(\"EOF\", Read());\n-  ASSERT_EQ(3, DroppedBytes());\n-  ASSERT_EQ(\"OK\", MatchError(\"missing start\"));\n-}\n-\n-TEST(LogTest, UnexpectedLastType) {\n-  Write(\"foo\");\n-  SetByte(6, kLastType);\n-  FixChecksum(0, 3);\n-  ASSERT_EQ(\"EOF\", Read());\n-  ASSERT_EQ(3, DroppedBytes());\n-  ASSERT_EQ(\"OK\", MatchError(\"missing start\"));\n-}\n-\n-TEST(LogTest, UnexpectedFullType) {\n-  Write(\"foo\");\n-  Write(\"bar\");\n-  SetByte(6, kFirstType);\n-  FixChecksum(0, 3);\n-  ASSERT_EQ(\"bar\", Read());\n-  ASSERT_EQ(\"EOF\", Read());\n-  ASSERT_EQ(3, DroppedBytes());\n-  ASSERT_EQ(\"OK\", MatchError(\"partial record without end\"));\n-}\n-\n-TEST(LogTest, UnexpectedFirstType) {\n-  Write(\"foo\");\n-  Write(BigString(\"bar\", 100000));\n-  SetByte(6, kFirstType);\n-  FixChecksum(0, 3);\n-  ASSERT_EQ(BigString(\"bar\", 100000), Read());\n-  ASSERT_EQ(\"EOF\", Read());\n-  ASSERT_EQ(3, DroppedBytes());\n-  ASSERT_EQ(\"OK\", MatchError(\"partial record without end\"));\n-}\n-\n-TEST(LogTest, ErrorJoinsRecords) {\n-  // Consider two fragmented records:\n-  //    first(R1) last(R1) first(R2) last(R2)\n-  // where the middle two fragments disappear.  We do not want\n-  // first(R1),last(R2) to get joined and returned as a valid record.\n-\n-  // Write records that span two blocks\n-  Write(BigString(\"foo\", kBlockSize));\n-  Write(BigString(\"bar\", kBlockSize));\n-  Write(\"correct\");\n-\n-  // Wipe the middle block\n-  for (int offset = kBlockSize; offset < 2*kBlockSize; offset++) {\n-    SetByte(offset, 'x');\n-  }\n-\n-  ASSERT_EQ(\"correct\", Read());\n-  ASSERT_EQ(\"EOF\", Read());\n-  const int dropped = DroppedBytes();\n-  ASSERT_LE(dropped, 2*kBlockSize + 100);\n-  ASSERT_GE(dropped, 2*kBlockSize);\n-}\n-\n-TEST(LogTest, ReadStart) {\n-  CheckInitialOffsetRecord(0, 0);\n-}\n-\n-TEST(LogTest, ReadSecondOneOff) {\n-  CheckInitialOffsetRecord(1, 1);\n-}\n-\n-TEST(LogTest, ReadSecondTenThousand) {\n-  CheckInitialOffsetRecord(10000, 1);\n-}\n-\n-TEST(LogTest, ReadSecondStart) {\n-  CheckInitialOffsetRecord(10007, 1);\n-}\n-\n-TEST(LogTest, ReadThirdOneOff) {\n-  CheckInitialOffsetRecord(10008, 2);\n-}\n-\n-TEST(LogTest, ReadThirdStart) {\n-  CheckInitialOffsetRecord(20014, 2);\n-}\n-\n-TEST(LogTest, ReadFourthOneOff) {\n-  CheckInitialOffsetRecord(20015, 3);\n-}\n-\n-TEST(LogTest, ReadFourthFirstBlockTrailer) {\n-  CheckInitialOffsetRecord(log::kBlockSize - 4, 3);\n-}\n-\n-TEST(LogTest, ReadFourthMiddleBlock) {\n-  CheckInitialOffsetRecord(log::kBlockSize + 1, 3);\n-}\n-\n-TEST(LogTest, ReadFourthLastBlock) {\n-  CheckInitialOffsetRecord(2 * log::kBlockSize + 1, 3);\n-}\n-\n-TEST(LogTest, ReadFourthStart) {\n-  CheckInitialOffsetRecord(\n-      2 * (kHeaderSize + 1000) + (2 * log::kBlockSize - 1000) + 3 * kHeaderSize,\n-      3);\n-}\n-\n-TEST(LogTest, ReadEnd) {\n-  CheckOffsetPastEndReturnsNoRecords(0);\n-}\n-\n-TEST(LogTest, ReadPastEnd) {\n-  CheckOffsetPastEndReturnsNoRecords(5);\n-}\n-\n-}  // namespace log\n-}  // namespace leveldb\n-\n-int main(int argc, char** argv) {\n-  return leveldb::test::RunAllTests();\n-}"
      },
      {
        "sha": "2da99ac08866397270663a1203075944162ba290",
        "filename": "src/leveldb/db/log_writer.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 103,
        "changes": 103,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/log_writer.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/log_writer.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/log_writer.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,103 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/log_writer.h\"\n-\n-#include <stdint.h>\n-#include \"leveldb/env.h\"\n-#include \"util/coding.h\"\n-#include \"util/crc32c.h\"\n-\n-namespace leveldb {\n-namespace log {\n-\n-Writer::Writer(WritableFile* dest)\n-    : dest_(dest),\n-      block_offset_(0) {\n-  for (int i = 0; i <= kMaxRecordType; i++) {\n-    char t = static_cast<char>(i);\n-    type_crc_[i] = crc32c::Value(&t, 1);\n-  }\n-}\n-\n-Writer::~Writer() {\n-}\n-\n-Status Writer::AddRecord(const Slice& slice) {\n-  const char* ptr = slice.data();\n-  size_t left = slice.size();\n-\n-  // Fragment the record if necessary and emit it.  Note that if slice\n-  // is empty, we still want to iterate once to emit a single\n-  // zero-length record\n-  Status s;\n-  bool begin = true;\n-  do {\n-    const int leftover = kBlockSize - block_offset_;\n-    assert(leftover >= 0);\n-    if (leftover < kHeaderSize) {\n-      // Switch to a new block\n-      if (leftover > 0) {\n-        // Fill the trailer (literal below relies on kHeaderSize being 7)\n-        assert(kHeaderSize == 7);\n-        dest_->Append(Slice(\"\\x00\\x00\\x00\\x00\\x00\\x00\", leftover));\n-      }\n-      block_offset_ = 0;\n-    }\n-\n-    // Invariant: we never leave < kHeaderSize bytes in a block.\n-    assert(kBlockSize - block_offset_ - kHeaderSize >= 0);\n-\n-    const size_t avail = kBlockSize - block_offset_ - kHeaderSize;\n-    const size_t fragment_length = (left < avail) ? left : avail;\n-\n-    RecordType type;\n-    const bool end = (left == fragment_length);\n-    if (begin && end) {\n-      type = kFullType;\n-    } else if (begin) {\n-      type = kFirstType;\n-    } else if (end) {\n-      type = kLastType;\n-    } else {\n-      type = kMiddleType;\n-    }\n-\n-    s = EmitPhysicalRecord(type, ptr, fragment_length);\n-    ptr += fragment_length;\n-    left -= fragment_length;\n-    begin = false;\n-  } while (s.ok() && left > 0);\n-  return s;\n-}\n-\n-Status Writer::EmitPhysicalRecord(RecordType t, const char* ptr, size_t n) {\n-  assert(n <= 0xffff);  // Must fit in two bytes\n-  assert(block_offset_ + kHeaderSize + n <= kBlockSize);\n-\n-  // Format the header\n-  char buf[kHeaderSize];\n-  buf[4] = static_cast<char>(n & 0xff);\n-  buf[5] = static_cast<char>(n >> 8);\n-  buf[6] = static_cast<char>(t);\n-\n-  // Compute the crc of the record type and the payload.\n-  uint32_t crc = crc32c::Extend(type_crc_[t], ptr, n);\n-  crc = crc32c::Mask(crc);                 // Adjust for storage\n-  EncodeFixed32(buf, crc);\n-\n-  // Write the header and the payload\n-  Status s = dest_->Append(Slice(buf, kHeaderSize));\n-  if (s.ok()) {\n-    s = dest_->Append(Slice(ptr, n));\n-    if (s.ok()) {\n-      s = dest_->Flush();\n-    }\n-  }\n-  block_offset_ += kHeaderSize + n;\n-  return s;\n-}\n-\n-}  // namespace log\n-}  // namespace leveldb"
      },
      {
        "sha": "a3a954d96732542fac9aef1345ebd952075f737c",
        "filename": "src/leveldb/db/log_writer.h",
        "status": "removed",
        "additions": 0,
        "deletions": 48,
        "changes": 48,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/log_writer.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/log_writer.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/log_writer.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,48 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_DB_LOG_WRITER_H_\n-#define STORAGE_LEVELDB_DB_LOG_WRITER_H_\n-\n-#include <stdint.h>\n-#include \"db/log_format.h\"\n-#include \"leveldb/slice.h\"\n-#include \"leveldb/status.h\"\n-\n-namespace leveldb {\n-\n-class WritableFile;\n-\n-namespace log {\n-\n-class Writer {\n- public:\n-  // Create a writer that will append data to \"*dest\".\n-  // \"*dest\" must be initially empty.\n-  // \"*dest\" must remain live while this Writer is in use.\n-  explicit Writer(WritableFile* dest);\n-  ~Writer();\n-\n-  Status AddRecord(const Slice& slice);\n-\n- private:\n-  WritableFile* dest_;\n-  int block_offset_;       // Current offset in block\n-\n-  // crc32c values for all supported record types.  These are\n-  // pre-computed to reduce the overhead of computing the crc of the\n-  // record type stored in the header.\n-  uint32_t type_crc_[kMaxRecordType + 1];\n-\n-  Status EmitPhysicalRecord(RecordType type, const char* ptr, size_t length);\n-\n-  // No copying allowed\n-  Writer(const Writer&);\n-  void operator=(const Writer&);\n-};\n-\n-}  // namespace log\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_DB_LOG_WRITER_H_"
      },
      {
        "sha": "bfec0a7e7a1dc210b44dd527b9547e33e829d9bb",
        "filename": "src/leveldb/db/memtable.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 145,
        "changes": 145,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/memtable.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/memtable.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/memtable.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,145 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/memtable.h\"\n-#include \"db/dbformat.h\"\n-#include \"leveldb/comparator.h\"\n-#include \"leveldb/env.h\"\n-#include \"leveldb/iterator.h\"\n-#include \"util/coding.h\"\n-\n-namespace leveldb {\n-\n-static Slice GetLengthPrefixedSlice(const char* data) {\n-  uint32_t len;\n-  const char* p = data;\n-  p = GetVarint32Ptr(p, p + 5, &len);  // +5: we assume \"p\" is not corrupted\n-  return Slice(p, len);\n-}\n-\n-MemTable::MemTable(const InternalKeyComparator& cmp)\n-    : comparator_(cmp),\n-      refs_(0),\n-      table_(comparator_, &arena_) {\n-}\n-\n-MemTable::~MemTable() {\n-  assert(refs_ == 0);\n-}\n-\n-size_t MemTable::ApproximateMemoryUsage() { return arena_.MemoryUsage(); }\n-\n-int MemTable::KeyComparator::operator()(const char* aptr, const char* bptr)\n-    const {\n-  // Internal keys are encoded as length-prefixed strings.\n-  Slice a = GetLengthPrefixedSlice(aptr);\n-  Slice b = GetLengthPrefixedSlice(bptr);\n-  return comparator.Compare(a, b);\n-}\n-\n-// Encode a suitable internal key target for \"target\" and return it.\n-// Uses *scratch as scratch space, and the returned pointer will point\n-// into this scratch space.\n-static const char* EncodeKey(std::string* scratch, const Slice& target) {\n-  scratch->clear();\n-  PutVarint32(scratch, target.size());\n-  scratch->append(target.data(), target.size());\n-  return scratch->data();\n-}\n-\n-class MemTableIterator: public Iterator {\n- public:\n-  explicit MemTableIterator(MemTable::Table* table) : iter_(table) { }\n-\n-  virtual bool Valid() const { return iter_.Valid(); }\n-  virtual void Seek(const Slice& k) { iter_.Seek(EncodeKey(&tmp_, k)); }\n-  virtual void SeekToFirst() { iter_.SeekToFirst(); }\n-  virtual void SeekToLast() { iter_.SeekToLast(); }\n-  virtual void Next() { iter_.Next(); }\n-  virtual void Prev() { iter_.Prev(); }\n-  virtual Slice key() const { return GetLengthPrefixedSlice(iter_.key()); }\n-  virtual Slice value() const {\n-    Slice key_slice = GetLengthPrefixedSlice(iter_.key());\n-    return GetLengthPrefixedSlice(key_slice.data() + key_slice.size());\n-  }\n-\n-  virtual Status status() const { return Status::OK(); }\n-\n- private:\n-  MemTable::Table::Iterator iter_;\n-  std::string tmp_;       // For passing to EncodeKey\n-\n-  // No copying allowed\n-  MemTableIterator(const MemTableIterator&);\n-  void operator=(const MemTableIterator&);\n-};\n-\n-Iterator* MemTable::NewIterator() {\n-  return new MemTableIterator(&table_);\n-}\n-\n-void MemTable::Add(SequenceNumber s, ValueType type,\n-                   const Slice& key,\n-                   const Slice& value) {\n-  // Format of an entry is concatenation of:\n-  //  key_size     : varint32 of internal_key.size()\n-  //  key bytes    : char[internal_key.size()]\n-  //  value_size   : varint32 of value.size()\n-  //  value bytes  : char[value.size()]\n-  size_t key_size = key.size();\n-  size_t val_size = value.size();\n-  size_t internal_key_size = key_size + 8;\n-  const size_t encoded_len =\n-      VarintLength(internal_key_size) + internal_key_size +\n-      VarintLength(val_size) + val_size;\n-  char* buf = arena_.Allocate(encoded_len);\n-  char* p = EncodeVarint32(buf, internal_key_size);\n-  memcpy(p, key.data(), key_size);\n-  p += key_size;\n-  EncodeFixed64(p, (s << 8) | type);\n-  p += 8;\n-  p = EncodeVarint32(p, val_size);\n-  memcpy(p, value.data(), val_size);\n-  assert((p + val_size) - buf == encoded_len);\n-  table_.Insert(buf);\n-}\n-\n-bool MemTable::Get(const LookupKey& key, std::string* value, Status* s) {\n-  Slice memkey = key.memtable_key();\n-  Table::Iterator iter(&table_);\n-  iter.Seek(memkey.data());\n-  if (iter.Valid()) {\n-    // entry format is:\n-    //    klength  varint32\n-    //    userkey  char[klength]\n-    //    tag      uint64\n-    //    vlength  varint32\n-    //    value    char[vlength]\n-    // Check that it belongs to same user key.  We do not check the\n-    // sequence number since the Seek() call above should have skipped\n-    // all entries with overly large sequence numbers.\n-    const char* entry = iter.key();\n-    uint32_t key_length;\n-    const char* key_ptr = GetVarint32Ptr(entry, entry+5, &key_length);\n-    if (comparator_.comparator.user_comparator()->Compare(\n-            Slice(key_ptr, key_length - 8),\n-            key.user_key()) == 0) {\n-      // Correct user key\n-      const uint64_t tag = DecodeFixed64(key_ptr + key_length - 8);\n-      switch (static_cast<ValueType>(tag & 0xff)) {\n-        case kTypeValue: {\n-          Slice v = GetLengthPrefixedSlice(key_ptr + key_length);\n-          value->assign(v.data(), v.size());\n-          return true;\n-        }\n-        case kTypeDeletion:\n-          *s = Status::NotFound(Slice());\n-          return true;\n-      }\n-    }\n-  }\n-  return false;\n-}\n-\n-}  // namespace leveldb"
      },
      {
        "sha": "92e90bb099f3356520c42d6bb887b2a35c6bbe39",
        "filename": "src/leveldb/db/memtable.h",
        "status": "removed",
        "additions": 0,
        "deletions": 91,
        "changes": 91,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/memtable.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/memtable.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/memtable.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,91 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_DB_MEMTABLE_H_\n-#define STORAGE_LEVELDB_DB_MEMTABLE_H_\n-\n-#include <string>\n-#include \"leveldb/db.h\"\n-#include \"db/dbformat.h\"\n-#include \"db/skiplist.h\"\n-#include \"util/arena.h\"\n-\n-namespace leveldb {\n-\n-class InternalKeyComparator;\n-class Mutex;\n-class MemTableIterator;\n-\n-class MemTable {\n- public:\n-  // MemTables are reference counted.  The initial reference count\n-  // is zero and the caller must call Ref() at least once.\n-  explicit MemTable(const InternalKeyComparator& comparator);\n-\n-  // Increase reference count.\n-  void Ref() { ++refs_; }\n-\n-  // Drop reference count.  Delete if no more references exist.\n-  void Unref() {\n-    --refs_;\n-    assert(refs_ >= 0);\n-    if (refs_ <= 0) {\n-      delete this;\n-    }\n-  }\n-\n-  // Returns an estimate of the number of bytes of data in use by this\n-  // data structure.\n-  //\n-  // REQUIRES: external synchronization to prevent simultaneous\n-  // operations on the same MemTable.\n-  size_t ApproximateMemoryUsage();\n-\n-  // Return an iterator that yields the contents of the memtable.\n-  //\n-  // The caller must ensure that the underlying MemTable remains live\n-  // while the returned iterator is live.  The keys returned by this\n-  // iterator are internal keys encoded by AppendInternalKey in the\n-  // db/format.{h,cc} module.\n-  Iterator* NewIterator();\n-\n-  // Add an entry into memtable that maps key to value at the\n-  // specified sequence number and with the specified type.\n-  // Typically value will be empty if type==kTypeDeletion.\n-  void Add(SequenceNumber seq, ValueType type,\n-           const Slice& key,\n-           const Slice& value);\n-\n-  // If memtable contains a value for key, store it in *value and return true.\n-  // If memtable contains a deletion for key, store a NotFound() error\n-  // in *status and return true.\n-  // Else, return false.\n-  bool Get(const LookupKey& key, std::string* value, Status* s);\n-\n- private:\n-  ~MemTable();  // Private since only Unref() should be used to delete it\n-\n-  struct KeyComparator {\n-    const InternalKeyComparator comparator;\n-    explicit KeyComparator(const InternalKeyComparator& c) : comparator(c) { }\n-    int operator()(const char* a, const char* b) const;\n-  };\n-  friend class MemTableIterator;\n-  friend class MemTableBackwardIterator;\n-\n-  typedef SkipList<const char*, KeyComparator> Table;\n-\n-  KeyComparator comparator_;\n-  int refs_;\n-  Arena arena_;\n-  Table table_;\n-\n-  // No copying allowed\n-  MemTable(const MemTable&);\n-  void operator=(const MemTable&);\n-};\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_DB_MEMTABLE_H_"
      },
      {
        "sha": "022d52f3debe0c0e89a6825ca3778c5894c3783c",
        "filename": "src/leveldb/db/repair.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 389,
        "changes": 389,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/repair.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/repair.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/repair.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,389 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// We recover the contents of the descriptor from the other files we find.\n-// (1) Any log files are first converted to tables\n-// (2) We scan every table to compute\n-//     (a) smallest/largest for the table\n-//     (b) largest sequence number in the table\n-// (3) We generate descriptor contents:\n-//      - log number is set to zero\n-//      - next-file-number is set to 1 + largest file number we found\n-//      - last-sequence-number is set to largest sequence# found across\n-//        all tables (see 2c)\n-//      - compaction pointers are cleared\n-//      - every table file is added at level 0\n-//\n-// Possible optimization 1:\n-//   (a) Compute total size and use to pick appropriate max-level M\n-//   (b) Sort tables by largest sequence# in the table\n-//   (c) For each table: if it overlaps earlier table, place in level-0,\n-//       else place in level-M.\n-// Possible optimization 2:\n-//   Store per-table metadata (smallest, largest, largest-seq#, ...)\n-//   in the table's meta section to speed up ScanTable.\n-\n-#include \"db/builder.h\"\n-#include \"db/db_impl.h\"\n-#include \"db/dbformat.h\"\n-#include \"db/filename.h\"\n-#include \"db/log_reader.h\"\n-#include \"db/log_writer.h\"\n-#include \"db/memtable.h\"\n-#include \"db/table_cache.h\"\n-#include \"db/version_edit.h\"\n-#include \"db/write_batch_internal.h\"\n-#include \"leveldb/comparator.h\"\n-#include \"leveldb/db.h\"\n-#include \"leveldb/env.h\"\n-\n-namespace leveldb {\n-\n-namespace {\n-\n-class Repairer {\n- public:\n-  Repairer(const std::string& dbname, const Options& options)\n-      : dbname_(dbname),\n-        env_(options.env),\n-        icmp_(options.comparator),\n-        ipolicy_(options.filter_policy),\n-        options_(SanitizeOptions(dbname, &icmp_, &ipolicy_, options)),\n-        owns_info_log_(options_.info_log != options.info_log),\n-        owns_cache_(options_.block_cache != options.block_cache),\n-        next_file_number_(1) {\n-    // TableCache can be small since we expect each table to be opened once.\n-    table_cache_ = new TableCache(dbname_, &options_, 10);\n-  }\n-\n-  ~Repairer() {\n-    delete table_cache_;\n-    if (owns_info_log_) {\n-      delete options_.info_log;\n-    }\n-    if (owns_cache_) {\n-      delete options_.block_cache;\n-    }\n-  }\n-\n-  Status Run() {\n-    Status status = FindFiles();\n-    if (status.ok()) {\n-      ConvertLogFilesToTables();\n-      ExtractMetaData();\n-      status = WriteDescriptor();\n-    }\n-    if (status.ok()) {\n-      unsigned long long bytes = 0;\n-      for (size_t i = 0; i < tables_.size(); i++) {\n-        bytes += tables_[i].meta.file_size;\n-      }\n-      Log(options_.info_log,\n-          \"**** Repaired leveldb %s; \"\n-          \"recovered %d files; %llu bytes. \"\n-          \"Some data may have been lost. \"\n-          \"****\",\n-          dbname_.c_str(),\n-          static_cast<int>(tables_.size()),\n-          bytes);\n-    }\n-    return status;\n-  }\n-\n- private:\n-  struct TableInfo {\n-    FileMetaData meta;\n-    SequenceNumber max_sequence;\n-  };\n-\n-  std::string const dbname_;\n-  Env* const env_;\n-  InternalKeyComparator const icmp_;\n-  InternalFilterPolicy const ipolicy_;\n-  Options const options_;\n-  bool owns_info_log_;\n-  bool owns_cache_;\n-  TableCache* table_cache_;\n-  VersionEdit edit_;\n-\n-  std::vector<std::string> manifests_;\n-  std::vector<uint64_t> table_numbers_;\n-  std::vector<uint64_t> logs_;\n-  std::vector<TableInfo> tables_;\n-  uint64_t next_file_number_;\n-\n-  Status FindFiles() {\n-    std::vector<std::string> filenames;\n-    Status status = env_->GetChildren(dbname_, &filenames);\n-    if (!status.ok()) {\n-      return status;\n-    }\n-    if (filenames.empty()) {\n-      return Status::IOError(dbname_, \"repair found no files\");\n-    }\n-\n-    uint64_t number;\n-    FileType type;\n-    for (size_t i = 0; i < filenames.size(); i++) {\n-      if (ParseFileName(filenames[i], &number, &type)) {\n-        if (type == kDescriptorFile) {\n-          manifests_.push_back(filenames[i]);\n-        } else {\n-          if (number + 1 > next_file_number_) {\n-            next_file_number_ = number + 1;\n-          }\n-          if (type == kLogFile) {\n-            logs_.push_back(number);\n-          } else if (type == kTableFile) {\n-            table_numbers_.push_back(number);\n-          } else {\n-            // Ignore other files\n-          }\n-        }\n-      }\n-    }\n-    return status;\n-  }\n-\n-  void ConvertLogFilesToTables() {\n-    for (size_t i = 0; i < logs_.size(); i++) {\n-      std::string logname = LogFileName(dbname_, logs_[i]);\n-      Status status = ConvertLogToTable(logs_[i]);\n-      if (!status.ok()) {\n-        Log(options_.info_log, \"Log #%llu: ignoring conversion error: %s\",\n-            (unsigned long long) logs_[i],\n-            status.ToString().c_str());\n-      }\n-      ArchiveFile(logname);\n-    }\n-  }\n-\n-  Status ConvertLogToTable(uint64_t log) {\n-    struct LogReporter : public log::Reader::Reporter {\n-      Env* env;\n-      Logger* info_log;\n-      uint64_t lognum;\n-      virtual void Corruption(size_t bytes, const Status& s) {\n-        // We print error messages for corruption, but continue repairing.\n-        Log(info_log, \"Log #%llu: dropping %d bytes; %s\",\n-            (unsigned long long) lognum,\n-            static_cast<int>(bytes),\n-            s.ToString().c_str());\n-      }\n-    };\n-\n-    // Open the log file\n-    std::string logname = LogFileName(dbname_, log);\n-    SequentialFile* lfile;\n-    Status status = env_->NewSequentialFile(logname, &lfile);\n-    if (!status.ok()) {\n-      return status;\n-    }\n-\n-    // Create the log reader.\n-    LogReporter reporter;\n-    reporter.env = env_;\n-    reporter.info_log = options_.info_log;\n-    reporter.lognum = log;\n-    // We intentially make log::Reader do checksumming so that\n-    // corruptions cause entire commits to be skipped instead of\n-    // propagating bad information (like overly large sequence\n-    // numbers).\n-    log::Reader reader(lfile, &reporter, false/*do not checksum*/,\n-                       0/*initial_offset*/);\n-\n-    // Read all the records and add to a memtable\n-    std::string scratch;\n-    Slice record;\n-    WriteBatch batch;\n-    MemTable* mem = new MemTable(icmp_);\n-    mem->Ref();\n-    int counter = 0;\n-    while (reader.ReadRecord(&record, &scratch)) {\n-      if (record.size() < 12) {\n-        reporter.Corruption(\n-            record.size(), Status::Corruption(\"log record too small\"));\n-        continue;\n-      }\n-      WriteBatchInternal::SetContents(&batch, record);\n-      status = WriteBatchInternal::InsertInto(&batch, mem);\n-      if (status.ok()) {\n-        counter += WriteBatchInternal::Count(&batch);\n-      } else {\n-        Log(options_.info_log, \"Log #%llu: ignoring %s\",\n-            (unsigned long long) log,\n-            status.ToString().c_str());\n-        status = Status::OK();  // Keep going with rest of file\n-      }\n-    }\n-    delete lfile;\n-\n-    // Do not record a version edit for this conversion to a Table\n-    // since ExtractMetaData() will also generate edits.\n-    FileMetaData meta;\n-    meta.number = next_file_number_++;\n-    Iterator* iter = mem->NewIterator();\n-    status = BuildTable(dbname_, env_, options_, table_cache_, iter, &meta);\n-    delete iter;\n-    mem->Unref();\n-    mem = NULL;\n-    if (status.ok()) {\n-      if (meta.file_size > 0) {\n-        table_numbers_.push_back(meta.number);\n-      }\n-    }\n-    Log(options_.info_log, \"Log #%llu: %d ops saved to Table #%llu %s\",\n-        (unsigned long long) log,\n-        counter,\n-        (unsigned long long) meta.number,\n-        status.ToString().c_str());\n-    return status;\n-  }\n-\n-  void ExtractMetaData() {\n-    std::vector<TableInfo> kept;\n-    for (size_t i = 0; i < table_numbers_.size(); i++) {\n-      TableInfo t;\n-      t.meta.number = table_numbers_[i];\n-      Status status = ScanTable(&t);\n-      if (!status.ok()) {\n-        std::string fname = TableFileName(dbname_, table_numbers_[i]);\n-        Log(options_.info_log, \"Table #%llu: ignoring %s\",\n-            (unsigned long long) table_numbers_[i],\n-            status.ToString().c_str());\n-        ArchiveFile(fname);\n-      } else {\n-        tables_.push_back(t);\n-      }\n-    }\n-  }\n-\n-  Status ScanTable(TableInfo* t) {\n-    std::string fname = TableFileName(dbname_, t->meta.number);\n-    int counter = 0;\n-    Status status = env_->GetFileSize(fname, &t->meta.file_size);\n-    if (status.ok()) {\n-      Iterator* iter = table_cache_->NewIterator(\n-          ReadOptions(), t->meta.number, t->meta.file_size);\n-      bool empty = true;\n-      ParsedInternalKey parsed;\n-      t->max_sequence = 0;\n-      for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n-        Slice key = iter->key();\n-        if (!ParseInternalKey(key, &parsed)) {\n-          Log(options_.info_log, \"Table #%llu: unparsable key %s\",\n-              (unsigned long long) t->meta.number,\n-              EscapeString(key).c_str());\n-          continue;\n-        }\n-\n-        counter++;\n-        if (empty) {\n-          empty = false;\n-          t->meta.smallest.DecodeFrom(key);\n-        }\n-        t->meta.largest.DecodeFrom(key);\n-        if (parsed.sequence > t->max_sequence) {\n-          t->max_sequence = parsed.sequence;\n-        }\n-      }\n-      if (!iter->status().ok()) {\n-        status = iter->status();\n-      }\n-      delete iter;\n-    }\n-    Log(options_.info_log, \"Table #%llu: %d entries %s\",\n-        (unsigned long long) t->meta.number,\n-        counter,\n-        status.ToString().c_str());\n-    return status;\n-  }\n-\n-  Status WriteDescriptor() {\n-    std::string tmp = TempFileName(dbname_, 1);\n-    WritableFile* file;\n-    Status status = env_->NewWritableFile(tmp, &file);\n-    if (!status.ok()) {\n-      return status;\n-    }\n-\n-    SequenceNumber max_sequence = 0;\n-    for (size_t i = 0; i < tables_.size(); i++) {\n-      if (max_sequence < tables_[i].max_sequence) {\n-        max_sequence = tables_[i].max_sequence;\n-      }\n-    }\n-\n-    edit_.SetComparatorName(icmp_.user_comparator()->Name());\n-    edit_.SetLogNumber(0);\n-    edit_.SetNextFile(next_file_number_);\n-    edit_.SetLastSequence(max_sequence);\n-\n-    for (size_t i = 0; i < tables_.size(); i++) {\n-      // TODO(opt): separate out into multiple levels\n-      const TableInfo& t = tables_[i];\n-      edit_.AddFile(0, t.meta.number, t.meta.file_size,\n-                    t.meta.smallest, t.meta.largest);\n-    }\n-\n-    //fprintf(stderr, \"NewDescriptor:\\n%s\\n\", edit_.DebugString().c_str());\n-    {\n-      log::Writer log(file);\n-      std::string record;\n-      edit_.EncodeTo(&record);\n-      status = log.AddRecord(record);\n-    }\n-    if (status.ok()) {\n-      status = file->Close();\n-    }\n-    delete file;\n-    file = NULL;\n-\n-    if (!status.ok()) {\n-      env_->DeleteFile(tmp);\n-    } else {\n-      // Discard older manifests\n-      for (size_t i = 0; i < manifests_.size(); i++) {\n-        ArchiveFile(dbname_ + \"/\" + manifests_[i]);\n-      }\n-\n-      // Install new manifest\n-      status = env_->RenameFile(tmp, DescriptorFileName(dbname_, 1));\n-      if (status.ok()) {\n-        status = SetCurrentFile(env_, dbname_, 1);\n-      } else {\n-        env_->DeleteFile(tmp);\n-      }\n-    }\n-    return status;\n-  }\n-\n-  void ArchiveFile(const std::string& fname) {\n-    // Move into another directory.  E.g., for\n-    //    dir/foo\n-    // rename to\n-    //    dir/lost/foo\n-    const char* slash = strrchr(fname.c_str(), '/');\n-    std::string new_dir;\n-    if (slash != NULL) {\n-      new_dir.assign(fname.data(), slash - fname.data());\n-    }\n-    new_dir.append(\"/lost\");\n-    env_->CreateDir(new_dir);  // Ignore error\n-    std::string new_file = new_dir;\n-    new_file.append(\"/\");\n-    new_file.append((slash == NULL) ? fname.c_str() : slash + 1);\n-    Status s = env_->RenameFile(fname, new_file);\n-    Log(options_.info_log, \"Archiving %s: %s\\n\",\n-        fname.c_str(), s.ToString().c_str());\n-  }\n-};\n-}  // namespace\n-\n-Status RepairDB(const std::string& dbname, const Options& options) {\n-  Repairer repairer(dbname, options);\n-  return repairer.Run();\n-}\n-\n-}  // namespace leveldb"
      },
      {
        "sha": "af85be6d01621b249f1756b2af5fd6078dd2ee25",
        "filename": "src/leveldb/db/skiplist.h",
        "status": "removed",
        "additions": 0,
        "deletions": 379,
        "changes": 379,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/skiplist.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/skiplist.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/skiplist.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,379 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// Thread safety\n-// -------------\n-//\n-// Writes require external synchronization, most likely a mutex.\n-// Reads require a guarantee that the SkipList will not be destroyed\n-// while the read is in progress.  Apart from that, reads progress\n-// without any internal locking or synchronization.\n-//\n-// Invariants:\n-//\n-// (1) Allocated nodes are never deleted until the SkipList is\n-// destroyed.  This is trivially guaranteed by the code since we\n-// never delete any skip list nodes.\n-//\n-// (2) The contents of a Node except for the next/prev pointers are\n-// immutable after the Node has been linked into the SkipList.\n-// Only Insert() modifies the list, and it is careful to initialize\n-// a node and use release-stores to publish the nodes in one or\n-// more lists.\n-//\n-// ... prev vs. next pointer ordering ...\n-\n-#include <assert.h>\n-#include <stdlib.h>\n-#include \"port/port.h\"\n-#include \"util/arena.h\"\n-#include \"util/random.h\"\n-\n-namespace leveldb {\n-\n-class Arena;\n-\n-template<typename Key, class Comparator>\n-class SkipList {\n- private:\n-  struct Node;\n-\n- public:\n-  // Create a new SkipList object that will use \"cmp\" for comparing keys,\n-  // and will allocate memory using \"*arena\".  Objects allocated in the arena\n-  // must remain allocated for the lifetime of the skiplist object.\n-  explicit SkipList(Comparator cmp, Arena* arena);\n-\n-  // Insert key into the list.\n-  // REQUIRES: nothing that compares equal to key is currently in the list.\n-  void Insert(const Key& key);\n-\n-  // Returns true iff an entry that compares equal to key is in the list.\n-  bool Contains(const Key& key) const;\n-\n-  // Iteration over the contents of a skip list\n-  class Iterator {\n-   public:\n-    // Initialize an iterator over the specified list.\n-    // The returned iterator is not valid.\n-    explicit Iterator(const SkipList* list);\n-\n-    // Returns true iff the iterator is positioned at a valid node.\n-    bool Valid() const;\n-\n-    // Returns the key at the current position.\n-    // REQUIRES: Valid()\n-    const Key& key() const;\n-\n-    // Advances to the next position.\n-    // REQUIRES: Valid()\n-    void Next();\n-\n-    // Advances to the previous position.\n-    // REQUIRES: Valid()\n-    void Prev();\n-\n-    // Advance to the first entry with a key >= target\n-    void Seek(const Key& target);\n-\n-    // Position at the first entry in list.\n-    // Final state of iterator is Valid() iff list is not empty.\n-    void SeekToFirst();\n-\n-    // Position at the last entry in list.\n-    // Final state of iterator is Valid() iff list is not empty.\n-    void SeekToLast();\n-\n-   private:\n-    const SkipList* list_;\n-    Node* node_;\n-    // Intentionally copyable\n-  };\n-\n- private:\n-  enum { kMaxHeight = 12 };\n-\n-  // Immutable after construction\n-  Comparator const compare_;\n-  Arena* const arena_;    // Arena used for allocations of nodes\n-\n-  Node* const head_;\n-\n-  // Modified only by Insert().  Read racily by readers, but stale\n-  // values are ok.\n-  port::AtomicPointer max_height_;   // Height of the entire list\n-\n-  inline int GetMaxHeight() const {\n-    return static_cast<int>(\n-        reinterpret_cast<intptr_t>(max_height_.NoBarrier_Load()));\n-  }\n-\n-  // Read/written only by Insert().\n-  Random rnd_;\n-\n-  Node* NewNode(const Key& key, int height);\n-  int RandomHeight();\n-  bool Equal(const Key& a, const Key& b) const { return (compare_(a, b) == 0); }\n-\n-  // Return true if key is greater than the data stored in \"n\"\n-  bool KeyIsAfterNode(const Key& key, Node* n) const;\n-\n-  // Return the earliest node that comes at or after key.\n-  // Return NULL if there is no such node.\n-  //\n-  // If prev is non-NULL, fills prev[level] with pointer to previous\n-  // node at \"level\" for every level in [0..max_height_-1].\n-  Node* FindGreaterOrEqual(const Key& key, Node** prev) const;\n-\n-  // Return the latest node with a key < key.\n-  // Return head_ if there is no such node.\n-  Node* FindLessThan(const Key& key) const;\n-\n-  // Return the last node in the list.\n-  // Return head_ if list is empty.\n-  Node* FindLast() const;\n-\n-  // No copying allowed\n-  SkipList(const SkipList&);\n-  void operator=(const SkipList&);\n-};\n-\n-// Implementation details follow\n-template<typename Key, class Comparator>\n-struct SkipList<Key,Comparator>::Node {\n-  explicit Node(const Key& k) : key(k) { }\n-\n-  Key const key;\n-\n-  // Accessors/mutators for links.  Wrapped in methods so we can\n-  // add the appropriate barriers as necessary.\n-  Node* Next(int n) {\n-    assert(n >= 0);\n-    // Use an 'acquire load' so that we observe a fully initialized\n-    // version of the returned Node.\n-    return reinterpret_cast<Node*>(next_[n].Acquire_Load());\n-  }\n-  void SetNext(int n, Node* x) {\n-    assert(n >= 0);\n-    // Use a 'release store' so that anybody who reads through this\n-    // pointer observes a fully initialized version of the inserted node.\n-    next_[n].Release_Store(x);\n-  }\n-\n-  // No-barrier variants that can be safely used in a few locations.\n-  Node* NoBarrier_Next(int n) {\n-    assert(n >= 0);\n-    return reinterpret_cast<Node*>(next_[n].NoBarrier_Load());\n-  }\n-  void NoBarrier_SetNext(int n, Node* x) {\n-    assert(n >= 0);\n-    next_[n].NoBarrier_Store(x);\n-  }\n-\n- private:\n-  // Array of length equal to the node height.  next_[0] is lowest level link.\n-  port::AtomicPointer next_[1];\n-};\n-\n-template<typename Key, class Comparator>\n-typename SkipList<Key,Comparator>::Node*\n-SkipList<Key,Comparator>::NewNode(const Key& key, int height) {\n-  char* mem = arena_->AllocateAligned(\n-      sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1));\n-  return new (mem) Node(key);\n-}\n-\n-template<typename Key, class Comparator>\n-inline SkipList<Key,Comparator>::Iterator::Iterator(const SkipList* list) {\n-  list_ = list;\n-  node_ = NULL;\n-}\n-\n-template<typename Key, class Comparator>\n-inline bool SkipList<Key,Comparator>::Iterator::Valid() const {\n-  return node_ != NULL;\n-}\n-\n-template<typename Key, class Comparator>\n-inline const Key& SkipList<Key,Comparator>::Iterator::key() const {\n-  assert(Valid());\n-  return node_->key;\n-}\n-\n-template<typename Key, class Comparator>\n-inline void SkipList<Key,Comparator>::Iterator::Next() {\n-  assert(Valid());\n-  node_ = node_->Next(0);\n-}\n-\n-template<typename Key, class Comparator>\n-inline void SkipList<Key,Comparator>::Iterator::Prev() {\n-  // Instead of using explicit \"prev\" links, we just search for the\n-  // last node that falls before key.\n-  assert(Valid());\n-  node_ = list_->FindLessThan(node_->key);\n-  if (node_ == list_->head_) {\n-    node_ = NULL;\n-  }\n-}\n-\n-template<typename Key, class Comparator>\n-inline void SkipList<Key,Comparator>::Iterator::Seek(const Key& target) {\n-  node_ = list_->FindGreaterOrEqual(target, NULL);\n-}\n-\n-template<typename Key, class Comparator>\n-inline void SkipList<Key,Comparator>::Iterator::SeekToFirst() {\n-  node_ = list_->head_->Next(0);\n-}\n-\n-template<typename Key, class Comparator>\n-inline void SkipList<Key,Comparator>::Iterator::SeekToLast() {\n-  node_ = list_->FindLast();\n-  if (node_ == list_->head_) {\n-    node_ = NULL;\n-  }\n-}\n-\n-template<typename Key, class Comparator>\n-int SkipList<Key,Comparator>::RandomHeight() {\n-  // Increase height with probability 1 in kBranching\n-  static const unsigned int kBranching = 4;\n-  int height = 1;\n-  while (height < kMaxHeight && ((rnd_.Next() % kBranching) == 0)) {\n-    height++;\n-  }\n-  assert(height > 0);\n-  assert(height <= kMaxHeight);\n-  return height;\n-}\n-\n-template<typename Key, class Comparator>\n-bool SkipList<Key,Comparator>::KeyIsAfterNode(const Key& key, Node* n) const {\n-  // NULL n is considered infinite\n-  return (n != NULL) && (compare_(n->key, key) < 0);\n-}\n-\n-template<typename Key, class Comparator>\n-typename SkipList<Key,Comparator>::Node* SkipList<Key,Comparator>::FindGreaterOrEqual(const Key& key, Node** prev)\n-    const {\n-  Node* x = head_;\n-  int level = GetMaxHeight() - 1;\n-  while (true) {\n-    Node* next = x->Next(level);\n-    if (KeyIsAfterNode(key, next)) {\n-      // Keep searching in this list\n-      x = next;\n-    } else {\n-      if (prev != NULL) prev[level] = x;\n-      if (level == 0) {\n-        return next;\n-      } else {\n-        // Switch to next list\n-        level--;\n-      }\n-    }\n-  }\n-}\n-\n-template<typename Key, class Comparator>\n-typename SkipList<Key,Comparator>::Node*\n-SkipList<Key,Comparator>::FindLessThan(const Key& key) const {\n-  Node* x = head_;\n-  int level = GetMaxHeight() - 1;\n-  while (true) {\n-    assert(x == head_ || compare_(x->key, key) < 0);\n-    Node* next = x->Next(level);\n-    if (next == NULL || compare_(next->key, key) >= 0) {\n-      if (level == 0) {\n-        return x;\n-      } else {\n-        // Switch to next list\n-        level--;\n-      }\n-    } else {\n-      x = next;\n-    }\n-  }\n-}\n-\n-template<typename Key, class Comparator>\n-typename SkipList<Key,Comparator>::Node* SkipList<Key,Comparator>::FindLast()\n-    const {\n-  Node* x = head_;\n-  int level = GetMaxHeight() - 1;\n-  while (true) {\n-    Node* next = x->Next(level);\n-    if (next == NULL) {\n-      if (level == 0) {\n-        return x;\n-      } else {\n-        // Switch to next list\n-        level--;\n-      }\n-    } else {\n-      x = next;\n-    }\n-  }\n-}\n-\n-template<typename Key, class Comparator>\n-SkipList<Key,Comparator>::SkipList(Comparator cmp, Arena* arena)\n-    : compare_(cmp),\n-      arena_(arena),\n-      head_(NewNode(0 /* any key will do */, kMaxHeight)),\n-      max_height_(reinterpret_cast<void*>(1)),\n-      rnd_(0xdeadbeef) {\n-  for (int i = 0; i < kMaxHeight; i++) {\n-    head_->SetNext(i, NULL);\n-  }\n-}\n-\n-template<typename Key, class Comparator>\n-void SkipList<Key,Comparator>::Insert(const Key& key) {\n-  // TODO(opt): We can use a barrier-free variant of FindGreaterOrEqual()\n-  // here since Insert() is externally synchronized.\n-  Node* prev[kMaxHeight];\n-  Node* x = FindGreaterOrEqual(key, prev);\n-\n-  // Our data structure does not allow duplicate insertion\n-  assert(x == NULL || !Equal(key, x->key));\n-\n-  int height = RandomHeight();\n-  if (height > GetMaxHeight()) {\n-    for (int i = GetMaxHeight(); i < height; i++) {\n-      prev[i] = head_;\n-    }\n-    //fprintf(stderr, \"Change height from %d to %d\\n\", max_height_, height);\n-\n-    // It is ok to mutate max_height_ without any synchronization\n-    // with concurrent readers.  A concurrent reader that observes\n-    // the new value of max_height_ will see either the old value of\n-    // new level pointers from head_ (NULL), or a new value set in\n-    // the loop below.  In the former case the reader will\n-    // immediately drop to the next level since NULL sorts after all\n-    // keys.  In the latter case the reader will use the new node.\n-    max_height_.NoBarrier_Store(reinterpret_cast<void*>(height));\n-  }\n-\n-  x = NewNode(key, height);\n-  for (int i = 0; i < height; i++) {\n-    // NoBarrier_SetNext() suffices since we will add a barrier when\n-    // we publish a pointer to \"x\" in prev[i].\n-    x->NoBarrier_SetNext(i, prev[i]->NoBarrier_Next(i));\n-    prev[i]->SetNext(i, x);\n-  }\n-}\n-\n-template<typename Key, class Comparator>\n-bool SkipList<Key,Comparator>::Contains(const Key& key) const {\n-  Node* x = FindGreaterOrEqual(key, NULL);\n-  if (x != NULL && Equal(key, x->key)) {\n-    return true;\n-  } else {\n-    return false;\n-  }\n-}\n-\n-}  // namespace leveldb"
      },
      {
        "sha": "c78f4b4fb1a0fc7fb5aadbefa0f7d19eaf06ba1b",
        "filename": "src/leveldb/db/skiplist_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 378,
        "changes": 378,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/skiplist_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/skiplist_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/skiplist_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,378 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/skiplist.h\"\n-#include <set>\n-#include \"leveldb/env.h\"\n-#include \"util/arena.h\"\n-#include \"util/hash.h\"\n-#include \"util/random.h\"\n-#include \"util/testharness.h\"\n-\n-namespace leveldb {\n-\n-typedef uint64_t Key;\n-\n-struct Comparator {\n-  int operator()(const Key& a, const Key& b) const {\n-    if (a < b) {\n-      return -1;\n-    } else if (a > b) {\n-      return +1;\n-    } else {\n-      return 0;\n-    }\n-  }\n-};\n-\n-class SkipTest { };\n-\n-TEST(SkipTest, Empty) {\n-  Arena arena;\n-  Comparator cmp;\n-  SkipList<Key, Comparator> list(cmp, &arena);\n-  ASSERT_TRUE(!list.Contains(10));\n-\n-  SkipList<Key, Comparator>::Iterator iter(&list);\n-  ASSERT_TRUE(!iter.Valid());\n-  iter.SeekToFirst();\n-  ASSERT_TRUE(!iter.Valid());\n-  iter.Seek(100);\n-  ASSERT_TRUE(!iter.Valid());\n-  iter.SeekToLast();\n-  ASSERT_TRUE(!iter.Valid());\n-}\n-\n-TEST(SkipTest, InsertAndLookup) {\n-  const int N = 2000;\n-  const int R = 5000;\n-  Random rnd(1000);\n-  std::set<Key> keys;\n-  Arena arena;\n-  Comparator cmp;\n-  SkipList<Key, Comparator> list(cmp, &arena);\n-  for (int i = 0; i < N; i++) {\n-    Key key = rnd.Next() % R;\n-    if (keys.insert(key).second) {\n-      list.Insert(key);\n-    }\n-  }\n-\n-  for (int i = 0; i < R; i++) {\n-    if (list.Contains(i)) {\n-      ASSERT_EQ(keys.count(i), 1);\n-    } else {\n-      ASSERT_EQ(keys.count(i), 0);\n-    }\n-  }\n-\n-  // Simple iterator tests\n-  {\n-    SkipList<Key, Comparator>::Iterator iter(&list);\n-    ASSERT_TRUE(!iter.Valid());\n-\n-    iter.Seek(0);\n-    ASSERT_TRUE(iter.Valid());\n-    ASSERT_EQ(*(keys.begin()), iter.key());\n-\n-    iter.SeekToFirst();\n-    ASSERT_TRUE(iter.Valid());\n-    ASSERT_EQ(*(keys.begin()), iter.key());\n-\n-    iter.SeekToLast();\n-    ASSERT_TRUE(iter.Valid());\n-    ASSERT_EQ(*(keys.rbegin()), iter.key());\n-  }\n-\n-  // Forward iteration test\n-  for (int i = 0; i < R; i++) {\n-    SkipList<Key, Comparator>::Iterator iter(&list);\n-    iter.Seek(i);\n-\n-    // Compare against model iterator\n-    std::set<Key>::iterator model_iter = keys.lower_bound(i);\n-    for (int j = 0; j < 3; j++) {\n-      if (model_iter == keys.end()) {\n-        ASSERT_TRUE(!iter.Valid());\n-        break;\n-      } else {\n-        ASSERT_TRUE(iter.Valid());\n-        ASSERT_EQ(*model_iter, iter.key());\n-        ++model_iter;\n-        iter.Next();\n-      }\n-    }\n-  }\n-\n-  // Backward iteration test\n-  {\n-    SkipList<Key, Comparator>::Iterator iter(&list);\n-    iter.SeekToLast();\n-\n-    // Compare against model iterator\n-    for (std::set<Key>::reverse_iterator model_iter = keys.rbegin();\n-         model_iter != keys.rend();\n-         ++model_iter) {\n-      ASSERT_TRUE(iter.Valid());\n-      ASSERT_EQ(*model_iter, iter.key());\n-      iter.Prev();\n-    }\n-    ASSERT_TRUE(!iter.Valid());\n-  }\n-}\n-\n-// We want to make sure that with a single writer and multiple\n-// concurrent readers (with no synchronization other than when a\n-// reader's iterator is created), the reader always observes all the\n-// data that was present in the skip list when the iterator was\n-// constructor.  Because insertions are happening concurrently, we may\n-// also observe new values that were inserted since the iterator was\n-// constructed, but we should never miss any values that were present\n-// at iterator construction time.\n-//\n-// We generate multi-part keys:\n-//     <key,gen,hash>\n-// where:\n-//     key is in range [0..K-1]\n-//     gen is a generation number for key\n-//     hash is hash(key,gen)\n-//\n-// The insertion code picks a random key, sets gen to be 1 + the last\n-// generation number inserted for that key, and sets hash to Hash(key,gen).\n-//\n-// At the beginning of a read, we snapshot the last inserted\n-// generation number for each key.  We then iterate, including random\n-// calls to Next() and Seek().  For every key we encounter, we\n-// check that it is either expected given the initial snapshot or has\n-// been concurrently added since the iterator started.\n-class ConcurrentTest {\n- private:\n-  static const uint32_t K = 4;\n-\n-  static uint64_t key(Key key) { return (key >> 40); }\n-  static uint64_t gen(Key key) { return (key >> 8) & 0xffffffffu; }\n-  static uint64_t hash(Key key) { return key & 0xff; }\n-\n-  static uint64_t HashNumbers(uint64_t k, uint64_t g) {\n-    uint64_t data[2] = { k, g };\n-    return Hash(reinterpret_cast<char*>(data), sizeof(data), 0);\n-  }\n-\n-  static Key MakeKey(uint64_t k, uint64_t g) {\n-    assert(sizeof(Key) == sizeof(uint64_t));\n-    assert(k <= K);  // We sometimes pass K to seek to the end of the skiplist\n-    assert(g <= 0xffffffffu);\n-    return ((k << 40) | (g << 8) | (HashNumbers(k, g) & 0xff));\n-  }\n-\n-  static bool IsValidKey(Key k) {\n-    return hash(k) == (HashNumbers(key(k), gen(k)) & 0xff);\n-  }\n-\n-  static Key RandomTarget(Random* rnd) {\n-    switch (rnd->Next() % 10) {\n-      case 0:\n-        // Seek to beginning\n-        return MakeKey(0, 0);\n-      case 1:\n-        // Seek to end\n-        return MakeKey(K, 0);\n-      default:\n-        // Seek to middle\n-        return MakeKey(rnd->Next() % K, 0);\n-    }\n-  }\n-\n-  // Per-key generation\n-  struct State {\n-    port::AtomicPointer generation[K];\n-    void Set(int k, intptr_t v) {\n-      generation[k].Release_Store(reinterpret_cast<void*>(v));\n-    }\n-    intptr_t Get(int k) {\n-      return reinterpret_cast<intptr_t>(generation[k].Acquire_Load());\n-    }\n-\n-    State() {\n-      for (int k = 0; k < K; k++) {\n-        Set(k, 0);\n-      }\n-    }\n-  };\n-\n-  // Current state of the test\n-  State current_;\n-\n-  Arena arena_;\n-\n-  // SkipList is not protected by mu_.  We just use a single writer\n-  // thread to modify it.\n-  SkipList<Key, Comparator> list_;\n-\n- public:\n-  ConcurrentTest() : list_(Comparator(), &arena_) { }\n-\n-  // REQUIRES: External synchronization\n-  void WriteStep(Random* rnd) {\n-    const uint32_t k = rnd->Next() % K;\n-    const intptr_t g = current_.Get(k) + 1;\n-    const Key key = MakeKey(k, g);\n-    list_.Insert(key);\n-    current_.Set(k, g);\n-  }\n-\n-  void ReadStep(Random* rnd) {\n-    // Remember the initial committed state of the skiplist.\n-    State initial_state;\n-    for (int k = 0; k < K; k++) {\n-      initial_state.Set(k, current_.Get(k));\n-    }\n-\n-    Key pos = RandomTarget(rnd);\n-    SkipList<Key, Comparator>::Iterator iter(&list_);\n-    iter.Seek(pos);\n-    while (true) {\n-      Key current;\n-      if (!iter.Valid()) {\n-        current = MakeKey(K, 0);\n-      } else {\n-        current = iter.key();\n-        ASSERT_TRUE(IsValidKey(current)) << current;\n-      }\n-      ASSERT_LE(pos, current) << \"should not go backwards\";\n-\n-      // Verify that everything in [pos,current) was not present in\n-      // initial_state.\n-      while (pos < current) {\n-        ASSERT_LT(key(pos), K) << pos;\n-\n-        // Note that generation 0 is never inserted, so it is ok if\n-        // <*,0,*> is missing.\n-        ASSERT_TRUE((gen(pos) == 0) ||\n-                    (gen(pos) > initial_state.Get(key(pos)))\n-                    ) << \"key: \" << key(pos)\n-                      << \"; gen: \" << gen(pos)\n-                      << \"; initgen: \"\n-                      << initial_state.Get(key(pos));\n-\n-        // Advance to next key in the valid key space\n-        if (key(pos) < key(current)) {\n-          pos = MakeKey(key(pos) + 1, 0);\n-        } else {\n-          pos = MakeKey(key(pos), gen(pos) + 1);\n-        }\n-      }\n-\n-      if (!iter.Valid()) {\n-        break;\n-      }\n-\n-      if (rnd->Next() % 2) {\n-        iter.Next();\n-        pos = MakeKey(key(pos), gen(pos) + 1);\n-      } else {\n-        Key new_target = RandomTarget(rnd);\n-        if (new_target > pos) {\n-          pos = new_target;\n-          iter.Seek(new_target);\n-        }\n-      }\n-    }\n-  }\n-};\n-const uint32_t ConcurrentTest::K;\n-\n-// Simple test that does single-threaded testing of the ConcurrentTest\n-// scaffolding.\n-TEST(SkipTest, ConcurrentWithoutThreads) {\n-  ConcurrentTest test;\n-  Random rnd(test::RandomSeed());\n-  for (int i = 0; i < 10000; i++) {\n-    test.ReadStep(&rnd);\n-    test.WriteStep(&rnd);\n-  }\n-}\n-\n-class TestState {\n- public:\n-  ConcurrentTest t_;\n-  int seed_;\n-  port::AtomicPointer quit_flag_;\n-\n-  enum ReaderState {\n-    STARTING,\n-    RUNNING,\n-    DONE\n-  };\n-\n-  explicit TestState(int s)\n-      : seed_(s),\n-        quit_flag_(NULL),\n-        state_(STARTING),\n-        state_cv_(&mu_) {}\n-\n-  void Wait(ReaderState s) {\n-    mu_.Lock();\n-    while (state_ != s) {\n-      state_cv_.Wait();\n-    }\n-    mu_.Unlock();\n-  }\n-\n-  void Change(ReaderState s) {\n-    mu_.Lock();\n-    state_ = s;\n-    state_cv_.Signal();\n-    mu_.Unlock();\n-  }\n-\n- private:\n-  port::Mutex mu_;\n-  ReaderState state_;\n-  port::CondVar state_cv_;\n-};\n-\n-static void ConcurrentReader(void* arg) {\n-  TestState* state = reinterpret_cast<TestState*>(arg);\n-  Random rnd(state->seed_);\n-  int64_t reads = 0;\n-  state->Change(TestState::RUNNING);\n-  while (!state->quit_flag_.Acquire_Load()) {\n-    state->t_.ReadStep(&rnd);\n-    ++reads;\n-  }\n-  state->Change(TestState::DONE);\n-}\n-\n-static void RunConcurrent(int run) {\n-  const int seed = test::RandomSeed() + (run * 100);\n-  Random rnd(seed);\n-  const int N = 1000;\n-  const int kSize = 1000;\n-  for (int i = 0; i < N; i++) {\n-    if ((i % 100) == 0) {\n-      fprintf(stderr, \"Run %d of %d\\n\", i, N);\n-    }\n-    TestState state(seed + 1);\n-    Env::Default()->Schedule(ConcurrentReader, &state);\n-    state.Wait(TestState::RUNNING);\n-    for (int i = 0; i < kSize; i++) {\n-      state.t_.WriteStep(&rnd);\n-    }\n-    state.quit_flag_.Release_Store(&state);  // Any non-NULL arg will do\n-    state.Wait(TestState::DONE);\n-  }\n-}\n-\n-TEST(SkipTest, Concurrent1) { RunConcurrent(1); }\n-TEST(SkipTest, Concurrent2) { RunConcurrent(2); }\n-TEST(SkipTest, Concurrent3) { RunConcurrent(3); }\n-TEST(SkipTest, Concurrent4) { RunConcurrent(4); }\n-TEST(SkipTest, Concurrent5) { RunConcurrent(5); }\n-\n-}  // namespace leveldb\n-\n-int main(int argc, char** argv) {\n-  return leveldb::test::RunAllTests();\n-}"
      },
      {
        "sha": "e7f8fd2c37cf8a482c45f5524aa729e74f7b3aa7",
        "filename": "src/leveldb/db/snapshot.h",
        "status": "removed",
        "additions": 0,
        "deletions": 66,
        "changes": 66,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/snapshot.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/snapshot.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/snapshot.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,66 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_DB_SNAPSHOT_H_\n-#define STORAGE_LEVELDB_DB_SNAPSHOT_H_\n-\n-#include \"leveldb/db.h\"\n-\n-namespace leveldb {\n-\n-class SnapshotList;\n-\n-// Snapshots are kept in a doubly-linked list in the DB.\n-// Each SnapshotImpl corresponds to a particular sequence number.\n-class SnapshotImpl : public Snapshot {\n- public:\n-  SequenceNumber number_;  // const after creation\n-\n- private:\n-  friend class SnapshotList;\n-\n-  // SnapshotImpl is kept in a doubly-linked circular list\n-  SnapshotImpl* prev_;\n-  SnapshotImpl* next_;\n-\n-  SnapshotList* list_;                 // just for sanity checks\n-};\n-\n-class SnapshotList {\n- public:\n-  SnapshotList() {\n-    list_.prev_ = &list_;\n-    list_.next_ = &list_;\n-  }\n-\n-  bool empty() const { return list_.next_ == &list_; }\n-  SnapshotImpl* oldest() const { assert(!empty()); return list_.next_; }\n-  SnapshotImpl* newest() const { assert(!empty()); return list_.prev_; }\n-\n-  const SnapshotImpl* New(SequenceNumber seq) {\n-    SnapshotImpl* s = new SnapshotImpl;\n-    s->number_ = seq;\n-    s->list_ = this;\n-    s->next_ = &list_;\n-    s->prev_ = list_.prev_;\n-    s->prev_->next_ = s;\n-    s->next_->prev_ = s;\n-    return s;\n-  }\n-\n-  void Delete(const SnapshotImpl* s) {\n-    assert(s->list_ == this);\n-    s->prev_->next_ = s->next_;\n-    s->next_->prev_ = s->prev_;\n-    delete s;\n-  }\n-\n- private:\n-  // Dummy head of doubly-linked list of snapshots\n-  SnapshotImpl list_;\n-};\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_DB_SNAPSHOT_H_"
      },
      {
        "sha": "497db270766d8857ddb355ad09ed0892e4ab2daa",
        "filename": "src/leveldb/db/table_cache.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 121,
        "changes": 121,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/table_cache.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/table_cache.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/table_cache.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,121 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/table_cache.h\"\n-\n-#include \"db/filename.h\"\n-#include \"leveldb/env.h\"\n-#include \"leveldb/table.h\"\n-#include \"util/coding.h\"\n-\n-namespace leveldb {\n-\n-struct TableAndFile {\n-  RandomAccessFile* file;\n-  Table* table;\n-};\n-\n-static void DeleteEntry(const Slice& key, void* value) {\n-  TableAndFile* tf = reinterpret_cast<TableAndFile*>(value);\n-  delete tf->table;\n-  delete tf->file;\n-  delete tf;\n-}\n-\n-static void UnrefEntry(void* arg1, void* arg2) {\n-  Cache* cache = reinterpret_cast<Cache*>(arg1);\n-  Cache::Handle* h = reinterpret_cast<Cache::Handle*>(arg2);\n-  cache->Release(h);\n-}\n-\n-TableCache::TableCache(const std::string& dbname,\n-                       const Options* options,\n-                       int entries)\n-    : env_(options->env),\n-      dbname_(dbname),\n-      options_(options),\n-      cache_(NewLRUCache(entries)) {\n-}\n-\n-TableCache::~TableCache() {\n-  delete cache_;\n-}\n-\n-Status TableCache::FindTable(uint64_t file_number, uint64_t file_size,\n-                             Cache::Handle** handle) {\n-  Status s;\n-  char buf[sizeof(file_number)];\n-  EncodeFixed64(buf, file_number);\n-  Slice key(buf, sizeof(buf));\n-  *handle = cache_->Lookup(key);\n-  if (*handle == NULL) {\n-    std::string fname = TableFileName(dbname_, file_number);\n-    RandomAccessFile* file = NULL;\n-    Table* table = NULL;\n-    s = env_->NewRandomAccessFile(fname, &file);\n-    if (s.ok()) {\n-      s = Table::Open(*options_, file, file_size, &table);\n-    }\n-\n-    if (!s.ok()) {\n-      assert(table == NULL);\n-      delete file;\n-      // We do not cache error results so that if the error is transient,\n-      // or somebody repairs the file, we recover automatically.\n-    } else {\n-      TableAndFile* tf = new TableAndFile;\n-      tf->file = file;\n-      tf->table = table;\n-      *handle = cache_->Insert(key, tf, 1, &DeleteEntry);\n-    }\n-  }\n-  return s;\n-}\n-\n-Iterator* TableCache::NewIterator(const ReadOptions& options,\n-                                  uint64_t file_number,\n-                                  uint64_t file_size,\n-                                  Table** tableptr) {\n-  if (tableptr != NULL) {\n-    *tableptr = NULL;\n-  }\n-\n-  Cache::Handle* handle = NULL;\n-  Status s = FindTable(file_number, file_size, &handle);\n-  if (!s.ok()) {\n-    return NewErrorIterator(s);\n-  }\n-\n-  Table* table = reinterpret_cast<TableAndFile*>(cache_->Value(handle))->table;\n-  Iterator* result = table->NewIterator(options);\n-  result->RegisterCleanup(&UnrefEntry, cache_, handle);\n-  if (tableptr != NULL) {\n-    *tableptr = table;\n-  }\n-  return result;\n-}\n-\n-Status TableCache::Get(const ReadOptions& options,\n-                       uint64_t file_number,\n-                       uint64_t file_size,\n-                       const Slice& k,\n-                       void* arg,\n-                       void (*saver)(void*, const Slice&, const Slice&)) {\n-  Cache::Handle* handle = NULL;\n-  Status s = FindTable(file_number, file_size, &handle);\n-  if (s.ok()) {\n-    Table* t = reinterpret_cast<TableAndFile*>(cache_->Value(handle))->table;\n-    s = t->InternalGet(options, k, arg, saver);\n-    cache_->Release(handle);\n-  }\n-  return s;\n-}\n-\n-void TableCache::Evict(uint64_t file_number) {\n-  char buf[sizeof(file_number)];\n-  EncodeFixed64(buf, file_number);\n-  cache_->Erase(Slice(buf, sizeof(buf)));\n-}\n-\n-}  // namespace leveldb"
      },
      {
        "sha": "8cf4aaf12d8ed1a02bd7d1962b79cc8506575b6f",
        "filename": "src/leveldb/db/table_cache.h",
        "status": "removed",
        "additions": 0,
        "deletions": 61,
        "changes": 61,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/table_cache.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/table_cache.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/table_cache.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,61 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// Thread-safe (provides internal synchronization)\n-\n-#ifndef STORAGE_LEVELDB_DB_TABLE_CACHE_H_\n-#define STORAGE_LEVELDB_DB_TABLE_CACHE_H_\n-\n-#include <string>\n-#include <stdint.h>\n-#include \"db/dbformat.h\"\n-#include \"leveldb/cache.h\"\n-#include \"leveldb/table.h\"\n-#include \"port/port.h\"\n-\n-namespace leveldb {\n-\n-class Env;\n-\n-class TableCache {\n- public:\n-  TableCache(const std::string& dbname, const Options* options, int entries);\n-  ~TableCache();\n-\n-  // Return an iterator for the specified file number (the corresponding\n-  // file length must be exactly \"file_size\" bytes).  If \"tableptr\" is\n-  // non-NULL, also sets \"*tableptr\" to point to the Table object\n-  // underlying the returned iterator, or NULL if no Table object underlies\n-  // the returned iterator.  The returned \"*tableptr\" object is owned by\n-  // the cache and should not be deleted, and is valid for as long as the\n-  // returned iterator is live.\n-  Iterator* NewIterator(const ReadOptions& options,\n-                        uint64_t file_number,\n-                        uint64_t file_size,\n-                        Table** tableptr = NULL);\n-\n-  // If a seek to internal key \"k\" in specified file finds an entry,\n-  // call (*handle_result)(arg, found_key, found_value).\n-  Status Get(const ReadOptions& options,\n-             uint64_t file_number,\n-             uint64_t file_size,\n-             const Slice& k,\n-             void* arg,\n-             void (*handle_result)(void*, const Slice&, const Slice&));\n-\n-  // Evict any entry for the specified file number\n-  void Evict(uint64_t file_number);\n-\n- private:\n-  Env* const env_;\n-  const std::string dbname_;\n-  const Options* options_;\n-  Cache* cache_;\n-\n-  Status FindTable(uint64_t file_number, uint64_t file_size, Cache::Handle**);\n-};\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_DB_TABLE_CACHE_H_"
      },
      {
        "sha": "f10a2d58b211cb16becb0ac0298210f0dacbd2a5",
        "filename": "src/leveldb/db/version_edit.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 266,
        "changes": 266,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/version_edit.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/version_edit.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_edit.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,266 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/version_edit.h\"\n-\n-#include \"db/version_set.h\"\n-#include \"util/coding.h\"\n-\n-namespace leveldb {\n-\n-// Tag numbers for serialized VersionEdit.  These numbers are written to\n-// disk and should not be changed.\n-enum Tag {\n-  kComparator           = 1,\n-  kLogNumber            = 2,\n-  kNextFileNumber       = 3,\n-  kLastSequence         = 4,\n-  kCompactPointer       = 5,\n-  kDeletedFile          = 6,\n-  kNewFile              = 7,\n-  // 8 was used for large value refs\n-  kPrevLogNumber        = 9\n-};\n-\n-void VersionEdit::Clear() {\n-  comparator_.clear();\n-  log_number_ = 0;\n-  prev_log_number_ = 0;\n-  last_sequence_ = 0;\n-  next_file_number_ = 0;\n-  has_comparator_ = false;\n-  has_log_number_ = false;\n-  has_prev_log_number_ = false;\n-  has_next_file_number_ = false;\n-  has_last_sequence_ = false;\n-  deleted_files_.clear();\n-  new_files_.clear();\n-}\n-\n-void VersionEdit::EncodeTo(std::string* dst) const {\n-  if (has_comparator_) {\n-    PutVarint32(dst, kComparator);\n-    PutLengthPrefixedSlice(dst, comparator_);\n-  }\n-  if (has_log_number_) {\n-    PutVarint32(dst, kLogNumber);\n-    PutVarint64(dst, log_number_);\n-  }\n-  if (has_prev_log_number_) {\n-    PutVarint32(dst, kPrevLogNumber);\n-    PutVarint64(dst, prev_log_number_);\n-  }\n-  if (has_next_file_number_) {\n-    PutVarint32(dst, kNextFileNumber);\n-    PutVarint64(dst, next_file_number_);\n-  }\n-  if (has_last_sequence_) {\n-    PutVarint32(dst, kLastSequence);\n-    PutVarint64(dst, last_sequence_);\n-  }\n-\n-  for (size_t i = 0; i < compact_pointers_.size(); i++) {\n-    PutVarint32(dst, kCompactPointer);\n-    PutVarint32(dst, compact_pointers_[i].first);  // level\n-    PutLengthPrefixedSlice(dst, compact_pointers_[i].second.Encode());\n-  }\n-\n-  for (DeletedFileSet::const_iterator iter = deleted_files_.begin();\n-       iter != deleted_files_.end();\n-       ++iter) {\n-    PutVarint32(dst, kDeletedFile);\n-    PutVarint32(dst, iter->first);   // level\n-    PutVarint64(dst, iter->second);  // file number\n-  }\n-\n-  for (size_t i = 0; i < new_files_.size(); i++) {\n-    const FileMetaData& f = new_files_[i].second;\n-    PutVarint32(dst, kNewFile);\n-    PutVarint32(dst, new_files_[i].first);  // level\n-    PutVarint64(dst, f.number);\n-    PutVarint64(dst, f.file_size);\n-    PutLengthPrefixedSlice(dst, f.smallest.Encode());\n-    PutLengthPrefixedSlice(dst, f.largest.Encode());\n-  }\n-}\n-\n-static bool GetInternalKey(Slice* input, InternalKey* dst) {\n-  Slice str;\n-  if (GetLengthPrefixedSlice(input, &str)) {\n-    dst->DecodeFrom(str);\n-    return true;\n-  } else {\n-    return false;\n-  }\n-}\n-\n-static bool GetLevel(Slice* input, int* level) {\n-  uint32_t v;\n-  if (GetVarint32(input, &v) &&\n-      v < config::kNumLevels) {\n-    *level = v;\n-    return true;\n-  } else {\n-    return false;\n-  }\n-}\n-\n-Status VersionEdit::DecodeFrom(const Slice& src) {\n-  Clear();\n-  Slice input = src;\n-  const char* msg = NULL;\n-  uint32_t tag;\n-\n-  // Temporary storage for parsing\n-  int level;\n-  uint64_t number;\n-  FileMetaData f;\n-  Slice str;\n-  InternalKey key;\n-\n-  while (msg == NULL && GetVarint32(&input, &tag)) {\n-    switch (tag) {\n-      case kComparator:\n-        if (GetLengthPrefixedSlice(&input, &str)) {\n-          comparator_ = str.ToString();\n-          has_comparator_ = true;\n-        } else {\n-          msg = \"comparator name\";\n-        }\n-        break;\n-\n-      case kLogNumber:\n-        if (GetVarint64(&input, &log_number_)) {\n-          has_log_number_ = true;\n-        } else {\n-          msg = \"log number\";\n-        }\n-        break;\n-\n-      case kPrevLogNumber:\n-        if (GetVarint64(&input, &prev_log_number_)) {\n-          has_prev_log_number_ = true;\n-        } else {\n-          msg = \"previous log number\";\n-        }\n-        break;\n-\n-      case kNextFileNumber:\n-        if (GetVarint64(&input, &next_file_number_)) {\n-          has_next_file_number_ = true;\n-        } else {\n-          msg = \"next file number\";\n-        }\n-        break;\n-\n-      case kLastSequence:\n-        if (GetVarint64(&input, &last_sequence_)) {\n-          has_last_sequence_ = true;\n-        } else {\n-          msg = \"last sequence number\";\n-        }\n-        break;\n-\n-      case kCompactPointer:\n-        if (GetLevel(&input, &level) &&\n-            GetInternalKey(&input, &key)) {\n-          compact_pointers_.push_back(std::make_pair(level, key));\n-        } else {\n-          msg = \"compaction pointer\";\n-        }\n-        break;\n-\n-      case kDeletedFile:\n-        if (GetLevel(&input, &level) &&\n-            GetVarint64(&input, &number)) {\n-          deleted_files_.insert(std::make_pair(level, number));\n-        } else {\n-          msg = \"deleted file\";\n-        }\n-        break;\n-\n-      case kNewFile:\n-        if (GetLevel(&input, &level) &&\n-            GetVarint64(&input, &f.number) &&\n-            GetVarint64(&input, &f.file_size) &&\n-            GetInternalKey(&input, &f.smallest) &&\n-            GetInternalKey(&input, &f.largest)) {\n-          new_files_.push_back(std::make_pair(level, f));\n-        } else {\n-          msg = \"new-file entry\";\n-        }\n-        break;\n-\n-      default:\n-        msg = \"unknown tag\";\n-        break;\n-    }\n-  }\n-\n-  if (msg == NULL && !input.empty()) {\n-    msg = \"invalid tag\";\n-  }\n-\n-  Status result;\n-  if (msg != NULL) {\n-    result = Status::Corruption(\"VersionEdit\", msg);\n-  }\n-  return result;\n-}\n-\n-std::string VersionEdit::DebugString() const {\n-  std::string r;\n-  r.append(\"VersionEdit {\");\n-  if (has_comparator_) {\n-    r.append(\"\\n  Comparator: \");\n-    r.append(comparator_);\n-  }\n-  if (has_log_number_) {\n-    r.append(\"\\n  LogNumber: \");\n-    AppendNumberTo(&r, log_number_);\n-  }\n-  if (has_prev_log_number_) {\n-    r.append(\"\\n  PrevLogNumber: \");\n-    AppendNumberTo(&r, prev_log_number_);\n-  }\n-  if (has_next_file_number_) {\n-    r.append(\"\\n  NextFile: \");\n-    AppendNumberTo(&r, next_file_number_);\n-  }\n-  if (has_last_sequence_) {\n-    r.append(\"\\n  LastSeq: \");\n-    AppendNumberTo(&r, last_sequence_);\n-  }\n-  for (size_t i = 0; i < compact_pointers_.size(); i++) {\n-    r.append(\"\\n  CompactPointer: \");\n-    AppendNumberTo(&r, compact_pointers_[i].first);\n-    r.append(\" \");\n-    r.append(compact_pointers_[i].second.DebugString());\n-  }\n-  for (DeletedFileSet::const_iterator iter = deleted_files_.begin();\n-       iter != deleted_files_.end();\n-       ++iter) {\n-    r.append(\"\\n  DeleteFile: \");\n-    AppendNumberTo(&r, iter->first);\n-    r.append(\" \");\n-    AppendNumberTo(&r, iter->second);\n-  }\n-  for (size_t i = 0; i < new_files_.size(); i++) {\n-    const FileMetaData& f = new_files_[i].second;\n-    r.append(\"\\n  AddFile: \");\n-    AppendNumberTo(&r, new_files_[i].first);\n-    r.append(\" \");\n-    AppendNumberTo(&r, f.number);\n-    r.append(\" \");\n-    AppendNumberTo(&r, f.file_size);\n-    r.append(\" \");\n-    r.append(f.smallest.DebugString());\n-    r.append(\" .. \");\n-    r.append(f.largest.DebugString());\n-  }\n-  r.append(\"\\n}\\n\");\n-  return r;\n-}\n-\n-}  // namespace leveldb"
      },
      {
        "sha": "eaef77b327c64a3756a5f2512bc1786530907cbd",
        "filename": "src/leveldb/db/version_edit.h",
        "status": "removed",
        "additions": 0,
        "deletions": 107,
        "changes": 107,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/version_edit.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/version_edit.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_edit.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,107 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_DB_VERSION_EDIT_H_\n-#define STORAGE_LEVELDB_DB_VERSION_EDIT_H_\n-\n-#include <set>\n-#include <utility>\n-#include <vector>\n-#include \"db/dbformat.h\"\n-\n-namespace leveldb {\n-\n-class VersionSet;\n-\n-struct FileMetaData {\n-  int refs;\n-  int allowed_seeks;          // Seeks allowed until compaction\n-  uint64_t number;\n-  uint64_t file_size;         // File size in bytes\n-  InternalKey smallest;       // Smallest internal key served by table\n-  InternalKey largest;        // Largest internal key served by table\n-\n-  FileMetaData() : refs(0), allowed_seeks(1 << 30), file_size(0) { }\n-};\n-\n-class VersionEdit {\n- public:\n-  VersionEdit() { Clear(); }\n-  ~VersionEdit() { }\n-\n-  void Clear();\n-\n-  void SetComparatorName(const Slice& name) {\n-    has_comparator_ = true;\n-    comparator_ = name.ToString();\n-  }\n-  void SetLogNumber(uint64_t num) {\n-    has_log_number_ = true;\n-    log_number_ = num;\n-  }\n-  void SetPrevLogNumber(uint64_t num) {\n-    has_prev_log_number_ = true;\n-    prev_log_number_ = num;\n-  }\n-  void SetNextFile(uint64_t num) {\n-    has_next_file_number_ = true;\n-    next_file_number_ = num;\n-  }\n-  void SetLastSequence(SequenceNumber seq) {\n-    has_last_sequence_ = true;\n-    last_sequence_ = seq;\n-  }\n-  void SetCompactPointer(int level, const InternalKey& key) {\n-    compact_pointers_.push_back(std::make_pair(level, key));\n-  }\n-\n-  // Add the specified file at the specified number.\n-  // REQUIRES: This version has not been saved (see VersionSet::SaveTo)\n-  // REQUIRES: \"smallest\" and \"largest\" are smallest and largest keys in file\n-  void AddFile(int level, uint64_t file,\n-               uint64_t file_size,\n-               const InternalKey& smallest,\n-               const InternalKey& largest) {\n-    FileMetaData f;\n-    f.number = file;\n-    f.file_size = file_size;\n-    f.smallest = smallest;\n-    f.largest = largest;\n-    new_files_.push_back(std::make_pair(level, f));\n-  }\n-\n-  // Delete the specified \"file\" from the specified \"level\".\n-  void DeleteFile(int level, uint64_t file) {\n-    deleted_files_.insert(std::make_pair(level, file));\n-  }\n-\n-  void EncodeTo(std::string* dst) const;\n-  Status DecodeFrom(const Slice& src);\n-\n-  std::string DebugString() const;\n-\n- private:\n-  friend class VersionSet;\n-\n-  typedef std::set< std::pair<int, uint64_t> > DeletedFileSet;\n-\n-  std::string comparator_;\n-  uint64_t log_number_;\n-  uint64_t prev_log_number_;\n-  uint64_t next_file_number_;\n-  SequenceNumber last_sequence_;\n-  bool has_comparator_;\n-  bool has_log_number_;\n-  bool has_prev_log_number_;\n-  bool has_next_file_number_;\n-  bool has_last_sequence_;\n-\n-  std::vector< std::pair<int, InternalKey> > compact_pointers_;\n-  DeletedFileSet deleted_files_;\n-  std::vector< std::pair<int, FileMetaData> > new_files_;\n-};\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_DB_VERSION_EDIT_H_"
      },
      {
        "sha": "280310b49d846e245df0d000ca1407724582daa2",
        "filename": "src/leveldb/db/version_edit_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 46,
        "changes": 46,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/version_edit_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/version_edit_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_edit_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,46 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/version_edit.h\"\n-#include \"util/testharness.h\"\n-\n-namespace leveldb {\n-\n-static void TestEncodeDecode(const VersionEdit& edit) {\n-  std::string encoded, encoded2;\n-  edit.EncodeTo(&encoded);\n-  VersionEdit parsed;\n-  Status s = parsed.DecodeFrom(encoded);\n-  ASSERT_TRUE(s.ok()) << s.ToString();\n-  parsed.EncodeTo(&encoded2);\n-  ASSERT_EQ(encoded, encoded2);\n-}\n-\n-class VersionEditTest { };\n-\n-TEST(VersionEditTest, EncodeDecode) {\n-  static const uint64_t kBig = 1ull << 50;\n-\n-  VersionEdit edit;\n-  for (int i = 0; i < 4; i++) {\n-    TestEncodeDecode(edit);\n-    edit.AddFile(3, kBig + 300 + i, kBig + 400 + i,\n-                 InternalKey(\"foo\", kBig + 500 + i, kTypeValue),\n-                 InternalKey(\"zoo\", kBig + 600 + i, kTypeDeletion));\n-    edit.DeleteFile(4, kBig + 700 + i);\n-    edit.SetCompactPointer(i, InternalKey(\"x\", kBig + 900 + i, kTypeValue));\n-  }\n-\n-  edit.SetComparatorName(\"foo\");\n-  edit.SetLogNumber(kBig + 100);\n-  edit.SetNextFile(kBig + 200);\n-  edit.SetLastSequence(kBig + 1000);\n-  TestEncodeDecode(edit);\n-}\n-\n-}  // namespace leveldb\n-\n-int main(int argc, char** argv) {\n-  return leveldb::test::RunAllTests();\n-}"
      },
      {
        "sha": "7d0a5de2b9f3e0559417a73204e6668bbed1793b",
        "filename": "src/leveldb/db/version_set.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 1438,
        "changes": 1438,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/version_set.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/version_set.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_set.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,1438 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/version_set.h\"\n-\n-#include <algorithm>\n-#include <stdio.h>\n-#include \"db/filename.h\"\n-#include \"db/log_reader.h\"\n-#include \"db/log_writer.h\"\n-#include \"db/memtable.h\"\n-#include \"db/table_cache.h\"\n-#include \"leveldb/env.h\"\n-#include \"leveldb/table_builder.h\"\n-#include \"table/merger.h\"\n-#include \"table/two_level_iterator.h\"\n-#include \"util/coding.h\"\n-#include \"util/logging.h\"\n-\n-namespace leveldb {\n-\n-static const int kTargetFileSize = 2 * 1048576;\n-\n-// Maximum bytes of overlaps in grandparent (i.e., level+2) before we\n-// stop building a single file in a level->level+1 compaction.\n-static const int64_t kMaxGrandParentOverlapBytes = 10 * kTargetFileSize;\n-\n-// Maximum number of bytes in all compacted files.  We avoid expanding\n-// the lower level file set of a compaction if it would make the\n-// total compaction cover more than this many bytes.\n-static const int64_t kExpandedCompactionByteSizeLimit = 25 * kTargetFileSize;\n-\n-static double MaxBytesForLevel(int level) {\n-  // Note: the result for level zero is not really used since we set\n-  // the level-0 compaction threshold based on number of files.\n-  double result = 10 * 1048576.0;  // Result for both level-0 and level-1\n-  while (level > 1) {\n-    result *= 10;\n-    level--;\n-  }\n-  return result;\n-}\n-\n-static uint64_t MaxFileSizeForLevel(int level) {\n-  return kTargetFileSize;  // We could vary per level to reduce number of files?\n-}\n-\n-static int64_t TotalFileSize(const std::vector<FileMetaData*>& files) {\n-  int64_t sum = 0;\n-  for (size_t i = 0; i < files.size(); i++) {\n-    sum += files[i]->file_size;\n-  }\n-  return sum;\n-}\n-\n-namespace {\n-std::string IntSetToString(const std::set<uint64_t>& s) {\n-  std::string result = \"{\";\n-  for (std::set<uint64_t>::const_iterator it = s.begin();\n-       it != s.end();\n-       ++it) {\n-    result += (result.size() > 1) ? \",\" : \"\";\n-    result += NumberToString(*it);\n-  }\n-  result += \"}\";\n-  return result;\n-}\n-}  // namespace\n-\n-Version::~Version() {\n-  assert(refs_ == 0);\n-\n-  // Remove from linked list\n-  prev_->next_ = next_;\n-  next_->prev_ = prev_;\n-\n-  // Drop references to files\n-  for (int level = 0; level < config::kNumLevels; level++) {\n-    for (size_t i = 0; i < files_[level].size(); i++) {\n-      FileMetaData* f = files_[level][i];\n-      assert(f->refs > 0);\n-      f->refs--;\n-      if (f->refs <= 0) {\n-        delete f;\n-      }\n-    }\n-  }\n-}\n-\n-int FindFile(const InternalKeyComparator& icmp,\n-             const std::vector<FileMetaData*>& files,\n-             const Slice& key) {\n-  uint32_t left = 0;\n-  uint32_t right = files.size();\n-  while (left < right) {\n-    uint32_t mid = (left + right) / 2;\n-    const FileMetaData* f = files[mid];\n-    if (icmp.InternalKeyComparator::Compare(f->largest.Encode(), key) < 0) {\n-      // Key at \"mid.largest\" is < \"target\".  Therefore all\n-      // files at or before \"mid\" are uninteresting.\n-      left = mid + 1;\n-    } else {\n-      // Key at \"mid.largest\" is >= \"target\".  Therefore all files\n-      // after \"mid\" are uninteresting.\n-      right = mid;\n-    }\n-  }\n-  return right;\n-}\n-\n-static bool AfterFile(const Comparator* ucmp,\n-                      const Slice* user_key, const FileMetaData* f) {\n-  // NULL user_key occurs before all keys and is therefore never after *f\n-  return (user_key != NULL &&\n-          ucmp->Compare(*user_key, f->largest.user_key()) > 0);\n-}\n-\n-static bool BeforeFile(const Comparator* ucmp,\n-                       const Slice* user_key, const FileMetaData* f) {\n-  // NULL user_key occurs after all keys and is therefore never before *f\n-  return (user_key != NULL &&\n-          ucmp->Compare(*user_key, f->smallest.user_key()) < 0);\n-}\n-\n-bool SomeFileOverlapsRange(\n-    const InternalKeyComparator& icmp,\n-    bool disjoint_sorted_files,\n-    const std::vector<FileMetaData*>& files,\n-    const Slice* smallest_user_key,\n-    const Slice* largest_user_key) {\n-  const Comparator* ucmp = icmp.user_comparator();\n-  if (!disjoint_sorted_files) {\n-    // Need to check against all files\n-    for (size_t i = 0; i < files.size(); i++) {\n-      const FileMetaData* f = files[i];\n-      if (AfterFile(ucmp, smallest_user_key, f) ||\n-          BeforeFile(ucmp, largest_user_key, f)) {\n-        // No overlap\n-      } else {\n-        return true;  // Overlap\n-      }\n-    }\n-    return false;\n-  }\n-\n-  // Binary search over file list\n-  uint32_t index = 0;\n-  if (smallest_user_key != NULL) {\n-    // Find the earliest possible internal key for smallest_user_key\n-    InternalKey small(*smallest_user_key, kMaxSequenceNumber,kValueTypeForSeek);\n-    index = FindFile(icmp, files, small.Encode());\n-  }\n-\n-  if (index >= files.size()) {\n-    // beginning of range is after all files, so no overlap.\n-    return false;\n-  }\n-\n-  return !BeforeFile(ucmp, largest_user_key, files[index]);\n-}\n-\n-// An internal iterator.  For a given version/level pair, yields\n-// information about the files in the level.  For a given entry, key()\n-// is the largest key that occurs in the file, and value() is an\n-// 16-byte value containing the file number and file size, both\n-// encoded using EncodeFixed64.\n-class Version::LevelFileNumIterator : public Iterator {\n- public:\n-  LevelFileNumIterator(const InternalKeyComparator& icmp,\n-                       const std::vector<FileMetaData*>* flist)\n-      : icmp_(icmp),\n-        flist_(flist),\n-        index_(flist->size()) {        // Marks as invalid\n-  }\n-  virtual bool Valid() const {\n-    return index_ < flist_->size();\n-  }\n-  virtual void Seek(const Slice& target) {\n-    index_ = FindFile(icmp_, *flist_, target);\n-  }\n-  virtual void SeekToFirst() { index_ = 0; }\n-  virtual void SeekToLast() {\n-    index_ = flist_->empty() ? 0 : flist_->size() - 1;\n-  }\n-  virtual void Next() {\n-    assert(Valid());\n-    index_++;\n-  }\n-  virtual void Prev() {\n-    assert(Valid());\n-    if (index_ == 0) {\n-      index_ = flist_->size();  // Marks as invalid\n-    } else {\n-      index_--;\n-    }\n-  }\n-  Slice key() const {\n-    assert(Valid());\n-    return (*flist_)[index_]->largest.Encode();\n-  }\n-  Slice value() const {\n-    assert(Valid());\n-    EncodeFixed64(value_buf_, (*flist_)[index_]->number);\n-    EncodeFixed64(value_buf_+8, (*flist_)[index_]->file_size);\n-    return Slice(value_buf_, sizeof(value_buf_));\n-  }\n-  virtual Status status() const { return Status::OK(); }\n- private:\n-  const InternalKeyComparator icmp_;\n-  const std::vector<FileMetaData*>* const flist_;\n-  uint32_t index_;\n-\n-  // Backing store for value().  Holds the file number and size.\n-  mutable char value_buf_[16];\n-};\n-\n-static Iterator* GetFileIterator(void* arg,\n-                                 const ReadOptions& options,\n-                                 const Slice& file_value) {\n-  TableCache* cache = reinterpret_cast<TableCache*>(arg);\n-  if (file_value.size() != 16) {\n-    return NewErrorIterator(\n-        Status::Corruption(\"FileReader invoked with unexpected value\"));\n-  } else {\n-    return cache->NewIterator(options,\n-                              DecodeFixed64(file_value.data()),\n-                              DecodeFixed64(file_value.data() + 8));\n-  }\n-}\n-\n-Iterator* Version::NewConcatenatingIterator(const ReadOptions& options,\n-                                            int level) const {\n-  return NewTwoLevelIterator(\n-      new LevelFileNumIterator(vset_->icmp_, &files_[level]),\n-      &GetFileIterator, vset_->table_cache_, options);\n-}\n-\n-void Version::AddIterators(const ReadOptions& options,\n-                           std::vector<Iterator*>* iters) {\n-  // Merge all level zero files together since they may overlap\n-  for (size_t i = 0; i < files_[0].size(); i++) {\n-    iters->push_back(\n-        vset_->table_cache_->NewIterator(\n-            options, files_[0][i]->number, files_[0][i]->file_size));\n-  }\n-\n-  // For levels > 0, we can use a concatenating iterator that sequentially\n-  // walks through the non-overlapping files in the level, opening them\n-  // lazily.\n-  for (int level = 1; level < config::kNumLevels; level++) {\n-    if (!files_[level].empty()) {\n-      iters->push_back(NewConcatenatingIterator(options, level));\n-    }\n-  }\n-}\n-\n-// Callback from TableCache::Get()\n-namespace {\n-enum SaverState {\n-  kNotFound,\n-  kFound,\n-  kDeleted,\n-  kCorrupt,\n-};\n-struct Saver {\n-  SaverState state;\n-  const Comparator* ucmp;\n-  Slice user_key;\n-  std::string* value;\n-};\n-}\n-static void SaveValue(void* arg, const Slice& ikey, const Slice& v) {\n-  Saver* s = reinterpret_cast<Saver*>(arg);\n-  ParsedInternalKey parsed_key;\n-  if (!ParseInternalKey(ikey, &parsed_key)) {\n-    s->state = kCorrupt;\n-  } else {\n-    if (s->ucmp->Compare(parsed_key.user_key, s->user_key) == 0) {\n-      s->state = (parsed_key.type == kTypeValue) ? kFound : kDeleted;\n-      if (s->state == kFound) {\n-        s->value->assign(v.data(), v.size());\n-      }\n-    }\n-  }\n-}\n-\n-static bool NewestFirst(FileMetaData* a, FileMetaData* b) {\n-  return a->number > b->number;\n-}\n-\n-Status Version::Get(const ReadOptions& options,\n-                    const LookupKey& k,\n-                    std::string* value,\n-                    GetStats* stats) {\n-  Slice ikey = k.internal_key();\n-  Slice user_key = k.user_key();\n-  const Comparator* ucmp = vset_->icmp_.user_comparator();\n-  Status s;\n-\n-  stats->seek_file = NULL;\n-  stats->seek_file_level = -1;\n-  FileMetaData* last_file_read = NULL;\n-  int last_file_read_level = -1;\n-\n-  // We can search level-by-level since entries never hop across\n-  // levels.  Therefore we are guaranteed that if we find data\n-  // in an smaller level, later levels are irrelevant.\n-  std::vector<FileMetaData*> tmp;\n-  FileMetaData* tmp2;\n-  for (int level = 0; level < config::kNumLevels; level++) {\n-    size_t num_files = files_[level].size();\n-    if (num_files == 0) continue;\n-\n-    // Get the list of files to search in this level\n-    FileMetaData* const* files = &files_[level][0];\n-    if (level == 0) {\n-      // Level-0 files may overlap each other.  Find all files that\n-      // overlap user_key and process them in order from newest to oldest.\n-      tmp.reserve(num_files);\n-      for (uint32_t i = 0; i < num_files; i++) {\n-        FileMetaData* f = files[i];\n-        if (ucmp->Compare(user_key, f->smallest.user_key()) >= 0 &&\n-            ucmp->Compare(user_key, f->largest.user_key()) <= 0) {\n-          tmp.push_back(f);\n-        }\n-      }\n-      if (tmp.empty()) continue;\n-\n-      std::sort(tmp.begin(), tmp.end(), NewestFirst);\n-      files = &tmp[0];\n-      num_files = tmp.size();\n-    } else {\n-      // Binary search to find earliest index whose largest key >= ikey.\n-      uint32_t index = FindFile(vset_->icmp_, files_[level], ikey);\n-      if (index >= num_files) {\n-        files = NULL;\n-        num_files = 0;\n-      } else {\n-        tmp2 = files[index];\n-        if (ucmp->Compare(user_key, tmp2->smallest.user_key()) < 0) {\n-          // All of \"tmp2\" is past any data for user_key\n-          files = NULL;\n-          num_files = 0;\n-        } else {\n-          files = &tmp2;\n-          num_files = 1;\n-        }\n-      }\n-    }\n-\n-    for (uint32_t i = 0; i < num_files; ++i) {\n-      if (last_file_read != NULL && stats->seek_file == NULL) {\n-        // We have had more than one seek for this read.  Charge the 1st file.\n-        stats->seek_file = last_file_read;\n-        stats->seek_file_level = last_file_read_level;\n-      }\n-\n-      FileMetaData* f = files[i];\n-      last_file_read = f;\n-      last_file_read_level = level;\n-\n-      Saver saver;\n-      saver.state = kNotFound;\n-      saver.ucmp = ucmp;\n-      saver.user_key = user_key;\n-      saver.value = value;\n-      s = vset_->table_cache_->Get(options, f->number, f->file_size,\n-                                   ikey, &saver, SaveValue);\n-      if (!s.ok()) {\n-        return s;\n-      }\n-      switch (saver.state) {\n-        case kNotFound:\n-          break;      // Keep searching in other files\n-        case kFound:\n-          return s;\n-        case kDeleted:\n-          s = Status::NotFound(Slice());  // Use empty error message for speed\n-          return s;\n-        case kCorrupt:\n-          s = Status::Corruption(\"corrupted key for \", user_key);\n-          return s;\n-      }\n-    }\n-  }\n-\n-  return Status::NotFound(Slice());  // Use an empty error message for speed\n-}\n-\n-bool Version::UpdateStats(const GetStats& stats) {\n-  FileMetaData* f = stats.seek_file;\n-  if (f != NULL) {\n-    f->allowed_seeks--;\n-    if (f->allowed_seeks <= 0 && file_to_compact_ == NULL) {\n-      file_to_compact_ = f;\n-      file_to_compact_level_ = stats.seek_file_level;\n-      return true;\n-    }\n-  }\n-  return false;\n-}\n-\n-void Version::Ref() {\n-  ++refs_;\n-}\n-\n-void Version::Unref() {\n-  assert(this != &vset_->dummy_versions_);\n-  assert(refs_ >= 1);\n-  --refs_;\n-  if (refs_ == 0) {\n-    delete this;\n-  }\n-}\n-\n-bool Version::OverlapInLevel(int level,\n-                             const Slice* smallest_user_key,\n-                             const Slice* largest_user_key) {\n-  return SomeFileOverlapsRange(vset_->icmp_, (level > 0), files_[level],\n-                               smallest_user_key, largest_user_key);\n-}\n-\n-int Version::PickLevelForMemTableOutput(\n-    const Slice& smallest_user_key,\n-    const Slice& largest_user_key) {\n-  int level = 0;\n-  if (!OverlapInLevel(0, &smallest_user_key, &largest_user_key)) {\n-    // Push to next level if there is no overlap in next level,\n-    // and the #bytes overlapping in the level after that are limited.\n-    InternalKey start(smallest_user_key, kMaxSequenceNumber, kValueTypeForSeek);\n-    InternalKey limit(largest_user_key, 0, static_cast<ValueType>(0));\n-    std::vector<FileMetaData*> overlaps;\n-    while (level < config::kMaxMemCompactLevel) {\n-      if (OverlapInLevel(level + 1, &smallest_user_key, &largest_user_key)) {\n-        break;\n-      }\n-      GetOverlappingInputs(level + 2, &start, &limit, &overlaps);\n-      const int64_t sum = TotalFileSize(overlaps);\n-      if (sum > kMaxGrandParentOverlapBytes) {\n-        break;\n-      }\n-      level++;\n-    }\n-  }\n-  return level;\n-}\n-\n-// Store in \"*inputs\" all files in \"level\" that overlap [begin,end]\n-void Version::GetOverlappingInputs(\n-    int level,\n-    const InternalKey* begin,\n-    const InternalKey* end,\n-    std::vector<FileMetaData*>* inputs) {\n-  inputs->clear();\n-  Slice user_begin, user_end;\n-  if (begin != NULL) {\n-    user_begin = begin->user_key();\n-  }\n-  if (end != NULL) {\n-    user_end = end->user_key();\n-  }\n-  const Comparator* user_cmp = vset_->icmp_.user_comparator();\n-  for (size_t i = 0; i < files_[level].size(); ) {\n-    FileMetaData* f = files_[level][i++];\n-    const Slice file_start = f->smallest.user_key();\n-    const Slice file_limit = f->largest.user_key();\n-    if (begin != NULL && user_cmp->Compare(file_limit, user_begin) < 0) {\n-      // \"f\" is completely before specified range; skip it\n-    } else if (end != NULL && user_cmp->Compare(file_start, user_end) > 0) {\n-      // \"f\" is completely after specified range; skip it\n-    } else {\n-      inputs->push_back(f);\n-      if (level == 0) {\n-        // Level-0 files may overlap each other.  So check if the newly\n-        // added file has expanded the range.  If so, restart search.\n-        if (begin != NULL && user_cmp->Compare(file_start, user_begin) < 0) {\n-          user_begin = file_start;\n-          inputs->clear();\n-          i = 0;\n-        } else if (end != NULL && user_cmp->Compare(file_limit, user_end) > 0) {\n-          user_end = file_limit;\n-          inputs->clear();\n-          i = 0;\n-        }\n-      }\n-    }\n-  }\n-}\n-\n-std::string Version::DebugString() const {\n-  std::string r;\n-  for (int level = 0; level < config::kNumLevels; level++) {\n-    // E.g.,\n-    //   --- level 1 ---\n-    //   17:123['a' .. 'd']\n-    //   20:43['e' .. 'g']\n-    r.append(\"--- level \");\n-    AppendNumberTo(&r, level);\n-    r.append(\" ---\\n\");\n-    const std::vector<FileMetaData*>& files = files_[level];\n-    for (size_t i = 0; i < files.size(); i++) {\n-      r.push_back(' ');\n-      AppendNumberTo(&r, files[i]->number);\n-      r.push_back(':');\n-      AppendNumberTo(&r, files[i]->file_size);\n-      r.append(\"[\");\n-      r.append(files[i]->smallest.DebugString());\n-      r.append(\" .. \");\n-      r.append(files[i]->largest.DebugString());\n-      r.append(\"]\\n\");\n-    }\n-  }\n-  return r;\n-}\n-\n-// A helper class so we can efficiently apply a whole sequence\n-// of edits to a particular state without creating intermediate\n-// Versions that contain full copies of the intermediate state.\n-class VersionSet::Builder {\n- private:\n-  // Helper to sort by v->files_[file_number].smallest\n-  struct BySmallestKey {\n-    const InternalKeyComparator* internal_comparator;\n-\n-    bool operator()(FileMetaData* f1, FileMetaData* f2) const {\n-      int r = internal_comparator->Compare(f1->smallest, f2->smallest);\n-      if (r != 0) {\n-        return (r < 0);\n-      } else {\n-        // Break ties by file number\n-        return (f1->number < f2->number);\n-      }\n-    }\n-  };\n-\n-  typedef std::set<FileMetaData*, BySmallestKey> FileSet;\n-  struct LevelState {\n-    std::set<uint64_t> deleted_files;\n-    FileSet* added_files;\n-  };\n-\n-  VersionSet* vset_;\n-  Version* base_;\n-  LevelState levels_[config::kNumLevels];\n-\n- public:\n-  // Initialize a builder with the files from *base and other info from *vset\n-  Builder(VersionSet* vset, Version* base)\n-      : vset_(vset),\n-        base_(base) {\n-    base_->Ref();\n-    BySmallestKey cmp;\n-    cmp.internal_comparator = &vset_->icmp_;\n-    for (int level = 0; level < config::kNumLevels; level++) {\n-      levels_[level].added_files = new FileSet(cmp);\n-    }\n-  }\n-\n-  ~Builder() {\n-    for (int level = 0; level < config::kNumLevels; level++) {\n-      const FileSet* added = levels_[level].added_files;\n-      std::vector<FileMetaData*> to_unref;\n-      to_unref.reserve(added->size());\n-      for (FileSet::const_iterator it = added->begin();\n-          it != added->end(); ++it) {\n-        to_unref.push_back(*it);\n-      }\n-      delete added;\n-      for (uint32_t i = 0; i < to_unref.size(); i++) {\n-        FileMetaData* f = to_unref[i];\n-        f->refs--;\n-        if (f->refs <= 0) {\n-          delete f;\n-        }\n-      }\n-    }\n-    base_->Unref();\n-  }\n-\n-  // Apply all of the edits in *edit to the current state.\n-  void Apply(VersionEdit* edit) {\n-    // Update compaction pointers\n-    for (size_t i = 0; i < edit->compact_pointers_.size(); i++) {\n-      const int level = edit->compact_pointers_[i].first;\n-      vset_->compact_pointer_[level] =\n-          edit->compact_pointers_[i].second.Encode().ToString();\n-    }\n-\n-    // Delete files\n-    const VersionEdit::DeletedFileSet& del = edit->deleted_files_;\n-    for (VersionEdit::DeletedFileSet::const_iterator iter = del.begin();\n-         iter != del.end();\n-         ++iter) {\n-      const int level = iter->first;\n-      const uint64_t number = iter->second;\n-      levels_[level].deleted_files.insert(number);\n-    }\n-\n-    // Add new files\n-    for (size_t i = 0; i < edit->new_files_.size(); i++) {\n-      const int level = edit->new_files_[i].first;\n-      FileMetaData* f = new FileMetaData(edit->new_files_[i].second);\n-      f->refs = 1;\n-\n-      // We arrange to automatically compact this file after\n-      // a certain number of seeks.  Let's assume:\n-      //   (1) One seek costs 10ms\n-      //   (2) Writing or reading 1MB costs 10ms (100MB/s)\n-      //   (3) A compaction of 1MB does 25MB of IO:\n-      //         1MB read from this level\n-      //         10-12MB read from next level (boundaries may be misaligned)\n-      //         10-12MB written to next level\n-      // This implies that 25 seeks cost the same as the compaction\n-      // of 1MB of data.  I.e., one seek costs approximately the\n-      // same as the compaction of 40KB of data.  We are a little\n-      // conservative and allow approximately one seek for every 16KB\n-      // of data before triggering a compaction.\n-      f->allowed_seeks = (f->file_size / 16384);\n-      if (f->allowed_seeks < 100) f->allowed_seeks = 100;\n-\n-      levels_[level].deleted_files.erase(f->number);\n-      levels_[level].added_files->insert(f);\n-    }\n-  }\n-\n-  // Save the current state in *v.\n-  void SaveTo(Version* v) {\n-    BySmallestKey cmp;\n-    cmp.internal_comparator = &vset_->icmp_;\n-    for (int level = 0; level < config::kNumLevels; level++) {\n-      // Merge the set of added files with the set of pre-existing files.\n-      // Drop any deleted files.  Store the result in *v.\n-      const std::vector<FileMetaData*>& base_files = base_->files_[level];\n-      std::vector<FileMetaData*>::const_iterator base_iter = base_files.begin();\n-      std::vector<FileMetaData*>::const_iterator base_end = base_files.end();\n-      const FileSet* added = levels_[level].added_files;\n-      v->files_[level].reserve(base_files.size() + added->size());\n-      for (FileSet::const_iterator added_iter = added->begin();\n-           added_iter != added->end();\n-           ++added_iter) {\n-        // Add all smaller files listed in base_\n-        for (std::vector<FileMetaData*>::const_iterator bpos\n-                 = std::upper_bound(base_iter, base_end, *added_iter, cmp);\n-             base_iter != bpos;\n-             ++base_iter) {\n-          MaybeAddFile(v, level, *base_iter);\n-        }\n-\n-        MaybeAddFile(v, level, *added_iter);\n-      }\n-\n-      // Add remaining base files\n-      for (; base_iter != base_end; ++base_iter) {\n-        MaybeAddFile(v, level, *base_iter);\n-      }\n-\n-#ifndef NDEBUG\n-      // Make sure there is no overlap in levels > 0\n-      if (level > 0) {\n-        for (uint32_t i = 1; i < v->files_[level].size(); i++) {\n-          const InternalKey& prev_end = v->files_[level][i-1]->largest;\n-          const InternalKey& this_begin = v->files_[level][i]->smallest;\n-          if (vset_->icmp_.Compare(prev_end, this_begin) >= 0) {\n-            fprintf(stderr, \"overlapping ranges in same level %s vs. %s\\n\",\n-                    prev_end.DebugString().c_str(),\n-                    this_begin.DebugString().c_str());\n-            abort();\n-          }\n-        }\n-      }\n-#endif\n-    }\n-  }\n-\n-  void MaybeAddFile(Version* v, int level, FileMetaData* f) {\n-    if (levels_[level].deleted_files.count(f->number) > 0) {\n-      // File is deleted: do nothing\n-    } else {\n-      std::vector<FileMetaData*>* files = &v->files_[level];\n-      if (level > 0 && !files->empty()) {\n-        // Must not overlap\n-        assert(vset_->icmp_.Compare((*files)[files->size()-1]->largest,\n-                                    f->smallest) < 0);\n-      }\n-      f->refs++;\n-      files->push_back(f);\n-    }\n-  }\n-};\n-\n-VersionSet::VersionSet(const std::string& dbname,\n-                       const Options* options,\n-                       TableCache* table_cache,\n-                       const InternalKeyComparator* cmp)\n-    : env_(options->env),\n-      dbname_(dbname),\n-      options_(options),\n-      table_cache_(table_cache),\n-      icmp_(*cmp),\n-      next_file_number_(2),\n-      manifest_file_number_(0),  // Filled by Recover()\n-      last_sequence_(0),\n-      log_number_(0),\n-      prev_log_number_(0),\n-      descriptor_file_(NULL),\n-      descriptor_log_(NULL),\n-      dummy_versions_(this),\n-      current_(NULL) {\n-  AppendVersion(new Version(this));\n-}\n-\n-VersionSet::~VersionSet() {\n-  current_->Unref();\n-  assert(dummy_versions_.next_ == &dummy_versions_);  // List must be empty\n-  delete descriptor_log_;\n-  delete descriptor_file_;\n-}\n-\n-void VersionSet::AppendVersion(Version* v) {\n-  // Make \"v\" current\n-  assert(v->refs_ == 0);\n-  assert(v != current_);\n-  if (current_ != NULL) {\n-    current_->Unref();\n-  }\n-  current_ = v;\n-  v->Ref();\n-\n-  // Append to linked list\n-  v->prev_ = dummy_versions_.prev_;\n-  v->next_ = &dummy_versions_;\n-  v->prev_->next_ = v;\n-  v->next_->prev_ = v;\n-}\n-\n-Status VersionSet::LogAndApply(VersionEdit* edit, port::Mutex* mu) {\n-  if (edit->has_log_number_) {\n-    assert(edit->log_number_ >= log_number_);\n-    assert(edit->log_number_ < next_file_number_);\n-  } else {\n-    edit->SetLogNumber(log_number_);\n-  }\n-\n-  if (!edit->has_prev_log_number_) {\n-    edit->SetPrevLogNumber(prev_log_number_);\n-  }\n-\n-  edit->SetNextFile(next_file_number_);\n-  edit->SetLastSequence(last_sequence_);\n-\n-  Version* v = new Version(this);\n-  {\n-    Builder builder(this, current_);\n-    builder.Apply(edit);\n-    builder.SaveTo(v);\n-  }\n-  Finalize(v);\n-\n-  // Initialize new descriptor log file if necessary by creating\n-  // a temporary file that contains a snapshot of the current version.\n-  std::string new_manifest_file;\n-  Status s;\n-  if (descriptor_log_ == NULL) {\n-    // No reason to unlock *mu here since we only hit this path in the\n-    // first call to LogAndApply (when opening the database).\n-    assert(descriptor_file_ == NULL);\n-    new_manifest_file = DescriptorFileName(dbname_, manifest_file_number_);\n-    edit->SetNextFile(next_file_number_);\n-    s = env_->NewWritableFile(new_manifest_file, &descriptor_file_);\n-    if (s.ok()) {\n-      descriptor_log_ = new log::Writer(descriptor_file_);\n-      s = WriteSnapshot(descriptor_log_);\n-    }\n-  }\n-\n-  // Unlock during expensive MANIFEST log write\n-  {\n-    mu->Unlock();\n-\n-    // Write new record to MANIFEST log\n-    if (s.ok()) {\n-      std::string record;\n-      edit->EncodeTo(&record);\n-      s = descriptor_log_->AddRecord(record);\n-      if (s.ok()) {\n-        s = descriptor_file_->Sync();\n-      }\n-      if (!s.ok()) {\n-        Log(options_->info_log, \"MANIFEST write: %s\\n\", s.ToString().c_str());\n-        if (ManifestContains(record)) {\n-          Log(options_->info_log,\n-              \"MANIFEST contains log record despite error; advancing to new \"\n-              \"version to prevent mismatch between in-memory and logged state\");\n-          s = Status::OK();\n-        }\n-      }\n-    }\n-\n-    // If we just created a new descriptor file, install it by writing a\n-    // new CURRENT file that points to it.\n-    if (s.ok() && !new_manifest_file.empty()) {\n-      s = SetCurrentFile(env_, dbname_, manifest_file_number_);\n-      // No need to double-check MANIFEST in case of error since it\n-      // will be discarded below.\n-    }\n-\n-    mu->Lock();\n-  }\n-\n-  // Install the new version\n-  if (s.ok()) {\n-    AppendVersion(v);\n-    log_number_ = edit->log_number_;\n-    prev_log_number_ = edit->prev_log_number_;\n-  } else {\n-    delete v;\n-    if (!new_manifest_file.empty()) {\n-      delete descriptor_log_;\n-      delete descriptor_file_;\n-      descriptor_log_ = NULL;\n-      descriptor_file_ = NULL;\n-      env_->DeleteFile(new_manifest_file);\n-    }\n-  }\n-\n-  return s;\n-}\n-\n-Status VersionSet::Recover() {\n-  struct LogReporter : public log::Reader::Reporter {\n-    Status* status;\n-    virtual void Corruption(size_t bytes, const Status& s) {\n-      if (this->status->ok()) *this->status = s;\n-    }\n-  };\n-\n-  // Read \"CURRENT\" file, which contains a pointer to the current manifest file\n-  std::string current;\n-  Status s = ReadFileToString(env_, CurrentFileName(dbname_), &current);\n-  if (!s.ok()) {\n-    return s;\n-  }\n-  if (current.empty() || current[current.size()-1] != '\\n') {\n-    return Status::Corruption(\"CURRENT file does not end with newline\");\n-  }\n-  current.resize(current.size() - 1);\n-\n-  std::string dscname = dbname_ + \"/\" + current;\n-  SequentialFile* file;\n-  s = env_->NewSequentialFile(dscname, &file);\n-  if (!s.ok()) {\n-    return s;\n-  }\n-\n-  bool have_log_number = false;\n-  bool have_prev_log_number = false;\n-  bool have_next_file = false;\n-  bool have_last_sequence = false;\n-  uint64_t next_file = 0;\n-  uint64_t last_sequence = 0;\n-  uint64_t log_number = 0;\n-  uint64_t prev_log_number = 0;\n-  Builder builder(this, current_);\n-\n-  {\n-    LogReporter reporter;\n-    reporter.status = &s;\n-    log::Reader reader(file, &reporter, true/*checksum*/, 0/*initial_offset*/);\n-    Slice record;\n-    std::string scratch;\n-    while (reader.ReadRecord(&record, &scratch) && s.ok()) {\n-      VersionEdit edit;\n-      s = edit.DecodeFrom(record);\n-      if (s.ok()) {\n-        if (edit.has_comparator_ &&\n-            edit.comparator_ != icmp_.user_comparator()->Name()) {\n-          s = Status::InvalidArgument(\n-              edit.comparator_ + \" does not match existing comparator \",\n-              icmp_.user_comparator()->Name());\n-        }\n-      }\n-\n-      if (s.ok()) {\n-        builder.Apply(&edit);\n-      }\n-\n-      if (edit.has_log_number_) {\n-        log_number = edit.log_number_;\n-        have_log_number = true;\n-      }\n-\n-      if (edit.has_prev_log_number_) {\n-        prev_log_number = edit.prev_log_number_;\n-        have_prev_log_number = true;\n-      }\n-\n-      if (edit.has_next_file_number_) {\n-        next_file = edit.next_file_number_;\n-        have_next_file = true;\n-      }\n-\n-      if (edit.has_last_sequence_) {\n-        last_sequence = edit.last_sequence_;\n-        have_last_sequence = true;\n-      }\n-    }\n-  }\n-  delete file;\n-  file = NULL;\n-\n-  if (s.ok()) {\n-    if (!have_next_file) {\n-      s = Status::Corruption(\"no meta-nextfile entry in descriptor\");\n-    } else if (!have_log_number) {\n-      s = Status::Corruption(\"no meta-lognumber entry in descriptor\");\n-    } else if (!have_last_sequence) {\n-      s = Status::Corruption(\"no last-sequence-number entry in descriptor\");\n-    }\n-\n-    if (!have_prev_log_number) {\n-      prev_log_number = 0;\n-    }\n-\n-    MarkFileNumberUsed(prev_log_number);\n-    MarkFileNumberUsed(log_number);\n-  }\n-\n-  if (s.ok()) {\n-    Version* v = new Version(this);\n-    builder.SaveTo(v);\n-    // Install recovered version\n-    Finalize(v);\n-    AppendVersion(v);\n-    manifest_file_number_ = next_file;\n-    next_file_number_ = next_file + 1;\n-    last_sequence_ = last_sequence;\n-    log_number_ = log_number;\n-    prev_log_number_ = prev_log_number;\n-  }\n-\n-  return s;\n-}\n-\n-void VersionSet::MarkFileNumberUsed(uint64_t number) {\n-  if (next_file_number_ <= number) {\n-    next_file_number_ = number + 1;\n-  }\n-}\n-\n-void VersionSet::Finalize(Version* v) {\n-  // Precomputed best level for next compaction\n-  int best_level = -1;\n-  double best_score = -1;\n-\n-  for (int level = 0; level < config::kNumLevels-1; level++) {\n-    double score;\n-    if (level == 0) {\n-      // We treat level-0 specially by bounding the number of files\n-      // instead of number of bytes for two reasons:\n-      //\n-      // (1) With larger write-buffer sizes, it is nice not to do too\n-      // many level-0 compactions.\n-      //\n-      // (2) The files in level-0 are merged on every read and\n-      // therefore we wish to avoid too many files when the individual\n-      // file size is small (perhaps because of a small write-buffer\n-      // setting, or very high compression ratios, or lots of\n-      // overwrites/deletions).\n-      score = v->files_[level].size() /\n-          static_cast<double>(config::kL0_CompactionTrigger);\n-    } else {\n-      // Compute the ratio of current size to size limit.\n-      const uint64_t level_bytes = TotalFileSize(v->files_[level]);\n-      score = static_cast<double>(level_bytes) / MaxBytesForLevel(level);\n-    }\n-\n-    if (score > best_score) {\n-      best_level = level;\n-      best_score = score;\n-    }\n-  }\n-\n-  v->compaction_level_ = best_level;\n-  v->compaction_score_ = best_score;\n-}\n-\n-Status VersionSet::WriteSnapshot(log::Writer* log) {\n-  // TODO: Break up into multiple records to reduce memory usage on recovery?\n-\n-  // Save metadata\n-  VersionEdit edit;\n-  edit.SetComparatorName(icmp_.user_comparator()->Name());\n-\n-  // Save compaction pointers\n-  for (int level = 0; level < config::kNumLevels; level++) {\n-    if (!compact_pointer_[level].empty()) {\n-      InternalKey key;\n-      key.DecodeFrom(compact_pointer_[level]);\n-      edit.SetCompactPointer(level, key);\n-    }\n-  }\n-\n-  // Save files\n-  for (int level = 0; level < config::kNumLevels; level++) {\n-    const std::vector<FileMetaData*>& files = current_->files_[level];\n-    for (size_t i = 0; i < files.size(); i++) {\n-      const FileMetaData* f = files[i];\n-      edit.AddFile(level, f->number, f->file_size, f->smallest, f->largest);\n-    }\n-  }\n-\n-  std::string record;\n-  edit.EncodeTo(&record);\n-  return log->AddRecord(record);\n-}\n-\n-int VersionSet::NumLevelFiles(int level) const {\n-  assert(level >= 0);\n-  assert(level < config::kNumLevels);\n-  return current_->files_[level].size();\n-}\n-\n-const char* VersionSet::LevelSummary(LevelSummaryStorage* scratch) const {\n-  // Update code if kNumLevels changes\n-  assert(config::kNumLevels == 7);\n-  snprintf(scratch->buffer, sizeof(scratch->buffer),\n-           \"files[ %d %d %d %d %d %d %d ]\",\n-           int(current_->files_[0].size()),\n-           int(current_->files_[1].size()),\n-           int(current_->files_[2].size()),\n-           int(current_->files_[3].size()),\n-           int(current_->files_[4].size()),\n-           int(current_->files_[5].size()),\n-           int(current_->files_[6].size()));\n-  return scratch->buffer;\n-}\n-\n-// Return true iff the manifest contains the specified record.\n-bool VersionSet::ManifestContains(const std::string& record) const {\n-  std::string fname = DescriptorFileName(dbname_, manifest_file_number_);\n-  Log(options_->info_log, \"ManifestContains: checking %s\\n\", fname.c_str());\n-  SequentialFile* file = NULL;\n-  Status s = env_->NewSequentialFile(fname, &file);\n-  if (!s.ok()) {\n-    Log(options_->info_log, \"ManifestContains: %s\\n\", s.ToString().c_str());\n-    return false;\n-  }\n-  log::Reader reader(file, NULL, true/*checksum*/, 0);\n-  Slice r;\n-  std::string scratch;\n-  bool result = false;\n-  while (reader.ReadRecord(&r, &scratch)) {\n-    if (r == Slice(record)) {\n-      result = true;\n-      break;\n-    }\n-  }\n-  delete file;\n-  Log(options_->info_log, \"ManifestContains: result = %d\\n\", result ? 1 : 0);\n-  return result;\n-}\n-\n-uint64_t VersionSet::ApproximateOffsetOf(Version* v, const InternalKey& ikey) {\n-  uint64_t result = 0;\n-  for (int level = 0; level < config::kNumLevels; level++) {\n-    const std::vector<FileMetaData*>& files = v->files_[level];\n-    for (size_t i = 0; i < files.size(); i++) {\n-      if (icmp_.Compare(files[i]->largest, ikey) <= 0) {\n-        // Entire file is before \"ikey\", so just add the file size\n-        result += files[i]->file_size;\n-      } else if (icmp_.Compare(files[i]->smallest, ikey) > 0) {\n-        // Entire file is after \"ikey\", so ignore\n-        if (level > 0) {\n-          // Files other than level 0 are sorted by meta->smallest, so\n-          // no further files in this level will contain data for\n-          // \"ikey\".\n-          break;\n-        }\n-      } else {\n-        // \"ikey\" falls in the range for this table.  Add the\n-        // approximate offset of \"ikey\" within the table.\n-        Table* tableptr;\n-        Iterator* iter = table_cache_->NewIterator(\n-            ReadOptions(), files[i]->number, files[i]->file_size, &tableptr);\n-        if (tableptr != NULL) {\n-          result += tableptr->ApproximateOffsetOf(ikey.Encode());\n-        }\n-        delete iter;\n-      }\n-    }\n-  }\n-  return result;\n-}\n-\n-void VersionSet::AddLiveFiles(std::set<uint64_t>* live) {\n-  for (Version* v = dummy_versions_.next_;\n-       v != &dummy_versions_;\n-       v = v->next_) {\n-    for (int level = 0; level < config::kNumLevels; level++) {\n-      const std::vector<FileMetaData*>& files = v->files_[level];\n-      for (size_t i = 0; i < files.size(); i++) {\n-        live->insert(files[i]->number);\n-      }\n-    }\n-  }\n-}\n-\n-int64_t VersionSet::NumLevelBytes(int level) const {\n-  assert(level >= 0);\n-  assert(level < config::kNumLevels);\n-  return TotalFileSize(current_->files_[level]);\n-}\n-\n-int64_t VersionSet::MaxNextLevelOverlappingBytes() {\n-  int64_t result = 0;\n-  std::vector<FileMetaData*> overlaps;\n-  for (int level = 1; level < config::kNumLevels - 1; level++) {\n-    for (size_t i = 0; i < current_->files_[level].size(); i++) {\n-      const FileMetaData* f = current_->files_[level][i];\n-      current_->GetOverlappingInputs(level+1, &f->smallest, &f->largest,\n-                                     &overlaps);\n-      const int64_t sum = TotalFileSize(overlaps);\n-      if (sum > result) {\n-        result = sum;\n-      }\n-    }\n-  }\n-  return result;\n-}\n-\n-// Stores the minimal range that covers all entries in inputs in\n-// *smallest, *largest.\n-// REQUIRES: inputs is not empty\n-void VersionSet::GetRange(const std::vector<FileMetaData*>& inputs,\n-                          InternalKey* smallest,\n-                          InternalKey* largest) {\n-  assert(!inputs.empty());\n-  smallest->Clear();\n-  largest->Clear();\n-  for (size_t i = 0; i < inputs.size(); i++) {\n-    FileMetaData* f = inputs[i];\n-    if (i == 0) {\n-      *smallest = f->smallest;\n-      *largest = f->largest;\n-    } else {\n-      if (icmp_.Compare(f->smallest, *smallest) < 0) {\n-        *smallest = f->smallest;\n-      }\n-      if (icmp_.Compare(f->largest, *largest) > 0) {\n-        *largest = f->largest;\n-      }\n-    }\n-  }\n-}\n-\n-// Stores the minimal range that covers all entries in inputs1 and inputs2\n-// in *smallest, *largest.\n-// REQUIRES: inputs is not empty\n-void VersionSet::GetRange2(const std::vector<FileMetaData*>& inputs1,\n-                           const std::vector<FileMetaData*>& inputs2,\n-                           InternalKey* smallest,\n-                           InternalKey* largest) {\n-  std::vector<FileMetaData*> all = inputs1;\n-  all.insert(all.end(), inputs2.begin(), inputs2.end());\n-  GetRange(all, smallest, largest);\n-}\n-\n-Iterator* VersionSet::MakeInputIterator(Compaction* c) {\n-  ReadOptions options;\n-  options.verify_checksums = options_->paranoid_checks;\n-  options.fill_cache = false;\n-\n-  // Level-0 files have to be merged together.  For other levels,\n-  // we will make a concatenating iterator per level.\n-  // TODO(opt): use concatenating iterator for level-0 if there is no overlap\n-  const int space = (c->level() == 0 ? c->inputs_[0].size() + 1 : 2);\n-  Iterator** list = new Iterator*[space];\n-  int num = 0;\n-  for (int which = 0; which < 2; which++) {\n-    if (!c->inputs_[which].empty()) {\n-      if (c->level() + which == 0) {\n-        const std::vector<FileMetaData*>& files = c->inputs_[which];\n-        for (size_t i = 0; i < files.size(); i++) {\n-          list[num++] = table_cache_->NewIterator(\n-              options, files[i]->number, files[i]->file_size);\n-        }\n-      } else {\n-        // Create concatenating iterator for the files from this level\n-        list[num++] = NewTwoLevelIterator(\n-            new Version::LevelFileNumIterator(icmp_, &c->inputs_[which]),\n-            &GetFileIterator, table_cache_, options);\n-      }\n-    }\n-  }\n-  assert(num <= space);\n-  Iterator* result = NewMergingIterator(&icmp_, list, num);\n-  delete[] list;\n-  return result;\n-}\n-\n-Compaction* VersionSet::PickCompaction() {\n-  Compaction* c;\n-  int level;\n-\n-  // We prefer compactions triggered by too much data in a level over\n-  // the compactions triggered by seeks.\n-  const bool size_compaction = (current_->compaction_score_ >= 1);\n-  const bool seek_compaction = (current_->file_to_compact_ != NULL);\n-  if (size_compaction) {\n-    level = current_->compaction_level_;\n-    assert(level >= 0);\n-    assert(level+1 < config::kNumLevels);\n-    c = new Compaction(level);\n-\n-    // Pick the first file that comes after compact_pointer_[level]\n-    for (size_t i = 0; i < current_->files_[level].size(); i++) {\n-      FileMetaData* f = current_->files_[level][i];\n-      if (compact_pointer_[level].empty() ||\n-          icmp_.Compare(f->largest.Encode(), compact_pointer_[level]) > 0) {\n-        c->inputs_[0].push_back(f);\n-        break;\n-      }\n-    }\n-    if (c->inputs_[0].empty()) {\n-      // Wrap-around to the beginning of the key space\n-      c->inputs_[0].push_back(current_->files_[level][0]);\n-    }\n-  } else if (seek_compaction) {\n-    level = current_->file_to_compact_level_;\n-    c = new Compaction(level);\n-    c->inputs_[0].push_back(current_->file_to_compact_);\n-  } else {\n-    return NULL;\n-  }\n-\n-  c->input_version_ = current_;\n-  c->input_version_->Ref();\n-\n-  // Files in level 0 may overlap each other, so pick up all overlapping ones\n-  if (level == 0) {\n-    InternalKey smallest, largest;\n-    GetRange(c->inputs_[0], &smallest, &largest);\n-    // Note that the next call will discard the file we placed in\n-    // c->inputs_[0] earlier and replace it with an overlapping set\n-    // which will include the picked file.\n-    current_->GetOverlappingInputs(0, &smallest, &largest, &c->inputs_[0]);\n-    assert(!c->inputs_[0].empty());\n-  }\n-\n-  SetupOtherInputs(c);\n-\n-  return c;\n-}\n-\n-void VersionSet::SetupOtherInputs(Compaction* c) {\n-  const int level = c->level();\n-  InternalKey smallest, largest;\n-  GetRange(c->inputs_[0], &smallest, &largest);\n-\n-  current_->GetOverlappingInputs(level+1, &smallest, &largest, &c->inputs_[1]);\n-\n-  // Get entire range covered by compaction\n-  InternalKey all_start, all_limit;\n-  GetRange2(c->inputs_[0], c->inputs_[1], &all_start, &all_limit);\n-\n-  // See if we can grow the number of inputs in \"level\" without\n-  // changing the number of \"level+1\" files we pick up.\n-  if (!c->inputs_[1].empty()) {\n-    std::vector<FileMetaData*> expanded0;\n-    current_->GetOverlappingInputs(level, &all_start, &all_limit, &expanded0);\n-    const int64_t inputs0_size = TotalFileSize(c->inputs_[0]);\n-    const int64_t inputs1_size = TotalFileSize(c->inputs_[1]);\n-    const int64_t expanded0_size = TotalFileSize(expanded0);\n-    if (expanded0.size() > c->inputs_[0].size() &&\n-        inputs1_size + expanded0_size < kExpandedCompactionByteSizeLimit) {\n-      InternalKey new_start, new_limit;\n-      GetRange(expanded0, &new_start, &new_limit);\n-      std::vector<FileMetaData*> expanded1;\n-      current_->GetOverlappingInputs(level+1, &new_start, &new_limit,\n-                                     &expanded1);\n-      if (expanded1.size() == c->inputs_[1].size()) {\n-        Log(options_->info_log,\n-            \"Expanding@%d %d+%d (%ld+%ld bytes) to %d+%d (%ld+%ld bytes)\\n\",\n-            level,\n-            int(c->inputs_[0].size()),\n-            int(c->inputs_[1].size()),\n-            long(inputs0_size), long(inputs1_size),\n-            int(expanded0.size()),\n-            int(expanded1.size()),\n-            long(expanded0_size), long(inputs1_size));\n-        smallest = new_start;\n-        largest = new_limit;\n-        c->inputs_[0] = expanded0;\n-        c->inputs_[1] = expanded1;\n-        GetRange2(c->inputs_[0], c->inputs_[1], &all_start, &all_limit);\n-      }\n-    }\n-  }\n-\n-  // Compute the set of grandparent files that overlap this compaction\n-  // (parent == level+1; grandparent == level+2)\n-  if (level + 2 < config::kNumLevels) {\n-    current_->GetOverlappingInputs(level + 2, &all_start, &all_limit,\n-                                   &c->grandparents_);\n-  }\n-\n-  if (false) {\n-    Log(options_->info_log, \"Compacting %d '%s' .. '%s'\",\n-        level,\n-        smallest.DebugString().c_str(),\n-        largest.DebugString().c_str());\n-  }\n-\n-  // Update the place where we will do the next compaction for this level.\n-  // We update this immediately instead of waiting for the VersionEdit\n-  // to be applied so that if the compaction fails, we will try a different\n-  // key range next time.\n-  compact_pointer_[level] = largest.Encode().ToString();\n-  c->edit_.SetCompactPointer(level, largest);\n-}\n-\n-Compaction* VersionSet::CompactRange(\n-    int level,\n-    const InternalKey* begin,\n-    const InternalKey* end) {\n-  std::vector<FileMetaData*> inputs;\n-  current_->GetOverlappingInputs(level, begin, end, &inputs);\n-  if (inputs.empty()) {\n-    return NULL;\n-  }\n-\n-  // Avoid compacting too much in one shot in case the range is large.\n-  const uint64_t limit = MaxFileSizeForLevel(level);\n-  uint64_t total = 0;\n-  for (size_t i = 0; i < inputs.size(); i++) {\n-    uint64_t s = inputs[i]->file_size;\n-    total += s;\n-    if (total >= limit) {\n-      inputs.resize(i + 1);\n-      break;\n-    }\n-  }\n-\n-  Compaction* c = new Compaction(level);\n-  c->input_version_ = current_;\n-  c->input_version_->Ref();\n-  c->inputs_[0] = inputs;\n-  SetupOtherInputs(c);\n-  return c;\n-}\n-\n-Compaction::Compaction(int level)\n-    : level_(level),\n-      max_output_file_size_(MaxFileSizeForLevel(level)),\n-      input_version_(NULL),\n-      grandparent_index_(0),\n-      seen_key_(false),\n-      overlapped_bytes_(0) {\n-  for (int i = 0; i < config::kNumLevels; i++) {\n-    level_ptrs_[i] = 0;\n-  }\n-}\n-\n-Compaction::~Compaction() {\n-  if (input_version_ != NULL) {\n-    input_version_->Unref();\n-  }\n-}\n-\n-bool Compaction::IsTrivialMove() const {\n-  // Avoid a move if there is lots of overlapping grandparent data.\n-  // Otherwise, the move could create a parent file that will require\n-  // a very expensive merge later on.\n-  return (num_input_files(0) == 1 &&\n-          num_input_files(1) == 0 &&\n-          TotalFileSize(grandparents_) <= kMaxGrandParentOverlapBytes);\n-}\n-\n-void Compaction::AddInputDeletions(VersionEdit* edit) {\n-  for (int which = 0; which < 2; which++) {\n-    for (size_t i = 0; i < inputs_[which].size(); i++) {\n-      edit->DeleteFile(level_ + which, inputs_[which][i]->number);\n-    }\n-  }\n-}\n-\n-bool Compaction::IsBaseLevelForKey(const Slice& user_key) {\n-  // Maybe use binary search to find right entry instead of linear search?\n-  const Comparator* user_cmp = input_version_->vset_->icmp_.user_comparator();\n-  for (int lvl = level_ + 2; lvl < config::kNumLevels; lvl++) {\n-    const std::vector<FileMetaData*>& files = input_version_->files_[lvl];\n-    for (; level_ptrs_[lvl] < files.size(); ) {\n-      FileMetaData* f = files[level_ptrs_[lvl]];\n-      if (user_cmp->Compare(user_key, f->largest.user_key()) <= 0) {\n-        // We've advanced far enough\n-        if (user_cmp->Compare(user_key, f->smallest.user_key()) >= 0) {\n-          // Key falls in this file's range, so definitely not base level\n-          return false;\n-        }\n-        break;\n-      }\n-      level_ptrs_[lvl]++;\n-    }\n-  }\n-  return true;\n-}\n-\n-bool Compaction::ShouldStopBefore(const Slice& internal_key) {\n-  // Scan to find earliest grandparent file that contains key.\n-  const InternalKeyComparator* icmp = &input_version_->vset_->icmp_;\n-  while (grandparent_index_ < grandparents_.size() &&\n-      icmp->Compare(internal_key,\n-                    grandparents_[grandparent_index_]->largest.Encode()) > 0) {\n-    if (seen_key_) {\n-      overlapped_bytes_ += grandparents_[grandparent_index_]->file_size;\n-    }\n-    grandparent_index_++;\n-  }\n-  seen_key_ = true;\n-\n-  if (overlapped_bytes_ > kMaxGrandParentOverlapBytes) {\n-    // Too much overlap for current output; start new output\n-    overlapped_bytes_ = 0;\n-    return true;\n-  } else {\n-    return false;\n-  }\n-}\n-\n-void Compaction::ReleaseInputs() {\n-  if (input_version_ != NULL) {\n-    input_version_->Unref();\n-    input_version_ = NULL;\n-  }\n-}\n-\n-}  // namespace leveldb"
      },
      {
        "sha": "9d084fdb7d00574c627291b1a48e059a8093d5b4",
        "filename": "src/leveldb/db/version_set.h",
        "status": "removed",
        "additions": 0,
        "deletions": 383,
        "changes": 383,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/version_set.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/version_set.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_set.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,383 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// The representation of a DBImpl consists of a set of Versions.  The\n-// newest version is called \"current\".  Older versions may be kept\n-// around to provide a consistent view to live iterators.\n-//\n-// Each Version keeps track of a set of Table files per level.  The\n-// entire set of versions is maintained in a VersionSet.\n-//\n-// Version,VersionSet are thread-compatible, but require external\n-// synchronization on all accesses.\n-\n-#ifndef STORAGE_LEVELDB_DB_VERSION_SET_H_\n-#define STORAGE_LEVELDB_DB_VERSION_SET_H_\n-\n-#include <map>\n-#include <set>\n-#include <vector>\n-#include \"db/dbformat.h\"\n-#include \"db/version_edit.h\"\n-#include \"port/port.h\"\n-#include \"port/thread_annotations.h\"\n-\n-namespace leveldb {\n-\n-namespace log { class Writer; }\n-\n-class Compaction;\n-class Iterator;\n-class MemTable;\n-class TableBuilder;\n-class TableCache;\n-class Version;\n-class VersionSet;\n-class WritableFile;\n-\n-// Return the smallest index i such that files[i]->largest >= key.\n-// Return files.size() if there is no such file.\n-// REQUIRES: \"files\" contains a sorted list of non-overlapping files.\n-extern int FindFile(const InternalKeyComparator& icmp,\n-                    const std::vector<FileMetaData*>& files,\n-                    const Slice& key);\n-\n-// Returns true iff some file in \"files\" overlaps the user key range\n-// [*smallest,*largest].\n-// smallest==NULL represents a key smaller than all keys in the DB.\n-// largest==NULL represents a key largest than all keys in the DB.\n-// REQUIRES: If disjoint_sorted_files, files[] contains disjoint ranges\n-//           in sorted order.\n-extern bool SomeFileOverlapsRange(\n-    const InternalKeyComparator& icmp,\n-    bool disjoint_sorted_files,\n-    const std::vector<FileMetaData*>& files,\n-    const Slice* smallest_user_key,\n-    const Slice* largest_user_key);\n-\n-class Version {\n- public:\n-  // Append to *iters a sequence of iterators that will\n-  // yield the contents of this Version when merged together.\n-  // REQUIRES: This version has been saved (see VersionSet::SaveTo)\n-  void AddIterators(const ReadOptions&, std::vector<Iterator*>* iters);\n-\n-  // Lookup the value for key.  If found, store it in *val and\n-  // return OK.  Else return a non-OK status.  Fills *stats.\n-  // REQUIRES: lock is not held\n-  struct GetStats {\n-    FileMetaData* seek_file;\n-    int seek_file_level;\n-  };\n-  Status Get(const ReadOptions&, const LookupKey& key, std::string* val,\n-             GetStats* stats);\n-\n-  // Adds \"stats\" into the current state.  Returns true if a new\n-  // compaction may need to be triggered, false otherwise.\n-  // REQUIRES: lock is held\n-  bool UpdateStats(const GetStats& stats);\n-\n-  // Reference count management (so Versions do not disappear out from\n-  // under live iterators)\n-  void Ref();\n-  void Unref();\n-\n-  void GetOverlappingInputs(\n-      int level,\n-      const InternalKey* begin,         // NULL means before all keys\n-      const InternalKey* end,           // NULL means after all keys\n-      std::vector<FileMetaData*>* inputs);\n-\n-  // Returns true iff some file in the specified level overlaps\n-  // some part of [*smallest_user_key,*largest_user_key].\n-  // smallest_user_key==NULL represents a key smaller than all keys in the DB.\n-  // largest_user_key==NULL represents a key largest than all keys in the DB.\n-  bool OverlapInLevel(int level,\n-                      const Slice* smallest_user_key,\n-                      const Slice* largest_user_key);\n-\n-  // Return the level at which we should place a new memtable compaction\n-  // result that covers the range [smallest_user_key,largest_user_key].\n-  int PickLevelForMemTableOutput(const Slice& smallest_user_key,\n-                                 const Slice& largest_user_key);\n-\n-  int NumFiles(int level) const { return files_[level].size(); }\n-\n-  // Return a human readable string that describes this version's contents.\n-  std::string DebugString() const;\n-\n- private:\n-  friend class Compaction;\n-  friend class VersionSet;\n-\n-  class LevelFileNumIterator;\n-  Iterator* NewConcatenatingIterator(const ReadOptions&, int level) const;\n-\n-  VersionSet* vset_;            // VersionSet to which this Version belongs\n-  Version* next_;               // Next version in linked list\n-  Version* prev_;               // Previous version in linked list\n-  int refs_;                    // Number of live refs to this version\n-\n-  // List of files per level\n-  std::vector<FileMetaData*> files_[config::kNumLevels];\n-\n-  // Next file to compact based on seek stats.\n-  FileMetaData* file_to_compact_;\n-  int file_to_compact_level_;\n-\n-  // Level that should be compacted next and its compaction score.\n-  // Score < 1 means compaction is not strictly needed.  These fields\n-  // are initialized by Finalize().\n-  double compaction_score_;\n-  int compaction_level_;\n-\n-  explicit Version(VersionSet* vset)\n-      : vset_(vset), next_(this), prev_(this), refs_(0),\n-        file_to_compact_(NULL),\n-        file_to_compact_level_(-1),\n-        compaction_score_(-1),\n-        compaction_level_(-1) {\n-  }\n-\n-  ~Version();\n-\n-  // No copying allowed\n-  Version(const Version&);\n-  void operator=(const Version&);\n-};\n-\n-class VersionSet {\n- public:\n-  VersionSet(const std::string& dbname,\n-             const Options* options,\n-             TableCache* table_cache,\n-             const InternalKeyComparator*);\n-  ~VersionSet();\n-\n-  // Apply *edit to the current version to form a new descriptor that\n-  // is both saved to persistent state and installed as the new\n-  // current version.  Will release *mu while actually writing to the file.\n-  // REQUIRES: *mu is held on entry.\n-  // REQUIRES: no other thread concurrently calls LogAndApply()\n-  Status LogAndApply(VersionEdit* edit, port::Mutex* mu)\n-      EXCLUSIVE_LOCKS_REQUIRED(mu);\n-\n-  // Recover the last saved descriptor from persistent storage.\n-  Status Recover();\n-\n-  // Return the current version.\n-  Version* current() const { return current_; }\n-\n-  // Return the current manifest file number\n-  uint64_t ManifestFileNumber() const { return manifest_file_number_; }\n-\n-  // Allocate and return a new file number\n-  uint64_t NewFileNumber() { return next_file_number_++; }\n-\n-  // Arrange to reuse \"file_number\" unless a newer file number has\n-  // already been allocated.\n-  // REQUIRES: \"file_number\" was returned by a call to NewFileNumber().\n-  void ReuseFileNumber(uint64_t file_number) {\n-    if (next_file_number_ == file_number + 1) {\n-      next_file_number_ = file_number;\n-    }\n-  }\n-\n-  // Return the number of Table files at the specified level.\n-  int NumLevelFiles(int level) const;\n-\n-  // Return the combined file size of all files at the specified level.\n-  int64_t NumLevelBytes(int level) const;\n-\n-  // Return the last sequence number.\n-  uint64_t LastSequence() const { return last_sequence_; }\n-\n-  // Set the last sequence number to s.\n-  void SetLastSequence(uint64_t s) {\n-    assert(s >= last_sequence_);\n-    last_sequence_ = s;\n-  }\n-\n-  // Mark the specified file number as used.\n-  void MarkFileNumberUsed(uint64_t number);\n-\n-  // Return the current log file number.\n-  uint64_t LogNumber() const { return log_number_; }\n-\n-  // Return the log file number for the log file that is currently\n-  // being compacted, or zero if there is no such log file.\n-  uint64_t PrevLogNumber() const { return prev_log_number_; }\n-\n-  // Pick level and inputs for a new compaction.\n-  // Returns NULL if there is no compaction to be done.\n-  // Otherwise returns a pointer to a heap-allocated object that\n-  // describes the compaction.  Caller should delete the result.\n-  Compaction* PickCompaction();\n-\n-  // Return a compaction object for compacting the range [begin,end] in\n-  // the specified level.  Returns NULL if there is nothing in that\n-  // level that overlaps the specified range.  Caller should delete\n-  // the result.\n-  Compaction* CompactRange(\n-      int level,\n-      const InternalKey* begin,\n-      const InternalKey* end);\n-\n-  // Return the maximum overlapping data (in bytes) at next level for any\n-  // file at a level >= 1.\n-  int64_t MaxNextLevelOverlappingBytes();\n-\n-  // Create an iterator that reads over the compaction inputs for \"*c\".\n-  // The caller should delete the iterator when no longer needed.\n-  Iterator* MakeInputIterator(Compaction* c);\n-\n-  // Returns true iff some level needs a compaction.\n-  bool NeedsCompaction() const {\n-    Version* v = current_;\n-    return (v->compaction_score_ >= 1) || (v->file_to_compact_ != NULL);\n-  }\n-\n-  // Add all files listed in any live version to *live.\n-  // May also mutate some internal state.\n-  void AddLiveFiles(std::set<uint64_t>* live);\n-\n-  // Return the approximate offset in the database of the data for\n-  // \"key\" as of version \"v\".\n-  uint64_t ApproximateOffsetOf(Version* v, const InternalKey& key);\n-\n-  // Return a human-readable short (single-line) summary of the number\n-  // of files per level.  Uses *scratch as backing store.\n-  struct LevelSummaryStorage {\n-    char buffer[100];\n-  };\n-  const char* LevelSummary(LevelSummaryStorage* scratch) const;\n-\n- private:\n-  class Builder;\n-\n-  friend class Compaction;\n-  friend class Version;\n-\n-  void Finalize(Version* v);\n-\n-  void GetRange(const std::vector<FileMetaData*>& inputs,\n-                InternalKey* smallest,\n-                InternalKey* largest);\n-\n-  void GetRange2(const std::vector<FileMetaData*>& inputs1,\n-                 const std::vector<FileMetaData*>& inputs2,\n-                 InternalKey* smallest,\n-                 InternalKey* largest);\n-\n-  void SetupOtherInputs(Compaction* c);\n-\n-  // Save current contents to *log\n-  Status WriteSnapshot(log::Writer* log);\n-\n-  void AppendVersion(Version* v);\n-\n-  bool ManifestContains(const std::string& record) const;\n-\n-  Env* const env_;\n-  const std::string dbname_;\n-  const Options* const options_;\n-  TableCache* const table_cache_;\n-  const InternalKeyComparator icmp_;\n-  uint64_t next_file_number_;\n-  uint64_t manifest_file_number_;\n-  uint64_t last_sequence_;\n-  uint64_t log_number_;\n-  uint64_t prev_log_number_;  // 0 or backing store for memtable being compacted\n-\n-  // Opened lazily\n-  WritableFile* descriptor_file_;\n-  log::Writer* descriptor_log_;\n-  Version dummy_versions_;  // Head of circular doubly-linked list of versions.\n-  Version* current_;        // == dummy_versions_.prev_\n-\n-  // Per-level key at which the next compaction at that level should start.\n-  // Either an empty string, or a valid InternalKey.\n-  std::string compact_pointer_[config::kNumLevels];\n-\n-  // No copying allowed\n-  VersionSet(const VersionSet&);\n-  void operator=(const VersionSet&);\n-};\n-\n-// A Compaction encapsulates information about a compaction.\n-class Compaction {\n- public:\n-  ~Compaction();\n-\n-  // Return the level that is being compacted.  Inputs from \"level\"\n-  // and \"level+1\" will be merged to produce a set of \"level+1\" files.\n-  int level() const { return level_; }\n-\n-  // Return the object that holds the edits to the descriptor done\n-  // by this compaction.\n-  VersionEdit* edit() { return &edit_; }\n-\n-  // \"which\" must be either 0 or 1\n-  int num_input_files(int which) const { return inputs_[which].size(); }\n-\n-  // Return the ith input file at \"level()+which\" (\"which\" must be 0 or 1).\n-  FileMetaData* input(int which, int i) const { return inputs_[which][i]; }\n-\n-  // Maximum size of files to build during this compaction.\n-  uint64_t MaxOutputFileSize() const { return max_output_file_size_; }\n-\n-  // Is this a trivial compaction that can be implemented by just\n-  // moving a single input file to the next level (no merging or splitting)\n-  bool IsTrivialMove() const;\n-\n-  // Add all inputs to this compaction as delete operations to *edit.\n-  void AddInputDeletions(VersionEdit* edit);\n-\n-  // Returns true if the information we have available guarantees that\n-  // the compaction is producing data in \"level+1\" for which no data exists\n-  // in levels greater than \"level+1\".\n-  bool IsBaseLevelForKey(const Slice& user_key);\n-\n-  // Returns true iff we should stop building the current output\n-  // before processing \"internal_key\".\n-  bool ShouldStopBefore(const Slice& internal_key);\n-\n-  // Release the input version for the compaction, once the compaction\n-  // is successful.\n-  void ReleaseInputs();\n-\n- private:\n-  friend class Version;\n-  friend class VersionSet;\n-\n-  explicit Compaction(int level);\n-\n-  int level_;\n-  uint64_t max_output_file_size_;\n-  Version* input_version_;\n-  VersionEdit edit_;\n-\n-  // Each compaction reads inputs from \"level_\" and \"level_+1\"\n-  std::vector<FileMetaData*> inputs_[2];      // The two sets of inputs\n-\n-  // State used to check for number of of overlapping grandparent files\n-  // (parent == level_ + 1, grandparent == level_ + 2)\n-  std::vector<FileMetaData*> grandparents_;\n-  size_t grandparent_index_;  // Index in grandparent_starts_\n-  bool seen_key_;             // Some output key has been seen\n-  int64_t overlapped_bytes_;  // Bytes of overlap between current output\n-                              // and grandparent files\n-\n-  // State for implementing IsBaseLevelForKey\n-\n-  // level_ptrs_ holds indices into input_version_->levels_: our state\n-  // is that we are positioned at one of the file ranges for each\n-  // higher level than the ones involved in this compaction (i.e. for\n-  // all L >= level_ + 2).\n-  size_t level_ptrs_[config::kNumLevels];\n-};\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_DB_VERSION_SET_H_"
      },
      {
        "sha": "501e34d1337d3917185b70369d9982db54e787aa",
        "filename": "src/leveldb/db/version_set_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 179,
        "changes": 179,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/version_set_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/version_set_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_set_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,179 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"db/version_set.h\"\n-#include \"util/logging.h\"\n-#include \"util/testharness.h\"\n-#include \"util/testutil.h\"\n-\n-namespace leveldb {\n-\n-class FindFileTest {\n- public:\n-  std::vector<FileMetaData*> files_;\n-  bool disjoint_sorted_files_;\n-\n-  FindFileTest() : disjoint_sorted_files_(true) { }\n-\n-  ~FindFileTest() {\n-    for (int i = 0; i < files_.size(); i++) {\n-      delete files_[i];\n-    }\n-  }\n-\n-  void Add(const char* smallest, const char* largest,\n-           SequenceNumber smallest_seq = 100,\n-           SequenceNumber largest_seq = 100) {\n-    FileMetaData* f = new FileMetaData;\n-    f->number = files_.size() + 1;\n-    f->smallest = InternalKey(smallest, smallest_seq, kTypeValue);\n-    f->largest = InternalKey(largest, largest_seq, kTypeValue);\n-    files_.push_back(f);\n-  }\n-\n-  int Find(const char* key) {\n-    InternalKey target(key, 100, kTypeValue);\n-    InternalKeyComparator cmp(BytewiseComparator());\n-    return FindFile(cmp, files_, target.Encode());\n-  }\n-\n-  bool Overlaps(const char* smallest, const char* largest) {\n-    InternalKeyComparator cmp(BytewiseComparator());\n-    Slice s(smallest != NULL ? smallest : \"\");\n-    Slice l(largest != NULL ? largest : \"\");\n-    return SomeFileOverlapsRange(cmp, disjoint_sorted_files_, files_,\n-                                 (smallest != NULL ? &s : NULL),\n-                                 (largest != NULL ? &l : NULL));\n-  }\n-};\n-\n-TEST(FindFileTest, Empty) {\n-  ASSERT_EQ(0, Find(\"foo\"));\n-  ASSERT_TRUE(! Overlaps(\"a\", \"z\"));\n-  ASSERT_TRUE(! Overlaps(NULL, \"z\"));\n-  ASSERT_TRUE(! Overlaps(\"a\", NULL));\n-  ASSERT_TRUE(! Overlaps(NULL, NULL));\n-}\n-\n-TEST(FindFileTest, Single) {\n-  Add(\"p\", \"q\");\n-  ASSERT_EQ(0, Find(\"a\"));\n-  ASSERT_EQ(0, Find(\"p\"));\n-  ASSERT_EQ(0, Find(\"p1\"));\n-  ASSERT_EQ(0, Find(\"q\"));\n-  ASSERT_EQ(1, Find(\"q1\"));\n-  ASSERT_EQ(1, Find(\"z\"));\n-\n-  ASSERT_TRUE(! Overlaps(\"a\", \"b\"));\n-  ASSERT_TRUE(! Overlaps(\"z1\", \"z2\"));\n-  ASSERT_TRUE(Overlaps(\"a\", \"p\"));\n-  ASSERT_TRUE(Overlaps(\"a\", \"q\"));\n-  ASSERT_TRUE(Overlaps(\"a\", \"z\"));\n-  ASSERT_TRUE(Overlaps(\"p\", \"p1\"));\n-  ASSERT_TRUE(Overlaps(\"p\", \"q\"));\n-  ASSERT_TRUE(Overlaps(\"p\", \"z\"));\n-  ASSERT_TRUE(Overlaps(\"p1\", \"p2\"));\n-  ASSERT_TRUE(Overlaps(\"p1\", \"z\"));\n-  ASSERT_TRUE(Overlaps(\"q\", \"q\"));\n-  ASSERT_TRUE(Overlaps(\"q\", \"q1\"));\n-\n-  ASSERT_TRUE(! Overlaps(NULL, \"j\"));\n-  ASSERT_TRUE(! Overlaps(\"r\", NULL));\n-  ASSERT_TRUE(Overlaps(NULL, \"p\"));\n-  ASSERT_TRUE(Overlaps(NULL, \"p1\"));\n-  ASSERT_TRUE(Overlaps(\"q\", NULL));\n-  ASSERT_TRUE(Overlaps(NULL, NULL));\n-}\n-\n-\n-TEST(FindFileTest, Multiple) {\n-  Add(\"150\", \"200\");\n-  Add(\"200\", \"250\");\n-  Add(\"300\", \"350\");\n-  Add(\"400\", \"450\");\n-  ASSERT_EQ(0, Find(\"100\"));\n-  ASSERT_EQ(0, Find(\"150\"));\n-  ASSERT_EQ(0, Find(\"151\"));\n-  ASSERT_EQ(0, Find(\"199\"));\n-  ASSERT_EQ(0, Find(\"200\"));\n-  ASSERT_EQ(1, Find(\"201\"));\n-  ASSERT_EQ(1, Find(\"249\"));\n-  ASSERT_EQ(1, Find(\"250\"));\n-  ASSERT_EQ(2, Find(\"251\"));\n-  ASSERT_EQ(2, Find(\"299\"));\n-  ASSERT_EQ(2, Find(\"300\"));\n-  ASSERT_EQ(2, Find(\"349\"));\n-  ASSERT_EQ(2, Find(\"350\"));\n-  ASSERT_EQ(3, Find(\"351\"));\n-  ASSERT_EQ(3, Find(\"400\"));\n-  ASSERT_EQ(3, Find(\"450\"));\n-  ASSERT_EQ(4, Find(\"451\"));\n-\n-  ASSERT_TRUE(! Overlaps(\"100\", \"149\"));\n-  ASSERT_TRUE(! Overlaps(\"251\", \"299\"));\n-  ASSERT_TRUE(! Overlaps(\"451\", \"500\"));\n-  ASSERT_TRUE(! Overlaps(\"351\", \"399\"));\n-\n-  ASSERT_TRUE(Overlaps(\"100\", \"150\"));\n-  ASSERT_TRUE(Overlaps(\"100\", \"200\"));\n-  ASSERT_TRUE(Overlaps(\"100\", \"300\"));\n-  ASSERT_TRUE(Overlaps(\"100\", \"400\"));\n-  ASSERT_TRUE(Overlaps(\"100\", \"500\"));\n-  ASSERT_TRUE(Overlaps(\"375\", \"400\"));\n-  ASSERT_TRUE(Overlaps(\"450\", \"450\"));\n-  ASSERT_TRUE(Overlaps(\"450\", \"500\"));\n-}\n-\n-TEST(FindFileTest, MultipleNullBoundaries) {\n-  Add(\"150\", \"200\");\n-  Add(\"200\", \"250\");\n-  Add(\"300\", \"350\");\n-  Add(\"400\", \"450\");\n-  ASSERT_TRUE(! Overlaps(NULL, \"149\"));\n-  ASSERT_TRUE(! Overlaps(\"451\", NULL));\n-  ASSERT_TRUE(Overlaps(NULL, NULL));\n-  ASSERT_TRUE(Overlaps(NULL, \"150\"));\n-  ASSERT_TRUE(Overlaps(NULL, \"199\"));\n-  ASSERT_TRUE(Overlaps(NULL, \"200\"));\n-  ASSERT_TRUE(Overlaps(NULL, \"201\"));\n-  ASSERT_TRUE(Overlaps(NULL, \"400\"));\n-  ASSERT_TRUE(Overlaps(NULL, \"800\"));\n-  ASSERT_TRUE(Overlaps(\"100\", NULL));\n-  ASSERT_TRUE(Overlaps(\"200\", NULL));\n-  ASSERT_TRUE(Overlaps(\"449\", NULL));\n-  ASSERT_TRUE(Overlaps(\"450\", NULL));\n-}\n-\n-TEST(FindFileTest, OverlapSequenceChecks) {\n-  Add(\"200\", \"200\", 5000, 3000);\n-  ASSERT_TRUE(! Overlaps(\"199\", \"199\"));\n-  ASSERT_TRUE(! Overlaps(\"201\", \"300\"));\n-  ASSERT_TRUE(Overlaps(\"200\", \"200\"));\n-  ASSERT_TRUE(Overlaps(\"190\", \"200\"));\n-  ASSERT_TRUE(Overlaps(\"200\", \"210\"));\n-}\n-\n-TEST(FindFileTest, OverlappingFiles) {\n-  Add(\"150\", \"600\");\n-  Add(\"400\", \"500\");\n-  disjoint_sorted_files_ = false;\n-  ASSERT_TRUE(! Overlaps(\"100\", \"149\"));\n-  ASSERT_TRUE(! Overlaps(\"601\", \"700\"));\n-  ASSERT_TRUE(Overlaps(\"100\", \"150\"));\n-  ASSERT_TRUE(Overlaps(\"100\", \"200\"));\n-  ASSERT_TRUE(Overlaps(\"100\", \"300\"));\n-  ASSERT_TRUE(Overlaps(\"100\", \"400\"));\n-  ASSERT_TRUE(Overlaps(\"100\", \"500\"));\n-  ASSERT_TRUE(Overlaps(\"375\", \"400\"));\n-  ASSERT_TRUE(Overlaps(\"450\", \"450\"));\n-  ASSERT_TRUE(Overlaps(\"450\", \"500\"));\n-  ASSERT_TRUE(Overlaps(\"450\", \"700\"));\n-  ASSERT_TRUE(Overlaps(\"600\", \"700\"));\n-}\n-\n-}  // namespace leveldb\n-\n-int main(int argc, char** argv) {\n-  return leveldb::test::RunAllTests();\n-}"
      },
      {
        "sha": "33f4a4257ea94e0105a9368de79d761ac7bf979a",
        "filename": "src/leveldb/db/write_batch.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 147,
        "changes": 147,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/write_batch.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/write_batch.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/write_batch.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,147 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// WriteBatch::rep_ :=\n-//    sequence: fixed64\n-//    count: fixed32\n-//    data: record[count]\n-// record :=\n-//    kTypeValue varstring varstring         |\n-//    kTypeDeletion varstring\n-// varstring :=\n-//    len: varint32\n-//    data: uint8[len]\n-\n-#include \"leveldb/write_batch.h\"\n-\n-#include \"leveldb/db.h\"\n-#include \"db/dbformat.h\"\n-#include \"db/memtable.h\"\n-#include \"db/write_batch_internal.h\"\n-#include \"util/coding.h\"\n-\n-namespace leveldb {\n-\n-// WriteBatch header has an 8-byte sequence number followed by a 4-byte count.\n-static const size_t kHeader = 12;\n-\n-WriteBatch::WriteBatch() {\n-  Clear();\n-}\n-\n-WriteBatch::~WriteBatch() { }\n-\n-WriteBatch::Handler::~Handler() { }\n-\n-void WriteBatch::Clear() {\n-  rep_.clear();\n-  rep_.resize(kHeader);\n-}\n-\n-Status WriteBatch::Iterate(Handler* handler) const {\n-  Slice input(rep_);\n-  if (input.size() < kHeader) {\n-    return Status::Corruption(\"malformed WriteBatch (too small)\");\n-  }\n-\n-  input.remove_prefix(kHeader);\n-  Slice key, value;\n-  int found = 0;\n-  while (!input.empty()) {\n-    found++;\n-    char tag = input[0];\n-    input.remove_prefix(1);\n-    switch (tag) {\n-      case kTypeValue:\n-        if (GetLengthPrefixedSlice(&input, &key) &&\n-            GetLengthPrefixedSlice(&input, &value)) {\n-          handler->Put(key, value);\n-        } else {\n-          return Status::Corruption(\"bad WriteBatch Put\");\n-        }\n-        break;\n-      case kTypeDeletion:\n-        if (GetLengthPrefixedSlice(&input, &key)) {\n-          handler->Delete(key);\n-        } else {\n-          return Status::Corruption(\"bad WriteBatch Delete\");\n-        }\n-        break;\n-      default:\n-        return Status::Corruption(\"unknown WriteBatch tag\");\n-    }\n-  }\n-  if (found != WriteBatchInternal::Count(this)) {\n-    return Status::Corruption(\"WriteBatch has wrong count\");\n-  } else {\n-    return Status::OK();\n-  }\n-}\n-\n-int WriteBatchInternal::Count(const WriteBatch* b) {\n-  return DecodeFixed32(b->rep_.data() + 8);\n-}\n-\n-void WriteBatchInternal::SetCount(WriteBatch* b, int n) {\n-  EncodeFixed32(&b->rep_[8], n);\n-}\n-\n-SequenceNumber WriteBatchInternal::Sequence(const WriteBatch* b) {\n-  return SequenceNumber(DecodeFixed64(b->rep_.data()));\n-}\n-\n-void WriteBatchInternal::SetSequence(WriteBatch* b, SequenceNumber seq) {\n-  EncodeFixed64(&b->rep_[0], seq);\n-}\n-\n-void WriteBatch::Put(const Slice& key, const Slice& value) {\n-  WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1);\n-  rep_.push_back(static_cast<char>(kTypeValue));\n-  PutLengthPrefixedSlice(&rep_, key);\n-  PutLengthPrefixedSlice(&rep_, value);\n-}\n-\n-void WriteBatch::Delete(const Slice& key) {\n-  WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1);\n-  rep_.push_back(static_cast<char>(kTypeDeletion));\n-  PutLengthPrefixedSlice(&rep_, key);\n-}\n-\n-namespace {\n-class MemTableInserter : public WriteBatch::Handler {\n- public:\n-  SequenceNumber sequence_;\n-  MemTable* mem_;\n-\n-  virtual void Put(const Slice& key, const Slice& value) {\n-    mem_->Add(sequence_, kTypeValue, key, value);\n-    sequence_++;\n-  }\n-  virtual void Delete(const Slice& key) {\n-    mem_->Add(sequence_, kTypeDeletion, key, Slice());\n-    sequence_++;\n-  }\n-};\n-}  // namespace\n-\n-Status WriteBatchInternal::InsertInto(const WriteBatch* b,\n-                                      MemTable* memtable) {\n-  MemTableInserter inserter;\n-  inserter.sequence_ = WriteBatchInternal::Sequence(b);\n-  inserter.mem_ = memtable;\n-  return b->Iterate(&inserter);\n-}\n-\n-void WriteBatchInternal::SetContents(WriteBatch* b, const Slice& contents) {\n-  assert(contents.size() >= kHeader);\n-  b->rep_.assign(contents.data(), contents.size());\n-}\n-\n-void WriteBatchInternal::Append(WriteBatch* dst, const WriteBatch* src) {\n-  SetCount(dst, Count(dst) + Count(src));\n-  assert(src->rep_.size() >= kHeader);\n-  dst->rep_.append(src->rep_.data() + kHeader, src->rep_.size() - kHeader);\n-}\n-\n-}  // namespace leveldb"
      },
      {
        "sha": "4423a7f31842457dea62d43547616b2f5ec852f8",
        "filename": "src/leveldb/db/write_batch_internal.h",
        "status": "removed",
        "additions": 0,
        "deletions": 49,
        "changes": 49,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/write_batch_internal.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/write_batch_internal.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/write_batch_internal.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,49 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_DB_WRITE_BATCH_INTERNAL_H_\n-#define STORAGE_LEVELDB_DB_WRITE_BATCH_INTERNAL_H_\n-\n-#include \"leveldb/write_batch.h\"\n-\n-namespace leveldb {\n-\n-class MemTable;\n-\n-// WriteBatchInternal provides static methods for manipulating a\n-// WriteBatch that we don't want in the public WriteBatch interface.\n-class WriteBatchInternal {\n- public:\n-  // Return the number of entries in the batch.\n-  static int Count(const WriteBatch* batch);\n-\n-  // Set the count for the number of entries in the batch.\n-  static void SetCount(WriteBatch* batch, int n);\n-\n-  // Return the seqeunce number for the start of this batch.\n-  static SequenceNumber Sequence(const WriteBatch* batch);\n-\n-  // Store the specified number as the seqeunce number for the start of\n-  // this batch.\n-  static void SetSequence(WriteBatch* batch, SequenceNumber seq);\n-\n-  static Slice Contents(const WriteBatch* batch) {\n-    return Slice(batch->rep_);\n-  }\n-\n-  static size_t ByteSize(const WriteBatch* batch) {\n-    return batch->rep_.size();\n-  }\n-\n-  static void SetContents(WriteBatch* batch, const Slice& contents);\n-\n-  static Status InsertInto(const WriteBatch* batch, MemTable* memtable);\n-\n-  static void Append(WriteBatch* dst, const WriteBatch* src);\n-};\n-\n-}  // namespace leveldb\n-\n-\n-#endif  // STORAGE_LEVELDB_DB_WRITE_BATCH_INTERNAL_H_"
      },
      {
        "sha": "9064e3d85eb35f32d20ef4c7456b0866d525aee8",
        "filename": "src/leveldb/db/write_batch_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 120,
        "changes": 120,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/write_batch_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/db/write_batch_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/write_batch_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,120 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"leveldb/db.h\"\n-\n-#include \"db/memtable.h\"\n-#include \"db/write_batch_internal.h\"\n-#include \"leveldb/env.h\"\n-#include \"util/logging.h\"\n-#include \"util/testharness.h\"\n-\n-namespace leveldb {\n-\n-static std::string PrintContents(WriteBatch* b) {\n-  InternalKeyComparator cmp(BytewiseComparator());\n-  MemTable* mem = new MemTable(cmp);\n-  mem->Ref();\n-  std::string state;\n-  Status s = WriteBatchInternal::InsertInto(b, mem);\n-  int count = 0;\n-  Iterator* iter = mem->NewIterator();\n-  for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n-    ParsedInternalKey ikey;\n-    ASSERT_TRUE(ParseInternalKey(iter->key(), &ikey));\n-    switch (ikey.type) {\n-      case kTypeValue:\n-        state.append(\"Put(\");\n-        state.append(ikey.user_key.ToString());\n-        state.append(\", \");\n-        state.append(iter->value().ToString());\n-        state.append(\")\");\n-        count++;\n-        break;\n-      case kTypeDeletion:\n-        state.append(\"Delete(\");\n-        state.append(ikey.user_key.ToString());\n-        state.append(\")\");\n-        count++;\n-        break;\n-    }\n-    state.append(\"@\");\n-    state.append(NumberToString(ikey.sequence));\n-  }\n-  delete iter;\n-  if (!s.ok()) {\n-    state.append(\"ParseError()\");\n-  } else if (count != WriteBatchInternal::Count(b)) {\n-    state.append(\"CountMismatch()\");\n-  }\n-  mem->Unref();\n-  return state;\n-}\n-\n-class WriteBatchTest { };\n-\n-TEST(WriteBatchTest, Empty) {\n-  WriteBatch batch;\n-  ASSERT_EQ(\"\", PrintContents(&batch));\n-  ASSERT_EQ(0, WriteBatchInternal::Count(&batch));\n-}\n-\n-TEST(WriteBatchTest, Multiple) {\n-  WriteBatch batch;\n-  batch.Put(Slice(\"foo\"), Slice(\"bar\"));\n-  batch.Delete(Slice(\"box\"));\n-  batch.Put(Slice(\"baz\"), Slice(\"boo\"));\n-  WriteBatchInternal::SetSequence(&batch, 100);\n-  ASSERT_EQ(100, WriteBatchInternal::Sequence(&batch));\n-  ASSERT_EQ(3, WriteBatchInternal::Count(&batch));\n-  ASSERT_EQ(\"Put(baz, boo)@102\"\n-            \"Delete(box)@101\"\n-            \"Put(foo, bar)@100\",\n-            PrintContents(&batch));\n-}\n-\n-TEST(WriteBatchTest, Corruption) {\n-  WriteBatch batch;\n-  batch.Put(Slice(\"foo\"), Slice(\"bar\"));\n-  batch.Delete(Slice(\"box\"));\n-  WriteBatchInternal::SetSequence(&batch, 200);\n-  Slice contents = WriteBatchInternal::Contents(&batch);\n-  WriteBatchInternal::SetContents(&batch,\n-                                  Slice(contents.data(),contents.size()-1));\n-  ASSERT_EQ(\"Put(foo, bar)@200\"\n-            \"ParseError()\",\n-            PrintContents(&batch));\n-}\n-\n-TEST(WriteBatchTest, Append) {\n-  WriteBatch b1, b2;\n-  WriteBatchInternal::SetSequence(&b1, 200);\n-  WriteBatchInternal::SetSequence(&b2, 300);\n-  WriteBatchInternal::Append(&b1, &b2);\n-  ASSERT_EQ(\"\",\n-            PrintContents(&b1));\n-  b2.Put(\"a\", \"va\");\n-  WriteBatchInternal::Append(&b1, &b2);\n-  ASSERT_EQ(\"Put(a, va)@200\",\n-            PrintContents(&b1));\n-  b2.Clear();\n-  b2.Put(\"b\", \"vb\");\n-  WriteBatchInternal::Append(&b1, &b2);\n-  ASSERT_EQ(\"Put(a, va)@200\"\n-            \"Put(b, vb)@201\",\n-            PrintContents(&b1));\n-  b2.Delete(\"foo\");\n-  WriteBatchInternal::Append(&b1, &b2);\n-  ASSERT_EQ(\"Put(a, va)@200\"\n-            \"Put(b, vb)@202\"\n-            \"Put(b, vb)@201\"\n-            \"Delete(foo)@203\",\n-            PrintContents(&b1));\n-}\n-\n-}  // namespace leveldb\n-\n-int main(int argc, char** argv) {\n-  return leveldb::test::RunAllTests();\n-}"
      },
      {
        "sha": "e63aaa8dcc289915176b12749dd9ec21b50aba02",
        "filename": "src/leveldb/doc/bench/db_bench_sqlite3.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 718,
        "changes": 718,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/bench/db_bench_sqlite3.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/bench/db_bench_sqlite3.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/bench/db_bench_sqlite3.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,718 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include <stdio.h>\n-#include <stdlib.h>\n-#include <sqlite3.h>\n-#include \"util/histogram.h\"\n-#include \"util/random.h\"\n-#include \"util/testutil.h\"\n-\n-// Comma-separated list of operations to run in the specified order\n-//   Actual benchmarks:\n-//\n-//   fillseq       -- write N values in sequential key order in async mode\n-//   fillseqsync   -- write N/100 values in sequential key order in sync mode\n-//   fillseqbatch  -- batch write N values in sequential key order in async mode\n-//   fillrandom    -- write N values in random key order in async mode\n-//   fillrandsync  -- write N/100 values in random key order in sync mode\n-//   fillrandbatch -- batch write N values in sequential key order in async mode\n-//   overwrite     -- overwrite N values in random key order in async mode\n-//   fillrand100K  -- write N/1000 100K values in random order in async mode\n-//   fillseq100K   -- write N/1000 100K values in sequential order in async mode\n-//   readseq       -- read N times sequentially\n-//   readrandom    -- read N times in random order\n-//   readrand100K  -- read N/1000 100K values in sequential order in async mode\n-static const char* FLAGS_benchmarks =\n-    \"fillseq,\"\n-    \"fillseqsync,\"\n-    \"fillseqbatch,\"\n-    \"fillrandom,\"\n-    \"fillrandsync,\"\n-    \"fillrandbatch,\"\n-    \"overwrite,\"\n-    \"overwritebatch,\"\n-    \"readrandom,\"\n-    \"readseq,\"\n-    \"fillrand100K,\"\n-    \"fillseq100K,\"\n-    \"readseq,\"\n-    \"readrand100K,\"\n-    ;\n-\n-// Number of key/values to place in database\n-static int FLAGS_num = 1000000;\n-\n-// Number of read operations to do.  If negative, do FLAGS_num reads.\n-static int FLAGS_reads = -1;\n-\n-// Size of each value\n-static int FLAGS_value_size = 100;\n-\n-// Print histogram of operation timings\n-static bool FLAGS_histogram = false;\n-\n-// Arrange to generate values that shrink to this fraction of\n-// their original size after compression\n-static double FLAGS_compression_ratio = 0.5;\n-\n-// Page size. Default 1 KB.\n-static int FLAGS_page_size = 1024;\n-\n-// Number of pages.\n-// Default cache size = FLAGS_page_size * FLAGS_num_pages = 4 MB.\n-static int FLAGS_num_pages = 4096;\n-\n-// If true, do not destroy the existing database.  If you set this\n-// flag and also specify a benchmark that wants a fresh database, that\n-// benchmark will fail.\n-static bool FLAGS_use_existing_db = false;\n-\n-// If true, we allow batch writes to occur\n-static bool FLAGS_transaction = true;\n-\n-// If true, we enable Write-Ahead Logging\n-static bool FLAGS_WAL_enabled = true;\n-\n-// Use the db with the following name.\n-static const char* FLAGS_db = NULL;\n-\n-inline\n-static void ExecErrorCheck(int status, char *err_msg) {\n-  if (status != SQLITE_OK) {\n-    fprintf(stderr, \"SQL error: %s\\n\", err_msg);\n-    sqlite3_free(err_msg);\n-    exit(1);\n-  }\n-}\n-\n-inline\n-static void StepErrorCheck(int status) {\n-  if (status != SQLITE_DONE) {\n-    fprintf(stderr, \"SQL step error: status = %d\\n\", status);\n-    exit(1);\n-  }\n-}\n-\n-inline\n-static void ErrorCheck(int status) {\n-  if (status != SQLITE_OK) {\n-    fprintf(stderr, \"sqlite3 error: status = %d\\n\", status);\n-    exit(1);\n-  }\n-}\n-\n-inline\n-static void WalCheckpoint(sqlite3* db_) {\n-  // Flush all writes to disk\n-  if (FLAGS_WAL_enabled) {\n-    sqlite3_wal_checkpoint_v2(db_, NULL, SQLITE_CHECKPOINT_FULL, NULL, NULL);\n-  }\n-}\n-\n-namespace leveldb {\n-\n-// Helper for quickly generating random data.\n-namespace {\n-class RandomGenerator {\n- private:\n-  std::string data_;\n-  int pos_;\n-\n- public:\n-  RandomGenerator() {\n-    // We use a limited amount of data over and over again and ensure\n-    // that it is larger than the compression window (32KB), and also\n-    // large enough to serve all typical value sizes we want to write.\n-    Random rnd(301);\n-    std::string piece;\n-    while (data_.size() < 1048576) {\n-      // Add a short fragment that is as compressible as specified\n-      // by FLAGS_compression_ratio.\n-      test::CompressibleString(&rnd, FLAGS_compression_ratio, 100, &piece);\n-      data_.append(piece);\n-    }\n-    pos_ = 0;\n-  }\n-\n-  Slice Generate(int len) {\n-    if (pos_ + len > data_.size()) {\n-      pos_ = 0;\n-      assert(len < data_.size());\n-    }\n-    pos_ += len;\n-    return Slice(data_.data() + pos_ - len, len);\n-  }\n-};\n-\n-static Slice TrimSpace(Slice s) {\n-  int start = 0;\n-  while (start < s.size() && isspace(s[start])) {\n-    start++;\n-  }\n-  int limit = s.size();\n-  while (limit > start && isspace(s[limit-1])) {\n-    limit--;\n-  }\n-  return Slice(s.data() + start, limit - start);\n-}\n-\n-}  // namespace\n-\n-class Benchmark {\n- private:\n-  sqlite3* db_;\n-  int db_num_;\n-  int num_;\n-  int reads_;\n-  double start_;\n-  double last_op_finish_;\n-  int64_t bytes_;\n-  std::string message_;\n-  Histogram hist_;\n-  RandomGenerator gen_;\n-  Random rand_;\n-\n-  // State kept for progress messages\n-  int done_;\n-  int next_report_;     // When to report next\n-\n-  void PrintHeader() {\n-    const int kKeySize = 16;\n-    PrintEnvironment();\n-    fprintf(stdout, \"Keys:       %d bytes each\\n\", kKeySize);\n-    fprintf(stdout, \"Values:     %d bytes each\\n\", FLAGS_value_size);\n-    fprintf(stdout, \"Entries:    %d\\n\", num_);\n-    fprintf(stdout, \"RawSize:    %.1f MB (estimated)\\n\",\n-            ((static_cast<int64_t>(kKeySize + FLAGS_value_size) * num_)\n-             / 1048576.0));\n-    PrintWarnings();\n-    fprintf(stdout, \"------------------------------------------------\\n\");\n-  }\n-\n-  void PrintWarnings() {\n-#if defined(__GNUC__) && !defined(__OPTIMIZE__)\n-    fprintf(stdout,\n-            \"WARNING: Optimization is disabled: benchmarks unnecessarily slow\\n\"\n-            );\n-#endif\n-#ifndef NDEBUG\n-    fprintf(stdout,\n-            \"WARNING: Assertions are enabled; benchmarks unnecessarily slow\\n\");\n-#endif\n-  }\n-\n-  void PrintEnvironment() {\n-    fprintf(stderr, \"SQLite:     version %s\\n\", SQLITE_VERSION);\n-\n-#if defined(__linux)\n-    time_t now = time(NULL);\n-    fprintf(stderr, \"Date:       %s\", ctime(&now));  // ctime() adds newline\n-\n-    FILE* cpuinfo = fopen(\"/proc/cpuinfo\", \"r\");\n-    if (cpuinfo != NULL) {\n-      char line[1000];\n-      int num_cpus = 0;\n-      std::string cpu_type;\n-      std::string cache_size;\n-      while (fgets(line, sizeof(line), cpuinfo) != NULL) {\n-        const char* sep = strchr(line, ':');\n-        if (sep == NULL) {\n-          continue;\n-        }\n-        Slice key = TrimSpace(Slice(line, sep - 1 - line));\n-        Slice val = TrimSpace(Slice(sep + 1));\n-        if (key == \"model name\") {\n-          ++num_cpus;\n-          cpu_type = val.ToString();\n-        } else if (key == \"cache size\") {\n-          cache_size = val.ToString();\n-        }\n-      }\n-      fclose(cpuinfo);\n-      fprintf(stderr, \"CPU:        %d * %s\\n\", num_cpus, cpu_type.c_str());\n-      fprintf(stderr, \"CPUCache:   %s\\n\", cache_size.c_str());\n-    }\n-#endif\n-  }\n-\n-  void Start() {\n-    start_ = Env::Default()->NowMicros() * 1e-6;\n-    bytes_ = 0;\n-    message_.clear();\n-    last_op_finish_ = start_;\n-    hist_.Clear();\n-    done_ = 0;\n-    next_report_ = 100;\n-  }\n-\n-  void FinishedSingleOp() {\n-    if (FLAGS_histogram) {\n-      double now = Env::Default()->NowMicros() * 1e-6;\n-      double micros = (now - last_op_finish_) * 1e6;\n-      hist_.Add(micros);\n-      if (micros > 20000) {\n-        fprintf(stderr, \"long op: %.1f micros%30s\\r\", micros, \"\");\n-        fflush(stderr);\n-      }\n-      last_op_finish_ = now;\n-    }\n-\n-    done_++;\n-    if (done_ >= next_report_) {\n-      if      (next_report_ < 1000)   next_report_ += 100;\n-      else if (next_report_ < 5000)   next_report_ += 500;\n-      else if (next_report_ < 10000)  next_report_ += 1000;\n-      else if (next_report_ < 50000)  next_report_ += 5000;\n-      else if (next_report_ < 100000) next_report_ += 10000;\n-      else if (next_report_ < 500000) next_report_ += 50000;\n-      else                            next_report_ += 100000;\n-      fprintf(stderr, \"... finished %d ops%30s\\r\", done_, \"\");\n-      fflush(stderr);\n-    }\n-  }\n-\n-  void Stop(const Slice& name) {\n-    double finish = Env::Default()->NowMicros() * 1e-6;\n-\n-    // Pretend at least one op was done in case we are running a benchmark\n-    // that does not call FinishedSingleOp().\n-    if (done_ < 1) done_ = 1;\n-\n-    if (bytes_ > 0) {\n-      char rate[100];\n-      snprintf(rate, sizeof(rate), \"%6.1f MB/s\",\n-               (bytes_ / 1048576.0) / (finish - start_));\n-      if (!message_.empty()) {\n-        message_  = std::string(rate) + \" \" + message_;\n-      } else {\n-        message_ = rate;\n-      }\n-    }\n-\n-    fprintf(stdout, \"%-12s : %11.3f micros/op;%s%s\\n\",\n-            name.ToString().c_str(),\n-            (finish - start_) * 1e6 / done_,\n-            (message_.empty() ? \"\" : \" \"),\n-            message_.c_str());\n-    if (FLAGS_histogram) {\n-      fprintf(stdout, \"Microseconds per op:\\n%s\\n\", hist_.ToString().c_str());\n-    }\n-    fflush(stdout);\n-  }\n-\n- public:\n-  enum Order {\n-    SEQUENTIAL,\n-    RANDOM\n-  };\n-  enum DBState {\n-    FRESH,\n-    EXISTING\n-  };\n-\n-  Benchmark()\n-  : db_(NULL),\n-    db_num_(0),\n-    num_(FLAGS_num),\n-    reads_(FLAGS_reads < 0 ? FLAGS_num : FLAGS_reads),\n-    bytes_(0),\n-    rand_(301) {\n-    std::vector<std::string> files;\n-    std::string test_dir;\n-    Env::Default()->GetTestDirectory(&test_dir);\n-    Env::Default()->GetChildren(test_dir, &files);\n-    if (!FLAGS_use_existing_db) {\n-      for (int i = 0; i < files.size(); i++) {\n-        if (Slice(files[i]).starts_with(\"dbbench_sqlite3\")) {\n-          std::string file_name(test_dir);\n-          file_name += \"/\";\n-          file_name += files[i];\n-          Env::Default()->DeleteFile(file_name.c_str());\n-        }\n-      }\n-    }\n-  }\n-\n-  ~Benchmark() {\n-    int status = sqlite3_close(db_);\n-    ErrorCheck(status);\n-  }\n-\n-  void Run() {\n-    PrintHeader();\n-    Open();\n-\n-    const char* benchmarks = FLAGS_benchmarks;\n-    while (benchmarks != NULL) {\n-      const char* sep = strchr(benchmarks, ',');\n-      Slice name;\n-      if (sep == NULL) {\n-        name = benchmarks;\n-        benchmarks = NULL;\n-      } else {\n-        name = Slice(benchmarks, sep - benchmarks);\n-        benchmarks = sep + 1;\n-      }\n-\n-      bytes_ = 0;\n-      Start();\n-\n-      bool known = true;\n-      bool write_sync = false;\n-      if (name == Slice(\"fillseq\")) {\n-        Write(write_sync, SEQUENTIAL, FRESH, num_, FLAGS_value_size, 1);\n-        WalCheckpoint(db_);\n-      } else if (name == Slice(\"fillseqbatch\")) {\n-        Write(write_sync, SEQUENTIAL, FRESH, num_, FLAGS_value_size, 1000);\n-        WalCheckpoint(db_);\n-      } else if (name == Slice(\"fillrandom\")) {\n-        Write(write_sync, RANDOM, FRESH, num_, FLAGS_value_size, 1);\n-        WalCheckpoint(db_);\n-      } else if (name == Slice(\"fillrandbatch\")) {\n-        Write(write_sync, RANDOM, FRESH, num_, FLAGS_value_size, 1000);\n-        WalCheckpoint(db_);\n-      } else if (name == Slice(\"overwrite\")) {\n-        Write(write_sync, RANDOM, EXISTING, num_, FLAGS_value_size, 1);\n-        WalCheckpoint(db_);\n-      } else if (name == Slice(\"overwritebatch\")) {\n-        Write(write_sync, RANDOM, EXISTING, num_, FLAGS_value_size, 1000);\n-        WalCheckpoint(db_);\n-      } else if (name == Slice(\"fillrandsync\")) {\n-        write_sync = true;\n-        Write(write_sync, RANDOM, FRESH, num_ / 100, FLAGS_value_size, 1);\n-        WalCheckpoint(db_);\n-      } else if (name == Slice(\"fillseqsync\")) {\n-        write_sync = true;\n-        Write(write_sync, SEQUENTIAL, FRESH, num_ / 100, FLAGS_value_size, 1);\n-        WalCheckpoint(db_);\n-      } else if (name == Slice(\"fillrand100K\")) {\n-        Write(write_sync, RANDOM, FRESH, num_ / 1000, 100 * 1000, 1);\n-        WalCheckpoint(db_);\n-      } else if (name == Slice(\"fillseq100K\")) {\n-        Write(write_sync, SEQUENTIAL, FRESH, num_ / 1000, 100 * 1000, 1);\n-        WalCheckpoint(db_);\n-      } else if (name == Slice(\"readseq\")) {\n-        ReadSequential();\n-      } else if (name == Slice(\"readrandom\")) {\n-        Read(RANDOM, 1);\n-      } else if (name == Slice(\"readrand100K\")) {\n-        int n = reads_;\n-        reads_ /= 1000;\n-        Read(RANDOM, 1);\n-        reads_ = n;\n-      } else {\n-        known = false;\n-        if (name != Slice()) {  // No error message for empty name\n-          fprintf(stderr, \"unknown benchmark '%s'\\n\", name.ToString().c_str());\n-        }\n-      }\n-      if (known) {\n-        Stop(name);\n-      }\n-    }\n-  }\n-\n-  void Open() {\n-    assert(db_ == NULL);\n-\n-    int status;\n-    char file_name[100];\n-    char* err_msg = NULL;\n-    db_num_++;\n-\n-    // Open database\n-    std::string tmp_dir;\n-    Env::Default()->GetTestDirectory(&tmp_dir);\n-    snprintf(file_name, sizeof(file_name),\n-             \"%s/dbbench_sqlite3-%d.db\",\n-             tmp_dir.c_str(),\n-             db_num_);\n-    status = sqlite3_open(file_name, &db_);\n-    if (status) {\n-      fprintf(stderr, \"open error: %s\\n\", sqlite3_errmsg(db_));\n-      exit(1);\n-    }\n-\n-    // Change SQLite cache size\n-    char cache_size[100];\n-    snprintf(cache_size, sizeof(cache_size), \"PRAGMA cache_size = %d\",\n-             FLAGS_num_pages);\n-    status = sqlite3_exec(db_, cache_size, NULL, NULL, &err_msg);\n-    ExecErrorCheck(status, err_msg);\n-\n-    // FLAGS_page_size is defaulted to 1024\n-    if (FLAGS_page_size != 1024) {\n-      char page_size[100];\n-      snprintf(page_size, sizeof(page_size), \"PRAGMA page_size = %d\",\n-               FLAGS_page_size);\n-      status = sqlite3_exec(db_, page_size, NULL, NULL, &err_msg);\n-      ExecErrorCheck(status, err_msg);\n-    }\n-\n-    // Change journal mode to WAL if WAL enabled flag is on\n-    if (FLAGS_WAL_enabled) {\n-      std::string WAL_stmt = \"PRAGMA journal_mode = WAL\";\n-\n-      // LevelDB's default cache size is a combined 4 MB\n-      std::string WAL_checkpoint = \"PRAGMA wal_autocheckpoint = 4096\";\n-      status = sqlite3_exec(db_, WAL_stmt.c_str(), NULL, NULL, &err_msg);\n-      ExecErrorCheck(status, err_msg);\n-      status = sqlite3_exec(db_, WAL_checkpoint.c_str(), NULL, NULL, &err_msg);\n-      ExecErrorCheck(status, err_msg);\n-    }\n-\n-    // Change locking mode to exclusive and create tables/index for database\n-    std::string locking_stmt = \"PRAGMA locking_mode = EXCLUSIVE\";\n-    std::string create_stmt =\n-          \"CREATE TABLE test (key blob, value blob, PRIMARY KEY(key))\";\n-    std::string stmt_array[] = { locking_stmt, create_stmt };\n-    int stmt_array_length = sizeof(stmt_array) / sizeof(std::string);\n-    for (int i = 0; i < stmt_array_length; i++) {\n-      status = sqlite3_exec(db_, stmt_array[i].c_str(), NULL, NULL, &err_msg);\n-      ExecErrorCheck(status, err_msg);\n-    }\n-  }\n-\n-  void Write(bool write_sync, Order order, DBState state,\n-             int num_entries, int value_size, int entries_per_batch) {\n-    // Create new database if state == FRESH\n-    if (state == FRESH) {\n-      if (FLAGS_use_existing_db) {\n-        message_ = \"skipping (--use_existing_db is true)\";\n-        return;\n-      }\n-      sqlite3_close(db_);\n-      db_ = NULL;\n-      Open();\n-      Start();\n-    }\n-\n-    if (num_entries != num_) {\n-      char msg[100];\n-      snprintf(msg, sizeof(msg), \"(%d ops)\", num_entries);\n-      message_ = msg;\n-    }\n-\n-    char* err_msg = NULL;\n-    int status;\n-\n-    sqlite3_stmt *replace_stmt, *begin_trans_stmt, *end_trans_stmt;\n-    std::string replace_str = \"REPLACE INTO test (key, value) VALUES (?, ?)\";\n-    std::string begin_trans_str = \"BEGIN TRANSACTION;\";\n-    std::string end_trans_str = \"END TRANSACTION;\";\n-\n-    // Check for synchronous flag in options\n-    std::string sync_stmt = (write_sync) ? \"PRAGMA synchronous = FULL\" :\n-                                           \"PRAGMA synchronous = OFF\";\n-    status = sqlite3_exec(db_, sync_stmt.c_str(), NULL, NULL, &err_msg);\n-    ExecErrorCheck(status, err_msg);\n-\n-    // Preparing sqlite3 statements\n-    status = sqlite3_prepare_v2(db_, replace_str.c_str(), -1,\n-                                &replace_stmt, NULL);\n-    ErrorCheck(status);\n-    status = sqlite3_prepare_v2(db_, begin_trans_str.c_str(), -1,\n-                                &begin_trans_stmt, NULL);\n-    ErrorCheck(status);\n-    status = sqlite3_prepare_v2(db_, end_trans_str.c_str(), -1,\n-                                &end_trans_stmt, NULL);\n-    ErrorCheck(status);\n-\n-    bool transaction = (entries_per_batch > 1);\n-    for (int i = 0; i < num_entries; i += entries_per_batch) {\n-      // Begin write transaction\n-      if (FLAGS_transaction && transaction) {\n-        status = sqlite3_step(begin_trans_stmt);\n-        StepErrorCheck(status);\n-        status = sqlite3_reset(begin_trans_stmt);\n-        ErrorCheck(status);\n-      }\n-\n-      // Create and execute SQL statements\n-      for (int j = 0; j < entries_per_batch; j++) {\n-        const char* value = gen_.Generate(value_size).data();\n-\n-        // Create values for key-value pair\n-        const int k = (order == SEQUENTIAL) ? i + j :\n-                      (rand_.Next() % num_entries);\n-        char key[100];\n-        snprintf(key, sizeof(key), \"%016d\", k);\n-\n-        // Bind KV values into replace_stmt\n-        status = sqlite3_bind_blob(replace_stmt, 1, key, 16, SQLITE_STATIC);\n-        ErrorCheck(status);\n-        status = sqlite3_bind_blob(replace_stmt, 2, value,\n-                                   value_size, SQLITE_STATIC);\n-        ErrorCheck(status);\n-\n-        // Execute replace_stmt\n-        bytes_ += value_size + strlen(key);\n-        status = sqlite3_step(replace_stmt);\n-        StepErrorCheck(status);\n-\n-        // Reset SQLite statement for another use\n-        status = sqlite3_clear_bindings(replace_stmt);\n-        ErrorCheck(status);\n-        status = sqlite3_reset(replace_stmt);\n-        ErrorCheck(status);\n-\n-        FinishedSingleOp();\n-      }\n-\n-      // End write transaction\n-      if (FLAGS_transaction && transaction) {\n-        status = sqlite3_step(end_trans_stmt);\n-        StepErrorCheck(status);\n-        status = sqlite3_reset(end_trans_stmt);\n-        ErrorCheck(status);\n-      }\n-    }\n-\n-    status = sqlite3_finalize(replace_stmt);\n-    ErrorCheck(status);\n-    status = sqlite3_finalize(begin_trans_stmt);\n-    ErrorCheck(status);\n-    status = sqlite3_finalize(end_trans_stmt);\n-    ErrorCheck(status);\n-  }\n-\n-  void Read(Order order, int entries_per_batch) {\n-    int status;\n-    sqlite3_stmt *read_stmt, *begin_trans_stmt, *end_trans_stmt;\n-\n-    std::string read_str = \"SELECT * FROM test WHERE key = ?\";\n-    std::string begin_trans_str = \"BEGIN TRANSACTION;\";\n-    std::string end_trans_str = \"END TRANSACTION;\";\n-\n-    // Preparing sqlite3 statements\n-    status = sqlite3_prepare_v2(db_, begin_trans_str.c_str(), -1,\n-                                &begin_trans_stmt, NULL);\n-    ErrorCheck(status);\n-    status = sqlite3_prepare_v2(db_, end_trans_str.c_str(), -1,\n-                                &end_trans_stmt, NULL);\n-    ErrorCheck(status);\n-    status = sqlite3_prepare_v2(db_, read_str.c_str(), -1, &read_stmt, NULL);\n-    ErrorCheck(status);\n-\n-    bool transaction = (entries_per_batch > 1);\n-    for (int i = 0; i < reads_; i += entries_per_batch) {\n-      // Begin read transaction\n-      if (FLAGS_transaction && transaction) {\n-        status = sqlite3_step(begin_trans_stmt);\n-        StepErrorCheck(status);\n-        status = sqlite3_reset(begin_trans_stmt);\n-        ErrorCheck(status);\n-      }\n-\n-      // Create and execute SQL statements\n-      for (int j = 0; j < entries_per_batch; j++) {\n-        // Create key value\n-        char key[100];\n-        int k = (order == SEQUENTIAL) ? i + j : (rand_.Next() % reads_);\n-        snprintf(key, sizeof(key), \"%016d\", k);\n-\n-        // Bind key value into read_stmt\n-        status = sqlite3_bind_blob(read_stmt, 1, key, 16, SQLITE_STATIC);\n-        ErrorCheck(status);\n-\n-        // Execute read statement\n-        while ((status = sqlite3_step(read_stmt)) == SQLITE_ROW) {}\n-        StepErrorCheck(status);\n-\n-        // Reset SQLite statement for another use\n-        status = sqlite3_clear_bindings(read_stmt);\n-        ErrorCheck(status);\n-        status = sqlite3_reset(read_stmt);\n-        ErrorCheck(status);\n-        FinishedSingleOp();\n-      }\n-\n-      // End read transaction\n-      if (FLAGS_transaction && transaction) {\n-        status = sqlite3_step(end_trans_stmt);\n-        StepErrorCheck(status);\n-        status = sqlite3_reset(end_trans_stmt);\n-        ErrorCheck(status);\n-      }\n-    }\n-\n-    status = sqlite3_finalize(read_stmt);\n-    ErrorCheck(status);\n-    status = sqlite3_finalize(begin_trans_stmt);\n-    ErrorCheck(status);\n-    status = sqlite3_finalize(end_trans_stmt);\n-    ErrorCheck(status);\n-  }\n-\n-  void ReadSequential() {\n-    int status;\n-    sqlite3_stmt *pStmt;\n-    std::string read_str = \"SELECT * FROM test ORDER BY key\";\n-\n-    status = sqlite3_prepare_v2(db_, read_str.c_str(), -1, &pStmt, NULL);\n-    ErrorCheck(status);\n-    for (int i = 0; i < reads_ && SQLITE_ROW == sqlite3_step(pStmt); i++) {\n-      bytes_ += sqlite3_column_bytes(pStmt, 1) + sqlite3_column_bytes(pStmt, 2);\n-      FinishedSingleOp();\n-    }\n-\n-    status = sqlite3_finalize(pStmt);\n-    ErrorCheck(status);\n-  }\n-\n-};\n-\n-}  // namespace leveldb\n-\n-int main(int argc, char** argv) {\n-  std::string default_db_path;\n-  for (int i = 1; i < argc; i++) {\n-    double d;\n-    int n;\n-    char junk;\n-    if (leveldb::Slice(argv[i]).starts_with(\"--benchmarks=\")) {\n-      FLAGS_benchmarks = argv[i] + strlen(\"--benchmarks=\");\n-    } else if (sscanf(argv[i], \"--histogram=%d%c\", &n, &junk) == 1 &&\n-               (n == 0 || n == 1)) {\n-      FLAGS_histogram = n;\n-    } else if (sscanf(argv[i], \"--compression_ratio=%lf%c\", &d, &junk) == 1) {\n-      FLAGS_compression_ratio = d;\n-    } else if (sscanf(argv[i], \"--use_existing_db=%d%c\", &n, &junk) == 1 &&\n-               (n == 0 || n == 1)) {\n-      FLAGS_use_existing_db = n;\n-    } else if (sscanf(argv[i], \"--num=%d%c\", &n, &junk) == 1) {\n-      FLAGS_num = n;\n-    } else if (sscanf(argv[i], \"--reads=%d%c\", &n, &junk) == 1) {\n-      FLAGS_reads = n;\n-    } else if (sscanf(argv[i], \"--value_size=%d%c\", &n, &junk) == 1) {\n-      FLAGS_value_size = n;\n-    } else if (leveldb::Slice(argv[i]) == leveldb::Slice(\"--no_transaction\")) {\n-      FLAGS_transaction = false;\n-    } else if (sscanf(argv[i], \"--page_size=%d%c\", &n, &junk) == 1) {\n-      FLAGS_page_size = n;\n-    } else if (sscanf(argv[i], \"--num_pages=%d%c\", &n, &junk) == 1) {\n-      FLAGS_num_pages = n;\n-    } else if (sscanf(argv[i], \"--WAL_enabled=%d%c\", &n, &junk) == 1 &&\n-               (n == 0 || n == 1)) {\n-      FLAGS_WAL_enabled = n;\n-    } else if (strncmp(argv[i], \"--db=\", 5) == 0) {\n-      FLAGS_db = argv[i] + 5;\n-    } else {\n-      fprintf(stderr, \"Invalid flag '%s'\\n\", argv[i]);\n-      exit(1);\n-    }\n-  }\n-\n-  // Choose a location for the test database if none given with --db=<path>\n-  if (FLAGS_db == NULL) {\n-      leveldb::Env::Default()->GetTestDirectory(&default_db_path);\n-      default_db_path += \"/dbbench\";\n-      FLAGS_db = default_db_path.c_str();\n-  }\n-\n-  leveldb::Benchmark benchmark;\n-  benchmark.Run();\n-  return 0;\n-}"
      },
      {
        "sha": "ed86f031c25fe931b0e3a05f4501269afd233f02",
        "filename": "src/leveldb/doc/bench/db_bench_tree_db.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 528,
        "changes": 528,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/bench/db_bench_tree_db.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/bench/db_bench_tree_db.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/bench/db_bench_tree_db.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,528 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include <stdio.h>\n-#include <stdlib.h>\n-#include <kcpolydb.h>\n-#include \"util/histogram.h\"\n-#include \"util/random.h\"\n-#include \"util/testutil.h\"\n-\n-// Comma-separated list of operations to run in the specified order\n-//   Actual benchmarks:\n-//\n-//   fillseq       -- write N values in sequential key order in async mode\n-//   fillrandom    -- write N values in random key order in async mode\n-//   overwrite     -- overwrite N values in random key order in async mode\n-//   fillseqsync   -- write N/100 values in sequential key order in sync mode\n-//   fillrandsync  -- write N/100 values in random key order in sync mode\n-//   fillrand100K  -- write N/1000 100K values in random order in async mode\n-//   fillseq100K   -- write N/1000 100K values in seq order in async mode\n-//   readseq       -- read N times sequentially\n-//   readseq100K   -- read N/1000 100K values in sequential order in async mode\n-//   readrand100K  -- read N/1000 100K values in sequential order in async mode\n-//   readrandom    -- read N times in random order\n-static const char* FLAGS_benchmarks =\n-    \"fillseq,\"\n-    \"fillseqsync,\"\n-    \"fillrandsync,\"\n-    \"fillrandom,\"\n-    \"overwrite,\"\n-    \"readrandom,\"\n-    \"readseq,\"\n-    \"fillrand100K,\"\n-    \"fillseq100K,\"\n-    \"readseq100K,\"\n-    \"readrand100K,\"\n-    ;\n-\n-// Number of key/values to place in database\n-static int FLAGS_num = 1000000;\n-\n-// Number of read operations to do.  If negative, do FLAGS_num reads.\n-static int FLAGS_reads = -1;\n-\n-// Size of each value\n-static int FLAGS_value_size = 100;\n-\n-// Arrange to generate values that shrink to this fraction of\n-// their original size after compression\n-static double FLAGS_compression_ratio = 0.5;\n-\n-// Print histogram of operation timings\n-static bool FLAGS_histogram = false;\n-\n-// Cache size. Default 4 MB\n-static int FLAGS_cache_size = 4194304;\n-\n-// Page size. Default 1 KB\n-static int FLAGS_page_size = 1024;\n-\n-// If true, do not destroy the existing database.  If you set this\n-// flag and also specify a benchmark that wants a fresh database, that\n-// benchmark will fail.\n-static bool FLAGS_use_existing_db = false;\n-\n-// Compression flag. If true, compression is on. If false, compression\n-// is off.\n-static bool FLAGS_compression = true;\n-\n-// Use the db with the following name.\n-static const char* FLAGS_db = NULL;\n-\n-inline\n-static void DBSynchronize(kyotocabinet::TreeDB* db_)\n-{\n-  // Synchronize will flush writes to disk\n-  if (!db_->synchronize()) {\n-    fprintf(stderr, \"synchronize error: %s\\n\", db_->error().name());\n-  }\n-}\n-\n-namespace leveldb {\n-\n-// Helper for quickly generating random data.\n-namespace {\n-class RandomGenerator {\n- private:\n-  std::string data_;\n-  int pos_;\n-\n- public:\n-  RandomGenerator() {\n-    // We use a limited amount of data over and over again and ensure\n-    // that it is larger than the compression window (32KB), and also\n-    // large enough to serve all typical value sizes we want to write.\n-    Random rnd(301);\n-    std::string piece;\n-    while (data_.size() < 1048576) {\n-      // Add a short fragment that is as compressible as specified\n-      // by FLAGS_compression_ratio.\n-      test::CompressibleString(&rnd, FLAGS_compression_ratio, 100, &piece);\n-      data_.append(piece);\n-    }\n-    pos_ = 0;\n-  }\n-\n-  Slice Generate(int len) {\n-    if (pos_ + len > data_.size()) {\n-      pos_ = 0;\n-      assert(len < data_.size());\n-    }\n-    pos_ += len;\n-    return Slice(data_.data() + pos_ - len, len);\n-  }\n-};\n-\n-static Slice TrimSpace(Slice s) {\n-  int start = 0;\n-  while (start < s.size() && isspace(s[start])) {\n-    start++;\n-  }\n-  int limit = s.size();\n-  while (limit > start && isspace(s[limit-1])) {\n-    limit--;\n-  }\n-  return Slice(s.data() + start, limit - start);\n-}\n-\n-}  // namespace\n-\n-class Benchmark {\n- private:\n-  kyotocabinet::TreeDB* db_;\n-  int db_num_;\n-  int num_;\n-  int reads_;\n-  double start_;\n-  double last_op_finish_;\n-  int64_t bytes_;\n-  std::string message_;\n-  Histogram hist_;\n-  RandomGenerator gen_;\n-  Random rand_;\n-  kyotocabinet::LZOCompressor<kyotocabinet::LZO::RAW> comp_;\n-\n-  // State kept for progress messages\n-  int done_;\n-  int next_report_;     // When to report next\n-\n-  void PrintHeader() {\n-    const int kKeySize = 16;\n-    PrintEnvironment();\n-    fprintf(stdout, \"Keys:       %d bytes each\\n\", kKeySize);\n-    fprintf(stdout, \"Values:     %d bytes each (%d bytes after compression)\\n\",\n-            FLAGS_value_size,\n-            static_cast<int>(FLAGS_value_size * FLAGS_compression_ratio + 0.5));\n-    fprintf(stdout, \"Entries:    %d\\n\", num_);\n-    fprintf(stdout, \"RawSize:    %.1f MB (estimated)\\n\",\n-            ((static_cast<int64_t>(kKeySize + FLAGS_value_size) * num_)\n-             / 1048576.0));\n-    fprintf(stdout, \"FileSize:   %.1f MB (estimated)\\n\",\n-            (((kKeySize + FLAGS_value_size * FLAGS_compression_ratio) * num_)\n-             / 1048576.0));\n-    PrintWarnings();\n-    fprintf(stdout, \"------------------------------------------------\\n\");\n-  }\n-\n-  void PrintWarnings() {\n-#if defined(__GNUC__) && !defined(__OPTIMIZE__)\n-    fprintf(stdout,\n-            \"WARNING: Optimization is disabled: benchmarks unnecessarily slow\\n\"\n-            );\n-#endif\n-#ifndef NDEBUG\n-    fprintf(stdout,\n-            \"WARNING: Assertions are enabled; benchmarks unnecessarily slow\\n\");\n-#endif\n-  }\n-\n-  void PrintEnvironment() {\n-    fprintf(stderr, \"Kyoto Cabinet:    version %s, lib ver %d, lib rev %d\\n\",\n-            kyotocabinet::VERSION, kyotocabinet::LIBVER, kyotocabinet::LIBREV);\n-\n-#if defined(__linux)\n-    time_t now = time(NULL);\n-    fprintf(stderr, \"Date:           %s\", ctime(&now));  // ctime() adds newline\n-\n-    FILE* cpuinfo = fopen(\"/proc/cpuinfo\", \"r\");\n-    if (cpuinfo != NULL) {\n-      char line[1000];\n-      int num_cpus = 0;\n-      std::string cpu_type;\n-      std::string cache_size;\n-      while (fgets(line, sizeof(line), cpuinfo) != NULL) {\n-        const char* sep = strchr(line, ':');\n-        if (sep == NULL) {\n-          continue;\n-        }\n-        Slice key = TrimSpace(Slice(line, sep - 1 - line));\n-        Slice val = TrimSpace(Slice(sep + 1));\n-        if (key == \"model name\") {\n-          ++num_cpus;\n-          cpu_type = val.ToString();\n-        } else if (key == \"cache size\") {\n-          cache_size = val.ToString();\n-        }\n-      }\n-      fclose(cpuinfo);\n-      fprintf(stderr, \"CPU:            %d * %s\\n\", num_cpus, cpu_type.c_str());\n-      fprintf(stderr, \"CPUCache:       %s\\n\", cache_size.c_str());\n-    }\n-#endif\n-  }\n-\n-  void Start() {\n-    start_ = Env::Default()->NowMicros() * 1e-6;\n-    bytes_ = 0;\n-    message_.clear();\n-    last_op_finish_ = start_;\n-    hist_.Clear();\n-    done_ = 0;\n-    next_report_ = 100;\n-  }\n-\n-  void FinishedSingleOp() {\n-    if (FLAGS_histogram) {\n-      double now = Env::Default()->NowMicros() * 1e-6;\n-      double micros = (now - last_op_finish_) * 1e6;\n-      hist_.Add(micros);\n-      if (micros > 20000) {\n-        fprintf(stderr, \"long op: %.1f micros%30s\\r\", micros, \"\");\n-        fflush(stderr);\n-      }\n-      last_op_finish_ = now;\n-    }\n-\n-    done_++;\n-    if (done_ >= next_report_) {\n-      if      (next_report_ < 1000)   next_report_ += 100;\n-      else if (next_report_ < 5000)   next_report_ += 500;\n-      else if (next_report_ < 10000)  next_report_ += 1000;\n-      else if (next_report_ < 50000)  next_report_ += 5000;\n-      else if (next_report_ < 100000) next_report_ += 10000;\n-      else if (next_report_ < 500000) next_report_ += 50000;\n-      else                            next_report_ += 100000;\n-      fprintf(stderr, \"... finished %d ops%30s\\r\", done_, \"\");\n-      fflush(stderr);\n-    }\n-  }\n-\n-  void Stop(const Slice& name) {\n-    double finish = Env::Default()->NowMicros() * 1e-6;\n-\n-    // Pretend at least one op was done in case we are running a benchmark\n-    // that does not call FinishedSingleOp().\n-    if (done_ < 1) done_ = 1;\n-\n-    if (bytes_ > 0) {\n-      char rate[100];\n-      snprintf(rate, sizeof(rate), \"%6.1f MB/s\",\n-               (bytes_ / 1048576.0) / (finish - start_));\n-      if (!message_.empty()) {\n-        message_  = std::string(rate) + \" \" + message_;\n-      } else {\n-        message_ = rate;\n-      }\n-    }\n-\n-    fprintf(stdout, \"%-12s : %11.3f micros/op;%s%s\\n\",\n-            name.ToString().c_str(),\n-            (finish - start_) * 1e6 / done_,\n-            (message_.empty() ? \"\" : \" \"),\n-            message_.c_str());\n-    if (FLAGS_histogram) {\n-      fprintf(stdout, \"Microseconds per op:\\n%s\\n\", hist_.ToString().c_str());\n-    }\n-    fflush(stdout);\n-  }\n-\n- public:\n-  enum Order {\n-    SEQUENTIAL,\n-    RANDOM\n-  };\n-  enum DBState {\n-    FRESH,\n-    EXISTING\n-  };\n-\n-  Benchmark()\n-  : db_(NULL),\n-    num_(FLAGS_num),\n-    reads_(FLAGS_reads < 0 ? FLAGS_num : FLAGS_reads),\n-    bytes_(0),\n-    rand_(301) {\n-    std::vector<std::string> files;\n-    std::string test_dir;\n-    Env::Default()->GetTestDirectory(&test_dir);\n-    Env::Default()->GetChildren(test_dir.c_str(), &files);\n-    if (!FLAGS_use_existing_db) {\n-      for (int i = 0; i < files.size(); i++) {\n-        if (Slice(files[i]).starts_with(\"dbbench_polyDB\")) {\n-          std::string file_name(test_dir);\n-          file_name += \"/\";\n-          file_name += files[i];\n-          Env::Default()->DeleteFile(file_name.c_str());\n-        }\n-      }\n-    }\n-  }\n-\n-  ~Benchmark() {\n-    if (!db_->close()) {\n-      fprintf(stderr, \"close error: %s\\n\", db_->error().name());\n-    }\n-  }\n-\n-  void Run() {\n-    PrintHeader();\n-    Open(false);\n-\n-    const char* benchmarks = FLAGS_benchmarks;\n-    while (benchmarks != NULL) {\n-      const char* sep = strchr(benchmarks, ',');\n-      Slice name;\n-      if (sep == NULL) {\n-        name = benchmarks;\n-        benchmarks = NULL;\n-      } else {\n-        name = Slice(benchmarks, sep - benchmarks);\n-        benchmarks = sep + 1;\n-      }\n-\n-      Start();\n-\n-      bool known = true;\n-      bool write_sync = false;\n-      if (name == Slice(\"fillseq\")) {\n-        Write(write_sync, SEQUENTIAL, FRESH, num_, FLAGS_value_size, 1);\n-        \n-      } else if (name == Slice(\"fillrandom\")) {\n-        Write(write_sync, RANDOM, FRESH, num_, FLAGS_value_size, 1);\n-        DBSynchronize(db_);\n-      } else if (name == Slice(\"overwrite\")) {\n-        Write(write_sync, RANDOM, EXISTING, num_, FLAGS_value_size, 1);\n-        DBSynchronize(db_);\n-      } else if (name == Slice(\"fillrandsync\")) {\n-        write_sync = true;\n-        Write(write_sync, RANDOM, FRESH, num_ / 100, FLAGS_value_size, 1);\n-        DBSynchronize(db_);\n-      } else if (name == Slice(\"fillseqsync\")) {\n-        write_sync = true;\n-        Write(write_sync, SEQUENTIAL, FRESH, num_ / 100, FLAGS_value_size, 1);\n-        DBSynchronize(db_);\n-      } else if (name == Slice(\"fillrand100K\")) {\n-        Write(write_sync, RANDOM, FRESH, num_ / 1000, 100 * 1000, 1);\n-        DBSynchronize(db_);\n-      } else if (name == Slice(\"fillseq100K\")) {\n-        Write(write_sync, SEQUENTIAL, FRESH, num_ / 1000, 100 * 1000, 1);\n-        DBSynchronize(db_);\n-      } else if (name == Slice(\"readseq\")) {\n-        ReadSequential();\n-      } else if (name == Slice(\"readrandom\")) {\n-        ReadRandom();\n-      } else if (name == Slice(\"readrand100K\")) {\n-        int n = reads_;\n-        reads_ /= 1000;\n-        ReadRandom();\n-        reads_ = n;\n-      } else if (name == Slice(\"readseq100K\")) {\n-        int n = reads_;\n-        reads_ /= 1000;\n-        ReadSequential();\n-        reads_ = n;\n-      } else {\n-        known = false;\n-        if (name != Slice()) {  // No error message for empty name\n-          fprintf(stderr, \"unknown benchmark '%s'\\n\", name.ToString().c_str());\n-        }\n-      }\n-      if (known) {\n-        Stop(name);\n-      }\n-    }\n-  }\n-\n- private:\n-    void Open(bool sync) {\n-    assert(db_ == NULL);\n-\n-    // Initialize db_\n-    db_ = new kyotocabinet::TreeDB();\n-    char file_name[100];\n-    db_num_++;\n-    std::string test_dir;\n-    Env::Default()->GetTestDirectory(&test_dir);\n-    snprintf(file_name, sizeof(file_name),\n-             \"%s/dbbench_polyDB-%d.kct\",\n-             test_dir.c_str(),\n-             db_num_);\n-\n-    // Create tuning options and open the database\n-    int open_options = kyotocabinet::PolyDB::OWRITER |\n-                       kyotocabinet::PolyDB::OCREATE;\n-    int tune_options = kyotocabinet::TreeDB::TSMALL |\n-        kyotocabinet::TreeDB::TLINEAR;\n-    if (FLAGS_compression) {\n-      tune_options |= kyotocabinet::TreeDB::TCOMPRESS;\n-      db_->tune_compressor(&comp_);\n-    }\n-    db_->tune_options(tune_options);\n-    db_->tune_page_cache(FLAGS_cache_size);\n-    db_->tune_page(FLAGS_page_size);\n-    db_->tune_map(256LL<<20);\n-    if (sync) {\n-      open_options |= kyotocabinet::PolyDB::OAUTOSYNC;\n-    }\n-    if (!db_->open(file_name, open_options)) {\n-      fprintf(stderr, \"open error: %s\\n\", db_->error().name());\n-    }\n-  }\n-\n-  void Write(bool sync, Order order, DBState state,\n-             int num_entries, int value_size, int entries_per_batch) {\n-    // Create new database if state == FRESH\n-    if (state == FRESH) {\n-      if (FLAGS_use_existing_db) {\n-        message_ = \"skipping (--use_existing_db is true)\";\n-        return;\n-      }\n-      delete db_;\n-      db_ = NULL;\n-      Open(sync);\n-      Start();  // Do not count time taken to destroy/open\n-    }\n-\n-    if (num_entries != num_) {\n-      char msg[100];\n-      snprintf(msg, sizeof(msg), \"(%d ops)\", num_entries);\n-      message_ = msg;\n-    }\n-\n-    // Write to database\n-    for (int i = 0; i < num_entries; i++)\n-    {\n-      const int k = (order == SEQUENTIAL) ? i : (rand_.Next() % num_entries);\n-      char key[100];\n-      snprintf(key, sizeof(key), \"%016d\", k);\n-      bytes_ += value_size + strlen(key);\n-      std::string cpp_key = key;\n-      if (!db_->set(cpp_key, gen_.Generate(value_size).ToString())) {\n-        fprintf(stderr, \"set error: %s\\n\", db_->error().name());\n-      }\n-      FinishedSingleOp();\n-    }\n-  }\n-\n-  void ReadSequential() {\n-    kyotocabinet::DB::Cursor* cur = db_->cursor();\n-    cur->jump();\n-    std::string ckey, cvalue;\n-    while (cur->get(&ckey, &cvalue, true)) {\n-      bytes_ += ckey.size() + cvalue.size();\n-      FinishedSingleOp();\n-    }\n-    delete cur;\n-  }\n-\n-  void ReadRandom() {\n-    std::string value;\n-    for (int i = 0; i < reads_; i++) {\n-      char key[100];\n-      const int k = rand_.Next() % reads_;\n-      snprintf(key, sizeof(key), \"%016d\", k);\n-      db_->get(key, &value);\n-      FinishedSingleOp();\n-    }\n-  }\n-};\n-\n-}  // namespace leveldb\n-\n-int main(int argc, char** argv) {\n-  std::string default_db_path;\n-  for (int i = 1; i < argc; i++) {\n-    double d;\n-    int n;\n-    char junk;\n-    if (leveldb::Slice(argv[i]).starts_with(\"--benchmarks=\")) {\n-      FLAGS_benchmarks = argv[i] + strlen(\"--benchmarks=\");\n-    } else if (sscanf(argv[i], \"--compression_ratio=%lf%c\", &d, &junk) == 1) {\n-      FLAGS_compression_ratio = d;\n-    } else if (sscanf(argv[i], \"--histogram=%d%c\", &n, &junk) == 1 &&\n-               (n == 0 || n == 1)) {\n-      FLAGS_histogram = n;\n-    } else if (sscanf(argv[i], \"--num=%d%c\", &n, &junk) == 1) {\n-      FLAGS_num = n;\n-    } else if (sscanf(argv[i], \"--reads=%d%c\", &n, &junk) == 1) {\n-      FLAGS_reads = n;\n-    } else if (sscanf(argv[i], \"--value_size=%d%c\", &n, &junk) == 1) {\n-      FLAGS_value_size = n;\n-    } else if (sscanf(argv[i], \"--cache_size=%d%c\", &n, &junk) == 1) {\n-      FLAGS_cache_size = n;\n-    } else if (sscanf(argv[i], \"--page_size=%d%c\", &n, &junk) == 1) {\n-      FLAGS_page_size = n;\n-    } else if (sscanf(argv[i], \"--compression=%d%c\", &n, &junk) == 1 &&\n-               (n == 0 || n == 1)) {\n-      FLAGS_compression = (n == 1) ? true : false;\n-    } else if (strncmp(argv[i], \"--db=\", 5) == 0) {\n-      FLAGS_db = argv[i] + 5;\n-    } else {\n-      fprintf(stderr, \"Invalid flag '%s'\\n\", argv[i]);\n-      exit(1);\n-    }\n-  }\n-\n-  // Choose a location for the test database if none given with --db=<path>\n-  if (FLAGS_db == NULL) {\n-      leveldb::Env::Default()->GetTestDirectory(&default_db_path);\n-      default_db_path += \"/dbbench\";\n-      FLAGS_db = default_db_path.c_str();\n-  }\n-\n-  leveldb::Benchmark benchmark;\n-  benchmark.Run();\n-  return 0;\n-}"
      },
      {
        "sha": "c4639772c175b463f6f41eeb0cd0c73fe7c16d68",
        "filename": "src/leveldb/doc/benchmark.html",
        "status": "removed",
        "additions": 0,
        "deletions": 459,
        "changes": 459,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/benchmark.html",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/benchmark.html",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/benchmark.html?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,459 +0,0 @@\n-<!DOCTYPE html>\n-<html>\n-<head>\n-<title>LevelDB Benchmarks</title>\n-<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n-<style>\n-body {\n-  font-family:Helvetica,sans-serif;\n-  padding:20px;\n-}\n-\n-h2 {\n-  padding-top:30px;\n-}\n-\n-table.bn {\n-  width:800px;\n-  border-collapse:collapse;\n-  border:0;\n-  padding:0;\n-}\n-\n-table.bnbase {\n-  width:650px;\n-}\n-\n-table.bn td {\n-  padding:2px 0;\n-}\n-\n-table.bn td.c1 {\n-  font-weight:bold;\n-  width:150px;\n-}\n-\n-table.bn td.c1 div.e {\n-  float:right;\n-  font-weight:normal;\n-}\n-\n-table.bn td.c2 {\n-  width:150px;\n-  text-align:right;\n-  padding:2px;\n-}\n-\n-table.bn td.c3 {\n-  width:350px;\n-}\n-\n-table.bn td.c4 {\n-  width:150px;\n-  font-size:small;\n-  padding-left:4px;\n-}\n-\n-/* chart bars */\n-div.bldb {\n-  background-color:#0255df;\n-}\n-\n-div.bkct {\n-  background-color:#df5555;\n-}\n-\n-div.bsql {\n-  background-color:#aadf55;\n-}\n-\n-.code {\n-  font-family:monospace;\n-  font-size:large;\n-}\n-\n-.todo {\n-  color: red;\n-}\n-\n-</style>\n-</head>\n-<body>\n-<h1>LevelDB Benchmarks</h1>\n-<p>Google, July 2011</p>\n-<hr>\n-\n-<p>In order to test LevelDB's performance, we benchmark it against other well-established database implementations. We compare LevelDB (revision 39) against <a href=\"http://www.sqlite.org/\">SQLite3</a> (version 3.7.6.3) and <a href=\"http://fallabs.com/kyotocabinet/spex.html\">Kyoto Cabinet's</a> (version 1.2.67) TreeDB (a B+Tree based key-value store). We would like to acknowledge Scott Hess and Mikio Hirabayashi for their suggestions and contributions to the SQLite3 and Kyoto Cabinet benchmarks, respectively.</p>\n-\n-<p>Benchmarks were all performed on a six-core Intel(R) Xeon(R) CPU X5650 @ 2.67GHz, with 12288 KB of total L3 cache and 12 GB of DDR3 RAM at 1333 MHz. (Note that LevelDB uses at most two CPUs since the benchmarks are single threaded: one to run the benchmark, and one for background compactions.) We ran the benchmarks on two machines (with identical processors), one with an Ext3 file system and one with an Ext4 file system. The machine with the Ext3 file system has a SATA Hitachi HDS721050CLA362 hard drive. The machine with the Ext4 file system has a SATA Samsung HD502HJ hard drive. Both hard drives spin at 7200 RPM and have hard drive write-caching enabled (using `hdparm -W 1 [device]`). The numbers reported below are the median of three measurements.</p>\n-\n-<h4>Benchmark Source Code</h4>\n-<p>We wrote benchmark tools for SQLite and Kyoto TreeDB based on LevelDB's <span class=\"code\">db_bench</span>. The code for each of the benchmarks resides here:</p>\n-<ul>\n-\t<li> <b>LevelDB:</b> <a href=\"http://code.google.com/p/leveldb/source/browse/trunk/db/db_bench.cc\">db/db_bench.cc</a>.</li>\n-\t<li> <b>SQLite:</b> <a href=\"http://code.google.com/p/leveldb/source/browse/#svn%2Ftrunk%2Fdoc%2Fbench%2Fdb_bench_sqlite3.cc\">doc/bench/db_bench_sqlite3.cc</a>.</li>\n-\t<li> <b>Kyoto TreeDB:</b> <a href=\"http://code.google.com/p/leveldb/source/browse/#svn%2Ftrunk%2Fdoc%2Fbench%2Fdb_bench_tree_db.cc\">doc/bench/db_bench_tree_db.cc</a>.</li>\n-</ul>\n-\n-<h4>Custom Build Specifications</h4>\n-<ul>\n-<li>LevelDB: LevelDB was compiled with the <a href=\"http://code.google.com/p/google-perftools\">tcmalloc</a> library and the <a href=\"http://code.google.com/p/snappy/\">Snappy</a> compression library (revision 33).  Assertions were disabled.</li>\n-<li>TreeDB: TreeDB was compiled using the <a href=\"http://www.oberhumer.com/opensource/lzo/\">LZO</a> compression library (version 2.03). Furthermore, we enabled the TSMALL and TLINEAR options when opening the database in order to reduce the footprint of each record.</li>\n-<li>SQLite: We tuned SQLite's performance, by setting its locking mode to exclusive.  We also enabled SQLite's <a href=\"http://www.sqlite.org/draft/wal.html\">write-ahead logging</a>.</li>\n-</ul>\n-\n-<h2>1. Baseline Performance</h2>\n-<p>This section gives the baseline performance of all the\n-databases.  Following sections show how performance changes as various\n-parameters are varied.  For the baseline:</p>\n-<ul>\n-\t<li> Each database is allowed 4 MB of cache memory.</li>\n-        <li> Databases are opened in <em>asynchronous</em> write mode.\n-             (LevelDB's sync option, TreeDB's OAUTOSYNC option, and\n-             SQLite3's synchronous options are all turned off).  I.e.,\n-             every write is pushed to the operating system, but the\n-             benchmark does not wait for the write to reach the disk.</li>\n-\t<li> Keys are 16 bytes each.</li>\n-        <li> Value are 100 bytes each (with enough redundancy so that\n-             a simple compressor shrinks them to 50% of their original\n-             size).</li>\n-\t<li> Sequential reads/writes traverse the key space in increasing order.</li>\n-\t<li> Random reads/writes traverse the key space in random order.</li>\n-</ul>\n-\n-<h3>A. Sequential Reads</h3>\n-<table class=\"bn bnbase\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">4,030,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">1,010,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:95px\">&nbsp;</div></td>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">383,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:33px\">&nbsp;</div></td>\n-</table>\n-<h3>B. Random Reads</h3>\n-<table class=\"bn bnbase\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">129,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:298px\">&nbsp;</div></td>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">151,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:350px\">&nbsp;</div></td>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">134,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:310px\">&nbsp;</div></td>\n-</table>\n-<h3>C. Sequential Writes</h3>\n-<table class=\"bn bnbase\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">779,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">342,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:154px\">&nbsp;</div></td>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">48,600 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:22px\">&nbsp;</div></td>\n-</table>\n-<h3>D. Random Writes</h3>\n-<table class=\"bn bnbase\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">164,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">88,500 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:188px\">&nbsp;</div></td>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">9,860 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:21px\">&nbsp;</div></td>\n-</table>\n-\n-<p>LevelDB outperforms both SQLite3 and TreeDB in sequential and random write operations and sequential read operations. Kyoto Cabinet has the fastest random read operations.</p>\n-\n-<h2>2. Write Performance under Different Configurations</h2>\n-<h3>A. Large Values </h3>\n-<p>For this benchmark, we start with an empty database, and write 100,000 byte values (~50% compressible). To keep the benchmark running time reasonable, we stop after writing 1000 values.</p>\n-<h4>Sequential Writes</h4>\n-<table class=\"bn bnbase\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">1,100 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:234px\">&nbsp;</div></td></tr>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">1,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:224px\">&nbsp;</div></td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">1,600 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:350px\">&nbsp;</div></td></tr>\n-</table>\n-<h4>Random Writes</h4>\n-<table class=\"bn bnbase\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">480 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:105px\">&nbsp;</div></td></tr>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">1,100 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:240px\">&nbsp;</div></td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">1,600 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:350px\">&nbsp;</div></td></tr>\n-</table>\n-<p>LevelDB doesn't perform as well with large values of 100,000 bytes each. This is because LevelDB writes keys and values at least twice: first time to the transaction log, and second time (during a compaction) to a sorted file.\n-With larger values, LevelDB's per-operation efficiency is swamped by the\n-cost of extra copies of large values.</p>\n-<h3>B. Batch Writes</h3>\n-<p>A batch write is a set of writes that are applied atomically to the underlying database. A single batch of N writes may be significantly faster than N individual writes. The following benchmark writes one thousand batches where each batch contains one thousand 100-byte values. TreeDB does not support batch writes and is omitted from this benchmark.</p>\n-<h4>Sequential Writes</h4>\n-<table class=\"bn\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">840,000 entries/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.08x baseline)</td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">124,000 entries/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:52px\">&nbsp;</div></td>\n-    <td class=\"c4\">(2.55x baseline)</td></tr>\n-</table>\n-<h4>Random Writes</h4>\n-<table class=\"bn\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">221,000 entries/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.35x baseline)</td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">22,000 entries/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:34px\">&nbsp;</div></td>\n-    <td class=\"c4\">(2.23x baseline)</td></tr>\n-</table>\n-\n-<p>Because of the way LevelDB persistent storage is organized, batches of\n-random writes are not much slower (only a factor of 4x) than batches\n-of sequential writes.</p>\n-\n-<h3>C. Synchronous Writes</h3>\n-<p>In the following benchmark, we enable the synchronous writing modes\n-of all of the databases.  Since this change significantly slows down the\n-benchmark, we stop after 10,000 writes. For synchronous write tests, we've\n-disabled hard drive write-caching (using `hdparm -W 0 [device]`).</p>\n-<ul>\n-    <li>For LevelDB, we set WriteOptions.sync = true.</li>\n-    <li>In TreeDB, we enabled TreeDB's OAUTOSYNC option.</li>\n-    <li>For SQLite3, we set \"PRAGMA synchronous = FULL\".</li>\n-</ul>\n-<h4>Sequential Writes</h4>\n-<table class=\"bn\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">100 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n-    <td class=\"c4\">(0.003x baseline)</td></tr>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">7 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:27px\">&nbsp;</div></td>\n-    <td class=\"c4\">(0.0004x baseline)</td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">88 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:315px\">&nbsp;</div></td>\n-    <td class=\"c4\">(0.002x baseline)</td></tr>\n-</table>\n-<h4>Random Writes</h4>\n-<table class=\"bn\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">100 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n-    <td class=\"c4\">(0.015x baseline)</td></tr>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">8 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:29px\">&nbsp;</div></td>\n-    <td class=\"c4\">(0.001x baseline)</td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">88 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:314px\">&nbsp;</div></td>\n-    <td class=\"c4\">(0.009x baseline)</td></tr>\n-</table>\n-\n-<p>Also see the <code>ext4</code> performance numbers below\n-since synchronous writes behave significantly differently\n-on <code>ext3</code> and <code>ext4</code>.</p>\n-\n-<h3>D. Turning Compression Off</h3>\n-\n-<p>In the baseline measurements, LevelDB and TreeDB were using\n-light-weight compression\n-(<a href=\"http://code.google.com/p/snappy/\">Snappy</a> for LevelDB,\n-and <a href=\"http://www.oberhumer.com/opensource/lzo/\">LZO</a> for\n-TreeDB). SQLite3, by default does not use compression.  The\n-experiments below show what happens when compression is disabled in\n-all of the databases (the SQLite3 numbers are just a copy of\n-its baseline measurements):</p>\n-\n-<h4>Sequential Writes</h4>\n-<table class=\"bn\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">594,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n-    <td class=\"c4\">(0.76x baseline)</td></tr>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">485,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:239px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.42x baseline)</td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">48,600 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:29px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.00x baseline)</td></tr>\n-</table>\n-<h4>Random Writes</h4>\n-<table class=\"bn\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">135,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:296px\">&nbsp;</div></td>\n-    <td class=\"c4\">(0.82x baseline)</td></tr>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">159,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:350px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.80x baseline)</td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">9,860 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:22px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.00x baseline)</td></tr>\n-</table>\n-\n-<p>LevelDB's write performance is better with compression than without\n-since compression decreases the amount of data that has to be written\n-to disk.  Therefore LevelDB users can leave compression enabled in\n-most scenarios without having worry about a tradeoff between space\n-usage and performance.  TreeDB's performance on the other hand is\n-better without compression than with compression.  Presumably this is\n-because TreeDB's compression library (LZO) is more expensive than\n-LevelDB's compression library (Snappy).<p>\n-\n-<h3>E. Using More Memory</h3>\n-<p>We increased the overall cache size for each database to 128 MB. For LevelDB, we partitioned 128 MB into a 120 MB write buffer and 8 MB of cache (up from 2 MB of write buffer and 2 MB of cache). For SQLite3, we kept the page size at 1024 bytes, but increased the number of pages to 131,072 (up from 4096). For TreeDB, we also kept the page size at 1024 bytes, but increased the cache size to 128 MB (up from 4 MB).</p>\n-<h4>Sequential Writes</h4>\n-<table class=\"bn\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">812,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.04x baseline)</td></tr>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">321,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:138px\">&nbsp;</div></td>\n-    <td class=\"c4\">(0.94x baseline)</td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">48,500 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:21px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.00x baseline)</td></tr>\n-</table>\n-<h4>Random Writes</h4>\n-<table class=\"bn\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">355,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n-    <td class=\"c4\">(2.16x baseline)</td></tr>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">284,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:280px\">&nbsp;</div></td>\n-    <td class=\"c4\">(3.21x baseline)</td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">9,670 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:10px\">&nbsp;</div></td>\n-    <td class=\"c4\">(0.98x baseline)</td></tr>\n-</table>\n-\n-<p>SQLite's performance does not change substantially when compared to\n-the baseline, but the random write performance for both LevelDB and\n-TreeDB increases significantly.  LevelDB's performance improves\n-because a larger write buffer reduces the need to merge sorted files\n-(since it creates a smaller number of larger sorted files).  TreeDB's\n-performance goes up because the entire database is available in memory\n-for fast in-place updates.</p>\n-\n-  <h2>3. Read Performance under Different Configurations</h2>\n-<h3>A. Larger Caches</h3>\n-<p>We increased the overall memory usage to 128 MB for each database.\n-For LevelDB, we allocated 8 MB to LevelDB's write buffer and 120 MB\n-to LevelDB's cache. The other databases don't differentiate between a\n-write buffer and a cache, so we simply set their cache size to 128\n-MB.</p>\n-<h4>Sequential Reads</h4>\n-<table class=\"bn\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">5,210,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.29x baseline)</td></tr>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">1,070,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:72px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.06x baseline)</td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">609,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:41px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.59x baseline)</td></tr>\n-</table>\n-\n-<h4>Random Reads</h4>\n-<table class=\"bn\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">190,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:144px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.47x baseline)</td></tr>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">463,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:350px\">&nbsp;</div></td>\n-    <td class=\"c4\">(3.07x baseline)</td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">186,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:141px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.39x baseline)</td></tr>\n-</table>\n-\n-<p>As expected, the read performance of all of the databases increases\n-when the caches are enlarged.  In particular, TreeDB seems to make\n-very effective use of a cache that is large enough to hold the entire\n-database.</p>\n-\n-<h3>B. No Compression Reads </h3>\n-<p>For this benchmark, we populated a database with 1 million entries consisting of 16 byte keys and 100 byte values. We compiled LevelDB and Kyoto Cabinet without compression support, so results that are read out from the database are already uncompressed. We've listed the SQLite3 baseline read performance as a point of comparison.</p>\n-<h4>Sequential Reads</h4>\n-<table class=\"bn\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">4,880,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.21x baseline)</td></tr>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">1,230,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:88px\">&nbsp;</div></td>\n-    <td class=\"c4\">(3.60x baseline)</td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">383,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:27px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.00x baseline)</td></tr>\n-</table>\n-<h4>Random Reads</h4>\n-<table class=\"bn\">\n-<tr><td class=\"c1\">LevelDB</td>\n-    <td class=\"c2\">149,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bldb\" style=\"width:300px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.16x baseline)</td></tr>\n-<tr><td class=\"c1\">Kyoto TreeDB</td>\n-    <td class=\"c2\">175,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bkct\" style=\"width:350px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.16x baseline)</td></tr>\n-<tr><td class=\"c1\">SQLite3</td>\n-    <td class=\"c2\">134,000 ops/sec</td>\n-    <td class=\"c3\"><div class=\"bsql\" style=\"width:268px\">&nbsp;</div></td>\n-    <td class=\"c4\">(1.00x baseline)</td></tr>\n-</table>\n-\n-<p>Performance of both LevelDB and TreeDB improves a small amount when\n-compression is disabled.  Note however that under different workloads,\n-performance may very well be better with compression if it allows more\n-of the working set to fit in memory.</p>\n-\n-<h2>Note about Ext4 Filesystems</h2>\n-<p>The preceding numbers are for an ext3 file system. Synchronous writes are much slower under <a href=\"http://en.wikipedia.org/wiki/Ext4\">ext4</a> (LevelDB drops to ~31 writes / second and TreeDB drops to ~5 writes / second; SQLite3's synchronous writes do not noticeably drop) due to ext4's different handling of <span class=\"code\">fsync</span> / <span class=\"code\">msync</span> calls. Even LevelDB's asynchronous write performance drops somewhat since it spreads its storage across multiple files and issues <span class=\"code\">fsync</span> calls when switching to a new file.</p>\n-\n-<h2>Acknowledgements</h2>\n-<p>Jeff Dean and Sanjay Ghemawat wrote LevelDB. Kevin Tseng wrote and compiled these benchmarks. Mikio Hirabayashi, Scott Hess, and Gabor Cselle provided help and advice.</p>\n-</body>\n-</html>"
      },
      {
        "sha": "700c564e433827c311cd8a26896ceaa7c8260818",
        "filename": "src/leveldb/doc/doc.css",
        "status": "removed",
        "additions": 0,
        "deletions": 89,
        "changes": 89,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/doc.css",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/doc.css",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/doc.css?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,89 +0,0 @@\n-body {\n-  margin-left: 0.5in;\n-  margin-right: 0.5in;\n-  background: white;\n-  color: black;\n-}\n-\n-h1 {\n-  margin-left: -0.2in;\n-  font-size: 14pt;\n-}\n-h2 {\n-  margin-left: -0in;\n-  font-size: 12pt;\n-}\n-h3 {\n-  margin-left: -0in;\n-}\n-h4 {\n-  margin-left: -0in;\n-}\n-hr {\n-  margin-left: -0in;\n-}\n-\n-/* Definition lists: definition term bold */\n-dt {\n-  font-weight: bold;\n-}\n-\n-address {\n-  text-align: center;\n-}\n-code,samp,var {\n-  color: blue;\n-}\n-kbd {\n-  color: #600000;\n-}\n-div.note p {\n-  float: right;\n-  width: 3in;\n-  margin-right: 0%;\n-  padding: 1px;\n-  border: 2px solid #6060a0;\n-  background-color: #fffff0;\n-}\n-\n-ul {\n-  margin-top: -0em;\n-  margin-bottom: -0em;\n-}\n-\n-ol {\n-  margin-top: -0em;\n-  margin-bottom: -0em;\n-}\n-\n-UL.nobullets {\n-  list-style-type: none;\n-  list-style-image: none;\n-  margin-left: -1em;\n-}\n-\n-p {\n-  margin: 1em 0 1em 0;\n-  padding: 0 0 0 0;\n-}\n-\n-pre {\n-  line-height: 1.3em;\n-  padding: 0.4em 0 0.8em 0;\n-  margin:  0 0 0 0;\n-  border:  0 0 0 0;\n-  color: blue;\n-}\n-\n-.datatable {\n-  margin-left: auto;\n-  margin-right: auto;\n-  margin-top: 2em;\n-  margin-bottom: 2em;\n-  border: 1px solid;\n-}\n-\n-.datatable td,th {\n-  padding: 0 0.5em 0 0.5em;\n-  text-align: right;\n-}"
      },
      {
        "sha": "e870795d231463b167d5b79efdc16b80107de93e",
        "filename": "src/leveldb/doc/impl.html",
        "status": "removed",
        "additions": 0,
        "deletions": 213,
        "changes": 213,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/impl.html",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/impl.html",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/impl.html?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,213 +0,0 @@\n-<!DOCTYPE html>\n-<html>\n-<head>\n-<link rel=\"stylesheet\" type=\"text/css\" href=\"doc.css\" />\n-<title>Leveldb file layout and compactions</title>\n-</head>\n-\n-<body>\n-\n-<h1>Files</h1>\n-\n-The implementation of leveldb is similar in spirit to the\n-representation of a single\n-<a href=\"http://labs.google.com/papers/bigtable.html\">\n-Bigtable tablet (section 5.3)</a>.\n-However the organization of the files that make up the representation\n-is somewhat different and is explained below.\n-\n-<p>\n-Each database is represented by a set of files stored in a directory.\n-There are several different types of files as documented below:\n-<p>\n-<h2>Log files</h2>\n-<p>\n-A log file (*.log) stores a sequence of recent updates.  Each update\n-is appended to the current log file.  When the log file reaches a\n-pre-determined size (approximately 4MB by default), it is converted\n-to a sorted table (see below) and a new log file is created for future\n-updates.\n-<p>\n-A copy of the current log file is kept in an in-memory structure (the\n-<code>memtable</code>).  This copy is consulted on every read so that read\n-operations reflect all logged updates.\n-<p>\n-<h2>Sorted tables</h2>\n-<p>\n-A sorted table (*.sst) stores a sequence of entries sorted by key.\n-Each entry is either a value for the key, or a deletion marker for the\n-key.  (Deletion markers are kept around to hide obsolete values\n-present in older sorted tables).\n-<p>\n-The set of sorted tables are organized into a sequence of levels.  The\n-sorted table generated from a log file is placed in a special <code>young</code>\n-level (also called level-0).  When the number of young files exceeds a\n-certain threshold (currently four), all of the young files are merged\n-together with all of the overlapping level-1 files to produce a\n-sequence of new level-1 files (we create a new level-1 file for every\n-2MB of data.)\n-<p>\n-Files in the young level may contain overlapping keys.  However files\n-in other levels have distinct non-overlapping key ranges.  Consider\n-level number L where L >= 1.  When the combined size of files in\n-level-L exceeds (10^L) MB (i.e., 10MB for level-1, 100MB for level-2,\n-...), one file in level-L, and all of the overlapping files in\n-level-(L+1) are merged to form a set of new files for level-(L+1).\n-These merges have the effect of gradually migrating new updates from\n-the young level to the largest level using only bulk reads and writes\n-(i.e., minimizing expensive seeks).\n-\n-<h2>Manifest</h2>\n-<p>\n-A MANIFEST file lists the set of sorted tables that make up each\n-level, the corresponding key ranges, and other important metadata.\n-A new MANIFEST file (with a new number embedded in the file name)\n-is created whenever the database is reopened.  The MANIFEST file is\n-formatted as a log, and changes made to the serving state (as files\n-are added or removed) are appended to this log.\n-<p>\n-<h2>Current</h2>\n-<p>\n-CURRENT is a simple text file that contains the name of the latest\n-MANIFEST file.\n-<p>\n-<h2>Info logs</h2>\n-<p>\n-Informational messages are printed to files named LOG and LOG.old.\n-<p>\n-<h2>Others</h2>\n-<p>\n-Other files used for miscellaneous purposes may also be present\n-(LOCK, *.dbtmp).\n-\n-<h1>Level 0</h1>\n-When the log file grows above a certain size (1MB by default):\n-<ul>\n-<li>Create a brand new memtable and log file and direct future updates here\n-<li>In the background:\n-<ul>\n-<li>Write the contents of the previous memtable to an sstable\n-<li>Discard the memtable\n-<li>Delete the old log file and the old memtable\n-<li>Add the new sstable to the young (level-0) level.\n-</ul>\n-</ul>\n-\n-<h1>Compactions</h1>\n-\n-<p>\n-When the size of level L exceeds its limit, we compact it in a\n-background thread.  The compaction picks a file from level L and all\n-overlapping files from the next level L+1.  Note that if a level-L\n-file overlaps only part of a level-(L+1) file, the entire file at\n-level-(L+1) is used as an input to the compaction and will be\n-discarded after the compaction.  Aside: because level-0 is special\n-(files in it may overlap each other), we treat compactions from\n-level-0 to level-1 specially: a level-0 compaction may pick more than\n-one level-0 file in case some of these files overlap each other.\n-\n-<p>\n-A compaction merges the contents of the picked files to produce a\n-sequence of level-(L+1) files.  We switch to producing a new\n-level-(L+1) file after the current output file has reached the target\n-file size (2MB).  We also switch to a new output file when the key\n-range of the current output file has grown enough to overlap more then\n-ten level-(L+2) files.  This last rule ensures that a later compaction\n-of a level-(L+1) file will not pick up too much data from level-(L+2).\n-\n-<p>\n-The old files are discarded and the new files are added to the serving\n-state.\n-\n-<p>\n-Compactions for a particular level rotate through the key space.  In\n-more detail, for each level L, we remember the ending key of the last\n-compaction at level L.  The next compaction for level L will pick the\n-first file that starts after this key (wrapping around to the\n-beginning of the key space if there is no such file).\n-\n-<p>\n-Compactions drop overwritten values.  They also drop deletion markers\n-if there are no higher numbered levels that contain a file whose range\n-overlaps the current key.\n-\n-<h2>Timing</h2>\n-\n-Level-0 compactions will read up to four 1MB files from level-0, and\n-at worst all the level-1 files (10MB).  I.e., we will read 14MB and\n-write 14MB.\n-\n-<p>\n-Other than the special level-0 compactions, we will pick one 2MB file\n-from level L.  In the worst case, this will overlap ~ 12 files from\n-level L+1 (10 because level-(L+1) is ten times the size of level-L,\n-and another two at the boundaries since the file ranges at level-L\n-will usually not be aligned with the file ranges at level-L+1).  The\n-compaction will therefore read 26MB and write 26MB.  Assuming a disk\n-IO rate of 100MB/s (ballpark range for modern drives), the worst\n-compaction cost will be approximately 0.5 second.\n-\n-<p>\n-If we throttle the background writing to something small, say 10% of\n-the full 100MB/s speed, a compaction may take up to 5 seconds.  If the\n-user is writing at 10MB/s, we might build up lots of level-0 files\n-(~50 to hold the 5*10MB).  This may signficantly increase the cost of\n-reads due to the overhead of merging more files together on every\n-read.\n-\n-<p>\n-Solution 1: To reduce this problem, we might want to increase the log\n-switching threshold when the number of level-0 files is large.  Though\n-the downside is that the larger this threshold, the more memory we will\n-need to hold the corresponding memtable.\n-\n-<p>\n-Solution 2: We might want to decrease write rate artificially when the\n-number of level-0 files goes up.\n-\n-<p>\n-Solution 3: We work on reducing the cost of very wide merges.\n-Perhaps most of the level-0 files will have their blocks sitting\n-uncompressed in the cache and we will only need to worry about the\n-O(N) complexity in the merging iterator.\n-\n-<h2>Number of files</h2>\n-\n-Instead of always making 2MB files, we could make larger files for\n-larger levels to reduce the total file count, though at the expense of\n-more bursty compactions.  Alternatively, we could shard the set of\n-files into multiple directories.\n-\n-<p>\n-An experiment on an <code>ext3</code> filesystem on Feb 04, 2011 shows\n-the following timings to do 100K file opens in directories with\n-varying number of files:\n-<table class=\"datatable\">\n-<tr><th>Files in directory</th><th>Microseconds to open a file</th></tr>\n-<tr><td>1000</td><td>9</td>\n-<tr><td>10000</td><td>10</td>\n-<tr><td>100000</td><td>16</td>\n-</table>\n-So maybe even the sharding is not necessary on modern filesystems?\n-\n-<h1>Recovery</h1>\n-\n-<ul>\n-<li> Read CURRENT to find name of the latest committed MANIFEST\n-<li> Read the named MANIFEST file\n-<li> Clean up stale files\n-<li> We could open all sstables here, but it is probably better to be lazy...\n-<li> Convert log chunk to a new level-0 sstable\n-<li> Start directing new writes to a new log file with recovered sequence#\n-</ul>\n-\n-<h1>Garbage collection of files</h1>\n-\n-<code>DeleteObsoleteFiles()</code> is called at the end of every\n-compaction and at the end of recovery.  It finds the names of all\n-files in the database.  It deletes all log files that are not the\n-current log file.  It deletes all table files that are not referenced\n-from some level and are not the output of an active compaction.\n-\n-</body>\n-</html>"
      },
      {
        "sha": "3ed0ed9d9e305ed630f39a201e3034e00d815b43",
        "filename": "src/leveldb/doc/index.html",
        "status": "removed",
        "additions": 0,
        "deletions": 549,
        "changes": 549,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/index.html",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/index.html",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/index.html?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,549 +0,0 @@\n-<!DOCTYPE html>\n-<html>\n-<head>\n-<link rel=\"stylesheet\" type=\"text/css\" href=\"doc.css\" />\n-<title>Leveldb</title>\n-</head>\n-\n-<body>\n-<h1>Leveldb</h1>\n-<address>Jeff Dean, Sanjay Ghemawat</address>\n-<p>\n-The <code>leveldb</code> library provides a persistent key value store.  Keys and\n-values are arbitrary byte arrays.  The keys are ordered within the key\n-value store according to a user-specified comparator function.\n-\n-<p>\n-<h1>Opening A Database</h1>\n-<p>\n-A <code>leveldb</code> database has a name which corresponds to a file system\n-directory.  All of the contents of database are stored in this\n-directory.  The following example shows how to open a database,\n-creating it if necessary:\n-<p>\n-<pre>\n-  #include &lt;assert&gt;\n-  #include \"leveldb/db.h\"\n-\n-  leveldb::DB* db;\n-  leveldb::Options options;\n-  options.create_if_missing = true;\n-  leveldb::Status status = leveldb::DB::Open(options, \"/tmp/testdb\", &amp;db);\n-  assert(status.ok());\n-  ...\n-</pre>\n-If you want to raise an error if the database already exists, add\n-the following line before the <code>leveldb::DB::Open</code> call:\n-<pre>\n-  options.error_if_exists = true;\n-</pre>\n-<h1>Status</h1>\n-<p>\n-You may have noticed the <code>leveldb::Status</code> type above.  Values of this\n-type are returned by most functions in <code>leveldb</code> that may encounter an\n-error.  You can check if such a result is ok, and also print an\n-associated error message:\n-<p>\n-<pre>\n-   leveldb::Status s = ...;\n-   if (!s.ok()) cerr &lt;&lt; s.ToString() &lt;&lt; endl;\n-</pre>\n-<h1>Closing A Database</h1>\n-<p>\n-When you are done with a database, just delete the database object.\n-Example:\n-<p>\n-<pre>\n-  ... open the db as described above ...\n-  ... do something with db ...\n-  delete db;\n-</pre>\n-<h1>Reads And Writes</h1>\n-<p>\n-The database provides <code>Put</code>, <code>Delete</code>, and <code>Get</code> methods to\n-modify/query the database.  For example, the following code\n-moves the value stored under key1 to key2.\n-<pre>\n-  std::string value;\n-  leveldb::Status s = db-&gt;Get(leveldb::ReadOptions(), key1, &amp;value);\n-  if (s.ok()) s = db-&gt;Put(leveldb::WriteOptions(), key2, value);\n-  if (s.ok()) s = db-&gt;Delete(leveldb::WriteOptions(), key1);\n-</pre>\n-\n-<h1>Atomic Updates</h1>\n-<p>\n-Note that if the process dies after the Put of key2 but before the\n-delete of key1, the same value may be left stored under multiple keys.\n-Such problems can be avoided by using the <code>WriteBatch</code> class to\n-atomically apply a set of updates:\n-<p>\n-<pre>\n-  #include \"leveldb/write_batch.h\"\n-  ...\n-  std::string value;\n-  leveldb::Status s = db-&gt;Get(leveldb::ReadOptions(), key1, &amp;value);\n-  if (s.ok()) {\n-    leveldb::WriteBatch batch;\n-    batch.Delete(key1);\n-    batch.Put(key2, value);\n-    s = db-&gt;Write(leveldb::WriteOptions(), &amp;batch);\n-  }\n-</pre>\n-The <code>WriteBatch</code> holds a sequence of edits to be made to the database,\n-and these edits within the batch are applied in order.  Note that we\n-called <code>Delete</code> before <code>Put</code> so that if <code>key1</code> is identical to <code>key2</code>,\n-we do not end up erroneously dropping the value entirely.\n-<p>\n-Apart from its atomicity benefits, <code>WriteBatch</code> may also be used to\n-speed up bulk updates by placing lots of individual mutations into the\n-same batch.\n-\n-<h1>Synchronous Writes</h1>\n-By default, each write to <code>leveldb</code> is asynchronous: it\n-returns after pushing the write from the process into the operating\n-system.  The transfer from operating system memory to the underlying\n-persistent storage happens asynchronously.  The <code>sync</code> flag\n-can be turned on for a particular write to make the write operation\n-not return until the data being written has been pushed all the way to\n-persistent storage.  (On Posix systems, this is implemented by calling\n-either <code>fsync(...)</code> or <code>fdatasync(...)</code> or\n-<code>msync(..., MS_SYNC)</code> before the write operation returns.)\n-<pre>\n-  leveldb::WriteOptions write_options;\n-  write_options.sync = true;\n-  db-&gt;Put(write_options, ...);\n-</pre>\n-Asynchronous writes are often more than a thousand times as fast as\n-synchronous writes.  The downside of asynchronous writes is that a\n-crash of the machine may cause the last few updates to be lost.  Note\n-that a crash of just the writing process (i.e., not a reboot) will not\n-cause any loss since even when <code>sync</code> is false, an update\n-is pushed from the process memory into the operating system before it\n-is considered done.\n-\n-<p>\n-Asynchronous writes can often be used safely.  For example, when\n-loading a large amount of data into the database you can handle lost\n-updates by restarting the bulk load after a crash.  A hybrid scheme is\n-also possible where every Nth write is synchronous, and in the event\n-of a crash, the bulk load is restarted just after the last synchronous\n-write finished by the previous run.  (The synchronous write can update\n-a marker that describes where to restart on a crash.)\n-\n-<p>\n-<code>WriteBatch</code> provides an alternative to asynchronous writes.\n-Multiple updates may be placed in the same <code>WriteBatch</code> and\n-applied together using a synchronous write (i.e.,\n-<code>write_options.sync</code> is set to true).  The extra cost of\n-the synchronous write will be amortized across all of the writes in\n-the batch.\n-\n-<p>\n-<h1>Concurrency</h1>\n-<p>\n-A database may only be opened by one process at a time.\n-The <code>leveldb</code> implementation acquires a lock from the\n-operating system to prevent misuse.  Within a single process, the\n-same <code>leveldb::DB</code> object may be safely shared by multiple\n-concurrent threads.  I.e., different threads may write into or fetch\n-iterators or call <code>Get</code> on the same database without any\n-external synchronization (the leveldb implementation will\n-automatically do the required synchronization).  However other objects\n-(like Iterator and WriteBatch) may require external synchronization.\n-If two threads share such an object, they must protect access to it\n-using their own locking protocol.  More details are available in\n-the public header files.\n-<p>\n-<h1>Iteration</h1>\n-<p>\n-The following example demonstrates how to print all key,value pairs\n-in a database.\n-<p>\n-<pre>\n-  leveldb::Iterator* it = db-&gt;NewIterator(leveldb::ReadOptions());\n-  for (it-&gt;SeekToFirst(); it-&gt;Valid(); it-&gt;Next()) {\n-    cout &lt;&lt; it-&gt;key().ToString() &lt;&lt; \": \"  &lt;&lt; it-&gt;value().ToString() &lt;&lt; endl;\n-  }\n-  assert(it-&gt;status().ok());  // Check for any errors found during the scan\n-  delete it;\n-</pre>\n-The following variation shows how to process just the keys in the\n-range <code>[start,limit)</code>:\n-<p>\n-<pre>\n-  for (it-&gt;Seek(start);\n-       it-&gt;Valid() &amp;&amp; it-&gt;key().ToString() &lt; limit;\n-       it-&gt;Next()) {\n-    ...\n-  }\n-</pre>\n-You can also process entries in reverse order.  (Caveat: reverse\n-iteration may be somewhat slower than forward iteration.)\n-<p>\n-<pre>\n-  for (it-&gt;SeekToLast(); it-&gt;Valid(); it-&gt;Prev()) {\n-    ...\n-  }\n-</pre>\n-<h1>Snapshots</h1>\n-<p>\n-Snapshots provide consistent read-only views over the entire state of\n-the key-value store.  <code>ReadOptions::snapshot</code> may be non-NULL to indicate\n-that a read should operate on a particular version of the DB state.\n-If <code>ReadOptions::snapshot</code> is NULL, the read will operate on an\n-implicit snapshot of the current state.\n-<p>\n-Snapshots are created by the DB::GetSnapshot() method:\n-<p>\n-<pre>\n-  leveldb::ReadOptions options;\n-  options.snapshot = db-&gt;GetSnapshot();\n-  ... apply some updates to db ...\n-  leveldb::Iterator* iter = db-&gt;NewIterator(options);\n-  ... read using iter to view the state when the snapshot was created ...\n-  delete iter;\n-  db-&gt;ReleaseSnapshot(options.snapshot);\n-</pre>\n-Note that when a snapshot is no longer needed, it should be released\n-using the DB::ReleaseSnapshot interface.  This allows the\n-implementation to get rid of state that was being maintained just to\n-support reading as of that snapshot.\n-<h1>Slice</h1>\n-<p>\n-The return value of the <code>it->key()</code> and <code>it->value()</code> calls above\n-are instances of the <code>leveldb::Slice</code> type.  <code>Slice</code> is a simple\n-structure that contains a length and a pointer to an external byte\n-array.  Returning a <code>Slice</code> is a cheaper alternative to returning a\n-<code>std::string</code> since we do not need to copy potentially large keys and\n-values.  In addition, <code>leveldb</code> methods do not return null-terminated\n-C-style strings since <code>leveldb</code> keys and values are allowed to\n-contain '\\0' bytes.\n-<p>\n-C++ strings and null-terminated C-style strings can be easily converted\n-to a Slice:\n-<p>\n-<pre>\n-   leveldb::Slice s1 = \"hello\";\n-\n-   std::string str(\"world\");\n-   leveldb::Slice s2 = str;\n-</pre>\n-A Slice can be easily converted back to a C++ string:\n-<pre>\n-   std::string str = s1.ToString();\n-   assert(str == std::string(\"hello\"));\n-</pre>\n-Be careful when using Slices since it is up to the caller to ensure that\n-the external byte array into which the Slice points remains live while\n-the Slice is in use.  For example, the following is buggy:\n-<p>\n-<pre>\n-   leveldb::Slice slice;\n-   if (...) {\n-     std::string str = ...;\n-     slice = str;\n-   }\n-   Use(slice);\n-</pre>\n-When the <code>if</code> statement goes out of scope, <code>str</code> will be destroyed and the\n-backing storage for <code>slice</code> will disappear.\n-<p>\n-<h1>Comparators</h1>\n-<p>\n-The preceding examples used the default ordering function for key,\n-which orders bytes lexicographically.  You can however supply a custom\n-comparator when opening a database.  For example, suppose each\n-database key consists of two numbers and we should sort by the first\n-number, breaking ties by the second number.  First, define a proper\n-subclass of <code>leveldb::Comparator</code> that expresses these rules:\n-<p>\n-<pre>\n-  class TwoPartComparator : public leveldb::Comparator {\n-   public:\n-    // Three-way comparison function:\n-    //   if a &lt; b: negative result\n-    //   if a &gt; b: positive result\n-    //   else: zero result\n-    int Compare(const leveldb::Slice&amp; a, const leveldb::Slice&amp; b) const {\n-      int a1, a2, b1, b2;\n-      ParseKey(a, &amp;a1, &amp;a2);\n-      ParseKey(b, &amp;b1, &amp;b2);\n-      if (a1 &lt; b1) return -1;\n-      if (a1 &gt; b1) return +1;\n-      if (a2 &lt; b2) return -1;\n-      if (a2 &gt; b2) return +1;\n-      return 0;\n-    }\n-\n-    // Ignore the following methods for now:\n-    const char* Name() const { return \"TwoPartComparator\"; }\n-    void FindShortestSeparator(std::string*, const leveldb::Slice&amp;) const { }\n-    void FindShortSuccessor(std::string*) const { }\n-  };\n-</pre>\n-Now create a database using this custom comparator:\n-<p>\n-<pre>\n-  TwoPartComparator cmp;\n-  leveldb::DB* db;\n-  leveldb::Options options;\n-  options.create_if_missing = true;\n-  options.comparator = &amp;cmp;\n-  leveldb::Status status = leveldb::DB::Open(options, \"/tmp/testdb\", &amp;db);\n-  ...\n-</pre>\n-<h2>Backwards compatibility</h2>\n-<p>\n-The result of the comparator's <code>Name</code> method is attached to the\n-database when it is created, and is checked on every subsequent\n-database open.  If the name changes, the <code>leveldb::DB::Open</code> call will\n-fail.  Therefore, change the name if and only if the new key format\n-and comparison function are incompatible with existing databases, and\n-it is ok to discard the contents of all existing databases.\n-<p>\n-You can however still gradually evolve your key format over time with\n-a little bit of pre-planning.  For example, you could store a version\n-number at the end of each key (one byte should suffice for most uses).\n-When you wish to switch to a new key format (e.g., adding an optional\n-third part to the keys processed by <code>TwoPartComparator</code>),\n-(a) keep the same comparator name (b) increment the version number\n-for new keys (c) change the comparator function so it uses the\n-version numbers found in the keys to decide how to interpret them.\n-<p>\n-<h1>Performance</h1>\n-<p>\n-Performance can be tuned by changing the default values of the\n-types defined in <code>include/leveldb/options.h</code>.\n-\n-<p>\n-<h2>Block size</h2>\n-<p>\n-<code>leveldb</code> groups adjacent keys together into the same block and such a\n-block is the unit of transfer to and from persistent storage.  The\n-default block size is approximately 4096 uncompressed bytes.\n-Applications that mostly do bulk scans over the contents of the\n-database may wish to increase this size.  Applications that do a lot\n-of point reads of small values may wish to switch to a smaller block\n-size if performance measurements indicate an improvement.  There isn't\n-much benefit in using blocks smaller than one kilobyte, or larger than\n-a few megabytes.  Also note that compression will be more effective\n-with larger block sizes.\n-<p>\n-<h2>Compression</h2>\n-<p>\n-Each block is individually compressed before being written to\n-persistent storage.  Compression is on by default since the default\n-compression method is very fast, and is automatically disabled for\n-uncompressible data.  In rare cases, applications may want to disable\n-compression entirely, but should only do so if benchmarks show a\n-performance improvement:\n-<p>\n-<pre>\n-  leveldb::Options options;\n-  options.compression = leveldb::kNoCompression;\n-  ... leveldb::DB::Open(options, name, ...) ....\n-</pre>\n-<h2>Cache</h2>\n-<p>\n-The contents of the database are stored in a set of files in the\n-filesystem and each file stores a sequence of compressed blocks.  If\n-<code>options.cache</code> is non-NULL, it is used to cache frequently used\n-uncompressed block contents.\n-<p>\n-<pre>\n-  #include \"leveldb/cache.h\"\n-\n-  leveldb::Options options;\n-  options.cache = leveldb::NewLRUCache(100 * 1048576);  // 100MB cache\n-  leveldb::DB* db;\n-  leveldb::DB::Open(options, name, &db);\n-  ... use the db ...\n-  delete db\n-  delete options.cache;\n-</pre>\n-Note that the cache holds uncompressed data, and therefore it should\n-be sized according to application level data sizes, without any\n-reduction from compression.  (Caching of compressed blocks is left to\n-the operating system buffer cache, or any custom <code>Env</code>\n-implementation provided by the client.)\n-<p>\n-When performing a bulk read, the application may wish to disable\n-caching so that the data processed by the bulk read does not end up\n-displacing most of the cached contents.  A per-iterator option can be\n-used to achieve this:\n-<p>\n-<pre>\n-  leveldb::ReadOptions options;\n-  options.fill_cache = false;\n-  leveldb::Iterator* it = db-&gt;NewIterator(options);\n-  for (it-&gt;SeekToFirst(); it-&gt;Valid(); it-&gt;Next()) {\n-    ...\n-  }\n-</pre>\n-<h2>Key Layout</h2>\n-<p>\n-Note that the unit of disk transfer and caching is a block.  Adjacent\n-keys (according to the database sort order) will usually be placed in\n-the same block.  Therefore the application can improve its performance\n-by placing keys that are accessed together near each other and placing\n-infrequently used keys in a separate region of the key space.\n-<p>\n-For example, suppose we are implementing a simple file system on top\n-of <code>leveldb</code>.  The types of entries we might wish to store are:\n-<p>\n-<pre>\n-   filename -&gt; permission-bits, length, list of file_block_ids\n-   file_block_id -&gt; data\n-</pre>\n-We might want to prefix <code>filename</code> keys with one letter (say '/') and the\n-<code>file_block_id</code> keys with a different letter (say '0') so that scans\n-over just the metadata do not force us to fetch and cache bulky file\n-contents.\n-<p>\n-<h2>Filters</h2>\n-<p>\n-Because of the way <code>leveldb</code> data is organized on disk,\n-a single <code>Get()</code> call may involve multiple reads from disk.\n-The optional <code>FilterPolicy</code> mechanism can be used to reduce\n-the number of disk reads substantially.\n-<pre>\n-   leveldb::Options options;\n-   options.filter_policy = NewBloomFilterPolicy(10);\n-   leveldb::DB* db;\n-   leveldb::DB::Open(options, \"/tmp/testdb\", &amp;db);\n-   ... use the database ...\n-   delete db;\n-   delete options.filter_policy;\n-</pre>\n-The preceding code associates a\n-<a href=\"http://en.wikipedia.org/wiki/Bloom_filter\">Bloom filter</a>\n-based filtering policy with the database.  Bloom filter based\n-filtering relies on keeping some number of bits of data in memory per\n-key (in this case 10 bits per key since that is the argument we passed\n-to NewBloomFilterPolicy).  This filter will reduce the number of unnecessary\n-disk reads needed for <code>Get()</code> calls by a factor of\n-approximately a 100.  Increasing the bits per key will lead to a\n-larger reduction at the cost of more memory usage.  We recommend that\n-applications whose working set does not fit in memory and that do a\n-lot of random reads set a filter policy.\n-<p>\n-If you are using a custom comparator, you should ensure that the filter\n-policy you are using is compatible with your comparator.  For example,\n-consider a comparator that ignores trailing spaces when comparing keys.\n-<code>NewBloomFilterPolicy</code> must not be used with such a comparator.\n-Instead, the application should provide a custom filter policy that\n-also ignores trailing spaces.  For example:\n-<pre>\n-  class CustomFilterPolicy : public leveldb::FilterPolicy {\n-   private:\n-    FilterPolicy* builtin_policy_;\n-   public:\n-    CustomFilterPolicy() : builtin_policy_(NewBloomFilterPolicy(10)) { }\n-    ~CustomFilterPolicy() { delete builtin_policy_; }\n-\n-    const char* Name() const { return \"IgnoreTrailingSpacesFilter\"; }\n-\n-    void CreateFilter(const Slice* keys, int n, std::string* dst) const {\n-      // Use builtin bloom filter code after removing trailing spaces\n-      std::vector&lt;Slice&gt; trimmed(n);\n-      for (int i = 0; i &lt; n; i++) {\n-        trimmed[i] = RemoveTrailingSpaces(keys[i]);\n-      }\n-      return builtin_policy_-&gt;CreateFilter(&amp;trimmed[i], n, dst);\n-    }\n-\n-    bool KeyMayMatch(const Slice& key, const Slice& filter) const {\n-      // Use builtin bloom filter code after removing trailing spaces\n-      return builtin_policy_-&gt;KeyMayMatch(RemoveTrailingSpaces(key), filter);\n-    }\n-  };\n-</pre>\n-<p>\n-Advanced applications may provide a filter policy that does not use\n-a bloom filter but uses some other mechanism for summarizing a set\n-of keys.  See <code>leveldb/filter_policy.h</code> for detail.\n-<p>\n-<h1>Checksums</h1>\n-<p>\n-<code>leveldb</code> associates checksums with all data it stores in the file system.\n-There are two separate controls provided over how aggressively these\n-checksums are verified:\n-<p>\n-<ul>\n-<li> <code>ReadOptions::verify_checksums</code> may be set to true to force\n-  checksum verification of all data that is read from the file system on\n-  behalf of a particular read.  By default, no such verification is\n-  done.\n-<p>\n-<li> <code>Options::paranoid_checks</code> may be set to true before opening a\n-  database to make the database implementation raise an error as soon as\n-  it detects an internal corruption.  Depending on which portion of the\n-  database has been corrupted, the error may be raised when the database\n-  is opened, or later by another database operation.  By default,\n-  paranoid checking is off so that the database can be used even if\n-  parts of its persistent storage have been corrupted.\n-<p>\n-  If a database is corrupted (perhaps it cannot be opened when\n-  paranoid checking is turned on), the <code>leveldb::RepairDB</code> function\n-  may be used to recover as much of the data as possible\n-<p>\n-</ul>\n-<h1>Approximate Sizes</h1>\n-<p>\n-The <code>GetApproximateSizes</code> method can used to get the approximate\n-number of bytes of file system space used by one or more key ranges.\n-<p>\n-<pre>\n-   leveldb::Range ranges[2];\n-   ranges[0] = leveldb::Range(\"a\", \"c\");\n-   ranges[1] = leveldb::Range(\"x\", \"z\");\n-   uint64_t sizes[2];\n-   leveldb::Status s = db-&gt;GetApproximateSizes(ranges, 2, sizes);\n-</pre>\n-The preceding call will set <code>sizes[0]</code> to the approximate number of\n-bytes of file system space used by the key range <code>[a..c)</code> and\n-<code>sizes[1]</code> to the approximate number of bytes used by the key range\n-<code>[x..z)</code>.\n-<p>\n-<h1>Environment</h1>\n-<p>\n-All file operations (and other operating system calls) issued by the\n-<code>leveldb</code> implementation are routed through a <code>leveldb::Env</code> object.\n-Sophisticated clients may wish to provide their own <code>Env</code>\n-implementation to get better control.  For example, an application may\n-introduce artificial delays in the file IO paths to limit the impact\n-of <code>leveldb</code> on other activities in the system.\n-<p>\n-<pre>\n-  class SlowEnv : public leveldb::Env {\n-    .. implementation of the Env interface ...\n-  };\n-\n-  SlowEnv env;\n-  leveldb::Options options;\n-  options.env = &amp;env;\n-  Status s = leveldb::DB::Open(options, ...);\n-</pre>\n-<h1>Porting</h1>\n-<p>\n-<code>leveldb</code> may be ported to a new platform by providing platform\n-specific implementations of the types/methods/functions exported by\n-<code>leveldb/port/port.h</code>.  See <code>leveldb/port/port_example.h</code> for more\n-details.\n-<p>\n-In addition, the new platform may need a new default <code>leveldb::Env</code>\n-implementation.  See <code>leveldb/util/env_posix.h</code> for an example.\n-\n-<h1>Other Information</h1>\n-\n-<p>\n-Details about the <code>leveldb</code> implementation may be found in\n-the following documents:\n-<ul>\n-<li> <a href=\"impl.html\">Implementation notes</a>\n-<li> <a href=\"table_format.txt\">Format of an immutable Table file</a>\n-<li> <a href=\"log_format.txt\">Format of a log file</a>\n-</ul>\n-\n-</body>\n-</html>"
      },
      {
        "sha": "5228f624de4347c114b87520c12a1ae39425d411",
        "filename": "src/leveldb/doc/log_format.txt",
        "status": "removed",
        "additions": 0,
        "deletions": 75,
        "changes": 75,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/log_format.txt",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/log_format.txt",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/log_format.txt?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,75 +0,0 @@\n-The log file contents are a sequence of 32KB blocks.  The only\n-exception is that the tail of the file may contain a partial block.\n-\n-Each block consists of a sequence of records:\n-   block := record* trailer?\n-   record :=\n-\tchecksum: uint32\t// crc32c of type and data[] ; little-endian\n-\tlength: uint16\t\t// little-endian\n-\ttype: uint8\t\t// One of FULL, FIRST, MIDDLE, LAST\n-\tdata: uint8[length]\n-\n-A record never starts within the last six bytes of a block (since it\n-won't fit).  Any leftover bytes here form the trailer, which must\n-consist entirely of zero bytes and must be skipped by readers.  \n-\n-Aside: if exactly seven bytes are left in the current block, and a new\n-non-zero length record is added, the writer must emit a FIRST record\n-(which contains zero bytes of user data) to fill up the trailing seven\n-bytes of the block and then emit all of the user data in subsequent\n-blocks.\n-\n-More types may be added in the future.  Some Readers may skip record\n-types they do not understand, others may report that some data was\n-skipped.\n-\n-FULL == 1\n-FIRST == 2\n-MIDDLE == 3\n-LAST == 4\n-\n-The FULL record contains the contents of an entire user record.\n-\n-FIRST, MIDDLE, LAST are types used for user records that have been\n-split into multiple fragments (typically because of block boundaries).\n-FIRST is the type of the first fragment of a user record, LAST is the\n-type of the last fragment of a user record, and MID is the type of all\n-interior fragments of a user record.\n-\n-Example: consider a sequence of user records:\n-   A: length 1000\n-   B: length 97270\n-   C: length 8000\n-A will be stored as a FULL record in the first block.\n-\n-B will be split into three fragments: first fragment occupies the rest\n-of the first block, second fragment occupies the entirety of the\n-second block, and the third fragment occupies a prefix of the third\n-block.  This will leave six bytes free in the third block, which will\n-be left empty as the trailer.\n-\n-C will be stored as a FULL record in the fourth block.\n-\n-===================\n-\n-Some benefits over the recordio format:\n-\n-(1) We do not need any heuristics for resyncing - just go to next\n-block boundary and scan.  If there is a corruption, skip to the next\n-block.  As a side-benefit, we do not get confused when part of the\n-contents of one log file are embedded as a record inside another log\n-file.\n-\n-(2) Splitting at approximate boundaries (e.g., for mapreduce) is\n-simple: find the next block boundary and skip records until we\n-hit a FULL or FIRST record.\n-\n-(3) We do not need extra buffering for large records.\n-\n-Some downsides compared to recordio format:\n-\n-(1) No packing of tiny records.  This could be fixed by adding a new\n-record type, so it is a shortcoming of the current implementation,\n-not necessarily the format.\n-\n-(2) No compression.  Again, this could be fixed by adding new record types."
      },
      {
        "sha": "ca8f9b4460ad85d9e09f14a959ed47bd2812edb5",
        "filename": "src/leveldb/doc/table_format.txt",
        "status": "removed",
        "additions": 0,
        "deletions": 104,
        "changes": 104,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/table_format.txt",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/doc/table_format.txt",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/table_format.txt?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,104 +0,0 @@\n-File format\n-===========\n-\n-  <beginning_of_file>\n-  [data block 1]\n-  [data block 2]\n-  ...\n-  [data block N]\n-  [meta block 1]\n-  ...\n-  [meta block K]\n-  [metaindex block]\n-  [index block]\n-  [Footer]        (fixed size; starts at file_size - sizeof(Footer))\n-  <end_of_file>\n-\n-The file contains internal pointers.  Each such pointer is called\n-a BlockHandle and contains the following information:\n-  offset:\t    varint64\n-  size:\t\t    varint64\n-See https://developers.google.com/protocol-buffers/docs/encoding#varints\n-for an explanation of varint64 format.\n-\n-(1) The sequence of key/value pairs in the file are stored in sorted\n-order and partitioned into a sequence of data blocks.  These blocks\n-come one after another at the beginning of the file.  Each data block\n-is formatted according to the code in block_builder.cc, and then\n-optionally compressed.\n-\n-(2) After the data blocks we store a bunch of meta blocks.  The\n-supported meta block types are described below.  More meta block types\n-may be added in the future.  Each meta block is again formatted using\n-block_builder.cc and then optionally compressed.\n-\n-(3) A \"metaindex\" block.  It contains one entry for every other meta\n-block where the key is the name of the meta block and the value is a\n-BlockHandle pointing to that meta block.\n-\n-(4) An \"index\" block.  This block contains one entry per data block,\n-where the key is a string >= last key in that data block and before\n-the first key in the successive data block.  The value is the\n-BlockHandle for the data block.\n-\n-(6) At the very end of the file is a fixed length footer that contains\n-the BlockHandle of the metaindex and index blocks as well as a magic number.\n-       metaindex_handle: char[p];    // Block handle for metaindex\n-       index_handle:     char[q];    // Block handle for index\n-       padding:          char[40-p-q]; // zeroed bytes to make fixed length\n-                                       // (40==2*BlockHandle::kMaxEncodedLength)\n-       magic:            fixed64;    // == 0xdb4775248b80fb57 (little-endian)\n-\n-\"filter\" Meta Block\n--------------------\n-\n-If a \"FilterPolicy\" was specified when the database was opened, a\n-filter block is stored in each table.  The \"metaindex\" block contains\n-an entry that maps from \"filter.<N>\" to the BlockHandle for the filter\n-block where \"<N>\" is the string returned by the filter policy's\n-\"Name()\" method.\n-\n-The filter block stores a sequence of filters, where filter i contains\n-the output of FilterPolicy::CreateFilter() on all keys that are stored\n-in a block whose file offset falls within the range\n-\n-    [ i*base ... (i+1)*base-1 ]\n-\n-Currently, \"base\" is 2KB.  So for example, if blocks X and Y start in\n-the range [ 0KB .. 2KB-1 ], all of the keys in X and Y will be\n-converted to a filter by calling FilterPolicy::CreateFilter(), and the\n-resulting filter will be stored as the first filter in the filter\n-block.\n-\n-The filter block is formatted as follows:\n-\n-     [filter 0]\n-     [filter 1]\n-     [filter 2]\n-     ...\n-     [filter N-1]\n-\n-     [offset of filter 0]                  : 4 bytes\n-     [offset of filter 1]                  : 4 bytes\n-     [offset of filter 2]                  : 4 bytes\n-     ...\n-     [offset of filter N-1]                : 4 bytes\n-\n-     [offset of beginning of offset array] : 4 bytes\n-     lg(base)                              : 1 byte\n-\n-The offset array at the end of the filter block allows efficient\n-mapping from a data block offset to the corresponding filter.\n-\n-\"stats\" Meta Block\n-------------------\n-\n-This meta block contains a bunch of stats.  The key is the name\n-of the statistic.  The value contains the statistic.\n-TODO(postrelease): record following stats.\n-  data size\n-  index size\n-  key size (uncompressed)\n-  value size (uncompressed)\n-  number of entries\n-  number of data blocks"
      },
      {
        "sha": "5879de121456a7c5c16457eb36d85c64ad0a1b61",
        "filename": "src/leveldb/helpers/memenv/memenv.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 384,
        "changes": 384,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/helpers/memenv/memenv.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/helpers/memenv/memenv.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/helpers/memenv/memenv.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,384 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"helpers/memenv/memenv.h\"\n-\n-#include \"leveldb/env.h\"\n-#include \"leveldb/status.h\"\n-#include \"port/port.h\"\n-#include \"util/mutexlock.h\"\n-#include <map>\n-#include <string.h>\n-#include <string>\n-#include <vector>\n-\n-namespace leveldb {\n-\n-namespace {\n-\n-class FileState {\n- public:\n-  // FileStates are reference counted. The initial reference count is zero\n-  // and the caller must call Ref() at least once.\n-  FileState() : refs_(0), size_(0) {}\n-\n-  // Increase the reference count.\n-  void Ref() {\n-    MutexLock lock(&refs_mutex_);\n-    ++refs_;\n-  }\n-\n-  // Decrease the reference count. Delete if this is the last reference.\n-  void Unref() {\n-    bool do_delete = false;\n-\n-    {\n-      MutexLock lock(&refs_mutex_);\n-      --refs_;\n-      assert(refs_ >= 0);\n-      if (refs_ <= 0) {\n-        do_delete = true;\n-      }\n-    }\n-\n-    if (do_delete) {\n-      delete this;\n-    }\n-  }\n-\n-  uint64_t Size() const { return size_; }\n-\n-  Status Read(uint64_t offset, size_t n, Slice* result, char* scratch) const {\n-    if (offset > size_) {\n-      return Status::IOError(\"Offset greater than file size.\");\n-    }\n-    const uint64_t available = size_ - offset;\n-    if (n > available) {\n-      n = available;\n-    }\n-    if (n == 0) {\n-      *result = Slice();\n-      return Status::OK();\n-    }\n-\n-    size_t block = offset / kBlockSize;\n-    size_t block_offset = offset % kBlockSize;\n-\n-    if (n <= kBlockSize - block_offset) {\n-      // The requested bytes are all in the first block.\n-      *result = Slice(blocks_[block] + block_offset, n);\n-      return Status::OK();\n-    }\n-\n-    size_t bytes_to_copy = n;\n-    char* dst = scratch;\n-\n-    while (bytes_to_copy > 0) {\n-      size_t avail = kBlockSize - block_offset;\n-      if (avail > bytes_to_copy) {\n-        avail = bytes_to_copy;\n-      }\n-      memcpy(dst, blocks_[block] + block_offset, avail);\n-\n-      bytes_to_copy -= avail;\n-      dst += avail;\n-      block++;\n-      block_offset = 0;\n-    }\n-\n-    *result = Slice(scratch, n);\n-    return Status::OK();\n-  }\n-\n-  Status Append(const Slice& data) {\n-    const char* src = data.data();\n-    size_t src_len = data.size();\n-\n-    while (src_len > 0) {\n-      size_t avail;\n-      size_t offset = size_ % kBlockSize;\n-\n-      if (offset != 0) {\n-        // There is some room in the last block.\n-        avail = kBlockSize - offset;\n-      } else {\n-        // No room in the last block; push new one.\n-        blocks_.push_back(new char[kBlockSize]);\n-        avail = kBlockSize;\n-      }\n-\n-      if (avail > src_len) {\n-        avail = src_len;\n-      }\n-      memcpy(blocks_.back() + offset, src, avail);\n-      src_len -= avail;\n-      src += avail;\n-      size_ += avail;\n-    }\n-\n-    return Status::OK();\n-  }\n-\n- private:\n-  // Private since only Unref() should be used to delete it.\n-  ~FileState() {\n-    for (std::vector<char*>::iterator i = blocks_.begin(); i != blocks_.end();\n-         ++i) {\n-      delete [] *i;\n-    }\n-  }\n-\n-  // No copying allowed.\n-  FileState(const FileState&);\n-  void operator=(const FileState&);\n-\n-  port::Mutex refs_mutex_;\n-  int refs_;  // Protected by refs_mutex_;\n-\n-  // The following fields are not protected by any mutex. They are only mutable\n-  // while the file is being written, and concurrent access is not allowed\n-  // to writable files.\n-  std::vector<char*> blocks_;\n-  uint64_t size_;\n-\n-  enum { kBlockSize = 8 * 1024 };\n-};\n-\n-class SequentialFileImpl : public SequentialFile {\n- public:\n-  explicit SequentialFileImpl(FileState* file) : file_(file), pos_(0) {\n-    file_->Ref();\n-  }\n-\n-  ~SequentialFileImpl() {\n-    file_->Unref();\n-  }\n-\n-  virtual Status Read(size_t n, Slice* result, char* scratch) {\n-    Status s = file_->Read(pos_, n, result, scratch);\n-    if (s.ok()) {\n-      pos_ += result->size();\n-    }\n-    return s;\n-  }\n-\n-  virtual Status Skip(uint64_t n) {\n-    if (pos_ > file_->Size()) {\n-      return Status::IOError(\"pos_ > file_->Size()\");\n-    }\n-    const size_t available = file_->Size() - pos_;\n-    if (n > available) {\n-      n = available;\n-    }\n-    pos_ += n;\n-    return Status::OK();\n-  }\n-\n- private:\n-  FileState* file_;\n-  size_t pos_;\n-};\n-\n-class RandomAccessFileImpl : public RandomAccessFile {\n- public:\n-  explicit RandomAccessFileImpl(FileState* file) : file_(file) {\n-    file_->Ref();\n-  }\n-\n-  ~RandomAccessFileImpl() {\n-    file_->Unref();\n-  }\n-\n-  virtual Status Read(uint64_t offset, size_t n, Slice* result,\n-                      char* scratch) const {\n-    return file_->Read(offset, n, result, scratch);\n-  }\n-\n- private:\n-  FileState* file_;\n-};\n-\n-class WritableFileImpl : public WritableFile {\n- public:\n-  WritableFileImpl(FileState* file) : file_(file) {\n-    file_->Ref();\n-  }\n-\n-  ~WritableFileImpl() {\n-    file_->Unref();\n-  }\n-\n-  virtual Status Append(const Slice& data) {\n-    return file_->Append(data);\n-  }\n-\n-  virtual Status Close() { return Status::OK(); }\n-  virtual Status Flush() { return Status::OK(); }\n-  virtual Status Sync() { return Status::OK(); }\n-\n- private:\n-  FileState* file_;\n-};\n-\n-class NoOpLogger : public Logger {\n- public:\n-  virtual void Logv(const char* format, va_list ap) { }\n-};\n-\n-class InMemoryEnv : public EnvWrapper {\n- public:\n-  explicit InMemoryEnv(Env* base_env) : EnvWrapper(base_env) { }\n-\n-  virtual ~InMemoryEnv() {\n-    for (FileSystem::iterator i = file_map_.begin(); i != file_map_.end(); ++i){\n-      i->second->Unref();\n-    }\n-  }\n-\n-  // Partial implementation of the Env interface.\n-  virtual Status NewSequentialFile(const std::string& fname,\n-                                   SequentialFile** result) {\n-    MutexLock lock(&mutex_);\n-    if (file_map_.find(fname) == file_map_.end()) {\n-      *result = NULL;\n-      return Status::IOError(fname, \"File not found\");\n-    }\n-\n-    *result = new SequentialFileImpl(file_map_[fname]);\n-    return Status::OK();\n-  }\n-\n-  virtual Status NewRandomAccessFile(const std::string& fname,\n-                                     RandomAccessFile** result) {\n-    MutexLock lock(&mutex_);\n-    if (file_map_.find(fname) == file_map_.end()) {\n-      *result = NULL;\n-      return Status::IOError(fname, \"File not found\");\n-    }\n-\n-    *result = new RandomAccessFileImpl(file_map_[fname]);\n-    return Status::OK();\n-  }\n-\n-  virtual Status NewWritableFile(const std::string& fname,\n-                                 WritableFile** result) {\n-    MutexLock lock(&mutex_);\n-    if (file_map_.find(fname) != file_map_.end()) {\n-      DeleteFileInternal(fname);\n-    }\n-\n-    FileState* file = new FileState();\n-    file->Ref();\n-    file_map_[fname] = file;\n-\n-    *result = new WritableFileImpl(file);\n-    return Status::OK();\n-  }\n-\n-  virtual bool FileExists(const std::string& fname) {\n-    MutexLock lock(&mutex_);\n-    return file_map_.find(fname) != file_map_.end();\n-  }\n-\n-  virtual Status GetChildren(const std::string& dir,\n-                             std::vector<std::string>* result) {\n-    MutexLock lock(&mutex_);\n-    result->clear();\n-\n-    for (FileSystem::iterator i = file_map_.begin(); i != file_map_.end(); ++i){\n-      const std::string& filename = i->first;\n-\n-      if (filename.size() >= dir.size() + 1 && filename[dir.size()] == '/' &&\n-          Slice(filename).starts_with(Slice(dir))) {\n-        result->push_back(filename.substr(dir.size() + 1));\n-      }\n-    }\n-\n-    return Status::OK();\n-  }\n-\n-  void DeleteFileInternal(const std::string& fname) {\n-    if (file_map_.find(fname) == file_map_.end()) {\n-      return;\n-    }\n-\n-    file_map_[fname]->Unref();\n-    file_map_.erase(fname);\n-  }\n-\n-  virtual Status DeleteFile(const std::string& fname) {\n-    MutexLock lock(&mutex_);\n-    if (file_map_.find(fname) == file_map_.end()) {\n-      return Status::IOError(fname, \"File not found\");\n-    }\n-\n-    DeleteFileInternal(fname);\n-    return Status::OK();\n-  }\n-\n-  virtual Status CreateDir(const std::string& dirname) {\n-    return Status::OK();\n-  }\n-\n-  virtual Status DeleteDir(const std::string& dirname) {\n-    return Status::OK();\n-  }\n-\n-  virtual Status GetFileSize(const std::string& fname, uint64_t* file_size) {\n-    MutexLock lock(&mutex_);\n-    if (file_map_.find(fname) == file_map_.end()) {\n-      return Status::IOError(fname, \"File not found\");\n-    }\n-\n-    *file_size = file_map_[fname]->Size();\n-    return Status::OK();\n-  }\n-\n-  virtual Status RenameFile(const std::string& src,\n-                            const std::string& target) {\n-    MutexLock lock(&mutex_);\n-    if (file_map_.find(src) == file_map_.end()) {\n-      return Status::IOError(src, \"File not found\");\n-    }\n-\n-    DeleteFileInternal(target);\n-    file_map_[target] = file_map_[src];\n-    file_map_.erase(src);\n-    return Status::OK();\n-  }\n-\n-  virtual Status LockFile(const std::string& fname, FileLock** lock) {\n-    *lock = new FileLock;\n-    return Status::OK();\n-  }\n-\n-  virtual Status UnlockFile(FileLock* lock) {\n-    delete lock;\n-    return Status::OK();\n-  }\n-\n-  virtual Status GetTestDirectory(std::string* path) {\n-    *path = \"/test\";\n-    return Status::OK();\n-  }\n-\n-  virtual Status NewLogger(const std::string& fname, Logger** result) {\n-    *result = new NoOpLogger;\n-    return Status::OK();\n-  }\n-\n- private:\n-  // Map from filenames to FileState objects, representing a simple file system.\n-  typedef std::map<std::string, FileState*> FileSystem;\n-  port::Mutex mutex_;\n-  FileSystem file_map_;  // Protected by mutex_.\n-};\n-\n-}  // namespace\n-\n-Env* NewMemEnv(Env* base_env) {\n-  return new InMemoryEnv(base_env);\n-}\n-\n-}  // namespace leveldb"
      },
      {
        "sha": "03b88de761dc732e09dec54baddd34e94ee17613",
        "filename": "src/leveldb/helpers/memenv/memenv.h",
        "status": "removed",
        "additions": 0,
        "deletions": 20,
        "changes": 20,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/helpers/memenv/memenv.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/helpers/memenv/memenv.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/helpers/memenv/memenv.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,20 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_HELPERS_MEMENV_MEMENV_H_\n-#define STORAGE_LEVELDB_HELPERS_MEMENV_MEMENV_H_\n-\n-namespace leveldb {\n-\n-class Env;\n-\n-// Returns a new environment that stores its data in memory and delegates\n-// all non-file-storage tasks to base_env. The caller must delete the result\n-// when it is no longer needed.\n-// *base_env must remain live while the result is in use.\n-Env* NewMemEnv(Env* base_env);\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_HELPERS_MEMENV_MEMENV_H_"
      },
      {
        "sha": "a44310fed80cd7f210d64b2c8e79ceb74284217a",
        "filename": "src/leveldb/helpers/memenv/memenv_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 232,
        "changes": 232,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/helpers/memenv/memenv_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/helpers/memenv/memenv_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/helpers/memenv/memenv_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,232 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"helpers/memenv/memenv.h\"\n-\n-#include \"db/db_impl.h\"\n-#include \"leveldb/db.h\"\n-#include \"leveldb/env.h\"\n-#include \"util/testharness.h\"\n-#include <string>\n-#include <vector>\n-\n-namespace leveldb {\n-\n-class MemEnvTest {\n- public:\n-  Env* env_;\n-\n-  MemEnvTest()\n-      : env_(NewMemEnv(Env::Default())) {\n-  }\n-  ~MemEnvTest() {\n-    delete env_;\n-  }\n-};\n-\n-TEST(MemEnvTest, Basics) {\n-  uint64_t file_size;\n-  WritableFile* writable_file;\n-  std::vector<std::string> children;\n-\n-  ASSERT_OK(env_->CreateDir(\"/dir\"));\n-\n-  // Check that the directory is empty.\n-  ASSERT_TRUE(!env_->FileExists(\"/dir/non_existent\"));\n-  ASSERT_TRUE(!env_->GetFileSize(\"/dir/non_existent\", &file_size).ok());\n-  ASSERT_OK(env_->GetChildren(\"/dir\", &children));\n-  ASSERT_EQ(0, children.size());\n-\n-  // Create a file.\n-  ASSERT_OK(env_->NewWritableFile(\"/dir/f\", &writable_file));\n-  delete writable_file;\n-\n-  // Check that the file exists.\n-  ASSERT_TRUE(env_->FileExists(\"/dir/f\"));\n-  ASSERT_OK(env_->GetFileSize(\"/dir/f\", &file_size));\n-  ASSERT_EQ(0, file_size);\n-  ASSERT_OK(env_->GetChildren(\"/dir\", &children));\n-  ASSERT_EQ(1, children.size());\n-  ASSERT_EQ(\"f\", children[0]);\n-\n-  // Write to the file.\n-  ASSERT_OK(env_->NewWritableFile(\"/dir/f\", &writable_file));\n-  ASSERT_OK(writable_file->Append(\"abc\"));\n-  delete writable_file;\n-\n-  // Check for expected size.\n-  ASSERT_OK(env_->GetFileSize(\"/dir/f\", &file_size));\n-  ASSERT_EQ(3, file_size);\n-\n-  // Check that renaming works.\n-  ASSERT_TRUE(!env_->RenameFile(\"/dir/non_existent\", \"/dir/g\").ok());\n-  ASSERT_OK(env_->RenameFile(\"/dir/f\", \"/dir/g\"));\n-  ASSERT_TRUE(!env_->FileExists(\"/dir/f\"));\n-  ASSERT_TRUE(env_->FileExists(\"/dir/g\"));\n-  ASSERT_OK(env_->GetFileSize(\"/dir/g\", &file_size));\n-  ASSERT_EQ(3, file_size);\n-\n-  // Check that opening non-existent file fails.\n-  SequentialFile* seq_file;\n-  RandomAccessFile* rand_file;\n-  ASSERT_TRUE(!env_->NewSequentialFile(\"/dir/non_existent\", &seq_file).ok());\n-  ASSERT_TRUE(!seq_file);\n-  ASSERT_TRUE(!env_->NewRandomAccessFile(\"/dir/non_existent\", &rand_file).ok());\n-  ASSERT_TRUE(!rand_file);\n-\n-  // Check that deleting works.\n-  ASSERT_TRUE(!env_->DeleteFile(\"/dir/non_existent\").ok());\n-  ASSERT_OK(env_->DeleteFile(\"/dir/g\"));\n-  ASSERT_TRUE(!env_->FileExists(\"/dir/g\"));\n-  ASSERT_OK(env_->GetChildren(\"/dir\", &children));\n-  ASSERT_EQ(0, children.size());\n-  ASSERT_OK(env_->DeleteDir(\"/dir\"));\n-}\n-\n-TEST(MemEnvTest, ReadWrite) {\n-  WritableFile* writable_file;\n-  SequentialFile* seq_file;\n-  RandomAccessFile* rand_file;\n-  Slice result;\n-  char scratch[100];\n-\n-  ASSERT_OK(env_->CreateDir(\"/dir\"));\n-\n-  ASSERT_OK(env_->NewWritableFile(\"/dir/f\", &writable_file));\n-  ASSERT_OK(writable_file->Append(\"hello \"));\n-  ASSERT_OK(writable_file->Append(\"world\"));\n-  delete writable_file;\n-\n-  // Read sequentially.\n-  ASSERT_OK(env_->NewSequentialFile(\"/dir/f\", &seq_file));\n-  ASSERT_OK(seq_file->Read(5, &result, scratch)); // Read \"hello\".\n-  ASSERT_EQ(0, result.compare(\"hello\"));\n-  ASSERT_OK(seq_file->Skip(1));\n-  ASSERT_OK(seq_file->Read(1000, &result, scratch)); // Read \"world\".\n-  ASSERT_EQ(0, result.compare(\"world\"));\n-  ASSERT_OK(seq_file->Read(1000, &result, scratch)); // Try reading past EOF.\n-  ASSERT_EQ(0, result.size());\n-  ASSERT_OK(seq_file->Skip(100)); // Try to skip past end of file.\n-  ASSERT_OK(seq_file->Read(1000, &result, scratch));\n-  ASSERT_EQ(0, result.size());\n-  delete seq_file;\n-\n-  // Random reads.\n-  ASSERT_OK(env_->NewRandomAccessFile(\"/dir/f\", &rand_file));\n-  ASSERT_OK(rand_file->Read(6, 5, &result, scratch)); // Read \"world\".\n-  ASSERT_EQ(0, result.compare(\"world\"));\n-  ASSERT_OK(rand_file->Read(0, 5, &result, scratch)); // Read \"hello\".\n-  ASSERT_EQ(0, result.compare(\"hello\"));\n-  ASSERT_OK(rand_file->Read(10, 100, &result, scratch)); // Read \"d\".\n-  ASSERT_EQ(0, result.compare(\"d\"));\n-\n-  // Too high offset.\n-  ASSERT_TRUE(!rand_file->Read(1000, 5, &result, scratch).ok());\n-  delete rand_file;\n-}\n-\n-TEST(MemEnvTest, Locks) {\n-  FileLock* lock;\n-\n-  // These are no-ops, but we test they return success.\n-  ASSERT_OK(env_->LockFile(\"some file\", &lock));\n-  ASSERT_OK(env_->UnlockFile(lock));\n-}\n-\n-TEST(MemEnvTest, Misc) {\n-  std::string test_dir;\n-  ASSERT_OK(env_->GetTestDirectory(&test_dir));\n-  ASSERT_TRUE(!test_dir.empty());\n-\n-  WritableFile* writable_file;\n-  ASSERT_OK(env_->NewWritableFile(\"/a/b\", &writable_file));\n-\n-  // These are no-ops, but we test they return success.\n-  ASSERT_OK(writable_file->Sync());\n-  ASSERT_OK(writable_file->Flush());\n-  ASSERT_OK(writable_file->Close());\n-  delete writable_file;\n-}\n-\n-TEST(MemEnvTest, LargeWrite) {\n-  const size_t kWriteSize = 300 * 1024;\n-  char* scratch = new char[kWriteSize * 2];\n-\n-  std::string write_data;\n-  for (size_t i = 0; i < kWriteSize; ++i) {\n-    write_data.append(1, static_cast<char>(i));\n-  }\n-\n-  WritableFile* writable_file;\n-  ASSERT_OK(env_->NewWritableFile(\"/dir/f\", &writable_file));\n-  ASSERT_OK(writable_file->Append(\"foo\"));\n-  ASSERT_OK(writable_file->Append(write_data));\n-  delete writable_file;\n-\n-  SequentialFile* seq_file;\n-  Slice result;\n-  ASSERT_OK(env_->NewSequentialFile(\"/dir/f\", &seq_file));\n-  ASSERT_OK(seq_file->Read(3, &result, scratch)); // Read \"foo\".\n-  ASSERT_EQ(0, result.compare(\"foo\"));\n-\n-  size_t read = 0;\n-  std::string read_data;\n-  while (read < kWriteSize) {\n-    ASSERT_OK(seq_file->Read(kWriteSize - read, &result, scratch));\n-    read_data.append(result.data(), result.size());\n-    read += result.size();\n-  }\n-  ASSERT_TRUE(write_data == read_data);\n-  delete seq_file;\n-  delete [] scratch;\n-}\n-\n-TEST(MemEnvTest, DBTest) {\n-  Options options;\n-  options.create_if_missing = true;\n-  options.env = env_;\n-  DB* db;\n-\n-  const Slice keys[] = {Slice(\"aaa\"), Slice(\"bbb\"), Slice(\"ccc\")};\n-  const Slice vals[] = {Slice(\"foo\"), Slice(\"bar\"), Slice(\"baz\")};\n-\n-  ASSERT_OK(DB::Open(options, \"/dir/db\", &db));\n-  for (size_t i = 0; i < 3; ++i) {\n-    ASSERT_OK(db->Put(WriteOptions(), keys[i], vals[i]));\n-  }\n-\n-  for (size_t i = 0; i < 3; ++i) {\n-    std::string res;\n-    ASSERT_OK(db->Get(ReadOptions(), keys[i], &res));\n-    ASSERT_TRUE(res == vals[i]);\n-  }\n-\n-  Iterator* iterator = db->NewIterator(ReadOptions());\n-  iterator->SeekToFirst();\n-  for (size_t i = 0; i < 3; ++i) {\n-    ASSERT_TRUE(iterator->Valid());\n-    ASSERT_TRUE(keys[i] == iterator->key());\n-    ASSERT_TRUE(vals[i] == iterator->value());\n-    iterator->Next();\n-  }\n-  ASSERT_TRUE(!iterator->Valid());\n-  delete iterator;\n-\n-  DBImpl* dbi = reinterpret_cast<DBImpl*>(db);\n-  ASSERT_OK(dbi->TEST_CompactMemTable());\n-\n-  for (size_t i = 0; i < 3; ++i) {\n-    std::string res;\n-    ASSERT_OK(db->Get(ReadOptions(), keys[i], &res));\n-    ASSERT_TRUE(res == vals[i]);\n-  }\n-\n-  delete db;\n-}\n-\n-}  // namespace leveldb\n-\n-int main(int argc, char** argv) {\n-  return leveldb::test::RunAllTests();\n-}"
      },
      {
        "sha": "1fa58866c3958c66d7c68b264aa4ee4ccdf51c68",
        "filename": "src/leveldb/include/leveldb/c.h",
        "status": "removed",
        "additions": 0,
        "deletions": 291,
        "changes": 291,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/c.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/c.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/c.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,291 +0,0 @@\n-/* Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-  Use of this source code is governed by a BSD-style license that can be\n-  found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-  C bindings for leveldb.  May be useful as a stable ABI that can be\n-  used by programs that keep leveldb in a shared library, or for\n-  a JNI api.\n-\n-  Does not support:\n-  . getters for the option types\n-  . custom comparators that implement key shortening\n-  . capturing post-write-snapshot\n-  . custom iter, db, env, cache implementations using just the C bindings\n-\n-  Some conventions:\n-\n-  (1) We expose just opaque struct pointers and functions to clients.\n-  This allows us to change internal representations without having to\n-  recompile clients.\n-\n-  (2) For simplicity, there is no equivalent to the Slice type.  Instead,\n-  the caller has to pass the pointer and length as separate\n-  arguments.\n-\n-  (3) Errors are represented by a null-terminated c string.  NULL\n-  means no error.  All operations that can raise an error are passed\n-  a \"char** errptr\" as the last argument.  One of the following must\n-  be true on entry:\n-     *errptr == NULL\n-     *errptr points to a malloc()ed null-terminated error message\n-       (On Windows, *errptr must have been malloc()-ed by this library.)\n-  On success, a leveldb routine leaves *errptr unchanged.\n-  On failure, leveldb frees the old value of *errptr and\n-  set *errptr to a malloc()ed error message.\n-\n-  (4) Bools have the type unsigned char (0 == false; rest == true)\n-\n-  (5) All of the pointer arguments must be non-NULL.\n-*/\n-\n-#ifndef STORAGE_LEVELDB_INCLUDE_C_H_\n-#define STORAGE_LEVELDB_INCLUDE_C_H_\n-\n-#ifdef __cplusplus\n-extern \"C\" {\n-#endif\n-\n-#include <stdarg.h>\n-#include <stddef.h>\n-#include <stdint.h>\n-\n-/* Exported types */\n-\n-typedef struct leveldb_t               leveldb_t;\n-typedef struct leveldb_cache_t         leveldb_cache_t;\n-typedef struct leveldb_comparator_t    leveldb_comparator_t;\n-typedef struct leveldb_env_t           leveldb_env_t;\n-typedef struct leveldb_filelock_t      leveldb_filelock_t;\n-typedef struct leveldb_filterpolicy_t  leveldb_filterpolicy_t;\n-typedef struct leveldb_iterator_t      leveldb_iterator_t;\n-typedef struct leveldb_logger_t        leveldb_logger_t;\n-typedef struct leveldb_options_t       leveldb_options_t;\n-typedef struct leveldb_randomfile_t    leveldb_randomfile_t;\n-typedef struct leveldb_readoptions_t   leveldb_readoptions_t;\n-typedef struct leveldb_seqfile_t       leveldb_seqfile_t;\n-typedef struct leveldb_snapshot_t      leveldb_snapshot_t;\n-typedef struct leveldb_writablefile_t  leveldb_writablefile_t;\n-typedef struct leveldb_writebatch_t    leveldb_writebatch_t;\n-typedef struct leveldb_writeoptions_t  leveldb_writeoptions_t;\n-\n-/* DB operations */\n-\n-extern leveldb_t* leveldb_open(\n-    const leveldb_options_t* options,\n-    const char* name,\n-    char** errptr);\n-\n-extern void leveldb_close(leveldb_t* db);\n-\n-extern void leveldb_put(\n-    leveldb_t* db,\n-    const leveldb_writeoptions_t* options,\n-    const char* key, size_t keylen,\n-    const char* val, size_t vallen,\n-    char** errptr);\n-\n-extern void leveldb_delete(\n-    leveldb_t* db,\n-    const leveldb_writeoptions_t* options,\n-    const char* key, size_t keylen,\n-    char** errptr);\n-\n-extern void leveldb_write(\n-    leveldb_t* db,\n-    const leveldb_writeoptions_t* options,\n-    leveldb_writebatch_t* batch,\n-    char** errptr);\n-\n-/* Returns NULL if not found.  A malloc()ed array otherwise.\n-   Stores the length of the array in *vallen. */\n-extern char* leveldb_get(\n-    leveldb_t* db,\n-    const leveldb_readoptions_t* options,\n-    const char* key, size_t keylen,\n-    size_t* vallen,\n-    char** errptr);\n-\n-extern leveldb_iterator_t* leveldb_create_iterator(\n-    leveldb_t* db,\n-    const leveldb_readoptions_t* options);\n-\n-extern const leveldb_snapshot_t* leveldb_create_snapshot(\n-    leveldb_t* db);\n-\n-extern void leveldb_release_snapshot(\n-    leveldb_t* db,\n-    const leveldb_snapshot_t* snapshot);\n-\n-/* Returns NULL if property name is unknown.\n-   Else returns a pointer to a malloc()-ed null-terminated value. */\n-extern char* leveldb_property_value(\n-    leveldb_t* db,\n-    const char* propname);\n-\n-extern void leveldb_approximate_sizes(\n-    leveldb_t* db,\n-    int num_ranges,\n-    const char* const* range_start_key, const size_t* range_start_key_len,\n-    const char* const* range_limit_key, const size_t* range_limit_key_len,\n-    uint64_t* sizes);\n-\n-extern void leveldb_compact_range(\n-    leveldb_t* db,\n-    const char* start_key, size_t start_key_len,\n-    const char* limit_key, size_t limit_key_len);\n-\n-/* Management operations */\n-\n-extern void leveldb_destroy_db(\n-    const leveldb_options_t* options,\n-    const char* name,\n-    char** errptr);\n-\n-extern void leveldb_repair_db(\n-    const leveldb_options_t* options,\n-    const char* name,\n-    char** errptr);\n-\n-/* Iterator */\n-\n-extern void leveldb_iter_destroy(leveldb_iterator_t*);\n-extern unsigned char leveldb_iter_valid(const leveldb_iterator_t*);\n-extern void leveldb_iter_seek_to_first(leveldb_iterator_t*);\n-extern void leveldb_iter_seek_to_last(leveldb_iterator_t*);\n-extern void leveldb_iter_seek(leveldb_iterator_t*, const char* k, size_t klen);\n-extern void leveldb_iter_next(leveldb_iterator_t*);\n-extern void leveldb_iter_prev(leveldb_iterator_t*);\n-extern const char* leveldb_iter_key(const leveldb_iterator_t*, size_t* klen);\n-extern const char* leveldb_iter_value(const leveldb_iterator_t*, size_t* vlen);\n-extern void leveldb_iter_get_error(const leveldb_iterator_t*, char** errptr);\n-\n-/* Write batch */\n-\n-extern leveldb_writebatch_t* leveldb_writebatch_create();\n-extern void leveldb_writebatch_destroy(leveldb_writebatch_t*);\n-extern void leveldb_writebatch_clear(leveldb_writebatch_t*);\n-extern void leveldb_writebatch_put(\n-    leveldb_writebatch_t*,\n-    const char* key, size_t klen,\n-    const char* val, size_t vlen);\n-extern void leveldb_writebatch_delete(\n-    leveldb_writebatch_t*,\n-    const char* key, size_t klen);\n-extern void leveldb_writebatch_iterate(\n-    leveldb_writebatch_t*,\n-    void* state,\n-    void (*put)(void*, const char* k, size_t klen, const char* v, size_t vlen),\n-    void (*deleted)(void*, const char* k, size_t klen));\n-\n-/* Options */\n-\n-extern leveldb_options_t* leveldb_options_create();\n-extern void leveldb_options_destroy(leveldb_options_t*);\n-extern void leveldb_options_set_comparator(\n-    leveldb_options_t*,\n-    leveldb_comparator_t*);\n-extern void leveldb_options_set_filter_policy(\n-    leveldb_options_t*,\n-    leveldb_filterpolicy_t*);\n-extern void leveldb_options_set_create_if_missing(\n-    leveldb_options_t*, unsigned char);\n-extern void leveldb_options_set_error_if_exists(\n-    leveldb_options_t*, unsigned char);\n-extern void leveldb_options_set_paranoid_checks(\n-    leveldb_options_t*, unsigned char);\n-extern void leveldb_options_set_env(leveldb_options_t*, leveldb_env_t*);\n-extern void leveldb_options_set_info_log(leveldb_options_t*, leveldb_logger_t*);\n-extern void leveldb_options_set_write_buffer_size(leveldb_options_t*, size_t);\n-extern void leveldb_options_set_max_open_files(leveldb_options_t*, int);\n-extern void leveldb_options_set_cache(leveldb_options_t*, leveldb_cache_t*);\n-extern void leveldb_options_set_block_size(leveldb_options_t*, size_t);\n-extern void leveldb_options_set_block_restart_interval(leveldb_options_t*, int);\n-\n-enum {\n-  leveldb_no_compression = 0,\n-  leveldb_snappy_compression = 1\n-};\n-extern void leveldb_options_set_compression(leveldb_options_t*, int);\n-\n-/* Comparator */\n-\n-extern leveldb_comparator_t* leveldb_comparator_create(\n-    void* state,\n-    void (*destructor)(void*),\n-    int (*compare)(\n-        void*,\n-        const char* a, size_t alen,\n-        const char* b, size_t blen),\n-    const char* (*name)(void*));\n-extern void leveldb_comparator_destroy(leveldb_comparator_t*);\n-\n-/* Filter policy */\n-\n-extern leveldb_filterpolicy_t* leveldb_filterpolicy_create(\n-    void* state,\n-    void (*destructor)(void*),\n-    char* (*create_filter)(\n-        void*,\n-        const char* const* key_array, const size_t* key_length_array,\n-        int num_keys,\n-        size_t* filter_length),\n-    unsigned char (*key_may_match)(\n-        void*,\n-        const char* key, size_t length,\n-        const char* filter, size_t filter_length),\n-    const char* (*name)(void*));\n-extern void leveldb_filterpolicy_destroy(leveldb_filterpolicy_t*);\n-\n-extern leveldb_filterpolicy_t* leveldb_filterpolicy_create_bloom(\n-    int bits_per_key);\n-\n-/* Read options */\n-\n-extern leveldb_readoptions_t* leveldb_readoptions_create();\n-extern void leveldb_readoptions_destroy(leveldb_readoptions_t*);\n-extern void leveldb_readoptions_set_verify_checksums(\n-    leveldb_readoptions_t*,\n-    unsigned char);\n-extern void leveldb_readoptions_set_fill_cache(\n-    leveldb_readoptions_t*, unsigned char);\n-extern void leveldb_readoptions_set_snapshot(\n-    leveldb_readoptions_t*,\n-    const leveldb_snapshot_t*);\n-\n-/* Write options */\n-\n-extern leveldb_writeoptions_t* leveldb_writeoptions_create();\n-extern void leveldb_writeoptions_destroy(leveldb_writeoptions_t*);\n-extern void leveldb_writeoptions_set_sync(\n-    leveldb_writeoptions_t*, unsigned char);\n-\n-/* Cache */\n-\n-extern leveldb_cache_t* leveldb_cache_create_lru(size_t capacity);\n-extern void leveldb_cache_destroy(leveldb_cache_t* cache);\n-\n-/* Env */\n-\n-extern leveldb_env_t* leveldb_create_default_env();\n-extern void leveldb_env_destroy(leveldb_env_t*);\n-\n-/* Utility */\n-\n-/* Calls free(ptr).\n-   REQUIRES: ptr was malloc()-ed and returned by one of the routines\n-   in this file.  Note that in certain cases (typically on Windows), you\n-   may need to call this routine instead of free(ptr) to dispose of\n-   malloc()-ed memory returned by this library. */\n-extern void leveldb_free(void* ptr);\n-\n-/* Return the major version number for this release. */\n-extern int leveldb_major_version();\n-\n-/* Return the minor version number for this release. */\n-extern int leveldb_minor_version();\n-\n-#ifdef __cplusplus\n-}  /* end extern \"C\" */\n-#endif\n-\n-#endif  /* STORAGE_LEVELDB_INCLUDE_C_H_ */"
      },
      {
        "sha": "5e3b47637d49e9f963b141dc5a011e1272615750",
        "filename": "src/leveldb/include/leveldb/cache.h",
        "status": "removed",
        "additions": 0,
        "deletions": 99,
        "changes": 99,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/cache.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/cache.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/cache.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,99 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// A Cache is an interface that maps keys to values.  It has internal\n-// synchronization and may be safely accessed concurrently from\n-// multiple threads.  It may automatically evict entries to make room\n-// for new entries.  Values have a specified charge against the cache\n-// capacity.  For example, a cache where the values are variable\n-// length strings, may use the length of the string as the charge for\n-// the string.\n-//\n-// A builtin cache implementation with a least-recently-used eviction\n-// policy is provided.  Clients may use their own implementations if\n-// they want something more sophisticated (like scan-resistance, a\n-// custom eviction policy, variable cache sizing, etc.)\n-\n-#ifndef STORAGE_LEVELDB_INCLUDE_CACHE_H_\n-#define STORAGE_LEVELDB_INCLUDE_CACHE_H_\n-\n-#include <stdint.h>\n-#include \"leveldb/slice.h\"\n-\n-namespace leveldb {\n-\n-class Cache;\n-\n-// Create a new cache with a fixed size capacity.  This implementation\n-// of Cache uses a least-recently-used eviction policy.\n-extern Cache* NewLRUCache(size_t capacity);\n-\n-class Cache {\n- public:\n-  Cache() { }\n-\n-  // Destroys all existing entries by calling the \"deleter\"\n-  // function that was passed to the constructor.\n-  virtual ~Cache();\n-\n-  // Opaque handle to an entry stored in the cache.\n-  struct Handle { };\n-\n-  // Insert a mapping from key->value into the cache and assign it\n-  // the specified charge against the total cache capacity.\n-  //\n-  // Returns a handle that corresponds to the mapping.  The caller\n-  // must call this->Release(handle) when the returned mapping is no\n-  // longer needed.\n-  //\n-  // When the inserted entry is no longer needed, the key and\n-  // value will be passed to \"deleter\".\n-  virtual Handle* Insert(const Slice& key, void* value, size_t charge,\n-                         void (*deleter)(const Slice& key, void* value)) = 0;\n-\n-  // If the cache has no mapping for \"key\", returns NULL.\n-  //\n-  // Else return a handle that corresponds to the mapping.  The caller\n-  // must call this->Release(handle) when the returned mapping is no\n-  // longer needed.\n-  virtual Handle* Lookup(const Slice& key) = 0;\n-\n-  // Release a mapping returned by a previous Lookup().\n-  // REQUIRES: handle must not have been released yet.\n-  // REQUIRES: handle must have been returned by a method on *this.\n-  virtual void Release(Handle* handle) = 0;\n-\n-  // Return the value encapsulated in a handle returned by a\n-  // successful Lookup().\n-  // REQUIRES: handle must not have been released yet.\n-  // REQUIRES: handle must have been returned by a method on *this.\n-  virtual void* Value(Handle* handle) = 0;\n-\n-  // If the cache contains entry for key, erase it.  Note that the\n-  // underlying entry will be kept around until all existing handles\n-  // to it have been released.\n-  virtual void Erase(const Slice& key) = 0;\n-\n-  // Return a new numeric id.  May be used by multiple clients who are\n-  // sharing the same cache to partition the key space.  Typically the\n-  // client will allocate a new id at startup and prepend the id to\n-  // its cache keys.\n-  virtual uint64_t NewId() = 0;\n-\n- private:\n-  void LRU_Remove(Handle* e);\n-  void LRU_Append(Handle* e);\n-  void Unref(Handle* e);\n-\n-  struct Rep;\n-  Rep* rep_;\n-\n-  // No copying allowed\n-  Cache(const Cache&);\n-  void operator=(const Cache&);\n-};\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_UTIL_CACHE_H_"
      },
      {
        "sha": "556b984c7694f6520088754f3017bf58c7cafc9d",
        "filename": "src/leveldb/include/leveldb/comparator.h",
        "status": "removed",
        "additions": 0,
        "deletions": 63,
        "changes": 63,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/comparator.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/comparator.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/comparator.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,63 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_INCLUDE_COMPARATOR_H_\n-#define STORAGE_LEVELDB_INCLUDE_COMPARATOR_H_\n-\n-#include <string>\n-\n-namespace leveldb {\n-\n-class Slice;\n-\n-// A Comparator object provides a total order across slices that are\n-// used as keys in an sstable or a database.  A Comparator implementation\n-// must be thread-safe since leveldb may invoke its methods concurrently\n-// from multiple threads.\n-class Comparator {\n- public:\n-  virtual ~Comparator();\n-\n-  // Three-way comparison.  Returns value:\n-  //   < 0 iff \"a\" < \"b\",\n-  //   == 0 iff \"a\" == \"b\",\n-  //   > 0 iff \"a\" > \"b\"\n-  virtual int Compare(const Slice& a, const Slice& b) const = 0;\n-\n-  // The name of the comparator.  Used to check for comparator\n-  // mismatches (i.e., a DB created with one comparator is\n-  // accessed using a different comparator.\n-  //\n-  // The client of this package should switch to a new name whenever\n-  // the comparator implementation changes in a way that will cause\n-  // the relative ordering of any two keys to change.\n-  //\n-  // Names starting with \"leveldb.\" are reserved and should not be used\n-  // by any clients of this package.\n-  virtual const char* Name() const = 0;\n-\n-  // Advanced functions: these are used to reduce the space requirements\n-  // for internal data structures like index blocks.\n-\n-  // If *start < limit, changes *start to a short string in [start,limit).\n-  // Simple comparator implementations may return with *start unchanged,\n-  // i.e., an implementation of this method that does nothing is correct.\n-  virtual void FindShortestSeparator(\n-      std::string* start,\n-      const Slice& limit) const = 0;\n-\n-  // Changes *key to a short string >= *key.\n-  // Simple comparator implementations may return with *key unchanged,\n-  // i.e., an implementation of this method that does nothing is correct.\n-  virtual void FindShortSuccessor(std::string* key) const = 0;\n-};\n-\n-// Return a builtin comparator that uses lexicographic byte-wise\n-// ordering.  The result remains the property of this module and\n-// must not be deleted.\n-extern const Comparator* BytewiseComparator();\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_INCLUDE_COMPARATOR_H_"
      },
      {
        "sha": "29d367447907063975608d742c8a29a799848fb5",
        "filename": "src/leveldb/include/leveldb/db.h",
        "status": "removed",
        "additions": 0,
        "deletions": 161,
        "changes": 161,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/db.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/db.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/db.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,161 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_INCLUDE_DB_H_\n-#define STORAGE_LEVELDB_INCLUDE_DB_H_\n-\n-#include <stdint.h>\n-#include <stdio.h>\n-#include \"leveldb/iterator.h\"\n-#include \"leveldb/options.h\"\n-\n-namespace leveldb {\n-\n-// Update Makefile if you change these\n-static const int kMajorVersion = 1;\n-static const int kMinorVersion = 9;\n-\n-struct Options;\n-struct ReadOptions;\n-struct WriteOptions;\n-class WriteBatch;\n-\n-// Abstract handle to particular state of a DB.\n-// A Snapshot is an immutable object and can therefore be safely\n-// accessed from multiple threads without any external synchronization.\n-class Snapshot {\n- protected:\n-  virtual ~Snapshot();\n-};\n-\n-// A range of keys\n-struct Range {\n-  Slice start;          // Included in the range\n-  Slice limit;          // Not included in the range\n-\n-  Range() { }\n-  Range(const Slice& s, const Slice& l) : start(s), limit(l) { }\n-};\n-\n-// A DB is a persistent ordered map from keys to values.\n-// A DB is safe for concurrent access from multiple threads without\n-// any external synchronization.\n-class DB {\n- public:\n-  // Open the database with the specified \"name\".\n-  // Stores a pointer to a heap-allocated database in *dbptr and returns\n-  // OK on success.\n-  // Stores NULL in *dbptr and returns a non-OK status on error.\n-  // Caller should delete *dbptr when it is no longer needed.\n-  static Status Open(const Options& options,\n-                     const std::string& name,\n-                     DB** dbptr);\n-\n-  DB() { }\n-  virtual ~DB();\n-\n-  // Set the database entry for \"key\" to \"value\".  Returns OK on success,\n-  // and a non-OK status on error.\n-  // Note: consider setting options.sync = true.\n-  virtual Status Put(const WriteOptions& options,\n-                     const Slice& key,\n-                     const Slice& value) = 0;\n-\n-  // Remove the database entry (if any) for \"key\".  Returns OK on\n-  // success, and a non-OK status on error.  It is not an error if \"key\"\n-  // did not exist in the database.\n-  // Note: consider setting options.sync = true.\n-  virtual Status Delete(const WriteOptions& options, const Slice& key) = 0;\n-\n-  // Apply the specified updates to the database.\n-  // Returns OK on success, non-OK on failure.\n-  // Note: consider setting options.sync = true.\n-  virtual Status Write(const WriteOptions& options, WriteBatch* updates) = 0;\n-\n-  // If the database contains an entry for \"key\" store the\n-  // corresponding value in *value and return OK.\n-  //\n-  // If there is no entry for \"key\" leave *value unchanged and return\n-  // a status for which Status::IsNotFound() returns true.\n-  //\n-  // May return some other Status on an error.\n-  virtual Status Get(const ReadOptions& options,\n-                     const Slice& key, std::string* value) = 0;\n-\n-  // Return a heap-allocated iterator over the contents of the database.\n-  // The result of NewIterator() is initially invalid (caller must\n-  // call one of the Seek methods on the iterator before using it).\n-  //\n-  // Caller should delete the iterator when it is no longer needed.\n-  // The returned iterator should be deleted before this db is deleted.\n-  virtual Iterator* NewIterator(const ReadOptions& options) = 0;\n-\n-  // Return a handle to the current DB state.  Iterators created with\n-  // this handle will all observe a stable snapshot of the current DB\n-  // state.  The caller must call ReleaseSnapshot(result) when the\n-  // snapshot is no longer needed.\n-  virtual const Snapshot* GetSnapshot() = 0;\n-\n-  // Release a previously acquired snapshot.  The caller must not\n-  // use \"snapshot\" after this call.\n-  virtual void ReleaseSnapshot(const Snapshot* snapshot) = 0;\n-\n-  // DB implementations can export properties about their state\n-  // via this method.  If \"property\" is a valid property understood by this\n-  // DB implementation, fills \"*value\" with its current value and returns\n-  // true.  Otherwise returns false.\n-  //\n-  //\n-  // Valid property names include:\n-  //\n-  //  \"leveldb.num-files-at-level<N>\" - return the number of files at level <N>,\n-  //     where <N> is an ASCII representation of a level number (e.g. \"0\").\n-  //  \"leveldb.stats\" - returns a multi-line string that describes statistics\n-  //     about the internal operation of the DB.\n-  //  \"leveldb.sstables\" - returns a multi-line string that describes all\n-  //     of the sstables that make up the db contents.\n-  virtual bool GetProperty(const Slice& property, std::string* value) = 0;\n-\n-  // For each i in [0,n-1], store in \"sizes[i]\", the approximate\n-  // file system space used by keys in \"[range[i].start .. range[i].limit)\".\n-  //\n-  // Note that the returned sizes measure file system space usage, so\n-  // if the user data compresses by a factor of ten, the returned\n-  // sizes will be one-tenth the size of the corresponding user data size.\n-  //\n-  // The results may not include the sizes of recently written data.\n-  virtual void GetApproximateSizes(const Range* range, int n,\n-                                   uint64_t* sizes) = 0;\n-\n-  // Compact the underlying storage for the key range [*begin,*end].\n-  // In particular, deleted and overwritten versions are discarded,\n-  // and the data is rearranged to reduce the cost of operations\n-  // needed to access the data.  This operation should typically only\n-  // be invoked by users who understand the underlying implementation.\n-  //\n-  // begin==NULL is treated as a key before all keys in the database.\n-  // end==NULL is treated as a key after all keys in the database.\n-  // Therefore the following call will compact the entire database:\n-  //    db->CompactRange(NULL, NULL);\n-  virtual void CompactRange(const Slice* begin, const Slice* end) = 0;\n-\n- private:\n-  // No copying allowed\n-  DB(const DB&);\n-  void operator=(const DB&);\n-};\n-\n-// Destroy the contents of the specified database.\n-// Be very careful using this method.\n-Status DestroyDB(const std::string& name, const Options& options);\n-\n-// If a DB cannot be opened, you may attempt to call this method to\n-// resurrect as much of the contents of the database as possible.\n-// Some data may be lost, so be careful when calling this function\n-// on a database that contains important information.\n-Status RepairDB(const std::string& dbname, const Options& options);\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_INCLUDE_DB_H_"
      },
      {
        "sha": "fa32289f581fd4d222dc74ea177a78138b71fbc2",
        "filename": "src/leveldb/include/leveldb/env.h",
        "status": "removed",
        "additions": 0,
        "deletions": 333,
        "changes": 333,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/env.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/env.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/env.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,333 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// An Env is an interface used by the leveldb implementation to access\n-// operating system functionality like the filesystem etc.  Callers\n-// may wish to provide a custom Env object when opening a database to\n-// get fine gain control; e.g., to rate limit file system operations.\n-//\n-// All Env implementations are safe for concurrent access from\n-// multiple threads without any external synchronization.\n-\n-#ifndef STORAGE_LEVELDB_INCLUDE_ENV_H_\n-#define STORAGE_LEVELDB_INCLUDE_ENV_H_\n-\n-#include <cstdarg>\n-#include <string>\n-#include <vector>\n-#include <stdint.h>\n-#include \"leveldb/status.h\"\n-\n-namespace leveldb {\n-\n-class FileLock;\n-class Logger;\n-class RandomAccessFile;\n-class SequentialFile;\n-class Slice;\n-class WritableFile;\n-\n-class Env {\n- public:\n-  Env() { }\n-  virtual ~Env();\n-\n-  // Return a default environment suitable for the current operating\n-  // system.  Sophisticated users may wish to provide their own Env\n-  // implementation instead of relying on this default environment.\n-  //\n-  // The result of Default() belongs to leveldb and must never be deleted.\n-  static Env* Default();\n-\n-  // Create a brand new sequentially-readable file with the specified name.\n-  // On success, stores a pointer to the new file in *result and returns OK.\n-  // On failure stores NULL in *result and returns non-OK.  If the file does\n-  // not exist, returns a non-OK status.\n-  //\n-  // The returned file will only be accessed by one thread at a time.\n-  virtual Status NewSequentialFile(const std::string& fname,\n-                                   SequentialFile** result) = 0;\n-\n-  // Create a brand new random access read-only file with the\n-  // specified name.  On success, stores a pointer to the new file in\n-  // *result and returns OK.  On failure stores NULL in *result and\n-  // returns non-OK.  If the file does not exist, returns a non-OK\n-  // status.\n-  //\n-  // The returned file may be concurrently accessed by multiple threads.\n-  virtual Status NewRandomAccessFile(const std::string& fname,\n-                                     RandomAccessFile** result) = 0;\n-\n-  // Create an object that writes to a new file with the specified\n-  // name.  Deletes any existing file with the same name and creates a\n-  // new file.  On success, stores a pointer to the new file in\n-  // *result and returns OK.  On failure stores NULL in *result and\n-  // returns non-OK.\n-  //\n-  // The returned file will only be accessed by one thread at a time.\n-  virtual Status NewWritableFile(const std::string& fname,\n-                                 WritableFile** result) = 0;\n-\n-  // Returns true iff the named file exists.\n-  virtual bool FileExists(const std::string& fname) = 0;\n-\n-  // Store in *result the names of the children of the specified directory.\n-  // The names are relative to \"dir\".\n-  // Original contents of *results are dropped.\n-  virtual Status GetChildren(const std::string& dir,\n-                             std::vector<std::string>* result) = 0;\n-\n-  // Delete the named file.\n-  virtual Status DeleteFile(const std::string& fname) = 0;\n-\n-  // Create the specified directory.\n-  virtual Status CreateDir(const std::string& dirname) = 0;\n-\n-  // Delete the specified directory.\n-  virtual Status DeleteDir(const std::string& dirname) = 0;\n-\n-  // Store the size of fname in *file_size.\n-  virtual Status GetFileSize(const std::string& fname, uint64_t* file_size) = 0;\n-\n-  // Rename file src to target.\n-  virtual Status RenameFile(const std::string& src,\n-                            const std::string& target) = 0;\n-\n-  // Lock the specified file.  Used to prevent concurrent access to\n-  // the same db by multiple processes.  On failure, stores NULL in\n-  // *lock and returns non-OK.\n-  //\n-  // On success, stores a pointer to the object that represents the\n-  // acquired lock in *lock and returns OK.  The caller should call\n-  // UnlockFile(*lock) to release the lock.  If the process exits,\n-  // the lock will be automatically released.\n-  //\n-  // If somebody else already holds the lock, finishes immediately\n-  // with a failure.  I.e., this call does not wait for existing locks\n-  // to go away.\n-  //\n-  // May create the named file if it does not already exist.\n-  virtual Status LockFile(const std::string& fname, FileLock** lock) = 0;\n-\n-  // Release the lock acquired by a previous successful call to LockFile.\n-  // REQUIRES: lock was returned by a successful LockFile() call\n-  // REQUIRES: lock has not already been unlocked.\n-  virtual Status UnlockFile(FileLock* lock) = 0;\n-\n-  // Arrange to run \"(*function)(arg)\" once in a background thread.\n-  //\n-  // \"function\" may run in an unspecified thread.  Multiple functions\n-  // added to the same Env may run concurrently in different threads.\n-  // I.e., the caller may not assume that background work items are\n-  // serialized.\n-  virtual void Schedule(\n-      void (*function)(void* arg),\n-      void* arg) = 0;\n-\n-  // Start a new thread, invoking \"function(arg)\" within the new thread.\n-  // When \"function(arg)\" returns, the thread will be destroyed.\n-  virtual void StartThread(void (*function)(void* arg), void* arg) = 0;\n-\n-  // *path is set to a temporary directory that can be used for testing. It may\n-  // or many not have just been created. The directory may or may not differ\n-  // between runs of the same process, but subsequent calls will return the\n-  // same directory.\n-  virtual Status GetTestDirectory(std::string* path) = 0;\n-\n-  // Create and return a log file for storing informational messages.\n-  virtual Status NewLogger(const std::string& fname, Logger** result) = 0;\n-\n-  // Returns the number of micro-seconds since some fixed point in time. Only\n-  // useful for computing deltas of time.\n-  virtual uint64_t NowMicros() = 0;\n-\n-  // Sleep/delay the thread for the perscribed number of micro-seconds.\n-  virtual void SleepForMicroseconds(int micros) = 0;\n-\n- private:\n-  // No copying allowed\n-  Env(const Env&);\n-  void operator=(const Env&);\n-};\n-\n-// A file abstraction for reading sequentially through a file\n-class SequentialFile {\n- public:\n-  SequentialFile() { }\n-  virtual ~SequentialFile();\n-\n-  // Read up to \"n\" bytes from the file.  \"scratch[0..n-1]\" may be\n-  // written by this routine.  Sets \"*result\" to the data that was\n-  // read (including if fewer than \"n\" bytes were successfully read).\n-  // May set \"*result\" to point at data in \"scratch[0..n-1]\", so\n-  // \"scratch[0..n-1]\" must be live when \"*result\" is used.\n-  // If an error was encountered, returns a non-OK status.\n-  //\n-  // REQUIRES: External synchronization\n-  virtual Status Read(size_t n, Slice* result, char* scratch) = 0;\n-\n-  // Skip \"n\" bytes from the file. This is guaranteed to be no\n-  // slower that reading the same data, but may be faster.\n-  //\n-  // If end of file is reached, skipping will stop at the end of the\n-  // file, and Skip will return OK.\n-  //\n-  // REQUIRES: External synchronization\n-  virtual Status Skip(uint64_t n) = 0;\n-\n- private:\n-  // No copying allowed\n-  SequentialFile(const SequentialFile&);\n-  void operator=(const SequentialFile&);\n-};\n-\n-// A file abstraction for randomly reading the contents of a file.\n-class RandomAccessFile {\n- public:\n-  RandomAccessFile() { }\n-  virtual ~RandomAccessFile();\n-\n-  // Read up to \"n\" bytes from the file starting at \"offset\".\n-  // \"scratch[0..n-1]\" may be written by this routine.  Sets \"*result\"\n-  // to the data that was read (including if fewer than \"n\" bytes were\n-  // successfully read).  May set \"*result\" to point at data in\n-  // \"scratch[0..n-1]\", so \"scratch[0..n-1]\" must be live when\n-  // \"*result\" is used.  If an error was encountered, returns a non-OK\n-  // status.\n-  //\n-  // Safe for concurrent use by multiple threads.\n-  virtual Status Read(uint64_t offset, size_t n, Slice* result,\n-                      char* scratch) const = 0;\n-\n- private:\n-  // No copying allowed\n-  RandomAccessFile(const RandomAccessFile&);\n-  void operator=(const RandomAccessFile&);\n-};\n-\n-// A file abstraction for sequential writing.  The implementation\n-// must provide buffering since callers may append small fragments\n-// at a time to the file.\n-class WritableFile {\n- public:\n-  WritableFile() { }\n-  virtual ~WritableFile();\n-\n-  virtual Status Append(const Slice& data) = 0;\n-  virtual Status Close() = 0;\n-  virtual Status Flush() = 0;\n-  virtual Status Sync() = 0;\n-\n- private:\n-  // No copying allowed\n-  WritableFile(const WritableFile&);\n-  void operator=(const WritableFile&);\n-};\n-\n-// An interface for writing log messages.\n-class Logger {\n- public:\n-  Logger() { }\n-  virtual ~Logger();\n-\n-  // Write an entry to the log file with the specified format.\n-  virtual void Logv(const char* format, va_list ap) = 0;\n-\n- private:\n-  // No copying allowed\n-  Logger(const Logger&);\n-  void operator=(const Logger&);\n-};\n-\n-\n-// Identifies a locked file.\n-class FileLock {\n- public:\n-  FileLock() { }\n-  virtual ~FileLock();\n- private:\n-  // No copying allowed\n-  FileLock(const FileLock&);\n-  void operator=(const FileLock&);\n-};\n-\n-// Log the specified data to *info_log if info_log is non-NULL.\n-extern void Log(Logger* info_log, const char* format, ...)\n-#   if defined(__GNUC__) || defined(__clang__)\n-    __attribute__((__format__ (__printf__, 2, 3)))\n-#   endif\n-    ;\n-\n-// A utility routine: write \"data\" to the named file.\n-extern Status WriteStringToFile(Env* env, const Slice& data,\n-                                const std::string& fname);\n-\n-// A utility routine: read contents of named file into *data\n-extern Status ReadFileToString(Env* env, const std::string& fname,\n-                               std::string* data);\n-\n-// An implementation of Env that forwards all calls to another Env.\n-// May be useful to clients who wish to override just part of the\n-// functionality of another Env.\n-class EnvWrapper : public Env {\n- public:\n-  // Initialize an EnvWrapper that delegates all calls to *t\n-  explicit EnvWrapper(Env* t) : target_(t) { }\n-  virtual ~EnvWrapper();\n-\n-  // Return the target to which this Env forwards all calls\n-  Env* target() const { return target_; }\n-\n-  // The following text is boilerplate that forwards all methods to target()\n-  Status NewSequentialFile(const std::string& f, SequentialFile** r) {\n-    return target_->NewSequentialFile(f, r);\n-  }\n-  Status NewRandomAccessFile(const std::string& f, RandomAccessFile** r) {\n-    return target_->NewRandomAccessFile(f, r);\n-  }\n-  Status NewWritableFile(const std::string& f, WritableFile** r) {\n-    return target_->NewWritableFile(f, r);\n-  }\n-  bool FileExists(const std::string& f) { return target_->FileExists(f); }\n-  Status GetChildren(const std::string& dir, std::vector<std::string>* r) {\n-    return target_->GetChildren(dir, r);\n-  }\n-  Status DeleteFile(const std::string& f) { return target_->DeleteFile(f); }\n-  Status CreateDir(const std::string& d) { return target_->CreateDir(d); }\n-  Status DeleteDir(const std::string& d) { return target_->DeleteDir(d); }\n-  Status GetFileSize(const std::string& f, uint64_t* s) {\n-    return target_->GetFileSize(f, s);\n-  }\n-  Status RenameFile(const std::string& s, const std::string& t) {\n-    return target_->RenameFile(s, t);\n-  }\n-  Status LockFile(const std::string& f, FileLock** l) {\n-    return target_->LockFile(f, l);\n-  }\n-  Status UnlockFile(FileLock* l) { return target_->UnlockFile(l); }\n-  void Schedule(void (*f)(void*), void* a) {\n-    return target_->Schedule(f, a);\n-  }\n-  void StartThread(void (*f)(void*), void* a) {\n-    return target_->StartThread(f, a);\n-  }\n-  virtual Status GetTestDirectory(std::string* path) {\n-    return target_->GetTestDirectory(path);\n-  }\n-  virtual Status NewLogger(const std::string& fname, Logger** result) {\n-    return target_->NewLogger(fname, result);\n-  }\n-  uint64_t NowMicros() {\n-    return target_->NowMicros();\n-  }\n-  void SleepForMicroseconds(int micros) {\n-    target_->SleepForMicroseconds(micros);\n-  }\n- private:\n-  Env* target_;\n-};\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_INCLUDE_ENV_H_"
      },
      {
        "sha": "1fba08001fc335a14bde90fa5a1c5d58025ae038",
        "filename": "src/leveldb/include/leveldb/filter_policy.h",
        "status": "removed",
        "additions": 0,
        "deletions": 70,
        "changes": 70,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/filter_policy.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/filter_policy.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/filter_policy.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,70 +0,0 @@\n-// Copyright (c) 2012 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// A database can be configured with a custom FilterPolicy object.\n-// This object is responsible for creating a small filter from a set\n-// of keys.  These filters are stored in leveldb and are consulted\n-// automatically by leveldb to decide whether or not to read some\n-// information from disk. In many cases, a filter can cut down the\n-// number of disk seeks form a handful to a single disk seek per\n-// DB::Get() call.\n-//\n-// Most people will want to use the builtin bloom filter support (see\n-// NewBloomFilterPolicy() below).\n-\n-#ifndef STORAGE_LEVELDB_INCLUDE_FILTER_POLICY_H_\n-#define STORAGE_LEVELDB_INCLUDE_FILTER_POLICY_H_\n-\n-#include <string>\n-\n-namespace leveldb {\n-\n-class Slice;\n-\n-class FilterPolicy {\n- public:\n-  virtual ~FilterPolicy();\n-\n-  // Return the name of this policy.  Note that if the filter encoding\n-  // changes in an incompatible way, the name returned by this method\n-  // must be changed.  Otherwise, old incompatible filters may be\n-  // passed to methods of this type.\n-  virtual const char* Name() const = 0;\n-\n-  // keys[0,n-1] contains a list of keys (potentially with duplicates)\n-  // that are ordered according to the user supplied comparator.\n-  // Append a filter that summarizes keys[0,n-1] to *dst.\n-  //\n-  // Warning: do not change the initial contents of *dst.  Instead,\n-  // append the newly constructed filter to *dst.\n-  virtual void CreateFilter(const Slice* keys, int n, std::string* dst)\n-      const = 0;\n-\n-  // \"filter\" contains the data appended by a preceding call to\n-  // CreateFilter() on this class.  This method must return true if\n-  // the key was in the list of keys passed to CreateFilter().\n-  // This method may return true or false if the key was not on the\n-  // list, but it should aim to return false with a high probability.\n-  virtual bool KeyMayMatch(const Slice& key, const Slice& filter) const = 0;\n-};\n-\n-// Return a new filter policy that uses a bloom filter with approximately\n-// the specified number of bits per key.  A good value for bits_per_key\n-// is 10, which yields a filter with ~ 1% false positive rate.\n-//\n-// Callers must delete the result after any database that is using the\n-// result has been closed.\n-//\n-// Note: if you are using a custom comparator that ignores some parts\n-// of the keys being compared, you must not use NewBloomFilterPolicy()\n-// and must provide your own FilterPolicy that also ignores the\n-// corresponding parts of the keys.  For example, if the comparator\n-// ignores trailing spaces, it would be incorrect to use a\n-// FilterPolicy (like NewBloomFilterPolicy) that does not ignore\n-// trailing spaces in keys.\n-extern const FilterPolicy* NewBloomFilterPolicy(int bits_per_key);\n-\n-}\n-\n-#endif  // STORAGE_LEVELDB_INCLUDE_FILTER_POLICY_H_"
      },
      {
        "sha": "ad543eb46cde9af30f9250ee2eaa7f0979cc2994",
        "filename": "src/leveldb/include/leveldb/iterator.h",
        "status": "removed",
        "additions": 0,
        "deletions": 100,
        "changes": 100,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/iterator.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/iterator.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/iterator.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,100 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// An iterator yields a sequence of key/value pairs from a source.\n-// The following class defines the interface.  Multiple implementations\n-// are provided by this library.  In particular, iterators are provided\n-// to access the contents of a Table or a DB.\n-//\n-// Multiple threads can invoke const methods on an Iterator without\n-// external synchronization, but if any of the threads may call a\n-// non-const method, all threads accessing the same Iterator must use\n-// external synchronization.\n-\n-#ifndef STORAGE_LEVELDB_INCLUDE_ITERATOR_H_\n-#define STORAGE_LEVELDB_INCLUDE_ITERATOR_H_\n-\n-#include \"leveldb/slice.h\"\n-#include \"leveldb/status.h\"\n-\n-namespace leveldb {\n-\n-class Iterator {\n- public:\n-  Iterator();\n-  virtual ~Iterator();\n-\n-  // An iterator is either positioned at a key/value pair, or\n-  // not valid.  This method returns true iff the iterator is valid.\n-  virtual bool Valid() const = 0;\n-\n-  // Position at the first key in the source.  The iterator is Valid()\n-  // after this call iff the source is not empty.\n-  virtual void SeekToFirst() = 0;\n-\n-  // Position at the last key in the source.  The iterator is\n-  // Valid() after this call iff the source is not empty.\n-  virtual void SeekToLast() = 0;\n-\n-  // Position at the first key in the source that at or past target\n-  // The iterator is Valid() after this call iff the source contains\n-  // an entry that comes at or past target.\n-  virtual void Seek(const Slice& target) = 0;\n-\n-  // Moves to the next entry in the source.  After this call, Valid() is\n-  // true iff the iterator was not positioned at the last entry in the source.\n-  // REQUIRES: Valid()\n-  virtual void Next() = 0;\n-\n-  // Moves to the previous entry in the source.  After this call, Valid() is\n-  // true iff the iterator was not positioned at the first entry in source.\n-  // REQUIRES: Valid()\n-  virtual void Prev() = 0;\n-\n-  // Return the key for the current entry.  The underlying storage for\n-  // the returned slice is valid only until the next modification of\n-  // the iterator.\n-  // REQUIRES: Valid()\n-  virtual Slice key() const = 0;\n-\n-  // Return the value for the current entry.  The underlying storage for\n-  // the returned slice is valid only until the next modification of\n-  // the iterator.\n-  // REQUIRES: !AtEnd() && !AtStart()\n-  virtual Slice value() const = 0;\n-\n-  // If an error has occurred, return it.  Else return an ok status.\n-  virtual Status status() const = 0;\n-\n-  // Clients are allowed to register function/arg1/arg2 triples that\n-  // will be invoked when this iterator is destroyed.\n-  //\n-  // Note that unlike all of the preceding methods, this method is\n-  // not abstract and therefore clients should not override it.\n-  typedef void (*CleanupFunction)(void* arg1, void* arg2);\n-  void RegisterCleanup(CleanupFunction function, void* arg1, void* arg2);\n-\n- private:\n-  struct Cleanup {\n-    CleanupFunction function;\n-    void* arg1;\n-    void* arg2;\n-    Cleanup* next;\n-  };\n-  Cleanup cleanup_;\n-\n-  // No copying allowed\n-  Iterator(const Iterator&);\n-  void operator=(const Iterator&);\n-};\n-\n-// Return an empty iterator (yields nothing).\n-extern Iterator* NewEmptyIterator();\n-\n-// Return an empty iterator with the specified status.\n-extern Iterator* NewErrorIterator(const Status& status);\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_INCLUDE_ITERATOR_H_"
      },
      {
        "sha": "fdda718d3090638c7378f4418e4d024dd2e68bda",
        "filename": "src/leveldb/include/leveldb/options.h",
        "status": "removed",
        "additions": 0,
        "deletions": 195,
        "changes": 195,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/options.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/options.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/options.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,195 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_INCLUDE_OPTIONS_H_\n-#define STORAGE_LEVELDB_INCLUDE_OPTIONS_H_\n-\n-#include <stddef.h>\n-\n-namespace leveldb {\n-\n-class Cache;\n-class Comparator;\n-class Env;\n-class FilterPolicy;\n-class Logger;\n-class Snapshot;\n-\n-// DB contents are stored in a set of blocks, each of which holds a\n-// sequence of key,value pairs.  Each block may be compressed before\n-// being stored in a file.  The following enum describes which\n-// compression method (if any) is used to compress a block.\n-enum CompressionType {\n-  // NOTE: do not change the values of existing entries, as these are\n-  // part of the persistent format on disk.\n-  kNoCompression     = 0x0,\n-  kSnappyCompression = 0x1\n-};\n-\n-// Options to control the behavior of a database (passed to DB::Open)\n-struct Options {\n-  // -------------------\n-  // Parameters that affect behavior\n-\n-  // Comparator used to define the order of keys in the table.\n-  // Default: a comparator that uses lexicographic byte-wise ordering\n-  //\n-  // REQUIRES: The client must ensure that the comparator supplied\n-  // here has the same name and orders keys *exactly* the same as the\n-  // comparator provided to previous open calls on the same DB.\n-  const Comparator* comparator;\n-\n-  // If true, the database will be created if it is missing.\n-  // Default: false\n-  bool create_if_missing;\n-\n-  // If true, an error is raised if the database already exists.\n-  // Default: false\n-  bool error_if_exists;\n-\n-  // If true, the implementation will do aggressive checking of the\n-  // data it is processing and will stop early if it detects any\n-  // errors.  This may have unforeseen ramifications: for example, a\n-  // corruption of one DB entry may cause a large number of entries to\n-  // become unreadable or for the entire DB to become unopenable.\n-  // Default: false\n-  bool paranoid_checks;\n-\n-  // Use the specified object to interact with the environment,\n-  // e.g. to read/write files, schedule background work, etc.\n-  // Default: Env::Default()\n-  Env* env;\n-\n-  // Any internal progress/error information generated by the db will\n-  // be written to info_log if it is non-NULL, or to a file stored\n-  // in the same directory as the DB contents if info_log is NULL.\n-  // Default: NULL\n-  Logger* info_log;\n-\n-  // -------------------\n-  // Parameters that affect performance\n-\n-  // Amount of data to build up in memory (backed by an unsorted log\n-  // on disk) before converting to a sorted on-disk file.\n-  //\n-  // Larger values increase performance, especially during bulk loads.\n-  // Up to two write buffers may be held in memory at the same time,\n-  // so you may wish to adjust this parameter to control memory usage.\n-  // Also, a larger write buffer will result in a longer recovery time\n-  // the next time the database is opened.\n-  //\n-  // Default: 4MB\n-  size_t write_buffer_size;\n-\n-  // Number of open files that can be used by the DB.  You may need to\n-  // increase this if your database has a large working set (budget\n-  // one open file per 2MB of working set).\n-  //\n-  // Default: 1000\n-  int max_open_files;\n-\n-  // Control over blocks (user data is stored in a set of blocks, and\n-  // a block is the unit of reading from disk).\n-\n-  // If non-NULL, use the specified cache for blocks.\n-  // If NULL, leveldb will automatically create and use an 8MB internal cache.\n-  // Default: NULL\n-  Cache* block_cache;\n-\n-  // Approximate size of user data packed per block.  Note that the\n-  // block size specified here corresponds to uncompressed data.  The\n-  // actual size of the unit read from disk may be smaller if\n-  // compression is enabled.  This parameter can be changed dynamically.\n-  //\n-  // Default: 4K\n-  size_t block_size;\n-\n-  // Number of keys between restart points for delta encoding of keys.\n-  // This parameter can be changed dynamically.  Most clients should\n-  // leave this parameter alone.\n-  //\n-  // Default: 16\n-  int block_restart_interval;\n-\n-  // Compress blocks using the specified compression algorithm.  This\n-  // parameter can be changed dynamically.\n-  //\n-  // Default: kSnappyCompression, which gives lightweight but fast\n-  // compression.\n-  //\n-  // Typical speeds of kSnappyCompression on an Intel(R) Core(TM)2 2.4GHz:\n-  //    ~200-500MB/s compression\n-  //    ~400-800MB/s decompression\n-  // Note that these speeds are significantly faster than most\n-  // persistent storage speeds, and therefore it is typically never\n-  // worth switching to kNoCompression.  Even if the input data is\n-  // incompressible, the kSnappyCompression implementation will\n-  // efficiently detect that and will switch to uncompressed mode.\n-  CompressionType compression;\n-\n-  // If non-NULL, use the specified filter policy to reduce disk reads.\n-  // Many applications will benefit from passing the result of\n-  // NewBloomFilterPolicy() here.\n-  //\n-  // Default: NULL\n-  const FilterPolicy* filter_policy;\n-\n-  // Create an Options object with default values for all fields.\n-  Options();\n-};\n-\n-// Options that control read operations\n-struct ReadOptions {\n-  // If true, all data read from underlying storage will be\n-  // verified against corresponding checksums.\n-  // Default: false\n-  bool verify_checksums;\n-\n-  // Should the data read for this iteration be cached in memory?\n-  // Callers may wish to set this field to false for bulk scans.\n-  // Default: true\n-  bool fill_cache;\n-\n-  // If \"snapshot\" is non-NULL, read as of the supplied snapshot\n-  // (which must belong to the DB that is being read and which must\n-  // not have been released).  If \"snapshot\" is NULL, use an impliicit\n-  // snapshot of the state at the beginning of this read operation.\n-  // Default: NULL\n-  const Snapshot* snapshot;\n-\n-  ReadOptions()\n-      : verify_checksums(false),\n-        fill_cache(true),\n-        snapshot(NULL) {\n-  }\n-};\n-\n-// Options that control write operations\n-struct WriteOptions {\n-  // If true, the write will be flushed from the operating system\n-  // buffer cache (by calling WritableFile::Sync()) before the write\n-  // is considered complete.  If this flag is true, writes will be\n-  // slower.\n-  //\n-  // If this flag is false, and the machine crashes, some recent\n-  // writes may be lost.  Note that if it is just the process that\n-  // crashes (i.e., the machine does not reboot), no writes will be\n-  // lost even if sync==false.\n-  //\n-  // In other words, a DB write with sync==false has similar\n-  // crash semantics as the \"write()\" system call.  A DB write\n-  // with sync==true has similar crash semantics to a \"write()\"\n-  // system call followed by \"fsync()\".\n-  //\n-  // Default: false\n-  bool sync;\n-\n-  WriteOptions()\n-      : sync(false) {\n-  }\n-};\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_INCLUDE_OPTIONS_H_"
      },
      {
        "sha": "74ea8fa49af6782b54ba07528844e665e8ea8095",
        "filename": "src/leveldb/include/leveldb/slice.h",
        "status": "removed",
        "additions": 0,
        "deletions": 109,
        "changes": 109,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/slice.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/slice.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/slice.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,109 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// Slice is a simple structure containing a pointer into some external\n-// storage and a size.  The user of a Slice must ensure that the slice\n-// is not used after the corresponding external storage has been\n-// deallocated.\n-//\n-// Multiple threads can invoke const methods on a Slice without\n-// external synchronization, but if any of the threads may call a\n-// non-const method, all threads accessing the same Slice must use\n-// external synchronization.\n-\n-#ifndef STORAGE_LEVELDB_INCLUDE_SLICE_H_\n-#define STORAGE_LEVELDB_INCLUDE_SLICE_H_\n-\n-#include <assert.h>\n-#include <stddef.h>\n-#include <string.h>\n-#include <string>\n-\n-namespace leveldb {\n-\n-class Slice {\n- public:\n-  // Create an empty slice.\n-  Slice() : data_(\"\"), size_(0) { }\n-\n-  // Create a slice that refers to d[0,n-1].\n-  Slice(const char* d, size_t n) : data_(d), size_(n) { }\n-\n-  // Create a slice that refers to the contents of \"s\"\n-  Slice(const std::string& s) : data_(s.data()), size_(s.size()) { }\n-\n-  // Create a slice that refers to s[0,strlen(s)-1]\n-  Slice(const char* s) : data_(s), size_(strlen(s)) { }\n-\n-  // Return a pointer to the beginning of the referenced data\n-  const char* data() const { return data_; }\n-\n-  // Return the length (in bytes) of the referenced data\n-  size_t size() const { return size_; }\n-\n-  // Return true iff the length of the referenced data is zero\n-  bool empty() const { return size_ == 0; }\n-\n-  // Return the ith byte in the referenced data.\n-  // REQUIRES: n < size()\n-  char operator[](size_t n) const {\n-    assert(n < size());\n-    return data_[n];\n-  }\n-\n-  // Change this slice to refer to an empty array\n-  void clear() { data_ = \"\"; size_ = 0; }\n-\n-  // Drop the first \"n\" bytes from this slice.\n-  void remove_prefix(size_t n) {\n-    assert(n <= size());\n-    data_ += n;\n-    size_ -= n;\n-  }\n-\n-  // Return a string that contains the copy of the referenced data.\n-  std::string ToString() const { return std::string(data_, size_); }\n-\n-  // Three-way comparison.  Returns value:\n-  //   <  0 iff \"*this\" <  \"b\",\n-  //   == 0 iff \"*this\" == \"b\",\n-  //   >  0 iff \"*this\" >  \"b\"\n-  int compare(const Slice& b) const;\n-\n-  // Return true iff \"x\" is a prefix of \"*this\"\n-  bool starts_with(const Slice& x) const {\n-    return ((size_ >= x.size_) &&\n-            (memcmp(data_, x.data_, x.size_) == 0));\n-  }\n-\n- private:\n-  const char* data_;\n-  size_t size_;\n-\n-  // Intentionally copyable\n-};\n-\n-inline bool operator==(const Slice& x, const Slice& y) {\n-  return ((x.size() == y.size()) &&\n-          (memcmp(x.data(), y.data(), x.size()) == 0));\n-}\n-\n-inline bool operator!=(const Slice& x, const Slice& y) {\n-  return !(x == y);\n-}\n-\n-inline int Slice::compare(const Slice& b) const {\n-  const int min_len = (size_ < b.size_) ? size_ : b.size_;\n-  int r = memcmp(data_, b.data_, min_len);\n-  if (r == 0) {\n-    if (size_ < b.size_) r = -1;\n-    else if (size_ > b.size_) r = +1;\n-  }\n-  return r;\n-}\n-\n-}  // namespace leveldb\n-\n-\n-#endif  // STORAGE_LEVELDB_INCLUDE_SLICE_H_"
      },
      {
        "sha": "11dbd4b47ed3883b7dd5092c21685441f6000c26",
        "filename": "src/leveldb/include/leveldb/status.h",
        "status": "removed",
        "additions": 0,
        "deletions": 106,
        "changes": 106,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/status.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/status.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/status.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,106 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// A Status encapsulates the result of an operation.  It may indicate success,\n-// or it may indicate an error with an associated error message.\n-//\n-// Multiple threads can invoke const methods on a Status without\n-// external synchronization, but if any of the threads may call a\n-// non-const method, all threads accessing the same Status must use\n-// external synchronization.\n-\n-#ifndef STORAGE_LEVELDB_INCLUDE_STATUS_H_\n-#define STORAGE_LEVELDB_INCLUDE_STATUS_H_\n-\n-#include <string>\n-#include \"leveldb/slice.h\"\n-\n-namespace leveldb {\n-\n-class Status {\n- public:\n-  // Create a success status.\n-  Status() : state_(NULL) { }\n-  ~Status() { delete[] state_; }\n-\n-  // Copy the specified status.\n-  Status(const Status& s);\n-  void operator=(const Status& s);\n-\n-  // Return a success status.\n-  static Status OK() { return Status(); }\n-\n-  // Return error status of an appropriate type.\n-  static Status NotFound(const Slice& msg, const Slice& msg2 = Slice()) {\n-    return Status(kNotFound, msg, msg2);\n-  }\n-  static Status Corruption(const Slice& msg, const Slice& msg2 = Slice()) {\n-    return Status(kCorruption, msg, msg2);\n-  }\n-  static Status NotSupported(const Slice& msg, const Slice& msg2 = Slice()) {\n-    return Status(kNotSupported, msg, msg2);\n-  }\n-  static Status InvalidArgument(const Slice& msg, const Slice& msg2 = Slice()) {\n-    return Status(kInvalidArgument, msg, msg2);\n-  }\n-  static Status IOError(const Slice& msg, const Slice& msg2 = Slice()) {\n-    return Status(kIOError, msg, msg2);\n-  }\n-\n-  // Returns true iff the status indicates success.\n-  bool ok() const { return (state_ == NULL); }\n-\n-  // Returns true iff the status indicates a NotFound error.\n-  bool IsNotFound() const { return code() == kNotFound; }\n-\n-  // Returns true iff the status indicates a Corruption error.\n-  bool IsCorruption() const { return code() == kCorruption; }\n-\n-  // Returns true iff the status indicates an IOError.\n-  bool IsIOError() const { return code() == kIOError; }\n-\n-  // Return a string representation of this status suitable for printing.\n-  // Returns the string \"OK\" for success.\n-  std::string ToString() const;\n-\n- private:\n-  // OK status has a NULL state_.  Otherwise, state_ is a new[] array\n-  // of the following form:\n-  //    state_[0..3] == length of message\n-  //    state_[4]    == code\n-  //    state_[5..]  == message\n-  const char* state_;\n-\n-  enum Code {\n-    kOk = 0,\n-    kNotFound = 1,\n-    kCorruption = 2,\n-    kNotSupported = 3,\n-    kInvalidArgument = 4,\n-    kIOError = 5\n-  };\n-\n-  Code code() const {\n-    return (state_ == NULL) ? kOk : static_cast<Code>(state_[4]);\n-  }\n-\n-  Status(Code code, const Slice& msg, const Slice& msg2);\n-  static const char* CopyState(const char* s);\n-};\n-\n-inline Status::Status(const Status& s) {\n-  state_ = (s.state_ == NULL) ? NULL : CopyState(s.state_);\n-}\n-inline void Status::operator=(const Status& s) {\n-  // The following condition catches both aliasing (when this == &s),\n-  // and the common case where both s and *this are ok.\n-  if (state_ != s.state_) {\n-    delete[] state_;\n-    state_ = (s.state_ == NULL) ? NULL : CopyState(s.state_);\n-  }\n-}\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_INCLUDE_STATUS_H_"
      },
      {
        "sha": "a9746c3f5ea90250d8bde12d9ec7e9091fd5bd51",
        "filename": "src/leveldb/include/leveldb/table.h",
        "status": "removed",
        "additions": 0,
        "deletions": 85,
        "changes": 85,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/table.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/table.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/table.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,85 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_INCLUDE_TABLE_H_\n-#define STORAGE_LEVELDB_INCLUDE_TABLE_H_\n-\n-#include <stdint.h>\n-#include \"leveldb/iterator.h\"\n-\n-namespace leveldb {\n-\n-class Block;\n-class BlockHandle;\n-class Footer;\n-struct Options;\n-class RandomAccessFile;\n-struct ReadOptions;\n-class TableCache;\n-\n-// A Table is a sorted map from strings to strings.  Tables are\n-// immutable and persistent.  A Table may be safely accessed from\n-// multiple threads without external synchronization.\n-class Table {\n- public:\n-  // Attempt to open the table that is stored in bytes [0..file_size)\n-  // of \"file\", and read the metadata entries necessary to allow\n-  // retrieving data from the table.\n-  //\n-  // If successful, returns ok and sets \"*table\" to the newly opened\n-  // table.  The client should delete \"*table\" when no longer needed.\n-  // If there was an error while initializing the table, sets \"*table\"\n-  // to NULL and returns a non-ok status.  Does not take ownership of\n-  // \"*source\", but the client must ensure that \"source\" remains live\n-  // for the duration of the returned table's lifetime.\n-  //\n-  // *file must remain live while this Table is in use.\n-  static Status Open(const Options& options,\n-                     RandomAccessFile* file,\n-                     uint64_t file_size,\n-                     Table** table);\n-\n-  ~Table();\n-\n-  // Returns a new iterator over the table contents.\n-  // The result of NewIterator() is initially invalid (caller must\n-  // call one of the Seek methods on the iterator before using it).\n-  Iterator* NewIterator(const ReadOptions&) const;\n-\n-  // Given a key, return an approximate byte offset in the file where\n-  // the data for that key begins (or would begin if the key were\n-  // present in the file).  The returned value is in terms of file\n-  // bytes, and so includes effects like compression of the underlying data.\n-  // E.g., the approximate offset of the last key in the table will\n-  // be close to the file length.\n-  uint64_t ApproximateOffsetOf(const Slice& key) const;\n-\n- private:\n-  struct Rep;\n-  Rep* rep_;\n-\n-  explicit Table(Rep* rep) { rep_ = rep; }\n-  static Iterator* BlockReader(void*, const ReadOptions&, const Slice&);\n-\n-  // Calls (*handle_result)(arg, ...) with the entry found after a call\n-  // to Seek(key).  May not make such a call if filter policy says\n-  // that key is not present.\n-  friend class TableCache;\n-  Status InternalGet(\n-      const ReadOptions&, const Slice& key,\n-      void* arg,\n-      void (*handle_result)(void* arg, const Slice& k, const Slice& v));\n-\n-\n-  void ReadMeta(const Footer& footer);\n-  void ReadFilter(const Slice& filter_handle_value);\n-\n-  // No copying allowed\n-  Table(const Table&);\n-  void operator=(const Table&);\n-};\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_INCLUDE_TABLE_H_"
      },
      {
        "sha": "5fd1dc71f1cb7541ef62397b6795946ad8c20652",
        "filename": "src/leveldb/include/leveldb/table_builder.h",
        "status": "removed",
        "additions": 0,
        "deletions": 92,
        "changes": 92,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/table_builder.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/table_builder.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/table_builder.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,92 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// TableBuilder provides the interface used to build a Table\n-// (an immutable and sorted map from keys to values).\n-//\n-// Multiple threads can invoke const methods on a TableBuilder without\n-// external synchronization, but if any of the threads may call a\n-// non-const method, all threads accessing the same TableBuilder must use\n-// external synchronization.\n-\n-#ifndef STORAGE_LEVELDB_INCLUDE_TABLE_BUILDER_H_\n-#define STORAGE_LEVELDB_INCLUDE_TABLE_BUILDER_H_\n-\n-#include <stdint.h>\n-#include \"leveldb/options.h\"\n-#include \"leveldb/status.h\"\n-\n-namespace leveldb {\n-\n-class BlockBuilder;\n-class BlockHandle;\n-class WritableFile;\n-\n-class TableBuilder {\n- public:\n-  // Create a builder that will store the contents of the table it is\n-  // building in *file.  Does not close the file.  It is up to the\n-  // caller to close the file after calling Finish().\n-  TableBuilder(const Options& options, WritableFile* file);\n-\n-  // REQUIRES: Either Finish() or Abandon() has been called.\n-  ~TableBuilder();\n-\n-  // Change the options used by this builder.  Note: only some of the\n-  // option fields can be changed after construction.  If a field is\n-  // not allowed to change dynamically and its value in the structure\n-  // passed to the constructor is different from its value in the\n-  // structure passed to this method, this method will return an error\n-  // without changing any fields.\n-  Status ChangeOptions(const Options& options);\n-\n-  // Add key,value to the table being constructed.\n-  // REQUIRES: key is after any previously added key according to comparator.\n-  // REQUIRES: Finish(), Abandon() have not been called\n-  void Add(const Slice& key, const Slice& value);\n-\n-  // Advanced operation: flush any buffered key/value pairs to file.\n-  // Can be used to ensure that two adjacent entries never live in\n-  // the same data block.  Most clients should not need to use this method.\n-  // REQUIRES: Finish(), Abandon() have not been called\n-  void Flush();\n-\n-  // Return non-ok iff some error has been detected.\n-  Status status() const;\n-\n-  // Finish building the table.  Stops using the file passed to the\n-  // constructor after this function returns.\n-  // REQUIRES: Finish(), Abandon() have not been called\n-  Status Finish();\n-\n-  // Indicate that the contents of this builder should be abandoned.  Stops\n-  // using the file passed to the constructor after this function returns.\n-  // If the caller is not going to call Finish(), it must call Abandon()\n-  // before destroying this builder.\n-  // REQUIRES: Finish(), Abandon() have not been called\n-  void Abandon();\n-\n-  // Number of calls to Add() so far.\n-  uint64_t NumEntries() const;\n-\n-  // Size of the file generated so far.  If invoked after a successful\n-  // Finish() call, returns the size of the final generated file.\n-  uint64_t FileSize() const;\n-\n- private:\n-  bool ok() const { return status().ok(); }\n-  void WriteBlock(BlockBuilder* block, BlockHandle* handle);\n-  void WriteRawBlock(const Slice& data, CompressionType, BlockHandle* handle);\n-\n-  struct Rep;\n-  Rep* rep_;\n-\n-  // No copying allowed\n-  TableBuilder(const TableBuilder&);\n-  void operator=(const TableBuilder&);\n-};\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_INCLUDE_TABLE_BUILDER_H_"
      },
      {
        "sha": "ee9aab68e0d83dc4d94835ee21cf926c1ff0c0db",
        "filename": "src/leveldb/include/leveldb/write_batch.h",
        "status": "removed",
        "additions": 0,
        "deletions": 64,
        "changes": 64,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/write_batch.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/include/leveldb/write_batch.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/write_batch.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,64 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// WriteBatch holds a collection of updates to apply atomically to a DB.\n-//\n-// The updates are applied in the order in which they are added\n-// to the WriteBatch.  For example, the value of \"key\" will be \"v3\"\n-// after the following batch is written:\n-//\n-//    batch.Put(\"key\", \"v1\");\n-//    batch.Delete(\"key\");\n-//    batch.Put(\"key\", \"v2\");\n-//    batch.Put(\"key\", \"v3\");\n-//\n-// Multiple threads can invoke const methods on a WriteBatch without\n-// external synchronization, but if any of the threads may call a\n-// non-const method, all threads accessing the same WriteBatch must use\n-// external synchronization.\n-\n-#ifndef STORAGE_LEVELDB_INCLUDE_WRITE_BATCH_H_\n-#define STORAGE_LEVELDB_INCLUDE_WRITE_BATCH_H_\n-\n-#include <string>\n-#include \"leveldb/status.h\"\n-\n-namespace leveldb {\n-\n-class Slice;\n-\n-class WriteBatch {\n- public:\n-  WriteBatch();\n-  ~WriteBatch();\n-\n-  // Store the mapping \"key->value\" in the database.\n-  void Put(const Slice& key, const Slice& value);\n-\n-  // If the database contains a mapping for \"key\", erase it.  Else do nothing.\n-  void Delete(const Slice& key);\n-\n-  // Clear all updates buffered in this batch.\n-  void Clear();\n-\n-  // Support for iterating over the contents of a batch.\n-  class Handler {\n-   public:\n-    virtual ~Handler();\n-    virtual void Put(const Slice& key, const Slice& value) = 0;\n-    virtual void Delete(const Slice& key) = 0;\n-  };\n-  Status Iterate(Handler* handler) const;\n-\n- private:\n-  friend class WriteBatchInternal;\n-\n-  std::string rep_;  // See comment in write_batch.cc for the format of rep_\n-\n-  // Intentionally copyable\n-};\n-\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_INCLUDE_WRITE_BATCH_H_"
      },
      {
        "sha": "422563e25ce0230d92373637799da8d28ab60223",
        "filename": "src/leveldb/port/README",
        "status": "removed",
        "additions": 0,
        "deletions": 10,
        "changes": 10,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/README",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/README",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/README?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,10 +0,0 @@\n-This directory contains interfaces and implementations that isolate the\n-rest of the package from platform details.\n-\n-Code in the rest of the package includes \"port.h\" from this directory.\n-\"port.h\" in turn includes a platform specific \"port_<platform>.h\" file\n-that provides the platform specific implementation.\n-\n-See port_posix.h for an example of what must be provided in a platform\n-specific header file.\n-"
      },
      {
        "sha": "e17bf435eab33e55a87046e705a1ecb04dad7e24",
        "filename": "src/leveldb/port/atomic_pointer.h",
        "status": "removed",
        "additions": 0,
        "deletions": 224,
        "changes": 224,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/atomic_pointer.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/atomic_pointer.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/atomic_pointer.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,224 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-// AtomicPointer provides storage for a lock-free pointer.\n-// Platform-dependent implementation of AtomicPointer:\n-// - If the platform provides a cheap barrier, we use it with raw pointers\n-// - If cstdatomic is present (on newer versions of gcc, it is), we use\n-//   a cstdatomic-based AtomicPointer.  However we prefer the memory\n-//   barrier based version, because at least on a gcc 4.4 32-bit build\n-//   on linux, we have encountered a buggy <cstdatomic>\n-//   implementation.  Also, some <cstdatomic> implementations are much\n-//   slower than a memory-barrier based implementation (~16ns for\n-//   <cstdatomic> based acquire-load vs. ~1ns for a barrier based\n-//   acquire-load).\n-// This code is based on atomicops-internals-* in Google's perftools:\n-// http://code.google.com/p/google-perftools/source/browse/#svn%2Ftrunk%2Fsrc%2Fbase\n-\n-#ifndef PORT_ATOMIC_POINTER_H_\n-#define PORT_ATOMIC_POINTER_H_\n-\n-#include <stdint.h>\n-#ifdef LEVELDB_CSTDATOMIC_PRESENT\n-#include <cstdatomic>\n-#endif\n-#ifdef OS_WIN\n-#include <windows.h>\n-#endif\n-#ifdef OS_MACOSX\n-#include <libkern/OSAtomic.h>\n-#endif\n-\n-#if defined(_M_X64) || defined(__x86_64__)\n-#define ARCH_CPU_X86_FAMILY 1\n-#elif defined(_M_IX86) || defined(__i386__) || defined(__i386)\n-#define ARCH_CPU_X86_FAMILY 1\n-#elif defined(__ARMEL__)\n-#define ARCH_CPU_ARM_FAMILY 1\n-#elif defined(__ppc__) || defined(__powerpc__) || defined(__powerpc64__)\n-#define ARCH_CPU_PPC_FAMILY 1\n-#endif\n-\n-namespace leveldb {\n-namespace port {\n-\n-// Define MemoryBarrier() if available\n-// Windows on x86\n-#if defined(OS_WIN) && defined(COMPILER_MSVC) && defined(ARCH_CPU_X86_FAMILY)\n-// windows.h already provides a MemoryBarrier(void) macro\n-// http://msdn.microsoft.com/en-us/library/ms684208(v=vs.85).aspx\n-#define LEVELDB_HAVE_MEMORY_BARRIER\n-\n-// Gcc on x86\n-#elif defined(ARCH_CPU_X86_FAMILY) && defined(__GNUC__)\n-inline void MemoryBarrier() {\n-  // See http://gcc.gnu.org/ml/gcc/2003-04/msg01180.html for a discussion on\n-  // this idiom. Also see http://en.wikipedia.org/wiki/Memory_ordering.\n-  __asm__ __volatile__(\"\" : : : \"memory\");\n-}\n-#define LEVELDB_HAVE_MEMORY_BARRIER\n-\n-// Sun Studio\n-#elif defined(ARCH_CPU_X86_FAMILY) && defined(__SUNPRO_CC)\n-inline void MemoryBarrier() {\n-  // See http://gcc.gnu.org/ml/gcc/2003-04/msg01180.html for a discussion on\n-  // this idiom. Also see http://en.wikipedia.org/wiki/Memory_ordering.\n-  asm volatile(\"\" : : : \"memory\");\n-}\n-#define LEVELDB_HAVE_MEMORY_BARRIER\n-\n-// Mac OS\n-#elif defined(OS_MACOSX)\n-inline void MemoryBarrier() {\n-  OSMemoryBarrier();\n-}\n-#define LEVELDB_HAVE_MEMORY_BARRIER\n-\n-// ARM Linux\n-#elif defined(ARCH_CPU_ARM_FAMILY) && defined(__linux__)\n-typedef void (*LinuxKernelMemoryBarrierFunc)(void);\n-// The Linux ARM kernel provides a highly optimized device-specific memory\n-// barrier function at a fixed memory address that is mapped in every\n-// user-level process.\n-//\n-// This beats using CPU-specific instructions which are, on single-core\n-// devices, un-necessary and very costly (e.g. ARMv7-A \"dmb\" takes more\n-// than 180ns on a Cortex-A8 like the one on a Nexus One). Benchmarking\n-// shows that the extra function call cost is completely negligible on\n-// multi-core devices.\n-//\n-inline void MemoryBarrier() {\n-  (*(LinuxKernelMemoryBarrierFunc)0xffff0fa0)();\n-}\n-#define LEVELDB_HAVE_MEMORY_BARRIER\n-\n-// PPC\n-#elif defined(ARCH_CPU_PPC_FAMILY) && defined(__GNUC__)\n-inline void MemoryBarrier() {\n-  // TODO for some powerpc expert: is there a cheaper suitable variant?\n-  // Perhaps by having separate barriers for acquire and release ops.\n-  asm volatile(\"sync\" : : : \"memory\");\n-}\n-#define LEVELDB_HAVE_MEMORY_BARRIER\n-\n-#endif\n-\n-// AtomicPointer built using platform-specific MemoryBarrier()\n-#if defined(LEVELDB_HAVE_MEMORY_BARRIER)\n-class AtomicPointer {\n- private:\n-  void* rep_;\n- public:\n-  AtomicPointer() { }\n-  explicit AtomicPointer(void* p) : rep_(p) {}\n-  inline void* NoBarrier_Load() const { return rep_; }\n-  inline void NoBarrier_Store(void* v) { rep_ = v; }\n-  inline void* Acquire_Load() const {\n-    void* result = rep_;\n-    MemoryBarrier();\n-    return result;\n-  }\n-  inline void Release_Store(void* v) {\n-    MemoryBarrier();\n-    rep_ = v;\n-  }\n-};\n-\n-// AtomicPointer based on <cstdatomic>\n-#elif defined(LEVELDB_CSTDATOMIC_PRESENT)\n-class AtomicPointer {\n- private:\n-  std::atomic<void*> rep_;\n- public:\n-  AtomicPointer() { }\n-  explicit AtomicPointer(void* v) : rep_(v) { }\n-  inline void* Acquire_Load() const {\n-    return rep_.load(std::memory_order_acquire);\n-  }\n-  inline void Release_Store(void* v) {\n-    rep_.store(v, std::memory_order_release);\n-  }\n-  inline void* NoBarrier_Load() const {\n-    return rep_.load(std::memory_order_relaxed);\n-  }\n-  inline void NoBarrier_Store(void* v) {\n-    rep_.store(v, std::memory_order_relaxed);\n-  }\n-};\n-\n-// Atomic pointer based on sparc memory barriers\n-#elif defined(__sparcv9) && defined(__GNUC__)\n-class AtomicPointer {\n- private:\n-  void* rep_;\n- public:\n-  AtomicPointer() { }\n-  explicit AtomicPointer(void* v) : rep_(v) { }\n-  inline void* Acquire_Load() const {\n-    void* val;\n-    __asm__ __volatile__ (\n-        \"ldx [%[rep_]], %[val] \\n\\t\"\n-         \"membar #LoadLoad|#LoadStore \\n\\t\"\n-        : [val] \"=r\" (val)\n-        : [rep_] \"r\" (&rep_)\n-        : \"memory\");\n-    return val;\n-  }\n-  inline void Release_Store(void* v) {\n-    __asm__ __volatile__ (\n-        \"membar #LoadStore|#StoreStore \\n\\t\"\n-        \"stx %[v], [%[rep_]] \\n\\t\"\n-        :\n-        : [rep_] \"r\" (&rep_), [v] \"r\" (v)\n-        : \"memory\");\n-  }\n-  inline void* NoBarrier_Load() const { return rep_; }\n-  inline void NoBarrier_Store(void* v) { rep_ = v; }\n-};\n-\n-// Atomic pointer based on ia64 acq/rel\n-#elif defined(__ia64) && defined(__GNUC__)\n-class AtomicPointer {\n- private:\n-  void* rep_;\n- public:\n-  AtomicPointer() { }\n-  explicit AtomicPointer(void* v) : rep_(v) { }\n-  inline void* Acquire_Load() const {\n-    void* val    ;\n-    __asm__ __volatile__ (\n-        \"ld8.acq %[val] = [%[rep_]] \\n\\t\"\n-        : [val] \"=r\" (val)\n-        : [rep_] \"r\" (&rep_)\n-        : \"memory\"\n-        );\n-    return val;\n-  }\n-  inline void Release_Store(void* v) {\n-    __asm__ __volatile__ (\n-        \"st8.rel [%[rep_]] = %[v]  \\n\\t\"\n-        :\n-        : [rep_] \"r\" (&rep_), [v] \"r\" (v)\n-        : \"memory\"\n-        );\n-  }\n-  inline void* NoBarrier_Load() const { return rep_; }\n-  inline void NoBarrier_Store(void* v) { rep_ = v; }\n-};\n-\n-// We have neither MemoryBarrier(), nor <cstdatomic>\n-#else\n-#error Please implement AtomicPointer for this platform.\n-\n-#endif\n-\n-#undef LEVELDB_HAVE_MEMORY_BARRIER\n-#undef ARCH_CPU_X86_FAMILY\n-#undef ARCH_CPU_ARM_FAMILY\n-#undef ARCH_CPU_PPC_FAMILY\n-\n-}  // namespace port\n-}  // namespace leveldb\n-\n-#endif  // PORT_ATOMIC_POINTER_H_"
      },
      {
        "sha": "4baafa8e22fd290cfd73ad4daf0b5245e0d109c1",
        "filename": "src/leveldb/port/port.h",
        "status": "removed",
        "additions": 0,
        "deletions": 21,
        "changes": 21,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/port.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/port.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/port.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,21 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_PORT_PORT_H_\n-#define STORAGE_LEVELDB_PORT_PORT_H_\n-\n-#include <string.h>\n-\n-// Include the appropriate platform specific file below.  If you are\n-// porting to a new platform, see \"port_example.h\" for documentation\n-// of what the new port_<platform>.h file must provide.\n-#if defined(LEVELDB_PLATFORM_POSIX)\n-#  include \"port/port_posix.h\"\n-#elif defined(LEVELDB_PLATFORM_CHROMIUM)\n-#  include \"port/port_chromium.h\"\n-#elif defined(LEVELDB_PLATFORM_WINDOWS)\n-#  include \"port/port_win.h\"\n-#endif\n-\n-#endif  // STORAGE_LEVELDB_PORT_PORT_H_"
      },
      {
        "sha": "ab9e489b32d8eb4ec8a43da07a20ad917fb35a1b",
        "filename": "src/leveldb/port/port_example.h",
        "status": "removed",
        "additions": 0,
        "deletions": 135,
        "changes": 135,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/port_example.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/port_example.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/port_example.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,135 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// This file contains the specification, but not the implementations,\n-// of the types/operations/etc. that should be defined by a platform\n-// specific port_<platform>.h file.  Use this file as a reference for\n-// how to port this package to a new platform.\n-\n-#ifndef STORAGE_LEVELDB_PORT_PORT_EXAMPLE_H_\n-#define STORAGE_LEVELDB_PORT_PORT_EXAMPLE_H_\n-\n-namespace leveldb {\n-namespace port {\n-\n-// TODO(jorlow): Many of these belong more in the environment class rather than\n-//               here. We should try moving them and see if it affects perf.\n-\n-// The following boolean constant must be true on a little-endian machine\n-// and false otherwise.\n-static const bool kLittleEndian = true /* or some other expression */;\n-\n-// ------------------ Threading -------------------\n-\n-// A Mutex represents an exclusive lock.\n-class Mutex {\n- public:\n-  Mutex();\n-  ~Mutex();\n-\n-  // Lock the mutex.  Waits until other lockers have exited.\n-  // Will deadlock if the mutex is already locked by this thread.\n-  void Lock();\n-\n-  // Unlock the mutex.\n-  // REQUIRES: This mutex was locked by this thread.\n-  void Unlock();\n-\n-  // Optionally crash if this thread does not hold this mutex.\n-  // The implementation must be fast, especially if NDEBUG is\n-  // defined.  The implementation is allowed to skip all checks.\n-  void AssertHeld();\n-};\n-\n-class CondVar {\n- public:\n-  explicit CondVar(Mutex* mu);\n-  ~CondVar();\n-\n-  // Atomically release *mu and block on this condition variable until\n-  // either a call to SignalAll(), or a call to Signal() that picks\n-  // this thread to wakeup.\n-  // REQUIRES: this thread holds *mu\n-  void Wait();\n-\n-  // If there are some threads waiting, wake up at least one of them.\n-  void Signal();\n-\n-  // Wake up all waiting threads.\n-  void SignallAll();\n-};\n-\n-// Thread-safe initialization.\n-// Used as follows:\n-//      static port::OnceType init_control = LEVELDB_ONCE_INIT;\n-//      static void Initializer() { ... do something ...; }\n-//      ...\n-//      port::InitOnce(&init_control, &Initializer);\n-typedef intptr_t OnceType;\n-#define LEVELDB_ONCE_INIT 0\n-extern void InitOnce(port::OnceType*, void (*initializer)());\n-\n-// A type that holds a pointer that can be read or written atomically\n-// (i.e., without word-tearing.)\n-class AtomicPointer {\n- private:\n-  intptr_t rep_;\n- public:\n-  // Initialize to arbitrary value\n-  AtomicPointer();\n-\n-  // Initialize to hold v\n-  explicit AtomicPointer(void* v) : rep_(v) { }\n-\n-  // Read and return the stored pointer with the guarantee that no\n-  // later memory access (read or write) by this thread can be\n-  // reordered ahead of this read.\n-  void* Acquire_Load() const;\n-\n-  // Set v as the stored pointer with the guarantee that no earlier\n-  // memory access (read or write) by this thread can be reordered\n-  // after this store.\n-  void Release_Store(void* v);\n-\n-  // Read the stored pointer with no ordering guarantees.\n-  void* NoBarrier_Load() const;\n-\n-  // Set va as the stored pointer with no ordering guarantees.\n-  void NoBarrier_Store(void* v);\n-};\n-\n-// ------------------ Compression -------------------\n-\n-// Store the snappy compression of \"input[0,input_length-1]\" in *output.\n-// Returns false if snappy is not supported by this port.\n-extern bool Snappy_Compress(const char* input, size_t input_length,\n-                            std::string* output);\n-\n-// If input[0,input_length-1] looks like a valid snappy compressed\n-// buffer, store the size of the uncompressed data in *result and\n-// return true.  Else return false.\n-extern bool Snappy_GetUncompressedLength(const char* input, size_t length,\n-                                         size_t* result);\n-\n-// Attempt to snappy uncompress input[0,input_length-1] into *output.\n-// Returns true if successful, false if the input is invalid lightweight\n-// compressed data.\n-//\n-// REQUIRES: at least the first \"n\" bytes of output[] must be writable\n-// where \"n\" is the result of a successful call to\n-// Snappy_GetUncompressedLength.\n-extern bool Snappy_Uncompress(const char* input_data, size_t input_length,\n-                              char* output);\n-\n-// ------------------ Miscellaneous -------------------\n-\n-// If heap profiling is not supported, returns false.\n-// Else repeatedly calls (*func)(arg, data, n) and then returns true.\n-// The concatenation of all \"data[0,n-1]\" fragments is the heap profile.\n-extern bool GetHeapProfile(void (*func)(void*, const char*, int), void* arg);\n-\n-}  // namespace port\n-}  // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_PORT_PORT_EXAMPLE_H_"
      },
      {
        "sha": "5ba127a5b91bfa036189aa29ee2aeb9b02a034d8",
        "filename": "src/leveldb/port/port_posix.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 54,
        "changes": 54,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/port_posix.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/port_posix.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/port_posix.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,54 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#include \"port/port_posix.h\"\n-\n-#include <cstdlib>\n-#include <stdio.h>\n-#include <string.h>\n-#include \"util/logging.h\"\n-\n-namespace leveldb {\n-namespace port {\n-\n-static void PthreadCall(const char* label, int result) {\n-  if (result != 0) {\n-    fprintf(stderr, \"pthread %s: %s\\n\", label, strerror(result));\n-    abort();\n-  }\n-}\n-\n-Mutex::Mutex() { PthreadCall(\"init mutex\", pthread_mutex_init(&mu_, NULL)); }\n-\n-Mutex::~Mutex() { PthreadCall(\"destroy mutex\", pthread_mutex_destroy(&mu_)); }\n-\n-void Mutex::Lock() { PthreadCall(\"lock\", pthread_mutex_lock(&mu_)); }\n-\n-void Mutex::Unlock() { PthreadCall(\"unlock\", pthread_mutex_unlock(&mu_)); }\n-\n-CondVar::CondVar(Mutex* mu)\n-    : mu_(mu) {\n-    PthreadCall(\"init cv\", pthread_cond_init(&cv_, NULL));\n-}\n-\n-CondVar::~CondVar() { PthreadCall(\"destroy cv\", pthread_cond_destroy(&cv_)); }\n-\n-void CondVar::Wait() {\n-  PthreadCall(\"wait\", pthread_cond_wait(&cv_, &mu_->mu_));\n-}\n-\n-void CondVar::Signal() {\n-  PthreadCall(\"signal\", pthread_cond_signal(&cv_));\n-}\n-\n-void CondVar::SignalAll() {\n-  PthreadCall(\"broadcast\", pthread_cond_broadcast(&cv_));\n-}\n-\n-void InitOnce(OnceType* once, void (*initializer)()) {\n-  PthreadCall(\"once\", pthread_once(once, initializer));\n-}\n-\n-}  // namespace port\n-}  // namespace leveldb"
      },
      {
        "sha": "f2b89bffb99c232c03922806868b0ccf663bc111",
        "filename": "src/leveldb/port/port_posix.h",
        "status": "removed",
        "additions": 0,
        "deletions": 157,
        "changes": 157,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/port_posix.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/port_posix.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/port_posix.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,157 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// See port_example.h for documentation for the following types/functions.\n-\n-#ifndef STORAGE_LEVELDB_PORT_PORT_POSIX_H_\n-#define STORAGE_LEVELDB_PORT_PORT_POSIX_H_\n-\n-#undef PLATFORM_IS_LITTLE_ENDIAN\n-#if defined(OS_MACOSX)\n-  #include <machine/endian.h>\n-  #if defined(__DARWIN_LITTLE_ENDIAN) && defined(__DARWIN_BYTE_ORDER)\n-    #define PLATFORM_IS_LITTLE_ENDIAN \\\n-        (__DARWIN_BYTE_ORDER == __DARWIN_LITTLE_ENDIAN)\n-  #endif\n-#elif defined(OS_SOLARIS)\n-  #include <sys/isa_defs.h>\n-  #ifdef _LITTLE_ENDIAN\n-    #define PLATFORM_IS_LITTLE_ENDIAN true\n-  #else\n-    #define PLATFORM_IS_LITTLE_ENDIAN false\n-  #endif\n-#elif defined(OS_FREEBSD)\n-  #include <sys/types.h>\n-  #include <sys/endian.h>\n-  #define PLATFORM_IS_LITTLE_ENDIAN (_BYTE_ORDER == _LITTLE_ENDIAN)\n-#elif defined(OS_OPENBSD) || defined(OS_NETBSD) ||\\\n-      defined(OS_DRAGONFLYBSD)\n-  #include <sys/types.h>\n-  #include <sys/endian.h>\n-#elif defined(OS_HPUX)\n-  #define PLATFORM_IS_LITTLE_ENDIAN false\n-#elif defined(OS_ANDROID)\n-  // Due to a bug in the NDK x86 <sys/endian.h> definition,\n-  // _BYTE_ORDER must be used instead of __BYTE_ORDER on Android.\n-  // See http://code.google.com/p/android/issues/detail?id=39824\n-  #include <endian.h>\n-  #define PLATFORM_IS_LITTLE_ENDIAN  (_BYTE_ORDER == _LITTLE_ENDIAN)\n-#else\n-  #include <endian.h>\n-#endif\n-\n-#include <pthread.h>\n-#ifdef SNAPPY\n-#include <snappy.h>\n-#endif\n-#include <stdint.h>\n-#include <string>\n-#include \"port/atomic_pointer.h\"\n-\n-#ifndef PLATFORM_IS_LITTLE_ENDIAN\n-#define PLATFORM_IS_LITTLE_ENDIAN (__BYTE_ORDER == __LITTLE_ENDIAN)\n-#endif\n-\n-#if defined(OS_MACOSX) || defined(OS_SOLARIS) || defined(OS_FREEBSD) ||\\\n-    defined(OS_NETBSD) || defined(OS_OPENBSD) || defined(OS_DRAGONFLYBSD) ||\\\n-    defined(OS_ANDROID) || defined(OS_HPUX)\n-// Use fread/fwrite/fflush on platforms without _unlocked variants\n-#define fread_unlocked fread\n-#define fwrite_unlocked fwrite\n-#define fflush_unlocked fflush\n-#endif\n-\n-#if defined(OS_MACOSX) || defined(OS_FREEBSD) ||\\\n-    defined(OS_OPENBSD) || defined(OS_DRAGONFLYBSD)\n-// Use fsync() on platforms without fdatasync()\n-#define fdatasync fsync\n-#endif\n-\n-#if defined(OS_ANDROID) && __ANDROID_API__ < 9\n-// fdatasync() was only introduced in API level 9 on Android. Use fsync()\n-// when targetting older platforms.\n-#define fdatasync fsync\n-#endif\n-\n-namespace leveldb {\n-namespace port {\n-\n-static const bool kLittleEndian = PLATFORM_IS_LITTLE_ENDIAN;\n-#undef PLATFORM_IS_LITTLE_ENDIAN\n-\n-class CondVar;\n-\n-class Mutex {\n- public:\n-  Mutex();\n-  ~Mutex();\n-\n-  void Lock();\n-  void Unlock();\n-  void AssertHeld() { }\n-\n- private:\n-  friend class CondVar;\n-  pthread_mutex_t mu_;\n-\n-  // No copying\n-  Mutex(const Mutex&);\n-  void operator=(const Mutex&);\n-};\n-\n-class CondVar {\n- public:\n-  explicit CondVar(Mutex* mu);\n-  ~CondVar();\n-  void Wait();\n-  void Signal();\n-  void SignalAll();\n- private:\n-  pthread_cond_t cv_;\n-  Mutex* mu_;\n-};\n-\n-typedef pthread_once_t OnceType;\n-#define LEVELDB_ONCE_INIT PTHREAD_ONCE_INIT\n-extern void InitOnce(OnceType* once, void (*initializer)());\n-\n-inline bool Snappy_Compress(const char* input, size_t length,\n-                            ::std::string* output) {\n-#ifdef SNAPPY\n-  output->resize(snappy::MaxCompressedLength(length));\n-  size_t outlen;\n-  snappy::RawCompress(input, length, &(*output)[0], &outlen);\n-  output->resize(outlen);\n-  return true;\n-#endif\n-\n-  return false;\n-}\n-\n-inline bool Snappy_GetUncompressedLength(const char* input, size_t length,\n-                                         size_t* result) {\n-#ifdef SNAPPY\n-  return snappy::GetUncompressedLength(input, length, result);\n-#else\n-  return false;\n-#endif\n-}\n-\n-inline bool Snappy_Uncompress(const char* input, size_t length,\n-                              char* output) {\n-#ifdef SNAPPY\n-  return snappy::RawUncompress(input, length, output);\n-#else\n-  return false;\n-#endif\n-}\n-\n-inline bool GetHeapProfile(void (*func)(void*, const char*, int), void* arg) {\n-  return false;\n-}\n-\n-} // namespace port\n-} // namespace leveldb\n-\n-#endif  // STORAGE_LEVELDB_PORT_PORT_POSIX_H_"
      },
      {
        "sha": "99c1d8e3460ddf39dc7ab32914a20df9e5f35e8a",
        "filename": "src/leveldb/port/port_win.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 149,
        "changes": 149,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/port_win.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/port_win.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/port_win.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,149 +0,0 @@\n-// LevelDB Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// See port_example.h for documentation for the following types/functions.\n-\n-// Redistribution and use in source and binary forms, with or without\n-// modification, are permitted provided that the following conditions are met:\n-// \n-//  * Redistributions of source code must retain the above copyright\n-//    notice, this list of conditions and the following disclaimer.\n-//  * Redistributions in binary form must reproduce the above copyright\n-//    notice, this list of conditions and the following disclaimer in the\n-//    documentation and/or other materials provided with the distribution.\n-//  * Neither the name of the University of California, Berkeley nor the\n-//    names of its contributors may be used to endorse or promote products\n-//    derived from this software without specific prior written permission.\n-// \n-// THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND ANY\n-// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n-// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n-// DISCLAIMED. IN NO EVENT SHALL THE REGENTS AND CONTRIBUTORS BE LIABLE FOR ANY\n-// DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n-// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n-// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n-// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n-// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n-//\n-\n-#include \"port/port_win.h\"\n-\n-#include <windows.h>\n-#include <cassert>\n-\n-namespace leveldb {\n-namespace port {\n-\n-Mutex::Mutex() :\n-    cs_(NULL) {\n-  assert(!cs_);\n-  cs_ = static_cast<void *>(new CRITICAL_SECTION());\n-  ::InitializeCriticalSection(static_cast<CRITICAL_SECTION *>(cs_));\n-  assert(cs_);\n-}\n-\n-Mutex::~Mutex() {\n-  assert(cs_);\n-  ::DeleteCriticalSection(static_cast<CRITICAL_SECTION *>(cs_));\n-  delete static_cast<CRITICAL_SECTION *>(cs_);\n-  cs_ = NULL;\n-  assert(!cs_);\n-}\n-\n-void Mutex::Lock() {\n-  assert(cs_);\n-  ::EnterCriticalSection(static_cast<CRITICAL_SECTION *>(cs_));\n-}\n-\n-void Mutex::Unlock() {\n-  assert(cs_);\n-  ::LeaveCriticalSection(static_cast<CRITICAL_SECTION *>(cs_));\n-}\n-\n-void Mutex::AssertHeld() {\n-  assert(cs_);\n-  assert(1);\n-}\n-\n-CondVar::CondVar(Mutex* mu) :\n-    waiting_(0), \n-    mu_(mu), \n-    sem1_(::CreateSemaphore(NULL, 0, 10000, NULL)), \n-    sem2_(::CreateSemaphore(NULL, 0, 10000, NULL)) {\n-  assert(mu_);\n-}\n-\n-CondVar::~CondVar() {\n-  ::CloseHandle(sem1_);\n-  ::CloseHandle(sem2_);\n-}\n-\n-void CondVar::Wait() {\n-  mu_->AssertHeld();\n-\n-  wait_mtx_.Lock();\n-  ++waiting_;\n-  wait_mtx_.Unlock();\n-\n-  mu_->Unlock();\n-\n-  // initiate handshake\n-  ::WaitForSingleObject(sem1_, INFINITE);\n-  ::ReleaseSemaphore(sem2_, 1, NULL);\n-  mu_->Lock();\n-}\n-\n-void CondVar::Signal() {\n-  wait_mtx_.Lock();\n-  if (waiting_ > 0) {\n-    --waiting_;\n-\n-    // finalize handshake\n-    ::ReleaseSemaphore(sem1_, 1, NULL);\n-    ::WaitForSingleObject(sem2_, INFINITE);\n-  }\n-  wait_mtx_.Unlock();\n-}\n-\n-void CondVar::SignalAll() {\n-  wait_mtx_.Lock();\n-  for(long i = 0; i < waiting_; ++i) {\n-    ::ReleaseSemaphore(sem1_, 1, NULL);\n-    while(waiting_ > 0) {\n-      --waiting_;\n-      ::WaitForSingleObject(sem2_, INFINITE);\n-    }\n-  }\n-  wait_mtx_.Unlock();\n-}\n-\n-AtomicPointer::AtomicPointer(void* v) {\n-  Release_Store(v);\n-}\n-\n-void InitOnce(OnceType* once, void (*initializer)()) {\n-  once->InitOnce(initializer);\n-}\n-\n-void* AtomicPointer::Acquire_Load() const {\n-  void * p = NULL;\n-  InterlockedExchangePointer(&p, rep_);\n-  return p;\n-}\n-\n-void AtomicPointer::Release_Store(void* v) {\n-  InterlockedExchangePointer(&rep_, v);\n-}\n-\n-void* AtomicPointer::NoBarrier_Load() const {\n-  return rep_;\n-}\n-\n-void AtomicPointer::NoBarrier_Store(void* v) {\n-  rep_ = v;\n-}\n-\n-}\n-}"
      },
      {
        "sha": "45bf2f0ea749d60abb2de3d7b357e7a70eac94de",
        "filename": "src/leveldb/port/port_win.h",
        "status": "removed",
        "additions": 0,
        "deletions": 174,
        "changes": 174,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/port_win.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/port_win.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/port_win.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,174 +0,0 @@\n-// LevelDB Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-//\n-// See port_example.h for documentation for the following types/functions.\n-\n-// Redistribution and use in source and binary forms, with or without\n-// modification, are permitted provided that the following conditions are met:\n-// \n-//  * Redistributions of source code must retain the above copyright\n-//    notice, this list of conditions and the following disclaimer.\n-//  * Redistributions in binary form must reproduce the above copyright\n-//    notice, this list of conditions and the following disclaimer in the\n-//    documentation and/or other materials provided with the distribution.\n-//  * Neither the name of the University of California, Berkeley nor the\n-//    names of its contributors may be used to endorse or promote products\n-//    derived from this software without specific prior written permission.\n-// \n-// THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND ANY\n-// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n-// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n-// DISCLAIMED. IN NO EVENT SHALL THE REGENTS AND CONTRIBUTORS BE LIABLE FOR ANY\n-// DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n-// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n-// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n-// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n-// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n-//\n-\n-#ifndef STORAGE_LEVELDB_PORT_PORT_WIN_H_\n-#define STORAGE_LEVELDB_PORT_PORT_WIN_H_\n-\n-#ifdef _MSC_VER\n-#define snprintf _snprintf\n-#define close _close\n-#define fread_unlocked _fread_nolock\n-#endif\n-\n-#include <string>\n-#include <stdint.h>\n-#ifdef SNAPPY\n-#include <snappy.h>\n-#endif\n-\n-namespace leveldb {\n-namespace port {\n-\n-// Windows is little endian (for now :p)\n-static const bool kLittleEndian = true;\n-\n-class CondVar;\n-\n-class Mutex {\n- public:\n-  Mutex();\n-  ~Mutex();\n-\n-  void Lock();\n-  void Unlock();\n-  void AssertHeld();\n-\n- private:\n-  friend class CondVar;\n-  // critical sections are more efficient than mutexes\n-  // but they are not recursive and can only be used to synchronize threads within the same process\n-  // we use opaque void * to avoid including windows.h in port_win.h\n-  void * cs_;\n-\n-  // No copying\n-  Mutex(const Mutex&);\n-  void operator=(const Mutex&);\n-};\n-\n-// the Win32 API offers a dependable condition variable mechanism, but only starting with\n-// Windows 2008 and Vista\n-// no matter what we will implement our own condition variable with a semaphore\n-// implementation as described in a paper written by Andrew D. Birrell in 2003\n-class CondVar {\n- public:\n-  explicit CondVar(Mutex* mu);\n-  ~CondVar();\n-  void Wait();\n-  void Signal();\n-  void SignalAll();\n- private:\n-  Mutex* mu_;\n-  \n-  Mutex wait_mtx_;\n-  long waiting_;\n-  \n-  void * sem1_;\n-  void * sem2_;\n-  \n-  \n-};\n-\n-class OnceType {\n-public:\n-//    OnceType() : init_(false) {}\n-    OnceType(const OnceType &once) : init_(once.init_) {}\n-    OnceType(bool f) : init_(f) {}\n-    void InitOnce(void (*initializer)()) {\n-        mutex_.Lock();\n-        if (!init_) {\n-            init_ = true;\n-            initializer();\n-        }\n-        mutex_.Unlock();\n-    }\n-\n-private:\n-    bool init_;\n-    Mutex mutex_;\n-};\n-\n-#define LEVELDB_ONCE_INIT false\n-extern void InitOnce(port::OnceType*, void (*initializer)());\n-\n-// Storage for a lock-free pointer\n-class AtomicPointer {\n- private:\n-  void * rep_;\n- public:\n-  AtomicPointer() : rep_(NULL) { }\n-  explicit AtomicPointer(void* v); \n-  void* Acquire_Load() const;\n-\n-  void Release_Store(void* v);\n-\n-  void* NoBarrier_Load() const;\n-\n-  void NoBarrier_Store(void* v);\n-};\n-\n-inline bool Snappy_Compress(const char* input, size_t length,\n-                            ::std::string* output) {\n-#ifdef SNAPPY\n-  output->resize(snappy::MaxCompressedLength(length));\n-  size_t outlen;\n-  snappy::RawCompress(input, length, &(*output)[0], &outlen);\n-  output->resize(outlen);\n-  return true;\n-#endif\n-\n-  return false;\n-}\n-\n-inline bool Snappy_GetUncompressedLength(const char* input, size_t length,\n-                                         size_t* result) {\n-#ifdef SNAPPY\n-  return snappy::GetUncompressedLength(input, length, result);\n-#else\n-  return false;\n-#endif\n-}\n-\n-inline bool Snappy_Uncompress(const char* input, size_t length,\n-                              char* output) {\n-#ifdef SNAPPY\n-  return snappy::RawUncompress(input, length, output);\n-#else\n-  return false;\n-#endif\n-}\n-\n-inline bool GetHeapProfile(void (*func)(void*, const char*, int), void* arg) {\n-  return false;\n-}\n-\n-}\n-}\n-\n-#endif  // STORAGE_LEVELDB_PORT_PORT_WIN_H_"
      },
      {
        "sha": "6f9b6a7924d68e16940254d6d21888eaae2a240d",
        "filename": "src/leveldb/port/thread_annotations.h",
        "status": "removed",
        "additions": 0,
        "deletions": 59,
        "changes": 59,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/thread_annotations.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/thread_annotations.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/thread_annotations.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,59 +0,0 @@\n-// Copyright (c) 2012 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-#ifndef STORAGE_LEVELDB_PORT_THREAD_ANNOTATIONS_H\n-\n-// Some environments provide custom macros to aid in static thread-safety\n-// analysis.  Provide empty definitions of such macros unless they are already\n-// defined.\n-\n-#ifndef EXCLUSIVE_LOCKS_REQUIRED\n-#define EXCLUSIVE_LOCKS_REQUIRED(...)\n-#endif\n-\n-#ifndef SHARED_LOCKS_REQUIRED\n-#define SHARED_LOCKS_REQUIRED(...)\n-#endif\n-\n-#ifndef LOCKS_EXCLUDED\n-#define LOCKS_EXCLUDED(...)\n-#endif\n-\n-#ifndef LOCK_RETURNED\n-#define LOCK_RETURNED(x)\n-#endif\n-\n-#ifndef LOCKABLE\n-#define LOCKABLE\n-#endif\n-\n-#ifndef SCOPED_LOCKABLE\n-#define SCOPED_LOCKABLE\n-#endif\n-\n-#ifndef EXCLUSIVE_LOCK_FUNCTION\n-#define EXCLUSIVE_LOCK_FUNCTION(...)\n-#endif\n-\n-#ifndef SHARED_LOCK_FUNCTION\n-#define SHARED_LOCK_FUNCTION(...)\n-#endif\n-\n-#ifndef EXCLUSIVE_TRYLOCK_FUNCTION\n-#define EXCLUSIVE_TRYLOCK_FUNCTION(...)\n-#endif\n-\n-#ifndef SHARED_TRYLOCK_FUNCTION\n-#define SHARED_TRYLOCK_FUNCTION(...)\n-#endif\n-\n-#ifndef UNLOCK_FUNCTION\n-#define UNLOCK_FUNCTION(...)\n-#endif\n-\n-#ifndef NO_THREAD_SAFETY_ANALYSIS\n-#define NO_THREAD_SAFETY_ANALYSIS\n-#endif\n-\n-#endif  // STORAGE_LEVELDB_PORT_THREAD_ANNOTATIONS_H"
      },
      {
        "sha": "39edd0db13f436dc57dd28ed4013ab4d55a28a31",
        "filename": "src/leveldb/port/win/stdint.h",
        "status": "removed",
        "additions": 0,
        "deletions": 24,
        "changes": 24,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/win/stdint.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/port/win/stdint.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/win/stdint.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370",
        "patch": "@@ -1,24 +0,0 @@\n-// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style license that can be\n-// found in the LICENSE file. See the AUTHORS file for names of contributors.\n-\n-// MSVC didn't ship with this file until the 2010 version.\n-\n-#ifndef STORAGE_LEVELDB_PORT_WIN_STDINT_H_\n-#define STORAGE_LEVELDB_PORT_WIN_STDINT_H_\n-\n-#if !defined(_MSC_VER)\n-#error This file should only be included when compiling with MSVC.\n-#endif\n-\n-// Define C99 equivalent types.\n-typedef signed char           int8_t;\n-typedef signed short          int16_t;\n-typedef signed int            int32_t;\n-typedef signed long long      int64_t;\n-typedef unsigned char         uint8_t;\n-typedef unsigned short        uint16_t;\n-typedef unsigned int          uint32_t;\n-typedef unsigned long long    uint64_t;\n-\n-#endif  // STORAGE_LEVELDB_PORT_WIN_STDINT_H_"
      },
      {
        "sha": "ab83c1112cc10c0ad65e628db91cdcc27dc9e8c4",
        "filename": "src/leveldb/table/block.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/block.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/block.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/block.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "2493eb9f9fd9da41aafcac80180c3f831928a32d",
        "filename": "src/leveldb/table/block.h",
        "status": "removed",
        "additions": 0,
        "deletions": 44,
        "changes": 44,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/block.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/block.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/block.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "db660cd07cf50ad1b54310c21b22108ab6994802",
        "filename": "src/leveldb/table/block_builder.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 109,
        "changes": 109,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/block_builder.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/block_builder.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/block_builder.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "5b545bd1afb4f4e65c36d8430dde09e0f543259f",
        "filename": "src/leveldb/table/block_builder.h",
        "status": "removed",
        "additions": 0,
        "deletions": 57,
        "changes": 57,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/block_builder.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/block_builder.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/block_builder.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "203e15c8bcb13b8776842052a725ad2a3909fcf5",
        "filename": "src/leveldb/table/filter_block.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 111,
        "changes": 111,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/filter_block.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/filter_block.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/filter_block.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "c67d010bd106756c456dea013c5babdf0a18494f",
        "filename": "src/leveldb/table/filter_block.h",
        "status": "removed",
        "additions": 0,
        "deletions": 68,
        "changes": 68,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/filter_block.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/filter_block.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/filter_block.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "3a2a07cf53ca606b2d0e9890e6b9cfa02a678398",
        "filename": "src/leveldb/table/filter_block_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 128,
        "changes": 128,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/filter_block_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/filter_block_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/filter_block_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "cda1decdf35476ecd5c44d7f3a8e69111e620124",
        "filename": "src/leveldb/table/format.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 145,
        "changes": 145,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/format.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/format.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/format.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "6c0b80c0179c7fffbf6ee2af802a10ec02d73998",
        "filename": "src/leveldb/table/format.h",
        "status": "removed",
        "additions": 0,
        "deletions": 108,
        "changes": 108,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/format.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/format.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/format.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "3d1c87fdece73d4c1ef16a0a762f70059b9443e6",
        "filename": "src/leveldb/table/iterator.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 67,
        "changes": 67,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/iterator.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/iterator.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/iterator.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "9e16b3dbedebe65f36fbbfa9e180fd1fa80e84a3",
        "filename": "src/leveldb/table/iterator_wrapper.h",
        "status": "removed",
        "additions": 0,
        "deletions": 63,
        "changes": 63,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/iterator_wrapper.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/iterator_wrapper.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/iterator_wrapper.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "2dde4dc21fde9d86e98f5a3f3b493745d07a22f7",
        "filename": "src/leveldb/table/merger.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 197,
        "changes": 197,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/merger.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/merger.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/merger.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "91ddd80faa35bfcf7edb81ee2f22ed3f29b58f98",
        "filename": "src/leveldb/table/merger.h",
        "status": "removed",
        "additions": 0,
        "deletions": 26,
        "changes": 26,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/merger.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/merger.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/merger.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "dbd6d3a1bf0867e285dd53207876605cca3bcf07",
        "filename": "src/leveldb/table/table.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 276,
        "changes": 276,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/table.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/table.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/table.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "62002c84f2b18f479fdb1cd3ab142c179e1f3a6f",
        "filename": "src/leveldb/table/table_builder.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 270,
        "changes": 270,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/table_builder.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/table_builder.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/table_builder.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "57cea25334e07501748b5fdac5a8872b89d31f1e",
        "filename": "src/leveldb/table/table_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 838,
        "changes": 838,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/table_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/table_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/table_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "7822ebab9c32ce579c42f9621545d7283e8332b9",
        "filename": "src/leveldb/table/two_level_iterator.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 182,
        "changes": 182,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/two_level_iterator.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/two_level_iterator.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/two_level_iterator.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "629ca34525414582e39df8ad7a48eff72e0e450f",
        "filename": "src/leveldb/table/two_level_iterator.h",
        "status": "removed",
        "additions": 0,
        "deletions": 34,
        "changes": 34,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/two_level_iterator.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/table/two_level_iterator.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/two_level_iterator.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "9551d6a3a27c8c2bd13cc7e48882aaecdb75ba20",
        "filename": "src/leveldb/util/arena.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 68,
        "changes": 68,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/arena.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/arena.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/arena.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "8f7dde226c450745646f29986d0302630db6f1f5",
        "filename": "src/leveldb/util/arena.h",
        "status": "removed",
        "additions": 0,
        "deletions": 68,
        "changes": 68,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/arena.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/arena.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/arena.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "63d1778034550a7394029df41d6b24f9053f5663",
        "filename": "src/leveldb/util/arena_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 68,
        "changes": 68,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/arena_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/arena_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/arena_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "d7941cd21fab64079cbef1f62060a48f0d86c74d",
        "filename": "src/leveldb/util/bloom.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 95,
        "changes": 95,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/bloom.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/bloom.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/bloom.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "0bf8e8d6ebeae860d7d553538511ef2ac6e80f22",
        "filename": "src/leveldb/util/bloom_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 160,
        "changes": 160,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/bloom_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/bloom_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/bloom_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "24f1f63f4f00289bed9ac700e5643e1ff626d099",
        "filename": "src/leveldb/util/cache.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 328,
        "changes": 328,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/cache.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/cache.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/cache.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "43716715a89f476700845c303e1651448ecef62c",
        "filename": "src/leveldb/util/cache_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 186,
        "changes": 186,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/cache_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/cache_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/cache_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "21e3186d5dcff984a11563fd0d09c714426a29c3",
        "filename": "src/leveldb/util/coding.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 194,
        "changes": 194,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/coding.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/coding.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/coding.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "3993c4a755dfa5a0f8b966f1a698a371fa08556d",
        "filename": "src/leveldb/util/coding.h",
        "status": "removed",
        "additions": 0,
        "deletions": 104,
        "changes": 104,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/coding.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/coding.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/coding.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "2c52b17b602b8dc1cb36bd49d2bafeabd4a35af1",
        "filename": "src/leveldb/util/coding_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 196,
        "changes": 196,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/coding_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/coding_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/coding_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "4b7b5724ef3be5f6c7ed9f95dcb47a99f2ee2f9b",
        "filename": "src/leveldb/util/comparator.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 81,
        "changes": 81,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/comparator.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/comparator.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/comparator.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "6db9e770774d7ebc0b0c3bc4a230b5b8254cd1f4",
        "filename": "src/leveldb/util/crc32c.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 332,
        "changes": 332,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/crc32c.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/crc32c.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/crc32c.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "1d7e5c075d86d6cf2551cfcd0fd9ac5adce6fc38",
        "filename": "src/leveldb/util/crc32c.h",
        "status": "removed",
        "additions": 0,
        "deletions": 45,
        "changes": 45,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/crc32c.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/crc32c.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/crc32c.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "4b957ee120c8f805c0240d3d90eaf16a759437ba",
        "filename": "src/leveldb/util/crc32c_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 72,
        "changes": 72,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/crc32c_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/crc32c_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/crc32c_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "c2600e964a260c87f22afc8c5a3e7788b4c7e350",
        "filename": "src/leveldb/util/env.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 96,
        "changes": 96,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/env.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/env.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/env.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "db81f56d11aa968b85ee45ecfef4dee37e578cbb",
        "filename": "src/leveldb/util/env_posix.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 701,
        "changes": 701,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/env_posix.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/env_posix.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/env_posix.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "b72cb4438425bca83d9a6ca0d207dbc8590efb2e",
        "filename": "src/leveldb/util/env_test.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 104,
        "changes": 104,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/env_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/env_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/env_test.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "ef2ecae8306623e613419782daed98044a117435",
        "filename": "src/leveldb/util/env_win.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 1031,
        "changes": 1031,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/env_win.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/env_win.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/env_win.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "7b045c8c91d6f6d600308e50966ccf56e53638bf",
        "filename": "src/leveldb/util/filter_policy.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 11,
        "changes": 11,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/filter_policy.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/filter_policy.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/filter_policy.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "ba1818082dff90aa6475bb272fa25ee0c22d6292",
        "filename": "src/leveldb/util/hash.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 45,
        "changes": 45,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/hash.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/hash.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/hash.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "8889d56be80e2f6342a1a292c6a0075d2481de80",
        "filename": "src/leveldb/util/hash.h",
        "status": "removed",
        "additions": 0,
        "deletions": 19,
        "changes": 19,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/hash.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/hash.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/hash.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "bb95f583eac6a6f916a83e409045f0a226bd9ae7",
        "filename": "src/leveldb/util/histogram.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 139,
        "changes": 139,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/histogram.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/histogram.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/histogram.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "1ef9f3c8abdfc50858be433110611086bb3c0da6",
        "filename": "src/leveldb/util/histogram.h",
        "status": "removed",
        "additions": 0,
        "deletions": 42,
        "changes": 42,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/histogram.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/histogram.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/histogram.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "22cf2785123c45ab77fa158256a45d8e700c1463",
        "filename": "src/leveldb/util/logging.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 81,
        "changes": 81,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/logging.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/logging.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/logging.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "b0c5da813e8658c9712b5529f4b219cb1a945508",
        "filename": "src/leveldb/util/logging.h",
        "status": "removed",
        "additions": 0,
        "deletions": 47,
        "changes": 47,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/logging.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/logging.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/logging.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "1ff5a9efa15efa166b427ef9611ccc58c96a3984",
        "filename": "src/leveldb/util/mutexlock.h",
        "status": "removed",
        "additions": 0,
        "deletions": 41,
        "changes": 41,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/mutexlock.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/mutexlock.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/mutexlock.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "76af5b9302d437d9847b2a93c87697232d027cac",
        "filename": "src/leveldb/util/options.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 29,
        "changes": 29,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/options.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/options.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/options.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "c063c2b7cb200dffa3c253fe76ee851910cfbe2f",
        "filename": "src/leveldb/util/posix_logger.h",
        "status": "removed",
        "additions": 0,
        "deletions": 98,
        "changes": 98,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/posix_logger.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/posix_logger.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/posix_logger.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "07538242ea559ad31396b994a5172f13ecb3d775",
        "filename": "src/leveldb/util/random.h",
        "status": "removed",
        "additions": 0,
        "deletions": 59,
        "changes": 59,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/random.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/random.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/random.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "a44f35b3149fa8fe88d9ca32dbf92fbb9f6d534c",
        "filename": "src/leveldb/util/status.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 75,
        "changes": 75,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/status.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/status.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/status.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "eb1bdd554a3ea2f06cd45053ab3df8c71c9610a8",
        "filename": "src/leveldb/util/testharness.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 77,
        "changes": 77,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/testharness.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/testharness.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/testharness.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "da4fe68bb4e76ee69af136d76f9417d349fa9605",
        "filename": "src/leveldb/util/testharness.h",
        "status": "removed",
        "additions": 0,
        "deletions": 138,
        "changes": 138,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/testharness.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/testharness.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/testharness.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "538d09516d217d614ac45cefe38bb1503d3b1d5c",
        "filename": "src/leveldb/util/testutil.cc",
        "status": "removed",
        "additions": 0,
        "deletions": 51,
        "changes": 51,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/testutil.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/testutil.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/testutil.cc?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      },
      {
        "sha": "824e655bd2c5d7955aa83887966ad0f54fb77192",
        "filename": "src/leveldb/util/testutil.h",
        "status": "removed",
        "additions": 0,
        "deletions": 53,
        "changes": 53,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/testutil.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/20b611770f6900209f125f7cd4bf45fdb1fe4370/src/leveldb/util/testutil.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/testutil.h?ref=20b611770f6900209f125f7cd4bf45fdb1fe4370"
      }
    ]
  },
  {
    "sha": "cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpjYjFlMzlmMGEzNWNjMmIzNmZiNzQ4YzI2ZjY5Y2QyN2UwZWQ1MzMy",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "sipa@ulyssis.org",
        "date": "2013-08-20T12:17:45Z"
      },
      "committer": {
        "name": "Pieter Wuille",
        "email": "sipa@ulyssis.org",
        "date": "2013-08-20T12:17:45Z"
      },
      "message": "Squashed 'src/leveldb/' content from commit a02ddf9\n\ngit-subtree-dir: src/leveldb\ngit-subtree-split: a02ddf9b14d145e88185ee209ab8b01d8826663a",
      "tree": {
        "sha": "dd1437e5bb46fbfe1c2bdf7a5bfbfe608761f67d",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/dd1437e5bb46fbfe1c2bdf7a5bfbfe608761f67d"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [],
    "stats": {
      "total": 27817,
      "additions": 27817,
      "deletions": 0
    },
    "files": [
      {
        "sha": "71d87a4eeb60b9599e6a7a23d61a659c7befa553",
        "filename": ".gitignore",
        "status": "added",
        "additions": 13,
        "deletions": 0,
        "changes": 13,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/.gitignore",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/.gitignore",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/.gitignore?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,13 @@\n+build_config.mk\n+*.a\n+*.o\n+*.dylib*\n+*.so\n+*.so.*\n+*_test\n+db_bench\n+leveldbutil\n+Release\n+Debug\n+Benchmark\n+vs2010.*"
      },
      {
        "sha": "fc40194ab94f41405bd48d085b9f3fcbe1704234",
        "filename": "AUTHORS",
        "status": "added",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/AUTHORS",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/AUTHORS",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/AUTHORS?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,11 @@\n+# Names should be added to this file like so:\n+# Name or Organization <email address>\n+\n+Google Inc.\n+\n+# Initial version authors:\n+Jeffrey Dean <jeff@google.com>\n+Sanjay Ghemawat <sanjay@google.com>\n+\n+# Partial list of contributors:\n+Kevin Regan <kevin.d.regan@gmail.com>"
      },
      {
        "sha": "8e80208cd72b3225c87d9111c4d7cab13af1c2ac",
        "filename": "LICENSE",
        "status": "added",
        "additions": 27,
        "deletions": 0,
        "changes": 27,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/LICENSE",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/LICENSE",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/LICENSE?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,27 @@\n+Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+\n+Redistribution and use in source and binary forms, with or without\n+modification, are permitted provided that the following conditions are\n+met:\n+\n+   * Redistributions of source code must retain the above copyright\n+notice, this list of conditions and the following disclaimer.\n+   * Redistributions in binary form must reproduce the above\n+copyright notice, this list of conditions and the following disclaimer\n+in the documentation and/or other materials provided with the\n+distribution.\n+   * Neither the name of Google Inc. nor the names of its\n+contributors may be used to endorse or promote products derived from\n+this software without specific prior written permission.\n+\n+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n+\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n+OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n+LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n+DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n+THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
      },
      {
        "sha": "96af7765be41edc6b8e83c646efcdb3b9c99d025",
        "filename": "Makefile",
        "status": "added",
        "additions": 206,
        "deletions": 0,
        "changes": 206,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/Makefile",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/Makefile",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/Makefile?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,206 @@\n+# Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+# Use of this source code is governed by a BSD-style license that can be\n+# found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#-----------------------------------------------\n+# Uncomment exactly one of the lines labelled (A), (B), and (C) below\n+# to switch between compilation modes.\n+\n+OPT ?= -O2 -DNDEBUG       # (A) Production use (optimized mode)\n+# OPT ?= -g2              # (B) Debug mode, w/ full line-level debugging symbols\n+# OPT ?= -O2 -g2 -DNDEBUG # (C) Profiling mode: opt, but w/debugging symbols\n+#-----------------------------------------------\n+\n+# detect what platform we're building on\n+$(shell CC=\"$(CC)\" CXX=\"$(CXX)\" TARGET_OS=\"$(TARGET_OS)\" \\\n+    ./build_detect_platform build_config.mk ./)\n+# this file is generated by the previous line to set build flags and sources\n+include build_config.mk\n+\n+CFLAGS += -I. -I./include $(PLATFORM_CCFLAGS) $(OPT)\n+CXXFLAGS += -I. -I./include $(PLATFORM_CXXFLAGS) $(OPT)\n+\n+LDFLAGS += $(PLATFORM_LDFLAGS)\n+LIBS += $(PLATFORM_LIBS)\n+\n+LIBOBJECTS = $(SOURCES:.cc=.o)\n+MEMENVOBJECTS = $(MEMENV_SOURCES:.cc=.o)\n+\n+TESTUTIL = ./util/testutil.o\n+TESTHARNESS = ./util/testharness.o $(TESTUTIL)\n+\n+TESTS = \\\n+\tarena_test \\\n+\tbloom_test \\\n+\tc_test \\\n+\tcache_test \\\n+\tcoding_test \\\n+\tcorruption_test \\\n+\tcrc32c_test \\\n+\tdb_test \\\n+\tdbformat_test \\\n+\tenv_test \\\n+\tfilename_test \\\n+\tfilter_block_test \\\n+\tissue178_test \\\n+\tlog_test \\\n+\tmemenv_test \\\n+\tskiplist_test \\\n+\ttable_test \\\n+\tversion_edit_test \\\n+\tversion_set_test \\\n+\twrite_batch_test\n+\n+PROGRAMS = db_bench leveldbutil $(TESTS)\n+BENCHMARKS = db_bench_sqlite3 db_bench_tree_db\n+\n+LIBRARY = libleveldb.a\n+MEMENVLIBRARY = libmemenv.a\n+\n+default: all\n+\n+# Should we build shared libraries?\n+ifneq ($(PLATFORM_SHARED_EXT),)\n+\n+ifneq ($(PLATFORM_SHARED_VERSIONED),true)\n+SHARED1 = libleveldb.$(PLATFORM_SHARED_EXT)\n+SHARED2 = $(SHARED1)\n+SHARED3 = $(SHARED1)\n+SHARED = $(SHARED1)\n+else\n+# Update db.h if you change these.\n+SHARED_MAJOR = 1\n+SHARED_MINOR = 12\n+SHARED1 = libleveldb.$(PLATFORM_SHARED_EXT)\n+SHARED2 = $(SHARED1).$(SHARED_MAJOR)\n+SHARED3 = $(SHARED1).$(SHARED_MAJOR).$(SHARED_MINOR)\n+SHARED = $(SHARED1) $(SHARED2) $(SHARED3)\n+$(SHARED1): $(SHARED3)\n+\tln -fs $(SHARED3) $(SHARED1)\n+$(SHARED2): $(SHARED3)\n+\tln -fs $(SHARED3) $(SHARED2)\n+endif\n+\n+$(SHARED3):\n+\t$(CXX) $(LDFLAGS) $(PLATFORM_SHARED_LDFLAGS)$(SHARED2) $(CXXFLAGS) $(PLATFORM_SHARED_CFLAGS) $(SOURCES) -o $(SHARED3) $(LIBS)\n+\n+endif  # PLATFORM_SHARED_EXT\n+\n+all: $(SHARED) $(LIBRARY)\n+\n+check: all $(PROGRAMS) $(TESTS)\n+\tfor t in $(TESTS); do echo \"***** Running $$t\"; ./$$t || exit 1; done\n+\n+clean:\n+\t-rm -f $(PROGRAMS) $(BENCHMARKS) $(LIBRARY) $(SHARED) $(MEMENVLIBRARY) */*.o */*/*.o ios-x86/*/*.o ios-arm/*/*.o build_config.mk\n+\t-rm -rf ios-x86/* ios-arm/*\n+\n+$(LIBRARY): $(LIBOBJECTS)\n+\trm -f $@\n+\t$(AR) -rs $@ $(LIBOBJECTS)\n+\n+db_bench: db/db_bench.o $(LIBOBJECTS) $(TESTUTIL)\n+\t$(CXX) $(LDFLAGS) db/db_bench.o $(LIBOBJECTS) $(TESTUTIL) -o $@ $(LIBS)\n+\n+db_bench_sqlite3: doc/bench/db_bench_sqlite3.o $(LIBOBJECTS) $(TESTUTIL)\n+\t$(CXX) $(LDFLAGS) doc/bench/db_bench_sqlite3.o $(LIBOBJECTS) $(TESTUTIL) -o $@ -lsqlite3 $(LIBS)\n+\n+db_bench_tree_db: doc/bench/db_bench_tree_db.o $(LIBOBJECTS) $(TESTUTIL)\n+\t$(CXX) $(LDFLAGS) doc/bench/db_bench_tree_db.o $(LIBOBJECTS) $(TESTUTIL) -o $@ -lkyotocabinet $(LIBS)\n+\n+leveldbutil: db/leveldb_main.o $(LIBOBJECTS)\n+\t$(CXX) $(LDFLAGS) db/leveldb_main.o $(LIBOBJECTS) -o $@ $(LIBS)\n+\n+arena_test: util/arena_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) util/arena_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+bloom_test: util/bloom_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) util/bloom_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+c_test: db/c_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/c_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+cache_test: util/cache_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) util/cache_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+coding_test: util/coding_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) util/coding_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+corruption_test: db/corruption_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/corruption_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+crc32c_test: util/crc32c_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) util/crc32c_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+db_test: db/db_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/db_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+dbformat_test: db/dbformat_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/dbformat_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+env_test: util/env_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) util/env_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+filename_test: db/filename_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/filename_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+filter_block_test: table/filter_block_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) table/filter_block_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+issue178_test: issues/issue178_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) issues/issue178_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+log_test: db/log_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/log_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+table_test: table/table_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) table/table_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+skiplist_test: db/skiplist_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/skiplist_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+version_edit_test: db/version_edit_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/version_edit_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+version_set_test: db/version_set_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/version_set_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+write_batch_test: db/write_batch_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/write_batch_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+$(MEMENVLIBRARY) : $(MEMENVOBJECTS)\n+\trm -f $@\n+\t$(AR) -rs $@ $(MEMENVOBJECTS)\n+\n+memenv_test : helpers/memenv/memenv_test.o $(MEMENVLIBRARY) $(LIBRARY) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) helpers/memenv/memenv_test.o $(MEMENVLIBRARY) $(LIBRARY) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+ifeq ($(PLATFORM), IOS)\n+# For iOS, create universal object files to be used on both the simulator and\n+# a device.\n+PLATFORMSROOT=/Applications/Xcode.app/Contents/Developer/Platforms\n+SIMULATORROOT=$(PLATFORMSROOT)/iPhoneSimulator.platform/Developer\n+DEVICEROOT=$(PLATFORMSROOT)/iPhoneOS.platform/Developer\n+IOSVERSION=$(shell defaults read $(PLATFORMSROOT)/iPhoneOS.platform/version CFBundleShortVersionString)\n+\n+.cc.o:\n+\tmkdir -p ios-x86/$(dir $@)\n+\t$(CXX) $(CXXFLAGS) -isysroot $(SIMULATORROOT)/SDKs/iPhoneSimulator$(IOSVERSION).sdk -arch i686 -c $< -o ios-x86/$@\n+\tmkdir -p ios-arm/$(dir $@)\n+\t$(DEVICEROOT)/usr/bin/$(CXX) $(CXXFLAGS) -isysroot $(DEVICEROOT)/SDKs/iPhoneOS$(IOSVERSION).sdk -arch armv6 -arch armv7 -c $< -o ios-arm/$@\n+\tlipo ios-x86/$@ ios-arm/$@ -create -output $@\n+\n+.c.o:\n+\tmkdir -p ios-x86/$(dir $@)\n+\t$(CC) $(CFLAGS) -isysroot $(SIMULATORROOT)/SDKs/iPhoneSimulator$(IOSVERSION).sdk -arch i686 -c $< -o ios-x86/$@\n+\tmkdir -p ios-arm/$(dir $@)\n+\t$(DEVICEROOT)/usr/bin/$(CC) $(CFLAGS) -isysroot $(DEVICEROOT)/SDKs/iPhoneOS$(IOSVERSION).sdk -arch armv6 -arch armv7 -c $< -o ios-arm/$@\n+\tlipo ios-x86/$@ ios-arm/$@ -create -output $@\n+\n+else\n+.cc.o:\n+\t$(CXX) $(CXXFLAGS) -c $< -o $@\n+\n+.c.o:\n+\t$(CC) $(CFLAGS) -c $< -o $@\n+endif"
      },
      {
        "sha": "3fd99242d7bbcfeffecfd01c0870f89382766baf",
        "filename": "NEWS",
        "status": "added",
        "additions": 17,
        "deletions": 0,
        "changes": 17,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/NEWS",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/NEWS",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/NEWS?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,17 @@\n+Release 1.2 2011-05-16\n+----------------------\n+\n+Fixes for larger databases (tested up to one billion 100-byte entries,\n+i.e., ~100GB).\n+\n+(1) Place hard limit on number of level-0 files.  This fixes errors\n+of the form \"too many open files\".\n+\n+(2) Fixed memtable management.  Before the fix, a heavy write burst\n+could cause unbounded memory usage.\n+\n+A fix for a logging bug where the reader would incorrectly complain\n+about corruption.\n+\n+Allow public access to WriteBatch contents so that users can easily\n+wrap a DB."
      },
      {
        "sha": "3618adeeedbea04a14e00d5a1ef33dd4f0a7be06",
        "filename": "README",
        "status": "added",
        "additions": 51,
        "deletions": 0,
        "changes": 51,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/README",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/README",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/README?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,51 @@\n+leveldb: A key-value store\n+Authors: Sanjay Ghemawat (sanjay@google.com) and Jeff Dean (jeff@google.com)\n+\n+The code under this directory implements a system for maintaining a\n+persistent key/value store.\n+\n+See doc/index.html for more explanation.\n+See doc/impl.html for a brief overview of the implementation.\n+\n+The public interface is in include/*.h.  Callers should not include or\n+rely on the details of any other header files in this package.  Those\n+internal APIs may be changed without warning.\n+\n+Guide to header files:\n+\n+include/db.h\n+    Main interface to the DB: Start here\n+\n+include/options.h\n+    Control over the behavior of an entire database, and also\n+    control over the behavior of individual reads and writes.\n+\n+include/comparator.h\n+    Abstraction for user-specified comparison function.  If you want\n+    just bytewise comparison of keys, you can use the default comparator,\n+    but clients can write their own comparator implementations if they\n+    want custom ordering (e.g. to handle different character\n+    encodings, etc.)\n+\n+include/iterator.h\n+    Interface for iterating over data. You can get an iterator\n+    from a DB object.\n+\n+include/write_batch.h\n+    Interface for atomically applying multiple updates to a database.\n+\n+include/slice.h\n+    A simple module for maintaining a pointer and a length into some\n+    other byte array.\n+\n+include/status.h\n+    Status is returned from many of the public interfaces and is used\n+    to report success and various kinds of errors.\n+\n+include/env.h\n+    Abstraction of the OS environment.  A posix implementation of\n+    this interface is in util/env_posix.cc\n+\n+include/table.h\n+include/table_builder.h\n+    Lower-level modules that most clients probably won't use directly"
      },
      {
        "sha": "e603c07137f1ea198a3de9ca3bbb369e1c069f61",
        "filename": "TODO",
        "status": "added",
        "additions": 14,
        "deletions": 0,
        "changes": 14,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/TODO",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/TODO",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/TODO?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,14 @@\n+ss\n+- Stats\n+\n+db\n+- Maybe implement DB::BulkDeleteForRange(start_key, end_key)\n+  that would blow away files whose ranges are entirely contained\n+  within [start_key..end_key]?  For Chrome, deletion of obsolete\n+  object stores, etc. can be done in the background anyway, so\n+  probably not that important.\n+- There have been requests for MultiGet.\n+\n+After a range is completely deleted, what gets rid of the\n+corresponding files if we do no future changes to that range.  Make\n+the conditions for triggering compactions fire in more situations?"
      },
      {
        "sha": "5b76c2448fe1e475b97d12235bdff53b857557cf",
        "filename": "WINDOWS.md",
        "status": "added",
        "additions": 39,
        "deletions": 0,
        "changes": 39,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/WINDOWS.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/WINDOWS.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/WINDOWS.md?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,39 @@\n+# Building LevelDB On Windows\n+\n+## Prereqs \n+\n+Install the [Windows Software Development Kit version 7.1](http://www.microsoft.com/downloads/dlx/en-us/listdetailsview.aspx?FamilyID=6b6c21d2-2006-4afa-9702-529fa782d63b).\n+\n+Download and extract the [Snappy source distribution](http://snappy.googlecode.com/files/snappy-1.0.5.tar.gz)\n+\n+1. Open the \"Windows SDK 7.1 Command Prompt\" :\n+   Start Menu -> \"Microsoft Windows SDK v7.1\" > \"Windows SDK 7.1 Command Prompt\"\n+2. Change the directory to the leveldb project\n+\n+## Building the Static lib \n+\n+* 32 bit Version \n+\n+        setenv /x86\n+        msbuild.exe /p:Configuration=Release /p:Platform=Win32 /p:Snappy=..\\snappy-1.0.5\n+\n+* 64 bit Version \n+\n+        setenv /x64\n+        msbuild.exe /p:Configuration=Release /p:Platform=x64 /p:Snappy=..\\snappy-1.0.5\n+\n+\n+## Building and Running the Benchmark app\n+\n+* 32 bit Version \n+\n+\t    setenv /x86\n+\t    msbuild.exe /p:Configuration=Benchmark /p:Platform=Win32 /p:Snappy=..\\snappy-1.0.5\n+\t\tBenchmark\\leveldb.exe\n+\n+* 64 bit Version \n+\n+\t    setenv /x64\n+\t    msbuild.exe /p:Configuration=Benchmark /p:Platform=x64 /p:Snappy=..\\snappy-1.0.5\n+\t    x64\\Benchmark\\leveldb.exe\n+"
      },
      {
        "sha": "bdfd64172cd1b9ca15dd436d96b9dba8904c281b",
        "filename": "build_detect_platform",
        "status": "added",
        "additions": 214,
        "deletions": 0,
        "changes": 214,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/build_detect_platform",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/build_detect_platform",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/build_detect_platform?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,214 @@\n+#!/bin/sh\n+#\n+# Detects OS we're compiling on and outputs a file specified by the first\n+# argument, which in turn gets read while processing Makefile.\n+#\n+# The output will set the following variables:\n+#   CC                          C Compiler path\n+#   CXX                         C++ Compiler path\n+#   PLATFORM_LDFLAGS            Linker flags\n+#   PLATFORM_LIBS               Libraries flags\n+#   PLATFORM_SHARED_EXT         Extension for shared libraries\n+#   PLATFORM_SHARED_LDFLAGS     Flags for building shared library\n+#                               This flag is embedded just before the name\n+#                               of the shared library without intervening spaces\n+#   PLATFORM_SHARED_CFLAGS      Flags for compiling objects for shared library\n+#   PLATFORM_CCFLAGS            C compiler flags\n+#   PLATFORM_CXXFLAGS           C++ compiler flags.  Will contain:\n+#   PLATFORM_SHARED_VERSIONED   Set to 'true' if platform supports versioned\n+#                               shared libraries, empty otherwise.\n+#\n+# The PLATFORM_CCFLAGS and PLATFORM_CXXFLAGS might include the following:\n+#\n+#       -DLEVELDB_CSTDATOMIC_PRESENT if <cstdatomic> is present\n+#       -DLEVELDB_PLATFORM_POSIX     for Posix-based platforms\n+#       -DSNAPPY                     if the Snappy library is present\n+#\n+\n+OUTPUT=$1\n+PREFIX=$2\n+if test -z \"$OUTPUT\" || test -z \"$PREFIX\"; then\n+  echo \"usage: $0 <output-filename> <directory_prefix>\" >&2\n+  exit 1\n+fi\n+\n+# Delete existing output, if it exists\n+rm -f $OUTPUT\n+touch $OUTPUT\n+\n+if test -z \"$CC\"; then\n+    CC=cc\n+fi\n+\n+if test -z \"$CXX\"; then\n+    CXX=g++\n+fi\n+\n+if test -z \"$TMPDIR\"; then\n+    TMPDIR=/tmp\n+fi\n+\n+# Detect OS\n+if test -z \"$TARGET_OS\"; then\n+    TARGET_OS=`uname -s`\n+fi\n+\n+COMMON_FLAGS=\n+CROSS_COMPILE=\n+PLATFORM_CCFLAGS=\n+PLATFORM_CXXFLAGS=\n+PLATFORM_LDFLAGS=\n+PLATFORM_LIBS=\n+PLATFORM_SHARED_EXT=\"so\"\n+PLATFORM_SHARED_LDFLAGS=\"-shared -Wl,-soname -Wl,\"\n+PLATFORM_SHARED_CFLAGS=\"-fPIC\"\n+PLATFORM_SHARED_VERSIONED=true\n+\n+MEMCMP_FLAG=\n+if [ \"$CXX\" = \"g++\" ]; then\n+    # Use libc's memcmp instead of GCC's memcmp.  This results in ~40%\n+    # performance improvement on readrandom under gcc 4.4.3 on Linux/x86.\n+    MEMCMP_FLAG=\"-fno-builtin-memcmp\"\n+fi\n+\n+case \"$TARGET_OS\" in\n+    Darwin)\n+        PLATFORM=OS_MACOSX\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -DOS_MACOSX\"\n+        PLATFORM_SHARED_EXT=dylib\n+        [ -z \"$INSTALL_PATH\" ] && INSTALL_PATH=`pwd`\n+        PLATFORM_SHARED_LDFLAGS=\"-dynamiclib -install_name $INSTALL_PATH/\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    Linux)\n+        PLATFORM=OS_LINUX\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -pthread -DOS_LINUX\"\n+        PLATFORM_LDFLAGS=\"-pthread\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    SunOS)\n+        PLATFORM=OS_SOLARIS\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_SOLARIS\"\n+        PLATFORM_LIBS=\"-lpthread -lrt\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    FreeBSD)\n+        PLATFORM=OS_FREEBSD\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_FREEBSD\"\n+        PLATFORM_LIBS=\"-lpthread\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    GNU/kFreeBSD)\n+        PLATFORM=OS_KFREEBSD\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_KFREEBSD\"\n+        PLATFORM_LIBS=\"-lpthread\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    NetBSD)\n+        PLATFORM=OS_NETBSD\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_NETBSD\"\n+        PLATFORM_LIBS=\"-lpthread -lgcc_s\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    OpenBSD)\n+        PLATFORM=OS_OPENBSD\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_OPENBSD\"\n+        PLATFORM_LDFLAGS=\"-pthread\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    DragonFly)\n+        PLATFORM=OS_DRAGONFLYBSD\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_DRAGONFLYBSD\"\n+        PLATFORM_LIBS=\"-lpthread\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    OS_ANDROID_CROSSCOMPILE)\n+        PLATFORM=OS_ANDROID\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_ANDROID -DLEVELDB_PLATFORM_POSIX\"\n+        PLATFORM_LDFLAGS=\"\"  # All pthread features are in the Android C library\n+        PORT_FILE=port/port_posix.cc\n+        CROSS_COMPILE=true\n+        ;;\n+    HP-UX)\n+        PLATFORM=OS_HPUX\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_HPUX\"\n+        PLATFORM_LDFLAGS=\"-pthread\"\n+        PORT_FILE=port/port_posix.cc\n+        # man ld: +h internal_name\n+        PLATFORM_SHARED_LDFLAGS=\"-shared -Wl,+h -Wl,\"\n+        ;;\n+    OS_WINDOWS_CROSSCOMPILE | NATIVE_WINDOWS)\n+        PLATFORM=OS_WINDOWS\n+        COMMON_FLAGS=\"-fno-builtin-memcmp -D_REENTRANT -DOS_WINDOWS -DLEVELDB_PLATFORM_WINDOWS -DWINVER=0x0500 -D__USE_MINGW_ANSI_STDIO=1\"\n+        PLATFORM_SOURCES=\"util/env_win.cc\"\n+        PLATFORM_LIBS=\"-lshlwapi\"\n+        PORT_FILE=port/port_win.cc\n+        CROSS_COMPILE=true\n+        ;;\n+    *)\n+        echo \"Unknown platform!\" >&2\n+        exit 1\n+esac\n+\n+# We want to make a list of all cc files within util, db, table, and helpers\n+# except for the test and benchmark files. By default, find will output a list\n+# of all files matching either rule, so we need to append -print to make the\n+# prune take effect.\n+DIRS=\"$PREFIX/db $PREFIX/util $PREFIX/table\"\n+\n+set -f # temporarily disable globbing so that our patterns aren't expanded\n+PRUNE_TEST=\"-name *test*.cc -prune\"\n+PRUNE_BENCH=\"-name *_bench.cc -prune\"\n+PRUNE_TOOL=\"-name leveldb_main.cc -prune\"\n+PORTABLE_FILES=`find $DIRS $PRUNE_TEST -o $PRUNE_BENCH -o $PRUNE_TOOL -o -name '*.cc' -print | sort | sed \"s,^$PREFIX/,,\" | tr \"\\n\" \" \"`\n+\n+set +f # re-enable globbing\n+\n+# The sources consist of the portable files, plus the platform-specific port\n+# file.\n+echo \"SOURCES=$PORTABLE_FILES $PORT_FILE\" >> $OUTPUT\n+echo \"MEMENV_SOURCES=helpers/memenv/memenv.cc\" >> $OUTPUT\n+\n+if [ \"$CROSS_COMPILE\" = \"true\" ]; then\n+    # Cross-compiling; do not try any compilation tests.\n+    true\n+else\n+    CXXOUTPUT=\"${TMPDIR}/leveldb_build_detect_platform-cxx.$$\"\n+\n+    # If -std=c++0x works, use <cstdatomic>.  Otherwise use port_posix.h.\n+    $CXX $CXXFLAGS -std=c++0x -x c++ - -o $CXXOUTPUT 2>/dev/null  <<EOF\n+      #include <cstdatomic>\n+      int main() {}\n+EOF\n+    if [ \"$?\" = 0 ]; then\n+        COMMON_FLAGS=\"$COMMON_FLAGS -DLEVELDB_PLATFORM_POSIX -DLEVELDB_CSTDATOMIC_PRESENT\"\n+        PLATFORM_CXXFLAGS=\"-std=c++0x\"\n+    else\n+        COMMON_FLAGS=\"$COMMON_FLAGS -DLEVELDB_PLATFORM_POSIX\"\n+    fi\n+\n+    # Test whether tcmalloc is available\n+    $CXX $CXXFLAGS -x c++ - -o $CXXOUTPUT -ltcmalloc 2>/dev/null  <<EOF\n+      int main() {}\n+EOF\n+    if [ \"$?\" = 0 ]; then\n+        PLATFORM_LIBS=\"$PLATFORM_LIBS -ltcmalloc\"\n+    fi\n+\n+    rm -f $CXXOUTPUT 2>/dev/null\n+fi\n+\n+PLATFORM_CCFLAGS=\"$PLATFORM_CCFLAGS $COMMON_FLAGS\"\n+PLATFORM_CXXFLAGS=\"$PLATFORM_CXXFLAGS $COMMON_FLAGS\"\n+\n+echo \"CC=$CC\" >> $OUTPUT\n+echo \"CXX=$CXX\" >> $OUTPUT\n+echo \"PLATFORM=$PLATFORM\" >> $OUTPUT\n+echo \"PLATFORM_LDFLAGS=$PLATFORM_LDFLAGS\" >> $OUTPUT\n+echo \"PLATFORM_LIBS=$PLATFORM_LIBS\" >> $OUTPUT\n+echo \"PLATFORM_CCFLAGS=$PLATFORM_CCFLAGS\" >> $OUTPUT\n+echo \"PLATFORM_CXXFLAGS=$PLATFORM_CXXFLAGS\" >> $OUTPUT\n+echo \"PLATFORM_SHARED_CFLAGS=$PLATFORM_SHARED_CFLAGS\" >> $OUTPUT\n+echo \"PLATFORM_SHARED_EXT=$PLATFORM_SHARED_EXT\" >> $OUTPUT\n+echo \"PLATFORM_SHARED_LDFLAGS=$PLATFORM_SHARED_LDFLAGS\" >> $OUTPUT\n+echo \"PLATFORM_SHARED_VERSIONED=$PLATFORM_SHARED_VERSIONED\" >> $OUTPUT"
      },
      {
        "sha": "f4198821973c94f2e28148f79a10ca2c48d8d1e6",
        "filename": "db/builder.cc",
        "status": "added",
        "additions": 88,
        "deletions": 0,
        "changes": 88,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/builder.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/builder.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/builder.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,88 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/builder.h\"\n+\n+#include \"db/filename.h\"\n+#include \"db/dbformat.h\"\n+#include \"db/table_cache.h\"\n+#include \"db/version_edit.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/iterator.h\"\n+\n+namespace leveldb {\n+\n+Status BuildTable(const std::string& dbname,\n+                  Env* env,\n+                  const Options& options,\n+                  TableCache* table_cache,\n+                  Iterator* iter,\n+                  FileMetaData* meta) {\n+  Status s;\n+  meta->file_size = 0;\n+  iter->SeekToFirst();\n+\n+  std::string fname = TableFileName(dbname, meta->number);\n+  if (iter->Valid()) {\n+    WritableFile* file;\n+    s = env->NewWritableFile(fname, &file);\n+    if (!s.ok()) {\n+      return s;\n+    }\n+\n+    TableBuilder* builder = new TableBuilder(options, file);\n+    meta->smallest.DecodeFrom(iter->key());\n+    for (; iter->Valid(); iter->Next()) {\n+      Slice key = iter->key();\n+      meta->largest.DecodeFrom(key);\n+      builder->Add(key, iter->value());\n+    }\n+\n+    // Finish and check for builder errors\n+    if (s.ok()) {\n+      s = builder->Finish();\n+      if (s.ok()) {\n+        meta->file_size = builder->FileSize();\n+        assert(meta->file_size > 0);\n+      }\n+    } else {\n+      builder->Abandon();\n+    }\n+    delete builder;\n+\n+    // Finish and check for file errors\n+    if (s.ok()) {\n+      s = file->Sync();\n+    }\n+    if (s.ok()) {\n+      s = file->Close();\n+    }\n+    delete file;\n+    file = NULL;\n+\n+    if (s.ok()) {\n+      // Verify that the table is usable\n+      Iterator* it = table_cache->NewIterator(ReadOptions(),\n+                                              meta->number,\n+                                              meta->file_size);\n+      s = it->status();\n+      delete it;\n+    }\n+  }\n+\n+  // Check for input iterator errors\n+  if (!iter->status().ok()) {\n+    s = iter->status();\n+  }\n+\n+  if (s.ok() && meta->file_size > 0) {\n+    // Keep it\n+  } else {\n+    env->DeleteFile(fname);\n+  }\n+  return s;\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "62431fcf44f4545490291d8ec1ab098c2fc2ba88",
        "filename": "db/builder.h",
        "status": "added",
        "additions": 34,
        "deletions": 0,
        "changes": 34,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/builder.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/builder.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/builder.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,34 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_BUILDER_H_\n+#define STORAGE_LEVELDB_DB_BUILDER_H_\n+\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+struct Options;\n+struct FileMetaData;\n+\n+class Env;\n+class Iterator;\n+class TableCache;\n+class VersionEdit;\n+\n+// Build a Table file from the contents of *iter.  The generated file\n+// will be named according to meta->number.  On success, the rest of\n+// *meta will be filled with metadata about the generated table.\n+// If no data is present in *iter, meta->file_size will be set to\n+// zero, and no Table file will be produced.\n+extern Status BuildTable(const std::string& dbname,\n+                         Env* env,\n+                         const Options& options,\n+                         TableCache* table_cache,\n+                         Iterator* iter,\n+                         FileMetaData* meta);\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_BUILDER_H_"
      },
      {
        "sha": "08ff0ad90ac00d63f05d5d71fb89f9f701894058",
        "filename": "db/c.cc",
        "status": "added",
        "additions": 595,
        "deletions": 0,
        "changes": 595,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/c.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/c.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/c.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,595 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"leveldb/c.h\"\n+\n+#include <stdlib.h>\n+#include <unistd.h>\n+#include \"leveldb/cache.h\"\n+#include \"leveldb/comparator.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/filter_policy.h\"\n+#include \"leveldb/iterator.h\"\n+#include \"leveldb/options.h\"\n+#include \"leveldb/status.h\"\n+#include \"leveldb/write_batch.h\"\n+\n+using leveldb::Cache;\n+using leveldb::Comparator;\n+using leveldb::CompressionType;\n+using leveldb::DB;\n+using leveldb::Env;\n+using leveldb::FileLock;\n+using leveldb::FilterPolicy;\n+using leveldb::Iterator;\n+using leveldb::kMajorVersion;\n+using leveldb::kMinorVersion;\n+using leveldb::Logger;\n+using leveldb::NewBloomFilterPolicy;\n+using leveldb::NewLRUCache;\n+using leveldb::Options;\n+using leveldb::RandomAccessFile;\n+using leveldb::Range;\n+using leveldb::ReadOptions;\n+using leveldb::SequentialFile;\n+using leveldb::Slice;\n+using leveldb::Snapshot;\n+using leveldb::Status;\n+using leveldb::WritableFile;\n+using leveldb::WriteBatch;\n+using leveldb::WriteOptions;\n+\n+extern \"C\" {\n+\n+struct leveldb_t              { DB*               rep; };\n+struct leveldb_iterator_t     { Iterator*         rep; };\n+struct leveldb_writebatch_t   { WriteBatch        rep; };\n+struct leveldb_snapshot_t     { const Snapshot*   rep; };\n+struct leveldb_readoptions_t  { ReadOptions       rep; };\n+struct leveldb_writeoptions_t { WriteOptions      rep; };\n+struct leveldb_options_t      { Options           rep; };\n+struct leveldb_cache_t        { Cache*            rep; };\n+struct leveldb_seqfile_t      { SequentialFile*   rep; };\n+struct leveldb_randomfile_t   { RandomAccessFile* rep; };\n+struct leveldb_writablefile_t { WritableFile*     rep; };\n+struct leveldb_logger_t       { Logger*           rep; };\n+struct leveldb_filelock_t     { FileLock*         rep; };\n+\n+struct leveldb_comparator_t : public Comparator {\n+  void* state_;\n+  void (*destructor_)(void*);\n+  int (*compare_)(\n+      void*,\n+      const char* a, size_t alen,\n+      const char* b, size_t blen);\n+  const char* (*name_)(void*);\n+\n+  virtual ~leveldb_comparator_t() {\n+    (*destructor_)(state_);\n+  }\n+\n+  virtual int Compare(const Slice& a, const Slice& b) const {\n+    return (*compare_)(state_, a.data(), a.size(), b.data(), b.size());\n+  }\n+\n+  virtual const char* Name() const {\n+    return (*name_)(state_);\n+  }\n+\n+  // No-ops since the C binding does not support key shortening methods.\n+  virtual void FindShortestSeparator(std::string*, const Slice&) const { }\n+  virtual void FindShortSuccessor(std::string* key) const { }\n+};\n+\n+struct leveldb_filterpolicy_t : public FilterPolicy {\n+  void* state_;\n+  void (*destructor_)(void*);\n+  const char* (*name_)(void*);\n+  char* (*create_)(\n+      void*,\n+      const char* const* key_array, const size_t* key_length_array,\n+      int num_keys,\n+      size_t* filter_length);\n+  unsigned char (*key_match_)(\n+      void*,\n+      const char* key, size_t length,\n+      const char* filter, size_t filter_length);\n+\n+  virtual ~leveldb_filterpolicy_t() {\n+    (*destructor_)(state_);\n+  }\n+\n+  virtual const char* Name() const {\n+    return (*name_)(state_);\n+  }\n+\n+  virtual void CreateFilter(const Slice* keys, int n, std::string* dst) const {\n+    std::vector<const char*> key_pointers(n);\n+    std::vector<size_t> key_sizes(n);\n+    for (int i = 0; i < n; i++) {\n+      key_pointers[i] = keys[i].data();\n+      key_sizes[i] = keys[i].size();\n+    }\n+    size_t len;\n+    char* filter = (*create_)(state_, &key_pointers[0], &key_sizes[0], n, &len);\n+    dst->append(filter, len);\n+    free(filter);\n+  }\n+\n+  virtual bool KeyMayMatch(const Slice& key, const Slice& filter) const {\n+    return (*key_match_)(state_, key.data(), key.size(),\n+                         filter.data(), filter.size());\n+  }\n+};\n+\n+struct leveldb_env_t {\n+  Env* rep;\n+  bool is_default;\n+};\n+\n+static bool SaveError(char** errptr, const Status& s) {\n+  assert(errptr != NULL);\n+  if (s.ok()) {\n+    return false;\n+  } else if (*errptr == NULL) {\n+    *errptr = strdup(s.ToString().c_str());\n+  } else {\n+    // TODO(sanjay): Merge with existing error?\n+    free(*errptr);\n+    *errptr = strdup(s.ToString().c_str());\n+  }\n+  return true;\n+}\n+\n+static char* CopyString(const std::string& str) {\n+  char* result = reinterpret_cast<char*>(malloc(sizeof(char) * str.size()));\n+  memcpy(result, str.data(), sizeof(char) * str.size());\n+  return result;\n+}\n+\n+leveldb_t* leveldb_open(\n+    const leveldb_options_t* options,\n+    const char* name,\n+    char** errptr) {\n+  DB* db;\n+  if (SaveError(errptr, DB::Open(options->rep, std::string(name), &db))) {\n+    return NULL;\n+  }\n+  leveldb_t* result = new leveldb_t;\n+  result->rep = db;\n+  return result;\n+}\n+\n+void leveldb_close(leveldb_t* db) {\n+  delete db->rep;\n+  delete db;\n+}\n+\n+void leveldb_put(\n+    leveldb_t* db,\n+    const leveldb_writeoptions_t* options,\n+    const char* key, size_t keylen,\n+    const char* val, size_t vallen,\n+    char** errptr) {\n+  SaveError(errptr,\n+            db->rep->Put(options->rep, Slice(key, keylen), Slice(val, vallen)));\n+}\n+\n+void leveldb_delete(\n+    leveldb_t* db,\n+    const leveldb_writeoptions_t* options,\n+    const char* key, size_t keylen,\n+    char** errptr) {\n+  SaveError(errptr, db->rep->Delete(options->rep, Slice(key, keylen)));\n+}\n+\n+\n+void leveldb_write(\n+    leveldb_t* db,\n+    const leveldb_writeoptions_t* options,\n+    leveldb_writebatch_t* batch,\n+    char** errptr) {\n+  SaveError(errptr, db->rep->Write(options->rep, &batch->rep));\n+}\n+\n+char* leveldb_get(\n+    leveldb_t* db,\n+    const leveldb_readoptions_t* options,\n+    const char* key, size_t keylen,\n+    size_t* vallen,\n+    char** errptr) {\n+  char* result = NULL;\n+  std::string tmp;\n+  Status s = db->rep->Get(options->rep, Slice(key, keylen), &tmp);\n+  if (s.ok()) {\n+    *vallen = tmp.size();\n+    result = CopyString(tmp);\n+  } else {\n+    *vallen = 0;\n+    if (!s.IsNotFound()) {\n+      SaveError(errptr, s);\n+    }\n+  }\n+  return result;\n+}\n+\n+leveldb_iterator_t* leveldb_create_iterator(\n+    leveldb_t* db,\n+    const leveldb_readoptions_t* options) {\n+  leveldb_iterator_t* result = new leveldb_iterator_t;\n+  result->rep = db->rep->NewIterator(options->rep);\n+  return result;\n+}\n+\n+const leveldb_snapshot_t* leveldb_create_snapshot(\n+    leveldb_t* db) {\n+  leveldb_snapshot_t* result = new leveldb_snapshot_t;\n+  result->rep = db->rep->GetSnapshot();\n+  return result;\n+}\n+\n+void leveldb_release_snapshot(\n+    leveldb_t* db,\n+    const leveldb_snapshot_t* snapshot) {\n+  db->rep->ReleaseSnapshot(snapshot->rep);\n+  delete snapshot;\n+}\n+\n+char* leveldb_property_value(\n+    leveldb_t* db,\n+    const char* propname) {\n+  std::string tmp;\n+  if (db->rep->GetProperty(Slice(propname), &tmp)) {\n+    // We use strdup() since we expect human readable output.\n+    return strdup(tmp.c_str());\n+  } else {\n+    return NULL;\n+  }\n+}\n+\n+void leveldb_approximate_sizes(\n+    leveldb_t* db,\n+    int num_ranges,\n+    const char* const* range_start_key, const size_t* range_start_key_len,\n+    const char* const* range_limit_key, const size_t* range_limit_key_len,\n+    uint64_t* sizes) {\n+  Range* ranges = new Range[num_ranges];\n+  for (int i = 0; i < num_ranges; i++) {\n+    ranges[i].start = Slice(range_start_key[i], range_start_key_len[i]);\n+    ranges[i].limit = Slice(range_limit_key[i], range_limit_key_len[i]);\n+  }\n+  db->rep->GetApproximateSizes(ranges, num_ranges, sizes);\n+  delete[] ranges;\n+}\n+\n+void leveldb_compact_range(\n+    leveldb_t* db,\n+    const char* start_key, size_t start_key_len,\n+    const char* limit_key, size_t limit_key_len) {\n+  Slice a, b;\n+  db->rep->CompactRange(\n+      // Pass NULL Slice if corresponding \"const char*\" is NULL\n+      (start_key ? (a = Slice(start_key, start_key_len), &a) : NULL),\n+      (limit_key ? (b = Slice(limit_key, limit_key_len), &b) : NULL));\n+}\n+\n+void leveldb_destroy_db(\n+    const leveldb_options_t* options,\n+    const char* name,\n+    char** errptr) {\n+  SaveError(errptr, DestroyDB(name, options->rep));\n+}\n+\n+void leveldb_repair_db(\n+    const leveldb_options_t* options,\n+    const char* name,\n+    char** errptr) {\n+  SaveError(errptr, RepairDB(name, options->rep));\n+}\n+\n+void leveldb_iter_destroy(leveldb_iterator_t* iter) {\n+  delete iter->rep;\n+  delete iter;\n+}\n+\n+unsigned char leveldb_iter_valid(const leveldb_iterator_t* iter) {\n+  return iter->rep->Valid();\n+}\n+\n+void leveldb_iter_seek_to_first(leveldb_iterator_t* iter) {\n+  iter->rep->SeekToFirst();\n+}\n+\n+void leveldb_iter_seek_to_last(leveldb_iterator_t* iter) {\n+  iter->rep->SeekToLast();\n+}\n+\n+void leveldb_iter_seek(leveldb_iterator_t* iter, const char* k, size_t klen) {\n+  iter->rep->Seek(Slice(k, klen));\n+}\n+\n+void leveldb_iter_next(leveldb_iterator_t* iter) {\n+  iter->rep->Next();\n+}\n+\n+void leveldb_iter_prev(leveldb_iterator_t* iter) {\n+  iter->rep->Prev();\n+}\n+\n+const char* leveldb_iter_key(const leveldb_iterator_t* iter, size_t* klen) {\n+  Slice s = iter->rep->key();\n+  *klen = s.size();\n+  return s.data();\n+}\n+\n+const char* leveldb_iter_value(const leveldb_iterator_t* iter, size_t* vlen) {\n+  Slice s = iter->rep->value();\n+  *vlen = s.size();\n+  return s.data();\n+}\n+\n+void leveldb_iter_get_error(const leveldb_iterator_t* iter, char** errptr) {\n+  SaveError(errptr, iter->rep->status());\n+}\n+\n+leveldb_writebatch_t* leveldb_writebatch_create() {\n+  return new leveldb_writebatch_t;\n+}\n+\n+void leveldb_writebatch_destroy(leveldb_writebatch_t* b) {\n+  delete b;\n+}\n+\n+void leveldb_writebatch_clear(leveldb_writebatch_t* b) {\n+  b->rep.Clear();\n+}\n+\n+void leveldb_writebatch_put(\n+    leveldb_writebatch_t* b,\n+    const char* key, size_t klen,\n+    const char* val, size_t vlen) {\n+  b->rep.Put(Slice(key, klen), Slice(val, vlen));\n+}\n+\n+void leveldb_writebatch_delete(\n+    leveldb_writebatch_t* b,\n+    const char* key, size_t klen) {\n+  b->rep.Delete(Slice(key, klen));\n+}\n+\n+void leveldb_writebatch_iterate(\n+    leveldb_writebatch_t* b,\n+    void* state,\n+    void (*put)(void*, const char* k, size_t klen, const char* v, size_t vlen),\n+    void (*deleted)(void*, const char* k, size_t klen)) {\n+  class H : public WriteBatch::Handler {\n+   public:\n+    void* state_;\n+    void (*put_)(void*, const char* k, size_t klen, const char* v, size_t vlen);\n+    void (*deleted_)(void*, const char* k, size_t klen);\n+    virtual void Put(const Slice& key, const Slice& value) {\n+      (*put_)(state_, key.data(), key.size(), value.data(), value.size());\n+    }\n+    virtual void Delete(const Slice& key) {\n+      (*deleted_)(state_, key.data(), key.size());\n+    }\n+  };\n+  H handler;\n+  handler.state_ = state;\n+  handler.put_ = put;\n+  handler.deleted_ = deleted;\n+  b->rep.Iterate(&handler);\n+}\n+\n+leveldb_options_t* leveldb_options_create() {\n+  return new leveldb_options_t;\n+}\n+\n+void leveldb_options_destroy(leveldb_options_t* options) {\n+  delete options;\n+}\n+\n+void leveldb_options_set_comparator(\n+    leveldb_options_t* opt,\n+    leveldb_comparator_t* cmp) {\n+  opt->rep.comparator = cmp;\n+}\n+\n+void leveldb_options_set_filter_policy(\n+    leveldb_options_t* opt,\n+    leveldb_filterpolicy_t* policy) {\n+  opt->rep.filter_policy = policy;\n+}\n+\n+void leveldb_options_set_create_if_missing(\n+    leveldb_options_t* opt, unsigned char v) {\n+  opt->rep.create_if_missing = v;\n+}\n+\n+void leveldb_options_set_error_if_exists(\n+    leveldb_options_t* opt, unsigned char v) {\n+  opt->rep.error_if_exists = v;\n+}\n+\n+void leveldb_options_set_paranoid_checks(\n+    leveldb_options_t* opt, unsigned char v) {\n+  opt->rep.paranoid_checks = v;\n+}\n+\n+void leveldb_options_set_env(leveldb_options_t* opt, leveldb_env_t* env) {\n+  opt->rep.env = (env ? env->rep : NULL);\n+}\n+\n+void leveldb_options_set_info_log(leveldb_options_t* opt, leveldb_logger_t* l) {\n+  opt->rep.info_log = (l ? l->rep : NULL);\n+}\n+\n+void leveldb_options_set_write_buffer_size(leveldb_options_t* opt, size_t s) {\n+  opt->rep.write_buffer_size = s;\n+}\n+\n+void leveldb_options_set_max_open_files(leveldb_options_t* opt, int n) {\n+  opt->rep.max_open_files = n;\n+}\n+\n+void leveldb_options_set_cache(leveldb_options_t* opt, leveldb_cache_t* c) {\n+  opt->rep.block_cache = c->rep;\n+}\n+\n+void leveldb_options_set_block_size(leveldb_options_t* opt, size_t s) {\n+  opt->rep.block_size = s;\n+}\n+\n+void leveldb_options_set_block_restart_interval(leveldb_options_t* opt, int n) {\n+  opt->rep.block_restart_interval = n;\n+}\n+\n+void leveldb_options_set_compression(leveldb_options_t* opt, int t) {\n+  opt->rep.compression = static_cast<CompressionType>(t);\n+}\n+\n+leveldb_comparator_t* leveldb_comparator_create(\n+    void* state,\n+    void (*destructor)(void*),\n+    int (*compare)(\n+        void*,\n+        const char* a, size_t alen,\n+        const char* b, size_t blen),\n+    const char* (*name)(void*)) {\n+  leveldb_comparator_t* result = new leveldb_comparator_t;\n+  result->state_ = state;\n+  result->destructor_ = destructor;\n+  result->compare_ = compare;\n+  result->name_ = name;\n+  return result;\n+}\n+\n+void leveldb_comparator_destroy(leveldb_comparator_t* cmp) {\n+  delete cmp;\n+}\n+\n+leveldb_filterpolicy_t* leveldb_filterpolicy_create(\n+    void* state,\n+    void (*destructor)(void*),\n+    char* (*create_filter)(\n+        void*,\n+        const char* const* key_array, const size_t* key_length_array,\n+        int num_keys,\n+        size_t* filter_length),\n+    unsigned char (*key_may_match)(\n+        void*,\n+        const char* key, size_t length,\n+        const char* filter, size_t filter_length),\n+    const char* (*name)(void*)) {\n+  leveldb_filterpolicy_t* result = new leveldb_filterpolicy_t;\n+  result->state_ = state;\n+  result->destructor_ = destructor;\n+  result->create_ = create_filter;\n+  result->key_match_ = key_may_match;\n+  result->name_ = name;\n+  return result;\n+}\n+\n+void leveldb_filterpolicy_destroy(leveldb_filterpolicy_t* filter) {\n+  delete filter;\n+}\n+\n+leveldb_filterpolicy_t* leveldb_filterpolicy_create_bloom(int bits_per_key) {\n+  // Make a leveldb_filterpolicy_t, but override all of its methods so\n+  // they delegate to a NewBloomFilterPolicy() instead of user\n+  // supplied C functions.\n+  struct Wrapper : public leveldb_filterpolicy_t {\n+    const FilterPolicy* rep_;\n+    ~Wrapper() { delete rep_; }\n+    const char* Name() const { return rep_->Name(); }\n+    void CreateFilter(const Slice* keys, int n, std::string* dst) const {\n+      return rep_->CreateFilter(keys, n, dst);\n+    }\n+    bool KeyMayMatch(const Slice& key, const Slice& filter) const {\n+      return rep_->KeyMayMatch(key, filter);\n+    }\n+    static void DoNothing(void*) { }\n+  };\n+  Wrapper* wrapper = new Wrapper;\n+  wrapper->rep_ = NewBloomFilterPolicy(bits_per_key);\n+  wrapper->state_ = NULL;\n+  wrapper->destructor_ = &Wrapper::DoNothing;\n+  return wrapper;\n+}\n+\n+leveldb_readoptions_t* leveldb_readoptions_create() {\n+  return new leveldb_readoptions_t;\n+}\n+\n+void leveldb_readoptions_destroy(leveldb_readoptions_t* opt) {\n+  delete opt;\n+}\n+\n+void leveldb_readoptions_set_verify_checksums(\n+    leveldb_readoptions_t* opt,\n+    unsigned char v) {\n+  opt->rep.verify_checksums = v;\n+}\n+\n+void leveldb_readoptions_set_fill_cache(\n+    leveldb_readoptions_t* opt, unsigned char v) {\n+  opt->rep.fill_cache = v;\n+}\n+\n+void leveldb_readoptions_set_snapshot(\n+    leveldb_readoptions_t* opt,\n+    const leveldb_snapshot_t* snap) {\n+  opt->rep.snapshot = (snap ? snap->rep : NULL);\n+}\n+\n+leveldb_writeoptions_t* leveldb_writeoptions_create() {\n+  return new leveldb_writeoptions_t;\n+}\n+\n+void leveldb_writeoptions_destroy(leveldb_writeoptions_t* opt) {\n+  delete opt;\n+}\n+\n+void leveldb_writeoptions_set_sync(\n+    leveldb_writeoptions_t* opt, unsigned char v) {\n+  opt->rep.sync = v;\n+}\n+\n+leveldb_cache_t* leveldb_cache_create_lru(size_t capacity) {\n+  leveldb_cache_t* c = new leveldb_cache_t;\n+  c->rep = NewLRUCache(capacity);\n+  return c;\n+}\n+\n+void leveldb_cache_destroy(leveldb_cache_t* cache) {\n+  delete cache->rep;\n+  delete cache;\n+}\n+\n+leveldb_env_t* leveldb_create_default_env() {\n+  leveldb_env_t* result = new leveldb_env_t;\n+  result->rep = Env::Default();\n+  result->is_default = true;\n+  return result;\n+}\n+\n+void leveldb_env_destroy(leveldb_env_t* env) {\n+  if (!env->is_default) delete env->rep;\n+  delete env;\n+}\n+\n+void leveldb_free(void* ptr) {\n+  free(ptr);\n+}\n+\n+int leveldb_major_version() {\n+  return kMajorVersion;\n+}\n+\n+int leveldb_minor_version() {\n+  return kMinorVersion;\n+}\n+\n+}  // end extern \"C\""
      },
      {
        "sha": "7cd5ee02076ab96303a2b1f25dbabbe721caaba7",
        "filename": "db/c_test.c",
        "status": "added",
        "additions": 390,
        "deletions": 0,
        "changes": 390,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/c_test.c",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/c_test.c",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/c_test.c?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,390 @@\n+/* Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+   Use of this source code is governed by a BSD-style license that can be\n+   found in the LICENSE file. See the AUTHORS file for names of contributors. */\n+\n+#include \"leveldb/c.h\"\n+\n+#include <stddef.h>\n+#include <stdio.h>\n+#include <stdlib.h>\n+#include <string.h>\n+#include <sys/types.h>\n+#include <unistd.h>\n+\n+const char* phase = \"\";\n+static char dbname[200];\n+\n+static void StartPhase(const char* name) {\n+  fprintf(stderr, \"=== Test %s\\n\", name);\n+  phase = name;\n+}\n+\n+static const char* GetTempDir(void) {\n+    const char* ret = getenv(\"TEST_TMPDIR\");\n+    if (ret == NULL || ret[0] == '\\0')\n+        ret = \"/tmp\";\n+    return ret;\n+}\n+\n+#define CheckNoError(err)                                               \\\n+  if ((err) != NULL) {                                                  \\\n+    fprintf(stderr, \"%s:%d: %s: %s\\n\", __FILE__, __LINE__, phase, (err)); \\\n+    abort();                                                            \\\n+  }\n+\n+#define CheckCondition(cond)                                            \\\n+  if (!(cond)) {                                                        \\\n+    fprintf(stderr, \"%s:%d: %s: %s\\n\", __FILE__, __LINE__, phase, #cond); \\\n+    abort();                                                            \\\n+  }\n+\n+static void CheckEqual(const char* expected, const char* v, size_t n) {\n+  if (expected == NULL && v == NULL) {\n+    // ok\n+  } else if (expected != NULL && v != NULL && n == strlen(expected) &&\n+             memcmp(expected, v, n) == 0) {\n+    // ok\n+    return;\n+  } else {\n+    fprintf(stderr, \"%s: expected '%s', got '%s'\\n\",\n+            phase,\n+            (expected ? expected : \"(null)\"),\n+            (v ? v : \"(null\"));\n+    abort();\n+  }\n+}\n+\n+static void Free(char** ptr) {\n+  if (*ptr) {\n+    free(*ptr);\n+    *ptr = NULL;\n+  }\n+}\n+\n+static void CheckGet(\n+    leveldb_t* db,\n+    const leveldb_readoptions_t* options,\n+    const char* key,\n+    const char* expected) {\n+  char* err = NULL;\n+  size_t val_len;\n+  char* val;\n+  val = leveldb_get(db, options, key, strlen(key), &val_len, &err);\n+  CheckNoError(err);\n+  CheckEqual(expected, val, val_len);\n+  Free(&val);\n+}\n+\n+static void CheckIter(leveldb_iterator_t* iter,\n+                      const char* key, const char* val) {\n+  size_t len;\n+  const char* str;\n+  str = leveldb_iter_key(iter, &len);\n+  CheckEqual(key, str, len);\n+  str = leveldb_iter_value(iter, &len);\n+  CheckEqual(val, str, len);\n+}\n+\n+// Callback from leveldb_writebatch_iterate()\n+static void CheckPut(void* ptr,\n+                     const char* k, size_t klen,\n+                     const char* v, size_t vlen) {\n+  int* state = (int*) ptr;\n+  CheckCondition(*state < 2);\n+  switch (*state) {\n+    case 0:\n+      CheckEqual(\"bar\", k, klen);\n+      CheckEqual(\"b\", v, vlen);\n+      break;\n+    case 1:\n+      CheckEqual(\"box\", k, klen);\n+      CheckEqual(\"c\", v, vlen);\n+      break;\n+  }\n+  (*state)++;\n+}\n+\n+// Callback from leveldb_writebatch_iterate()\n+static void CheckDel(void* ptr, const char* k, size_t klen) {\n+  int* state = (int*) ptr;\n+  CheckCondition(*state == 2);\n+  CheckEqual(\"bar\", k, klen);\n+  (*state)++;\n+}\n+\n+static void CmpDestroy(void* arg) { }\n+\n+static int CmpCompare(void* arg, const char* a, size_t alen,\n+                      const char* b, size_t blen) {\n+  int n = (alen < blen) ? alen : blen;\n+  int r = memcmp(a, b, n);\n+  if (r == 0) {\n+    if (alen < blen) r = -1;\n+    else if (alen > blen) r = +1;\n+  }\n+  return r;\n+}\n+\n+static const char* CmpName(void* arg) {\n+  return \"foo\";\n+}\n+\n+// Custom filter policy\n+static unsigned char fake_filter_result = 1;\n+static void FilterDestroy(void* arg) { }\n+static const char* FilterName(void* arg) {\n+  return \"TestFilter\";\n+}\n+static char* FilterCreate(\n+    void* arg,\n+    const char* const* key_array, const size_t* key_length_array,\n+    int num_keys,\n+    size_t* filter_length) {\n+  *filter_length = 4;\n+  char* result = malloc(4);\n+  memcpy(result, \"fake\", 4);\n+  return result;\n+}\n+unsigned char FilterKeyMatch(\n+    void* arg,\n+    const char* key, size_t length,\n+    const char* filter, size_t filter_length) {\n+  CheckCondition(filter_length == 4);\n+  CheckCondition(memcmp(filter, \"fake\", 4) == 0);\n+  return fake_filter_result;\n+}\n+\n+int main(int argc, char** argv) {\n+  leveldb_t* db;\n+  leveldb_comparator_t* cmp;\n+  leveldb_cache_t* cache;\n+  leveldb_env_t* env;\n+  leveldb_options_t* options;\n+  leveldb_readoptions_t* roptions;\n+  leveldb_writeoptions_t* woptions;\n+  char* err = NULL;\n+  int run = -1;\n+\n+  CheckCondition(leveldb_major_version() >= 1);\n+  CheckCondition(leveldb_minor_version() >= 1);\n+\n+  snprintf(dbname, sizeof(dbname),\n+           \"%s/leveldb_c_test-%d\",\n+           GetTempDir(),\n+           ((int) geteuid()));\n+\n+  StartPhase(\"create_objects\");\n+  cmp = leveldb_comparator_create(NULL, CmpDestroy, CmpCompare, CmpName);\n+  env = leveldb_create_default_env();\n+  cache = leveldb_cache_create_lru(100000);\n+\n+  options = leveldb_options_create();\n+  leveldb_options_set_comparator(options, cmp);\n+  leveldb_options_set_error_if_exists(options, 1);\n+  leveldb_options_set_cache(options, cache);\n+  leveldb_options_set_env(options, env);\n+  leveldb_options_set_info_log(options, NULL);\n+  leveldb_options_set_write_buffer_size(options, 100000);\n+  leveldb_options_set_paranoid_checks(options, 1);\n+  leveldb_options_set_max_open_files(options, 10);\n+  leveldb_options_set_block_size(options, 1024);\n+  leveldb_options_set_block_restart_interval(options, 8);\n+  leveldb_options_set_compression(options, leveldb_no_compression);\n+\n+  roptions = leveldb_readoptions_create();\n+  leveldb_readoptions_set_verify_checksums(roptions, 1);\n+  leveldb_readoptions_set_fill_cache(roptions, 0);\n+\n+  woptions = leveldb_writeoptions_create();\n+  leveldb_writeoptions_set_sync(woptions, 1);\n+\n+  StartPhase(\"destroy\");\n+  leveldb_destroy_db(options, dbname, &err);\n+  Free(&err);\n+\n+  StartPhase(\"open_error\");\n+  db = leveldb_open(options, dbname, &err);\n+  CheckCondition(err != NULL);\n+  Free(&err);\n+\n+  StartPhase(\"leveldb_free\");\n+  db = leveldb_open(options, dbname, &err);\n+  CheckCondition(err != NULL);\n+  leveldb_free(err);\n+  err = NULL;\n+\n+  StartPhase(\"open\");\n+  leveldb_options_set_create_if_missing(options, 1);\n+  db = leveldb_open(options, dbname, &err);\n+  CheckNoError(err);\n+  CheckGet(db, roptions, \"foo\", NULL);\n+\n+  StartPhase(\"put\");\n+  leveldb_put(db, woptions, \"foo\", 3, \"hello\", 5, &err);\n+  CheckNoError(err);\n+  CheckGet(db, roptions, \"foo\", \"hello\");\n+\n+  StartPhase(\"compactall\");\n+  leveldb_compact_range(db, NULL, 0, NULL, 0);\n+  CheckGet(db, roptions, \"foo\", \"hello\");\n+\n+  StartPhase(\"compactrange\");\n+  leveldb_compact_range(db, \"a\", 1, \"z\", 1);\n+  CheckGet(db, roptions, \"foo\", \"hello\");\n+\n+  StartPhase(\"writebatch\");\n+  {\n+    leveldb_writebatch_t* wb = leveldb_writebatch_create();\n+    leveldb_writebatch_put(wb, \"foo\", 3, \"a\", 1);\n+    leveldb_writebatch_clear(wb);\n+    leveldb_writebatch_put(wb, \"bar\", 3, \"b\", 1);\n+    leveldb_writebatch_put(wb, \"box\", 3, \"c\", 1);\n+    leveldb_writebatch_delete(wb, \"bar\", 3);\n+    leveldb_write(db, woptions, wb, &err);\n+    CheckNoError(err);\n+    CheckGet(db, roptions, \"foo\", \"hello\");\n+    CheckGet(db, roptions, \"bar\", NULL);\n+    CheckGet(db, roptions, \"box\", \"c\");\n+    int pos = 0;\n+    leveldb_writebatch_iterate(wb, &pos, CheckPut, CheckDel);\n+    CheckCondition(pos == 3);\n+    leveldb_writebatch_destroy(wb);\n+  }\n+\n+  StartPhase(\"iter\");\n+  {\n+    leveldb_iterator_t* iter = leveldb_create_iterator(db, roptions);\n+    CheckCondition(!leveldb_iter_valid(iter));\n+    leveldb_iter_seek_to_first(iter);\n+    CheckCondition(leveldb_iter_valid(iter));\n+    CheckIter(iter, \"box\", \"c\");\n+    leveldb_iter_next(iter);\n+    CheckIter(iter, \"foo\", \"hello\");\n+    leveldb_iter_prev(iter);\n+    CheckIter(iter, \"box\", \"c\");\n+    leveldb_iter_prev(iter);\n+    CheckCondition(!leveldb_iter_valid(iter));\n+    leveldb_iter_seek_to_last(iter);\n+    CheckIter(iter, \"foo\", \"hello\");\n+    leveldb_iter_seek(iter, \"b\", 1);\n+    CheckIter(iter, \"box\", \"c\");\n+    leveldb_iter_get_error(iter, &err);\n+    CheckNoError(err);\n+    leveldb_iter_destroy(iter);\n+  }\n+\n+  StartPhase(\"approximate_sizes\");\n+  {\n+    int i;\n+    int n = 20000;\n+    char keybuf[100];\n+    char valbuf[100];\n+    uint64_t sizes[2];\n+    const char* start[2] = { \"a\", \"k00000000000000010000\" };\n+    size_t start_len[2] = { 1, 21 };\n+    const char* limit[2] = { \"k00000000000000010000\", \"z\" };\n+    size_t limit_len[2] = { 21, 1 };\n+    leveldb_writeoptions_set_sync(woptions, 0);\n+    for (i = 0; i < n; i++) {\n+      snprintf(keybuf, sizeof(keybuf), \"k%020d\", i);\n+      snprintf(valbuf, sizeof(valbuf), \"v%020d\", i);\n+      leveldb_put(db, woptions, keybuf, strlen(keybuf), valbuf, strlen(valbuf),\n+                  &err);\n+      CheckNoError(err);\n+    }\n+    leveldb_approximate_sizes(db, 2, start, start_len, limit, limit_len, sizes);\n+    CheckCondition(sizes[0] > 0);\n+    CheckCondition(sizes[1] > 0);\n+  }\n+\n+  StartPhase(\"property\");\n+  {\n+    char* prop = leveldb_property_value(db, \"nosuchprop\");\n+    CheckCondition(prop == NULL);\n+    prop = leveldb_property_value(db, \"leveldb.stats\");\n+    CheckCondition(prop != NULL);\n+    Free(&prop);\n+  }\n+\n+  StartPhase(\"snapshot\");\n+  {\n+    const leveldb_snapshot_t* snap;\n+    snap = leveldb_create_snapshot(db);\n+    leveldb_delete(db, woptions, \"foo\", 3, &err);\n+    CheckNoError(err);\n+    leveldb_readoptions_set_snapshot(roptions, snap);\n+    CheckGet(db, roptions, \"foo\", \"hello\");\n+    leveldb_readoptions_set_snapshot(roptions, NULL);\n+    CheckGet(db, roptions, \"foo\", NULL);\n+    leveldb_release_snapshot(db, snap);\n+  }\n+\n+  StartPhase(\"repair\");\n+  {\n+    leveldb_close(db);\n+    leveldb_options_set_create_if_missing(options, 0);\n+    leveldb_options_set_error_if_exists(options, 0);\n+    leveldb_repair_db(options, dbname, &err);\n+    CheckNoError(err);\n+    db = leveldb_open(options, dbname, &err);\n+    CheckNoError(err);\n+    CheckGet(db, roptions, \"foo\", NULL);\n+    CheckGet(db, roptions, \"bar\", NULL);\n+    CheckGet(db, roptions, \"box\", \"c\");\n+    leveldb_options_set_create_if_missing(options, 1);\n+    leveldb_options_set_error_if_exists(options, 1);\n+  }\n+\n+  StartPhase(\"filter\");\n+  for (run = 0; run < 2; run++) {\n+    // First run uses custom filter, second run uses bloom filter\n+    CheckNoError(err);\n+    leveldb_filterpolicy_t* policy;\n+    if (run == 0) {\n+      policy = leveldb_filterpolicy_create(\n+          NULL, FilterDestroy, FilterCreate, FilterKeyMatch, FilterName);\n+    } else {\n+      policy = leveldb_filterpolicy_create_bloom(10);\n+    }\n+\n+    // Create new database\n+    leveldb_close(db);\n+    leveldb_destroy_db(options, dbname, &err);\n+    leveldb_options_set_filter_policy(options, policy);\n+    db = leveldb_open(options, dbname, &err);\n+    CheckNoError(err);\n+    leveldb_put(db, woptions, \"foo\", 3, \"foovalue\", 8, &err);\n+    CheckNoError(err);\n+    leveldb_put(db, woptions, \"bar\", 3, \"barvalue\", 8, &err);\n+    CheckNoError(err);\n+    leveldb_compact_range(db, NULL, 0, NULL, 0);\n+\n+    fake_filter_result = 1;\n+    CheckGet(db, roptions, \"foo\", \"foovalue\");\n+    CheckGet(db, roptions, \"bar\", \"barvalue\");\n+    if (phase == 0) {\n+      // Must not find value when custom filter returns false\n+      fake_filter_result = 0;\n+      CheckGet(db, roptions, \"foo\", NULL);\n+      CheckGet(db, roptions, \"bar\", NULL);\n+      fake_filter_result = 1;\n+\n+      CheckGet(db, roptions, \"foo\", \"foovalue\");\n+      CheckGet(db, roptions, \"bar\", \"barvalue\");\n+    }\n+    leveldb_options_set_filter_policy(options, NULL);\n+    leveldb_filterpolicy_destroy(policy);\n+  }\n+\n+  StartPhase(\"cleanup\");\n+  leveldb_close(db);\n+  leveldb_options_destroy(options);\n+  leveldb_readoptions_destroy(roptions);\n+  leveldb_writeoptions_destroy(woptions);\n+  leveldb_cache_destroy(cache);\n+  leveldb_comparator_destroy(cmp);\n+  leveldb_env_destroy(env);\n+\n+  fprintf(stderr, \"PASS\\n\");\n+  return 0;\n+}"
      },
      {
        "sha": "31b2d5f4166bc200122c1773b1a3e7fdcff7c640",
        "filename": "db/corruption_test.cc",
        "status": "added",
        "additions": 359,
        "deletions": 0,
        "changes": 359,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/corruption_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/corruption_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/corruption_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,359 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"leveldb/db.h\"\n+\n+#include <errno.h>\n+#include <fcntl.h>\n+#include <sys/stat.h>\n+#include <sys/types.h>\n+#include \"leveldb/cache.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/table.h\"\n+#include \"leveldb/write_batch.h\"\n+#include \"db/db_impl.h\"\n+#include \"db/filename.h\"\n+#include \"db/log_format.h\"\n+#include \"db/version_set.h\"\n+#include \"util/logging.h\"\n+#include \"util/testharness.h\"\n+#include \"util/testutil.h\"\n+\n+namespace leveldb {\n+\n+static const int kValueSize = 1000;\n+\n+class CorruptionTest {\n+ public:\n+  test::ErrorEnv env_;\n+  std::string dbname_;\n+  Cache* tiny_cache_;\n+  Options options_;\n+  DB* db_;\n+\n+  CorruptionTest() {\n+    tiny_cache_ = NewLRUCache(100);\n+    options_.env = &env_;\n+    dbname_ = test::TmpDir() + \"/db_test\";\n+    DestroyDB(dbname_, options_);\n+\n+    db_ = NULL;\n+    options_.create_if_missing = true;\n+    Reopen();\n+    options_.create_if_missing = false;\n+  }\n+\n+  ~CorruptionTest() {\n+     delete db_;\n+     DestroyDB(dbname_, Options());\n+     delete tiny_cache_;\n+  }\n+\n+  Status TryReopen(Options* options = NULL) {\n+    delete db_;\n+    db_ = NULL;\n+    Options opt = (options ? *options : options_);\n+    opt.env = &env_;\n+    opt.block_cache = tiny_cache_;\n+    return DB::Open(opt, dbname_, &db_);\n+  }\n+\n+  void Reopen(Options* options = NULL) {\n+    ASSERT_OK(TryReopen(options));\n+  }\n+\n+  void RepairDB() {\n+    delete db_;\n+    db_ = NULL;\n+    ASSERT_OK(::leveldb::RepairDB(dbname_, options_));\n+  }\n+\n+  void Build(int n) {\n+    std::string key_space, value_space;\n+    WriteBatch batch;\n+    for (int i = 0; i < n; i++) {\n+      //if ((i % 100) == 0) fprintf(stderr, \"@ %d of %d\\n\", i, n);\n+      Slice key = Key(i, &key_space);\n+      batch.Clear();\n+      batch.Put(key, Value(i, &value_space));\n+      ASSERT_OK(db_->Write(WriteOptions(), &batch));\n+    }\n+  }\n+\n+  void Check(int min_expected, int max_expected) {\n+    int next_expected = 0;\n+    int missed = 0;\n+    int bad_keys = 0;\n+    int bad_values = 0;\n+    int correct = 0;\n+    std::string value_space;\n+    Iterator* iter = db_->NewIterator(ReadOptions());\n+    for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+      uint64_t key;\n+      Slice in(iter->key());\n+      if (!ConsumeDecimalNumber(&in, &key) ||\n+          !in.empty() ||\n+          key < next_expected) {\n+        bad_keys++;\n+        continue;\n+      }\n+      missed += (key - next_expected);\n+      next_expected = key + 1;\n+      if (iter->value() != Value(key, &value_space)) {\n+        bad_values++;\n+      } else {\n+        correct++;\n+      }\n+    }\n+    delete iter;\n+\n+    fprintf(stderr,\n+            \"expected=%d..%d; got=%d; bad_keys=%d; bad_values=%d; missed=%d\\n\",\n+            min_expected, max_expected, correct, bad_keys, bad_values, missed);\n+    ASSERT_LE(min_expected, correct);\n+    ASSERT_GE(max_expected, correct);\n+  }\n+\n+  void Corrupt(FileType filetype, int offset, int bytes_to_corrupt) {\n+    // Pick file to corrupt\n+    std::vector<std::string> filenames;\n+    ASSERT_OK(env_.GetChildren(dbname_, &filenames));\n+    uint64_t number;\n+    FileType type;\n+    std::string fname;\n+    int picked_number = -1;\n+    for (int i = 0; i < filenames.size(); i++) {\n+      if (ParseFileName(filenames[i], &number, &type) &&\n+          type == filetype &&\n+          int(number) > picked_number) {  // Pick latest file\n+        fname = dbname_ + \"/\" + filenames[i];\n+        picked_number = number;\n+      }\n+    }\n+    ASSERT_TRUE(!fname.empty()) << filetype;\n+\n+    struct stat sbuf;\n+    if (stat(fname.c_str(), &sbuf) != 0) {\n+      const char* msg = strerror(errno);\n+      ASSERT_TRUE(false) << fname << \": \" << msg;\n+    }\n+\n+    if (offset < 0) {\n+      // Relative to end of file; make it absolute\n+      if (-offset > sbuf.st_size) {\n+        offset = 0;\n+      } else {\n+        offset = sbuf.st_size + offset;\n+      }\n+    }\n+    if (offset > sbuf.st_size) {\n+      offset = sbuf.st_size;\n+    }\n+    if (offset + bytes_to_corrupt > sbuf.st_size) {\n+      bytes_to_corrupt = sbuf.st_size - offset;\n+    }\n+\n+    // Do it\n+    std::string contents;\n+    Status s = ReadFileToString(Env::Default(), fname, &contents);\n+    ASSERT_TRUE(s.ok()) << s.ToString();\n+    for (int i = 0; i < bytes_to_corrupt; i++) {\n+      contents[i + offset] ^= 0x80;\n+    }\n+    s = WriteStringToFile(Env::Default(), contents, fname);\n+    ASSERT_TRUE(s.ok()) << s.ToString();\n+  }\n+\n+  int Property(const std::string& name) {\n+    std::string property;\n+    int result;\n+    if (db_->GetProperty(name, &property) &&\n+        sscanf(property.c_str(), \"%d\", &result) == 1) {\n+      return result;\n+    } else {\n+      return -1;\n+    }\n+  }\n+\n+  // Return the ith key\n+  Slice Key(int i, std::string* storage) {\n+    char buf[100];\n+    snprintf(buf, sizeof(buf), \"%016d\", i);\n+    storage->assign(buf, strlen(buf));\n+    return Slice(*storage);\n+  }\n+\n+  // Return the value to associate with the specified key\n+  Slice Value(int k, std::string* storage) {\n+    Random r(k);\n+    return test::RandomString(&r, kValueSize, storage);\n+  }\n+};\n+\n+TEST(CorruptionTest, Recovery) {\n+  Build(100);\n+  Check(100, 100);\n+  Corrupt(kLogFile, 19, 1);      // WriteBatch tag for first record\n+  Corrupt(kLogFile, log::kBlockSize + 1000, 1);  // Somewhere in second block\n+  Reopen();\n+\n+  // The 64 records in the first two log blocks are completely lost.\n+  Check(36, 36);\n+}\n+\n+TEST(CorruptionTest, RecoverWriteError) {\n+  env_.writable_file_error_ = true;\n+  Status s = TryReopen();\n+  ASSERT_TRUE(!s.ok());\n+}\n+\n+TEST(CorruptionTest, NewFileErrorDuringWrite) {\n+  // Do enough writing to force minor compaction\n+  env_.writable_file_error_ = true;\n+  const int num = 3 + (Options().write_buffer_size / kValueSize);\n+  std::string value_storage;\n+  Status s;\n+  for (int i = 0; s.ok() && i < num; i++) {\n+    WriteBatch batch;\n+    batch.Put(\"a\", Value(100, &value_storage));\n+    s = db_->Write(WriteOptions(), &batch);\n+  }\n+  ASSERT_TRUE(!s.ok());\n+  ASSERT_GE(env_.num_writable_file_errors_, 1);\n+  env_.writable_file_error_ = false;\n+  Reopen();\n+}\n+\n+TEST(CorruptionTest, TableFile) {\n+  Build(100);\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n+  dbi->TEST_CompactMemTable();\n+  dbi->TEST_CompactRange(0, NULL, NULL);\n+  dbi->TEST_CompactRange(1, NULL, NULL);\n+\n+  Corrupt(kTableFile, 100, 1);\n+  Check(99, 99);\n+}\n+\n+TEST(CorruptionTest, TableFileIndexData) {\n+  Build(10000);  // Enough to build multiple Tables\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n+  dbi->TEST_CompactMemTable();\n+\n+  Corrupt(kTableFile, -2000, 500);\n+  Reopen();\n+  Check(5000, 9999);\n+}\n+\n+TEST(CorruptionTest, MissingDescriptor) {\n+  Build(1000);\n+  RepairDB();\n+  Reopen();\n+  Check(1000, 1000);\n+}\n+\n+TEST(CorruptionTest, SequenceNumberRecovery) {\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v1\"));\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v2\"));\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v3\"));\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v4\"));\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v5\"));\n+  RepairDB();\n+  Reopen();\n+  std::string v;\n+  ASSERT_OK(db_->Get(ReadOptions(), \"foo\", &v));\n+  ASSERT_EQ(\"v5\", v);\n+  // Write something.  If sequence number was not recovered properly,\n+  // it will be hidden by an earlier write.\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v6\"));\n+  ASSERT_OK(db_->Get(ReadOptions(), \"foo\", &v));\n+  ASSERT_EQ(\"v6\", v);\n+  Reopen();\n+  ASSERT_OK(db_->Get(ReadOptions(), \"foo\", &v));\n+  ASSERT_EQ(\"v6\", v);\n+}\n+\n+TEST(CorruptionTest, CorruptedDescriptor) {\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"hello\"));\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n+  dbi->TEST_CompactMemTable();\n+  dbi->TEST_CompactRange(0, NULL, NULL);\n+\n+  Corrupt(kDescriptorFile, 0, 1000);\n+  Status s = TryReopen();\n+  ASSERT_TRUE(!s.ok());\n+\n+  RepairDB();\n+  Reopen();\n+  std::string v;\n+  ASSERT_OK(db_->Get(ReadOptions(), \"foo\", &v));\n+  ASSERT_EQ(\"hello\", v);\n+}\n+\n+TEST(CorruptionTest, CompactionInputError) {\n+  Build(10);\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n+  dbi->TEST_CompactMemTable();\n+  const int last = config::kMaxMemCompactLevel;\n+  ASSERT_EQ(1, Property(\"leveldb.num-files-at-level\" + NumberToString(last)));\n+\n+  Corrupt(kTableFile, 100, 1);\n+  Check(9, 9);\n+\n+  // Force compactions by writing lots of values\n+  Build(10000);\n+  Check(10000, 10000);\n+}\n+\n+TEST(CorruptionTest, CompactionInputErrorParanoid) {\n+  Options options;\n+  options.paranoid_checks = true;\n+  options.write_buffer_size = 1048576;\n+  Reopen(&options);\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n+\n+  // Fill levels >= 1 so memtable compaction outputs to level 1\n+  for (int level = 1; level < config::kNumLevels; level++) {\n+    dbi->Put(WriteOptions(), \"\", \"begin\");\n+    dbi->Put(WriteOptions(), \"~\", \"end\");\n+    dbi->TEST_CompactMemTable();\n+  }\n+\n+  Build(10);\n+  dbi->TEST_CompactMemTable();\n+  ASSERT_EQ(1, Property(\"leveldb.num-files-at-level0\"));\n+\n+  Corrupt(kTableFile, 100, 1);\n+  Check(9, 9);\n+\n+  // Write must eventually fail because of corrupted table\n+  Status s;\n+  std::string tmp1, tmp2;\n+  for (int i = 0; i < 10000 && s.ok(); i++) {\n+    s = db_->Put(WriteOptions(), Key(i, &tmp1), Value(i, &tmp2));\n+  }\n+  ASSERT_TRUE(!s.ok()) << \"write did not fail in corrupted paranoid db\";\n+}\n+\n+TEST(CorruptionTest, UnrelatedKeys) {\n+  Build(10);\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n+  dbi->TEST_CompactMemTable();\n+  Corrupt(kTableFile, 100, 1);\n+\n+  std::string tmp1, tmp2;\n+  ASSERT_OK(db_->Put(WriteOptions(), Key(1000, &tmp1), Value(1000, &tmp2)));\n+  std::string v;\n+  ASSERT_OK(db_->Get(ReadOptions(), Key(1000, &tmp1), &v));\n+  ASSERT_EQ(Value(1000, &tmp2).ToString(), v);\n+  dbi->TEST_CompactMemTable();\n+  ASSERT_OK(db_->Get(ReadOptions(), Key(1000, &tmp1), &v));\n+  ASSERT_EQ(Value(1000, &tmp2).ToString(), v);\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "7abdf87587dfc8d4fa0287d8ad45d82b2142d7fc",
        "filename": "db/db_bench.cc",
        "status": "added",
        "additions": 979,
        "deletions": 0,
        "changes": 979,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/db_bench.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/db_bench.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/db_bench.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,979 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include <sys/types.h>\n+#include <stdio.h>\n+#include <stdlib.h>\n+#include \"db/db_impl.h\"\n+#include \"db/version_set.h\"\n+#include \"leveldb/cache.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/write_batch.h\"\n+#include \"port/port.h\"\n+#include \"util/crc32c.h\"\n+#include \"util/histogram.h\"\n+#include \"util/mutexlock.h\"\n+#include \"util/random.h\"\n+#include \"util/testutil.h\"\n+\n+// Comma-separated list of operations to run in the specified order\n+//   Actual benchmarks:\n+//      fillseq       -- write N values in sequential key order in async mode\n+//      fillrandom    -- write N values in random key order in async mode\n+//      overwrite     -- overwrite N values in random key order in async mode\n+//      fillsync      -- write N/100 values in random key order in sync mode\n+//      fill100K      -- write N/1000 100K values in random order in async mode\n+//      deleteseq     -- delete N keys in sequential order\n+//      deleterandom  -- delete N keys in random order\n+//      readseq       -- read N times sequentially\n+//      readreverse   -- read N times in reverse order\n+//      readrandom    -- read N times in random order\n+//      readmissing   -- read N missing keys in random order\n+//      readhot       -- read N times in random order from 1% section of DB\n+//      seekrandom    -- N random seeks\n+//      crc32c        -- repeated crc32c of 4K of data\n+//      acquireload   -- load N*1000 times\n+//   Meta operations:\n+//      compact     -- Compact the entire DB\n+//      stats       -- Print DB stats\n+//      sstables    -- Print sstable info\n+//      heapprofile -- Dump a heap profile (if supported by this port)\n+static const char* FLAGS_benchmarks =\n+    \"fillseq,\"\n+    \"fillsync,\"\n+    \"fillrandom,\"\n+    \"overwrite,\"\n+    \"readrandom,\"\n+    \"readrandom,\"  // Extra run to allow previous compactions to quiesce\n+    \"readseq,\"\n+    \"readreverse,\"\n+    \"compact,\"\n+    \"readrandom,\"\n+    \"readseq,\"\n+    \"readreverse,\"\n+    \"fill100K,\"\n+    \"crc32c,\"\n+    \"snappycomp,\"\n+    \"snappyuncomp,\"\n+    \"acquireload,\"\n+    ;\n+\n+// Number of key/values to place in database\n+static int FLAGS_num = 1000000;\n+\n+// Number of read operations to do.  If negative, do FLAGS_num reads.\n+static int FLAGS_reads = -1;\n+\n+// Number of concurrent threads to run.\n+static int FLAGS_threads = 1;\n+\n+// Size of each value\n+static int FLAGS_value_size = 100;\n+\n+// Arrange to generate values that shrink to this fraction of\n+// their original size after compression\n+static double FLAGS_compression_ratio = 0.5;\n+\n+// Print histogram of operation timings\n+static bool FLAGS_histogram = false;\n+\n+// Number of bytes to buffer in memtable before compacting\n+// (initialized to default value by \"main\")\n+static int FLAGS_write_buffer_size = 0;\n+\n+// Number of bytes to use as a cache of uncompressed data.\n+// Negative means use default settings.\n+static int FLAGS_cache_size = -1;\n+\n+// Maximum number of files to keep open at the same time (use default if == 0)\n+static int FLAGS_open_files = 0;\n+\n+// Bloom filter bits per key.\n+// Negative means use default settings.\n+static int FLAGS_bloom_bits = -1;\n+\n+// If true, do not destroy the existing database.  If you set this\n+// flag and also specify a benchmark that wants a fresh database, that\n+// benchmark will fail.\n+static bool FLAGS_use_existing_db = false;\n+\n+// Use the db with the following name.\n+static const char* FLAGS_db = NULL;\n+\n+namespace leveldb {\n+\n+namespace {\n+\n+// Helper for quickly generating random data.\n+class RandomGenerator {\n+ private:\n+  std::string data_;\n+  int pos_;\n+\n+ public:\n+  RandomGenerator() {\n+    // We use a limited amount of data over and over again and ensure\n+    // that it is larger than the compression window (32KB), and also\n+    // large enough to serve all typical value sizes we want to write.\n+    Random rnd(301);\n+    std::string piece;\n+    while (data_.size() < 1048576) {\n+      // Add a short fragment that is as compressible as specified\n+      // by FLAGS_compression_ratio.\n+      test::CompressibleString(&rnd, FLAGS_compression_ratio, 100, &piece);\n+      data_.append(piece);\n+    }\n+    pos_ = 0;\n+  }\n+\n+  Slice Generate(int len) {\n+    if (pos_ + len > data_.size()) {\n+      pos_ = 0;\n+      assert(len < data_.size());\n+    }\n+    pos_ += len;\n+    return Slice(data_.data() + pos_ - len, len);\n+  }\n+};\n+\n+static Slice TrimSpace(Slice s) {\n+  int start = 0;\n+  while (start < s.size() && isspace(s[start])) {\n+    start++;\n+  }\n+  int limit = s.size();\n+  while (limit > start && isspace(s[limit-1])) {\n+    limit--;\n+  }\n+  return Slice(s.data() + start, limit - start);\n+}\n+\n+static void AppendWithSpace(std::string* str, Slice msg) {\n+  if (msg.empty()) return;\n+  if (!str->empty()) {\n+    str->push_back(' ');\n+  }\n+  str->append(msg.data(), msg.size());\n+}\n+\n+class Stats {\n+ private:\n+  double start_;\n+  double finish_;\n+  double seconds_;\n+  int done_;\n+  int next_report_;\n+  int64_t bytes_;\n+  double last_op_finish_;\n+  Histogram hist_;\n+  std::string message_;\n+\n+ public:\n+  Stats() { Start(); }\n+\n+  void Start() {\n+    next_report_ = 100;\n+    last_op_finish_ = start_;\n+    hist_.Clear();\n+    done_ = 0;\n+    bytes_ = 0;\n+    seconds_ = 0;\n+    start_ = Env::Default()->NowMicros();\n+    finish_ = start_;\n+    message_.clear();\n+  }\n+\n+  void Merge(const Stats& other) {\n+    hist_.Merge(other.hist_);\n+    done_ += other.done_;\n+    bytes_ += other.bytes_;\n+    seconds_ += other.seconds_;\n+    if (other.start_ < start_) start_ = other.start_;\n+    if (other.finish_ > finish_) finish_ = other.finish_;\n+\n+    // Just keep the messages from one thread\n+    if (message_.empty()) message_ = other.message_;\n+  }\n+\n+  void Stop() {\n+    finish_ = Env::Default()->NowMicros();\n+    seconds_ = (finish_ - start_) * 1e-6;\n+  }\n+\n+  void AddMessage(Slice msg) {\n+    AppendWithSpace(&message_, msg);\n+  }\n+\n+  void FinishedSingleOp() {\n+    if (FLAGS_histogram) {\n+      double now = Env::Default()->NowMicros();\n+      double micros = now - last_op_finish_;\n+      hist_.Add(micros);\n+      if (micros > 20000) {\n+        fprintf(stderr, \"long op: %.1f micros%30s\\r\", micros, \"\");\n+        fflush(stderr);\n+      }\n+      last_op_finish_ = now;\n+    }\n+\n+    done_++;\n+    if (done_ >= next_report_) {\n+      if      (next_report_ < 1000)   next_report_ += 100;\n+      else if (next_report_ < 5000)   next_report_ += 500;\n+      else if (next_report_ < 10000)  next_report_ += 1000;\n+      else if (next_report_ < 50000)  next_report_ += 5000;\n+      else if (next_report_ < 100000) next_report_ += 10000;\n+      else if (next_report_ < 500000) next_report_ += 50000;\n+      else                            next_report_ += 100000;\n+      fprintf(stderr, \"... finished %d ops%30s\\r\", done_, \"\");\n+      fflush(stderr);\n+    }\n+  }\n+\n+  void AddBytes(int64_t n) {\n+    bytes_ += n;\n+  }\n+\n+  void Report(const Slice& name) {\n+    // Pretend at least one op was done in case we are running a benchmark\n+    // that does not call FinishedSingleOp().\n+    if (done_ < 1) done_ = 1;\n+\n+    std::string extra;\n+    if (bytes_ > 0) {\n+      // Rate is computed on actual elapsed time, not the sum of per-thread\n+      // elapsed times.\n+      double elapsed = (finish_ - start_) * 1e-6;\n+      char rate[100];\n+      snprintf(rate, sizeof(rate), \"%6.1f MB/s\",\n+               (bytes_ / 1048576.0) / elapsed);\n+      extra = rate;\n+    }\n+    AppendWithSpace(&extra, message_);\n+\n+    fprintf(stdout, \"%-12s : %11.3f micros/op;%s%s\\n\",\n+            name.ToString().c_str(),\n+            seconds_ * 1e6 / done_,\n+            (extra.empty() ? \"\" : \" \"),\n+            extra.c_str());\n+    if (FLAGS_histogram) {\n+      fprintf(stdout, \"Microseconds per op:\\n%s\\n\", hist_.ToString().c_str());\n+    }\n+    fflush(stdout);\n+  }\n+};\n+\n+// State shared by all concurrent executions of the same benchmark.\n+struct SharedState {\n+  port::Mutex mu;\n+  port::CondVar cv;\n+  int total;\n+\n+  // Each thread goes through the following states:\n+  //    (1) initializing\n+  //    (2) waiting for others to be initialized\n+  //    (3) running\n+  //    (4) done\n+\n+  int num_initialized;\n+  int num_done;\n+  bool start;\n+\n+  SharedState() : cv(&mu) { }\n+};\n+\n+// Per-thread state for concurrent executions of the same benchmark.\n+struct ThreadState {\n+  int tid;             // 0..n-1 when running in n threads\n+  Random rand;         // Has different seeds for different threads\n+  Stats stats;\n+  SharedState* shared;\n+\n+  ThreadState(int index)\n+      : tid(index),\n+        rand(1000 + index) {\n+  }\n+};\n+\n+}  // namespace\n+\n+class Benchmark {\n+ private:\n+  Cache* cache_;\n+  const FilterPolicy* filter_policy_;\n+  DB* db_;\n+  int num_;\n+  int value_size_;\n+  int entries_per_batch_;\n+  WriteOptions write_options_;\n+  int reads_;\n+  int heap_counter_;\n+\n+  void PrintHeader() {\n+    const int kKeySize = 16;\n+    PrintEnvironment();\n+    fprintf(stdout, \"Keys:       %d bytes each\\n\", kKeySize);\n+    fprintf(stdout, \"Values:     %d bytes each (%d bytes after compression)\\n\",\n+            FLAGS_value_size,\n+            static_cast<int>(FLAGS_value_size * FLAGS_compression_ratio + 0.5));\n+    fprintf(stdout, \"Entries:    %d\\n\", num_);\n+    fprintf(stdout, \"RawSize:    %.1f MB (estimated)\\n\",\n+            ((static_cast<int64_t>(kKeySize + FLAGS_value_size) * num_)\n+             / 1048576.0));\n+    fprintf(stdout, \"FileSize:   %.1f MB (estimated)\\n\",\n+            (((kKeySize + FLAGS_value_size * FLAGS_compression_ratio) * num_)\n+             / 1048576.0));\n+    PrintWarnings();\n+    fprintf(stdout, \"------------------------------------------------\\n\");\n+  }\n+\n+  void PrintWarnings() {\n+#if defined(__GNUC__) && !defined(__OPTIMIZE__)\n+    fprintf(stdout,\n+            \"WARNING: Optimization is disabled: benchmarks unnecessarily slow\\n\"\n+            );\n+#endif\n+#ifndef NDEBUG\n+    fprintf(stdout,\n+            \"WARNING: Assertions are enabled; benchmarks unnecessarily slow\\n\");\n+#endif\n+\n+    // See if snappy is working by attempting to compress a compressible string\n+    const char text[] = \"yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\";\n+    std::string compressed;\n+    if (!port::Snappy_Compress(text, sizeof(text), &compressed)) {\n+      fprintf(stdout, \"WARNING: Snappy compression is not enabled\\n\");\n+    } else if (compressed.size() >= sizeof(text)) {\n+      fprintf(stdout, \"WARNING: Snappy compression is not effective\\n\");\n+    }\n+  }\n+\n+  void PrintEnvironment() {\n+    fprintf(stderr, \"LevelDB:    version %d.%d\\n\",\n+            kMajorVersion, kMinorVersion);\n+\n+#if defined(__linux)\n+    time_t now = time(NULL);\n+    fprintf(stderr, \"Date:       %s\", ctime(&now));  // ctime() adds newline\n+\n+    FILE* cpuinfo = fopen(\"/proc/cpuinfo\", \"r\");\n+    if (cpuinfo != NULL) {\n+      char line[1000];\n+      int num_cpus = 0;\n+      std::string cpu_type;\n+      std::string cache_size;\n+      while (fgets(line, sizeof(line), cpuinfo) != NULL) {\n+        const char* sep = strchr(line, ':');\n+        if (sep == NULL) {\n+          continue;\n+        }\n+        Slice key = TrimSpace(Slice(line, sep - 1 - line));\n+        Slice val = TrimSpace(Slice(sep + 1));\n+        if (key == \"model name\") {\n+          ++num_cpus;\n+          cpu_type = val.ToString();\n+        } else if (key == \"cache size\") {\n+          cache_size = val.ToString();\n+        }\n+      }\n+      fclose(cpuinfo);\n+      fprintf(stderr, \"CPU:        %d * %s\\n\", num_cpus, cpu_type.c_str());\n+      fprintf(stderr, \"CPUCache:   %s\\n\", cache_size.c_str());\n+    }\n+#endif\n+  }\n+\n+ public:\n+  Benchmark()\n+  : cache_(FLAGS_cache_size >= 0 ? NewLRUCache(FLAGS_cache_size) : NULL),\n+    filter_policy_(FLAGS_bloom_bits >= 0\n+                   ? NewBloomFilterPolicy(FLAGS_bloom_bits)\n+                   : NULL),\n+    db_(NULL),\n+    num_(FLAGS_num),\n+    value_size_(FLAGS_value_size),\n+    entries_per_batch_(1),\n+    reads_(FLAGS_reads < 0 ? FLAGS_num : FLAGS_reads),\n+    heap_counter_(0) {\n+    std::vector<std::string> files;\n+    Env::Default()->GetChildren(FLAGS_db, &files);\n+    for (int i = 0; i < files.size(); i++) {\n+      if (Slice(files[i]).starts_with(\"heap-\")) {\n+        Env::Default()->DeleteFile(std::string(FLAGS_db) + \"/\" + files[i]);\n+      }\n+    }\n+    if (!FLAGS_use_existing_db) {\n+      DestroyDB(FLAGS_db, Options());\n+    }\n+  }\n+\n+  ~Benchmark() {\n+    delete db_;\n+    delete cache_;\n+    delete filter_policy_;\n+  }\n+\n+  void Run() {\n+    PrintHeader();\n+    Open();\n+\n+    const char* benchmarks = FLAGS_benchmarks;\n+    while (benchmarks != NULL) {\n+      const char* sep = strchr(benchmarks, ',');\n+      Slice name;\n+      if (sep == NULL) {\n+        name = benchmarks;\n+        benchmarks = NULL;\n+      } else {\n+        name = Slice(benchmarks, sep - benchmarks);\n+        benchmarks = sep + 1;\n+      }\n+\n+      // Reset parameters that may be overriddden bwlow\n+      num_ = FLAGS_num;\n+      reads_ = (FLAGS_reads < 0 ? FLAGS_num : FLAGS_reads);\n+      value_size_ = FLAGS_value_size;\n+      entries_per_batch_ = 1;\n+      write_options_ = WriteOptions();\n+\n+      void (Benchmark::*method)(ThreadState*) = NULL;\n+      bool fresh_db = false;\n+      int num_threads = FLAGS_threads;\n+\n+      if (name == Slice(\"fillseq\")) {\n+        fresh_db = true;\n+        method = &Benchmark::WriteSeq;\n+      } else if (name == Slice(\"fillbatch\")) {\n+        fresh_db = true;\n+        entries_per_batch_ = 1000;\n+        method = &Benchmark::WriteSeq;\n+      } else if (name == Slice(\"fillrandom\")) {\n+        fresh_db = true;\n+        method = &Benchmark::WriteRandom;\n+      } else if (name == Slice(\"overwrite\")) {\n+        fresh_db = false;\n+        method = &Benchmark::WriteRandom;\n+      } else if (name == Slice(\"fillsync\")) {\n+        fresh_db = true;\n+        num_ /= 1000;\n+        write_options_.sync = true;\n+        method = &Benchmark::WriteRandom;\n+      } else if (name == Slice(\"fill100K\")) {\n+        fresh_db = true;\n+        num_ /= 1000;\n+        value_size_ = 100 * 1000;\n+        method = &Benchmark::WriteRandom;\n+      } else if (name == Slice(\"readseq\")) {\n+        method = &Benchmark::ReadSequential;\n+      } else if (name == Slice(\"readreverse\")) {\n+        method = &Benchmark::ReadReverse;\n+      } else if (name == Slice(\"readrandom\")) {\n+        method = &Benchmark::ReadRandom;\n+      } else if (name == Slice(\"readmissing\")) {\n+        method = &Benchmark::ReadMissing;\n+      } else if (name == Slice(\"seekrandom\")) {\n+        method = &Benchmark::SeekRandom;\n+      } else if (name == Slice(\"readhot\")) {\n+        method = &Benchmark::ReadHot;\n+      } else if (name == Slice(\"readrandomsmall\")) {\n+        reads_ /= 1000;\n+        method = &Benchmark::ReadRandom;\n+      } else if (name == Slice(\"deleteseq\")) {\n+        method = &Benchmark::DeleteSeq;\n+      } else if (name == Slice(\"deleterandom\")) {\n+        method = &Benchmark::DeleteRandom;\n+      } else if (name == Slice(\"readwhilewriting\")) {\n+        num_threads++;  // Add extra thread for writing\n+        method = &Benchmark::ReadWhileWriting;\n+      } else if (name == Slice(\"compact\")) {\n+        method = &Benchmark::Compact;\n+      } else if (name == Slice(\"crc32c\")) {\n+        method = &Benchmark::Crc32c;\n+      } else if (name == Slice(\"acquireload\")) {\n+        method = &Benchmark::AcquireLoad;\n+      } else if (name == Slice(\"snappycomp\")) {\n+        method = &Benchmark::SnappyCompress;\n+      } else if (name == Slice(\"snappyuncomp\")) {\n+        method = &Benchmark::SnappyUncompress;\n+      } else if (name == Slice(\"heapprofile\")) {\n+        HeapProfile();\n+      } else if (name == Slice(\"stats\")) {\n+        PrintStats(\"leveldb.stats\");\n+      } else if (name == Slice(\"sstables\")) {\n+        PrintStats(\"leveldb.sstables\");\n+      } else {\n+        if (name != Slice()) {  // No error message for empty name\n+          fprintf(stderr, \"unknown benchmark '%s'\\n\", name.ToString().c_str());\n+        }\n+      }\n+\n+      if (fresh_db) {\n+        if (FLAGS_use_existing_db) {\n+          fprintf(stdout, \"%-12s : skipped (--use_existing_db is true)\\n\",\n+                  name.ToString().c_str());\n+          method = NULL;\n+        } else {\n+          delete db_;\n+          db_ = NULL;\n+          DestroyDB(FLAGS_db, Options());\n+          Open();\n+        }\n+      }\n+\n+      if (method != NULL) {\n+        RunBenchmark(num_threads, name, method);\n+      }\n+    }\n+  }\n+\n+ private:\n+  struct ThreadArg {\n+    Benchmark* bm;\n+    SharedState* shared;\n+    ThreadState* thread;\n+    void (Benchmark::*method)(ThreadState*);\n+  };\n+\n+  static void ThreadBody(void* v) {\n+    ThreadArg* arg = reinterpret_cast<ThreadArg*>(v);\n+    SharedState* shared = arg->shared;\n+    ThreadState* thread = arg->thread;\n+    {\n+      MutexLock l(&shared->mu);\n+      shared->num_initialized++;\n+      if (shared->num_initialized >= shared->total) {\n+        shared->cv.SignalAll();\n+      }\n+      while (!shared->start) {\n+        shared->cv.Wait();\n+      }\n+    }\n+\n+    thread->stats.Start();\n+    (arg->bm->*(arg->method))(thread);\n+    thread->stats.Stop();\n+\n+    {\n+      MutexLock l(&shared->mu);\n+      shared->num_done++;\n+      if (shared->num_done >= shared->total) {\n+        shared->cv.SignalAll();\n+      }\n+    }\n+  }\n+\n+  void RunBenchmark(int n, Slice name,\n+                    void (Benchmark::*method)(ThreadState*)) {\n+    SharedState shared;\n+    shared.total = n;\n+    shared.num_initialized = 0;\n+    shared.num_done = 0;\n+    shared.start = false;\n+\n+    ThreadArg* arg = new ThreadArg[n];\n+    for (int i = 0; i < n; i++) {\n+      arg[i].bm = this;\n+      arg[i].method = method;\n+      arg[i].shared = &shared;\n+      arg[i].thread = new ThreadState(i);\n+      arg[i].thread->shared = &shared;\n+      Env::Default()->StartThread(ThreadBody, &arg[i]);\n+    }\n+\n+    shared.mu.Lock();\n+    while (shared.num_initialized < n) {\n+      shared.cv.Wait();\n+    }\n+\n+    shared.start = true;\n+    shared.cv.SignalAll();\n+    while (shared.num_done < n) {\n+      shared.cv.Wait();\n+    }\n+    shared.mu.Unlock();\n+\n+    for (int i = 1; i < n; i++) {\n+      arg[0].thread->stats.Merge(arg[i].thread->stats);\n+    }\n+    arg[0].thread->stats.Report(name);\n+\n+    for (int i = 0; i < n; i++) {\n+      delete arg[i].thread;\n+    }\n+    delete[] arg;\n+  }\n+\n+  void Crc32c(ThreadState* thread) {\n+    // Checksum about 500MB of data total\n+    const int size = 4096;\n+    const char* label = \"(4K per op)\";\n+    std::string data(size, 'x');\n+    int64_t bytes = 0;\n+    uint32_t crc = 0;\n+    while (bytes < 500 * 1048576) {\n+      crc = crc32c::Value(data.data(), size);\n+      thread->stats.FinishedSingleOp();\n+      bytes += size;\n+    }\n+    // Print so result is not dead\n+    fprintf(stderr, \"... crc=0x%x\\r\", static_cast<unsigned int>(crc));\n+\n+    thread->stats.AddBytes(bytes);\n+    thread->stats.AddMessage(label);\n+  }\n+\n+  void AcquireLoad(ThreadState* thread) {\n+    int dummy;\n+    port::AtomicPointer ap(&dummy);\n+    int count = 0;\n+    void *ptr = NULL;\n+    thread->stats.AddMessage(\"(each op is 1000 loads)\");\n+    while (count < 100000) {\n+      for (int i = 0; i < 1000; i++) {\n+        ptr = ap.Acquire_Load();\n+      }\n+      count++;\n+      thread->stats.FinishedSingleOp();\n+    }\n+    if (ptr == NULL) exit(1); // Disable unused variable warning.\n+  }\n+\n+  void SnappyCompress(ThreadState* thread) {\n+    RandomGenerator gen;\n+    Slice input = gen.Generate(Options().block_size);\n+    int64_t bytes = 0;\n+    int64_t produced = 0;\n+    bool ok = true;\n+    std::string compressed;\n+    while (ok && bytes < 1024 * 1048576) {  // Compress 1G\n+      ok = port::Snappy_Compress(input.data(), input.size(), &compressed);\n+      produced += compressed.size();\n+      bytes += input.size();\n+      thread->stats.FinishedSingleOp();\n+    }\n+\n+    if (!ok) {\n+      thread->stats.AddMessage(\"(snappy failure)\");\n+    } else {\n+      char buf[100];\n+      snprintf(buf, sizeof(buf), \"(output: %.1f%%)\",\n+               (produced * 100.0) / bytes);\n+      thread->stats.AddMessage(buf);\n+      thread->stats.AddBytes(bytes);\n+    }\n+  }\n+\n+  void SnappyUncompress(ThreadState* thread) {\n+    RandomGenerator gen;\n+    Slice input = gen.Generate(Options().block_size);\n+    std::string compressed;\n+    bool ok = port::Snappy_Compress(input.data(), input.size(), &compressed);\n+    int64_t bytes = 0;\n+    char* uncompressed = new char[input.size()];\n+    while (ok && bytes < 1024 * 1048576) {  // Compress 1G\n+      ok =  port::Snappy_Uncompress(compressed.data(), compressed.size(),\n+                                    uncompressed);\n+      bytes += input.size();\n+      thread->stats.FinishedSingleOp();\n+    }\n+    delete[] uncompressed;\n+\n+    if (!ok) {\n+      thread->stats.AddMessage(\"(snappy failure)\");\n+    } else {\n+      thread->stats.AddBytes(bytes);\n+    }\n+  }\n+\n+  void Open() {\n+    assert(db_ == NULL);\n+    Options options;\n+    options.create_if_missing = !FLAGS_use_existing_db;\n+    options.block_cache = cache_;\n+    options.write_buffer_size = FLAGS_write_buffer_size;\n+    options.max_open_files = FLAGS_open_files;\n+    options.filter_policy = filter_policy_;\n+    Status s = DB::Open(options, FLAGS_db, &db_);\n+    if (!s.ok()) {\n+      fprintf(stderr, \"open error: %s\\n\", s.ToString().c_str());\n+      exit(1);\n+    }\n+  }\n+\n+  void WriteSeq(ThreadState* thread) {\n+    DoWrite(thread, true);\n+  }\n+\n+  void WriteRandom(ThreadState* thread) {\n+    DoWrite(thread, false);\n+  }\n+\n+  void DoWrite(ThreadState* thread, bool seq) {\n+    if (num_ != FLAGS_num) {\n+      char msg[100];\n+      snprintf(msg, sizeof(msg), \"(%d ops)\", num_);\n+      thread->stats.AddMessage(msg);\n+    }\n+\n+    RandomGenerator gen;\n+    WriteBatch batch;\n+    Status s;\n+    int64_t bytes = 0;\n+    for (int i = 0; i < num_; i += entries_per_batch_) {\n+      batch.Clear();\n+      for (int j = 0; j < entries_per_batch_; j++) {\n+        const int k = seq ? i+j : (thread->rand.Next() % FLAGS_num);\n+        char key[100];\n+        snprintf(key, sizeof(key), \"%016d\", k);\n+        batch.Put(key, gen.Generate(value_size_));\n+        bytes += value_size_ + strlen(key);\n+        thread->stats.FinishedSingleOp();\n+      }\n+      s = db_->Write(write_options_, &batch);\n+      if (!s.ok()) {\n+        fprintf(stderr, \"put error: %s\\n\", s.ToString().c_str());\n+        exit(1);\n+      }\n+    }\n+    thread->stats.AddBytes(bytes);\n+  }\n+\n+  void ReadSequential(ThreadState* thread) {\n+    Iterator* iter = db_->NewIterator(ReadOptions());\n+    int i = 0;\n+    int64_t bytes = 0;\n+    for (iter->SeekToFirst(); i < reads_ && iter->Valid(); iter->Next()) {\n+      bytes += iter->key().size() + iter->value().size();\n+      thread->stats.FinishedSingleOp();\n+      ++i;\n+    }\n+    delete iter;\n+    thread->stats.AddBytes(bytes);\n+  }\n+\n+  void ReadReverse(ThreadState* thread) {\n+    Iterator* iter = db_->NewIterator(ReadOptions());\n+    int i = 0;\n+    int64_t bytes = 0;\n+    for (iter->SeekToLast(); i < reads_ && iter->Valid(); iter->Prev()) {\n+      bytes += iter->key().size() + iter->value().size();\n+      thread->stats.FinishedSingleOp();\n+      ++i;\n+    }\n+    delete iter;\n+    thread->stats.AddBytes(bytes);\n+  }\n+\n+  void ReadRandom(ThreadState* thread) {\n+    ReadOptions options;\n+    std::string value;\n+    int found = 0;\n+    for (int i = 0; i < reads_; i++) {\n+      char key[100];\n+      const int k = thread->rand.Next() % FLAGS_num;\n+      snprintf(key, sizeof(key), \"%016d\", k);\n+      if (db_->Get(options, key, &value).ok()) {\n+        found++;\n+      }\n+      thread->stats.FinishedSingleOp();\n+    }\n+    char msg[100];\n+    snprintf(msg, sizeof(msg), \"(%d of %d found)\", found, num_);\n+    thread->stats.AddMessage(msg);\n+  }\n+\n+  void ReadMissing(ThreadState* thread) {\n+    ReadOptions options;\n+    std::string value;\n+    for (int i = 0; i < reads_; i++) {\n+      char key[100];\n+      const int k = thread->rand.Next() % FLAGS_num;\n+      snprintf(key, sizeof(key), \"%016d.\", k);\n+      db_->Get(options, key, &value);\n+      thread->stats.FinishedSingleOp();\n+    }\n+  }\n+\n+  void ReadHot(ThreadState* thread) {\n+    ReadOptions options;\n+    std::string value;\n+    const int range = (FLAGS_num + 99) / 100;\n+    for (int i = 0; i < reads_; i++) {\n+      char key[100];\n+      const int k = thread->rand.Next() % range;\n+      snprintf(key, sizeof(key), \"%016d\", k);\n+      db_->Get(options, key, &value);\n+      thread->stats.FinishedSingleOp();\n+    }\n+  }\n+\n+  void SeekRandom(ThreadState* thread) {\n+    ReadOptions options;\n+    std::string value;\n+    int found = 0;\n+    for (int i = 0; i < reads_; i++) {\n+      Iterator* iter = db_->NewIterator(options);\n+      char key[100];\n+      const int k = thread->rand.Next() % FLAGS_num;\n+      snprintf(key, sizeof(key), \"%016d\", k);\n+      iter->Seek(key);\n+      if (iter->Valid() && iter->key() == key) found++;\n+      delete iter;\n+      thread->stats.FinishedSingleOp();\n+    }\n+    char msg[100];\n+    snprintf(msg, sizeof(msg), \"(%d of %d found)\", found, num_);\n+    thread->stats.AddMessage(msg);\n+  }\n+\n+  void DoDelete(ThreadState* thread, bool seq) {\n+    RandomGenerator gen;\n+    WriteBatch batch;\n+    Status s;\n+    for (int i = 0; i < num_; i += entries_per_batch_) {\n+      batch.Clear();\n+      for (int j = 0; j < entries_per_batch_; j++) {\n+        const int k = seq ? i+j : (thread->rand.Next() % FLAGS_num);\n+        char key[100];\n+        snprintf(key, sizeof(key), \"%016d\", k);\n+        batch.Delete(key);\n+        thread->stats.FinishedSingleOp();\n+      }\n+      s = db_->Write(write_options_, &batch);\n+      if (!s.ok()) {\n+        fprintf(stderr, \"del error: %s\\n\", s.ToString().c_str());\n+        exit(1);\n+      }\n+    }\n+  }\n+\n+  void DeleteSeq(ThreadState* thread) {\n+    DoDelete(thread, true);\n+  }\n+\n+  void DeleteRandom(ThreadState* thread) {\n+    DoDelete(thread, false);\n+  }\n+\n+  void ReadWhileWriting(ThreadState* thread) {\n+    if (thread->tid > 0) {\n+      ReadRandom(thread);\n+    } else {\n+      // Special thread that keeps writing until other threads are done.\n+      RandomGenerator gen;\n+      while (true) {\n+        {\n+          MutexLock l(&thread->shared->mu);\n+          if (thread->shared->num_done + 1 >= thread->shared->num_initialized) {\n+            // Other threads have finished\n+            break;\n+          }\n+        }\n+\n+        const int k = thread->rand.Next() % FLAGS_num;\n+        char key[100];\n+        snprintf(key, sizeof(key), \"%016d\", k);\n+        Status s = db_->Put(write_options_, key, gen.Generate(value_size_));\n+        if (!s.ok()) {\n+          fprintf(stderr, \"put error: %s\\n\", s.ToString().c_str());\n+          exit(1);\n+        }\n+      }\n+\n+      // Do not count any of the preceding work/delay in stats.\n+      thread->stats.Start();\n+    }\n+  }\n+\n+  void Compact(ThreadState* thread) {\n+    db_->CompactRange(NULL, NULL);\n+  }\n+\n+  void PrintStats(const char* key) {\n+    std::string stats;\n+    if (!db_->GetProperty(key, &stats)) {\n+      stats = \"(failed)\";\n+    }\n+    fprintf(stdout, \"\\n%s\\n\", stats.c_str());\n+  }\n+\n+  static void WriteToFile(void* arg, const char* buf, int n) {\n+    reinterpret_cast<WritableFile*>(arg)->Append(Slice(buf, n));\n+  }\n+\n+  void HeapProfile() {\n+    char fname[100];\n+    snprintf(fname, sizeof(fname), \"%s/heap-%04d\", FLAGS_db, ++heap_counter_);\n+    WritableFile* file;\n+    Status s = Env::Default()->NewWritableFile(fname, &file);\n+    if (!s.ok()) {\n+      fprintf(stderr, \"%s\\n\", s.ToString().c_str());\n+      return;\n+    }\n+    bool ok = port::GetHeapProfile(WriteToFile, file);\n+    delete file;\n+    if (!ok) {\n+      fprintf(stderr, \"heap profiling not supported\\n\");\n+      Env::Default()->DeleteFile(fname);\n+    }\n+  }\n+};\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  FLAGS_write_buffer_size = leveldb::Options().write_buffer_size;\n+  FLAGS_open_files = leveldb::Options().max_open_files;\n+  std::string default_db_path;\n+\n+  for (int i = 1; i < argc; i++) {\n+    double d;\n+    int n;\n+    char junk;\n+    if (leveldb::Slice(argv[i]).starts_with(\"--benchmarks=\")) {\n+      FLAGS_benchmarks = argv[i] + strlen(\"--benchmarks=\");\n+    } else if (sscanf(argv[i], \"--compression_ratio=%lf%c\", &d, &junk) == 1) {\n+      FLAGS_compression_ratio = d;\n+    } else if (sscanf(argv[i], \"--histogram=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_histogram = n;\n+    } else if (sscanf(argv[i], \"--use_existing_db=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_use_existing_db = n;\n+    } else if (sscanf(argv[i], \"--num=%d%c\", &n, &junk) == 1) {\n+      FLAGS_num = n;\n+    } else if (sscanf(argv[i], \"--reads=%d%c\", &n, &junk) == 1) {\n+      FLAGS_reads = n;\n+    } else if (sscanf(argv[i], \"--threads=%d%c\", &n, &junk) == 1) {\n+      FLAGS_threads = n;\n+    } else if (sscanf(argv[i], \"--value_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_value_size = n;\n+    } else if (sscanf(argv[i], \"--write_buffer_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_write_buffer_size = n;\n+    } else if (sscanf(argv[i], \"--cache_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_cache_size = n;\n+    } else if (sscanf(argv[i], \"--bloom_bits=%d%c\", &n, &junk) == 1) {\n+      FLAGS_bloom_bits = n;\n+    } else if (sscanf(argv[i], \"--open_files=%d%c\", &n, &junk) == 1) {\n+      FLAGS_open_files = n;\n+    } else if (strncmp(argv[i], \"--db=\", 5) == 0) {\n+      FLAGS_db = argv[i] + 5;\n+    } else {\n+      fprintf(stderr, \"Invalid flag '%s'\\n\", argv[i]);\n+      exit(1);\n+    }\n+  }\n+\n+  // Choose a location for the test database if none given with --db=<path>\n+  if (FLAGS_db == NULL) {\n+      leveldb::Env::Default()->GetTestDirectory(&default_db_path);\n+      default_db_path += \"/dbbench\";\n+      FLAGS_db = default_db_path.c_str();\n+  }\n+\n+  leveldb::Benchmark benchmark;\n+  benchmark.Run();\n+  return 0;\n+}"
      },
      {
        "sha": "395d3172ad3a16b24552057599c776228bf84917",
        "filename": "db/db_impl.cc",
        "status": "added",
        "additions": 1485,
        "deletions": 0,
        "changes": 1485,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/db_impl.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/db_impl.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/db_impl.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,1485 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/db_impl.h\"\n+\n+#include <algorithm>\n+#include <set>\n+#include <string>\n+#include <stdint.h>\n+#include <stdio.h>\n+#include <vector>\n+#include \"db/builder.h\"\n+#include \"db/db_iter.h\"\n+#include \"db/dbformat.h\"\n+#include \"db/filename.h\"\n+#include \"db/log_reader.h\"\n+#include \"db/log_writer.h\"\n+#include \"db/memtable.h\"\n+#include \"db/table_cache.h\"\n+#include \"db/version_set.h\"\n+#include \"db/write_batch_internal.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/status.h\"\n+#include \"leveldb/table.h\"\n+#include \"leveldb/table_builder.h\"\n+#include \"port/port.h\"\n+#include \"table/block.h\"\n+#include \"table/merger.h\"\n+#include \"table/two_level_iterator.h\"\n+#include \"util/coding.h\"\n+#include \"util/logging.h\"\n+#include \"util/mutexlock.h\"\n+\n+namespace leveldb {\n+\n+const int kNumNonTableCacheFiles = 10;\n+\n+// Information kept for every waiting writer\n+struct DBImpl::Writer {\n+  Status status;\n+  WriteBatch* batch;\n+  bool sync;\n+  bool done;\n+  port::CondVar cv;\n+\n+  explicit Writer(port::Mutex* mu) : cv(mu) { }\n+};\n+\n+struct DBImpl::CompactionState {\n+  Compaction* const compaction;\n+\n+  // Sequence numbers < smallest_snapshot are not significant since we\n+  // will never have to service a snapshot below smallest_snapshot.\n+  // Therefore if we have seen a sequence number S <= smallest_snapshot,\n+  // we can drop all entries for the same key with sequence numbers < S.\n+  SequenceNumber smallest_snapshot;\n+\n+  // Files produced by compaction\n+  struct Output {\n+    uint64_t number;\n+    uint64_t file_size;\n+    InternalKey smallest, largest;\n+  };\n+  std::vector<Output> outputs;\n+\n+  // State kept for output being generated\n+  WritableFile* outfile;\n+  TableBuilder* builder;\n+\n+  uint64_t total_bytes;\n+\n+  Output* current_output() { return &outputs[outputs.size()-1]; }\n+\n+  explicit CompactionState(Compaction* c)\n+      : compaction(c),\n+        outfile(NULL),\n+        builder(NULL),\n+        total_bytes(0) {\n+  }\n+};\n+\n+// Fix user-supplied options to be reasonable\n+template <class T,class V>\n+static void ClipToRange(T* ptr, V minvalue, V maxvalue) {\n+  if (static_cast<V>(*ptr) > maxvalue) *ptr = maxvalue;\n+  if (static_cast<V>(*ptr) < minvalue) *ptr = minvalue;\n+}\n+Options SanitizeOptions(const std::string& dbname,\n+                        const InternalKeyComparator* icmp,\n+                        const InternalFilterPolicy* ipolicy,\n+                        const Options& src) {\n+  Options result = src;\n+  result.comparator = icmp;\n+  result.filter_policy = (src.filter_policy != NULL) ? ipolicy : NULL;\n+  ClipToRange(&result.max_open_files,    64 + kNumNonTableCacheFiles, 50000);\n+  ClipToRange(&result.write_buffer_size, 64<<10,                      1<<30);\n+  ClipToRange(&result.block_size,        1<<10,                       4<<20);\n+  if (result.info_log == NULL) {\n+    // Open a log file in the same directory as the db\n+    src.env->CreateDir(dbname);  // In case it does not exist\n+    src.env->RenameFile(InfoLogFileName(dbname), OldInfoLogFileName(dbname));\n+    Status s = src.env->NewLogger(InfoLogFileName(dbname), &result.info_log);\n+    if (!s.ok()) {\n+      // No place suitable for logging\n+      result.info_log = NULL;\n+    }\n+  }\n+  if (result.block_cache == NULL) {\n+    result.block_cache = NewLRUCache(8 << 20);\n+  }\n+  return result;\n+}\n+\n+DBImpl::DBImpl(const Options& options, const std::string& dbname)\n+    : env_(options.env),\n+      internal_comparator_(options.comparator),\n+      internal_filter_policy_(options.filter_policy),\n+      options_(SanitizeOptions(\n+          dbname, &internal_comparator_, &internal_filter_policy_, options)),\n+      owns_info_log_(options_.info_log != options.info_log),\n+      owns_cache_(options_.block_cache != options.block_cache),\n+      dbname_(dbname),\n+      db_lock_(NULL),\n+      shutting_down_(NULL),\n+      bg_cv_(&mutex_),\n+      mem_(new MemTable(internal_comparator_)),\n+      imm_(NULL),\n+      logfile_(NULL),\n+      logfile_number_(0),\n+      log_(NULL),\n+      tmp_batch_(new WriteBatch),\n+      bg_compaction_scheduled_(false),\n+      manual_compaction_(NULL),\n+      consecutive_compaction_errors_(0) {\n+  mem_->Ref();\n+  has_imm_.Release_Store(NULL);\n+\n+  // Reserve ten files or so for other uses and give the rest to TableCache.\n+  const int table_cache_size = options.max_open_files - kNumNonTableCacheFiles;\n+  table_cache_ = new TableCache(dbname_, &options_, table_cache_size);\n+\n+  versions_ = new VersionSet(dbname_, &options_, table_cache_,\n+                             &internal_comparator_);\n+}\n+\n+DBImpl::~DBImpl() {\n+  // Wait for background work to finish\n+  mutex_.Lock();\n+  shutting_down_.Release_Store(this);  // Any non-NULL value is ok\n+  while (bg_compaction_scheduled_) {\n+    bg_cv_.Wait();\n+  }\n+  mutex_.Unlock();\n+\n+  if (db_lock_ != NULL) {\n+    env_->UnlockFile(db_lock_);\n+  }\n+\n+  delete versions_;\n+  if (mem_ != NULL) mem_->Unref();\n+  if (imm_ != NULL) imm_->Unref();\n+  delete tmp_batch_;\n+  delete log_;\n+  delete logfile_;\n+  delete table_cache_;\n+\n+  if (owns_info_log_) {\n+    delete options_.info_log;\n+  }\n+  if (owns_cache_) {\n+    delete options_.block_cache;\n+  }\n+}\n+\n+Status DBImpl::NewDB() {\n+  VersionEdit new_db;\n+  new_db.SetComparatorName(user_comparator()->Name());\n+  new_db.SetLogNumber(0);\n+  new_db.SetNextFile(2);\n+  new_db.SetLastSequence(0);\n+\n+  const std::string manifest = DescriptorFileName(dbname_, 1);\n+  WritableFile* file;\n+  Status s = env_->NewWritableFile(manifest, &file);\n+  if (!s.ok()) {\n+    return s;\n+  }\n+  {\n+    log::Writer log(file);\n+    std::string record;\n+    new_db.EncodeTo(&record);\n+    s = log.AddRecord(record);\n+    if (s.ok()) {\n+      s = file->Close();\n+    }\n+  }\n+  delete file;\n+  if (s.ok()) {\n+    // Make \"CURRENT\" file that points to the new manifest file.\n+    s = SetCurrentFile(env_, dbname_, 1);\n+  } else {\n+    env_->DeleteFile(manifest);\n+  }\n+  return s;\n+}\n+\n+void DBImpl::MaybeIgnoreError(Status* s) const {\n+  if (s->ok() || options_.paranoid_checks) {\n+    // No change needed\n+  } else {\n+    Log(options_.info_log, \"Ignoring error %s\", s->ToString().c_str());\n+    *s = Status::OK();\n+  }\n+}\n+\n+void DBImpl::DeleteObsoleteFiles() {\n+  // Make a set of all of the live files\n+  std::set<uint64_t> live = pending_outputs_;\n+  versions_->AddLiveFiles(&live);\n+\n+  std::vector<std::string> filenames;\n+  env_->GetChildren(dbname_, &filenames); // Ignoring errors on purpose\n+  uint64_t number;\n+  FileType type;\n+  for (size_t i = 0; i < filenames.size(); i++) {\n+    if (ParseFileName(filenames[i], &number, &type)) {\n+      bool keep = true;\n+      switch (type) {\n+        case kLogFile:\n+          keep = ((number >= versions_->LogNumber()) ||\n+                  (number == versions_->PrevLogNumber()));\n+          break;\n+        case kDescriptorFile:\n+          // Keep my manifest file, and any newer incarnations'\n+          // (in case there is a race that allows other incarnations)\n+          keep = (number >= versions_->ManifestFileNumber());\n+          break;\n+        case kTableFile:\n+          keep = (live.find(number) != live.end());\n+          break;\n+        case kTempFile:\n+          // Any temp files that are currently being written to must\n+          // be recorded in pending_outputs_, which is inserted into \"live\"\n+          keep = (live.find(number) != live.end());\n+          break;\n+        case kCurrentFile:\n+        case kDBLockFile:\n+        case kInfoLogFile:\n+          keep = true;\n+          break;\n+      }\n+\n+      if (!keep) {\n+        if (type == kTableFile) {\n+          table_cache_->Evict(number);\n+        }\n+        Log(options_.info_log, \"Delete type=%d #%lld\\n\",\n+            int(type),\n+            static_cast<unsigned long long>(number));\n+        env_->DeleteFile(dbname_ + \"/\" + filenames[i]);\n+      }\n+    }\n+  }\n+}\n+\n+Status DBImpl::Recover(VersionEdit* edit) {\n+  mutex_.AssertHeld();\n+\n+  // Ignore error from CreateDir since the creation of the DB is\n+  // committed only when the descriptor is created, and this directory\n+  // may already exist from a previous failed creation attempt.\n+  env_->CreateDir(dbname_);\n+  assert(db_lock_ == NULL);\n+  Status s = env_->LockFile(LockFileName(dbname_), &db_lock_);\n+  if (!s.ok()) {\n+    return s;\n+  }\n+\n+  if (!env_->FileExists(CurrentFileName(dbname_))) {\n+    if (options_.create_if_missing) {\n+      s = NewDB();\n+      if (!s.ok()) {\n+        return s;\n+      }\n+    } else {\n+      return Status::InvalidArgument(\n+          dbname_, \"does not exist (create_if_missing is false)\");\n+    }\n+  } else {\n+    if (options_.error_if_exists) {\n+      return Status::InvalidArgument(\n+          dbname_, \"exists (error_if_exists is true)\");\n+    }\n+  }\n+\n+  s = versions_->Recover();\n+  if (s.ok()) {\n+    SequenceNumber max_sequence(0);\n+\n+    // Recover from all newer log files than the ones named in the\n+    // descriptor (new log files may have been added by the previous\n+    // incarnation without registering them in the descriptor).\n+    //\n+    // Note that PrevLogNumber() is no longer used, but we pay\n+    // attention to it in case we are recovering a database\n+    // produced by an older version of leveldb.\n+    const uint64_t min_log = versions_->LogNumber();\n+    const uint64_t prev_log = versions_->PrevLogNumber();\n+    std::vector<std::string> filenames;\n+    s = env_->GetChildren(dbname_, &filenames);\n+    if (!s.ok()) {\n+      return s;\n+    }\n+    std::set<uint64_t> expected;\n+    versions_->AddLiveFiles(&expected);\n+    uint64_t number;\n+    FileType type;\n+    std::vector<uint64_t> logs;\n+    for (size_t i = 0; i < filenames.size(); i++) {\n+      if (ParseFileName(filenames[i], &number, &type)) {\n+        expected.erase(number);\n+        if (type == kLogFile && ((number >= min_log) || (number == prev_log)))\n+          logs.push_back(number);\n+      }\n+    }\n+    if (!expected.empty()) {\n+      char buf[50];\n+      snprintf(buf, sizeof(buf), \"%d missing files; e.g.\",\n+               static_cast<int>(expected.size()));\n+      return Status::Corruption(buf, TableFileName(dbname_, *(expected.begin())));\n+    }\n+\n+    // Recover in the order in which the logs were generated\n+    std::sort(logs.begin(), logs.end());\n+    for (size_t i = 0; i < logs.size(); i++) {\n+      s = RecoverLogFile(logs[i], edit, &max_sequence);\n+\n+      // The previous incarnation may not have written any MANIFEST\n+      // records after allocating this log number.  So we manually\n+      // update the file number allocation counter in VersionSet.\n+      versions_->MarkFileNumberUsed(logs[i]);\n+    }\n+\n+    if (s.ok()) {\n+      if (versions_->LastSequence() < max_sequence) {\n+        versions_->SetLastSequence(max_sequence);\n+      }\n+    }\n+  }\n+\n+  return s;\n+}\n+\n+Status DBImpl::RecoverLogFile(uint64_t log_number,\n+                              VersionEdit* edit,\n+                              SequenceNumber* max_sequence) {\n+  struct LogReporter : public log::Reader::Reporter {\n+    Env* env;\n+    Logger* info_log;\n+    const char* fname;\n+    Status* status;  // NULL if options_.paranoid_checks==false\n+    virtual void Corruption(size_t bytes, const Status& s) {\n+      Log(info_log, \"%s%s: dropping %d bytes; %s\",\n+          (this->status == NULL ? \"(ignoring error) \" : \"\"),\n+          fname, static_cast<int>(bytes), s.ToString().c_str());\n+      if (this->status != NULL && this->status->ok()) *this->status = s;\n+    }\n+  };\n+\n+  mutex_.AssertHeld();\n+\n+  // Open the log file\n+  std::string fname = LogFileName(dbname_, log_number);\n+  SequentialFile* file;\n+  Status status = env_->NewSequentialFile(fname, &file);\n+  if (!status.ok()) {\n+    MaybeIgnoreError(&status);\n+    return status;\n+  }\n+\n+  // Create the log reader.\n+  LogReporter reporter;\n+  reporter.env = env_;\n+  reporter.info_log = options_.info_log;\n+  reporter.fname = fname.c_str();\n+  reporter.status = (options_.paranoid_checks ? &status : NULL);\n+  // We intentially make log::Reader do checksumming even if\n+  // paranoid_checks==false so that corruptions cause entire commits\n+  // to be skipped instead of propagating bad information (like overly\n+  // large sequence numbers).\n+  log::Reader reader(file, &reporter, true/*checksum*/,\n+                     0/*initial_offset*/);\n+  Log(options_.info_log, \"Recovering log #%llu\",\n+      (unsigned long long) log_number);\n+\n+  // Read all the records and add to a memtable\n+  std::string scratch;\n+  Slice record;\n+  WriteBatch batch;\n+  MemTable* mem = NULL;\n+  while (reader.ReadRecord(&record, &scratch) &&\n+         status.ok()) {\n+    if (record.size() < 12) {\n+      reporter.Corruption(\n+          record.size(), Status::Corruption(\"log record too small\"));\n+      continue;\n+    }\n+    WriteBatchInternal::SetContents(&batch, record);\n+\n+    if (mem == NULL) {\n+      mem = new MemTable(internal_comparator_);\n+      mem->Ref();\n+    }\n+    status = WriteBatchInternal::InsertInto(&batch, mem);\n+    MaybeIgnoreError(&status);\n+    if (!status.ok()) {\n+      break;\n+    }\n+    const SequenceNumber last_seq =\n+        WriteBatchInternal::Sequence(&batch) +\n+        WriteBatchInternal::Count(&batch) - 1;\n+    if (last_seq > *max_sequence) {\n+      *max_sequence = last_seq;\n+    }\n+\n+    if (mem->ApproximateMemoryUsage() > options_.write_buffer_size) {\n+      status = WriteLevel0Table(mem, edit, NULL);\n+      if (!status.ok()) {\n+        // Reflect errors immediately so that conditions like full\n+        // file-systems cause the DB::Open() to fail.\n+        break;\n+      }\n+      mem->Unref();\n+      mem = NULL;\n+    }\n+  }\n+\n+  if (status.ok() && mem != NULL) {\n+    status = WriteLevel0Table(mem, edit, NULL);\n+    // Reflect errors immediately so that conditions like full\n+    // file-systems cause the DB::Open() to fail.\n+  }\n+\n+  if (mem != NULL) mem->Unref();\n+  delete file;\n+  return status;\n+}\n+\n+Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit,\n+                                Version* base) {\n+  mutex_.AssertHeld();\n+  const uint64_t start_micros = env_->NowMicros();\n+  FileMetaData meta;\n+  meta.number = versions_->NewFileNumber();\n+  pending_outputs_.insert(meta.number);\n+  Iterator* iter = mem->NewIterator();\n+  Log(options_.info_log, \"Level-0 table #%llu: started\",\n+      (unsigned long long) meta.number);\n+\n+  Status s;\n+  {\n+    mutex_.Unlock();\n+    s = BuildTable(dbname_, env_, options_, table_cache_, iter, &meta);\n+    mutex_.Lock();\n+  }\n+\n+  Log(options_.info_log, \"Level-0 table #%llu: %lld bytes %s\",\n+      (unsigned long long) meta.number,\n+      (unsigned long long) meta.file_size,\n+      s.ToString().c_str());\n+  delete iter;\n+  pending_outputs_.erase(meta.number);\n+\n+\n+  // Note that if file_size is zero, the file has been deleted and\n+  // should not be added to the manifest.\n+  int level = 0;\n+  if (s.ok() && meta.file_size > 0) {\n+    const Slice min_user_key = meta.smallest.user_key();\n+    const Slice max_user_key = meta.largest.user_key();\n+    if (base != NULL) {\n+      level = base->PickLevelForMemTableOutput(min_user_key, max_user_key);\n+    }\n+    edit->AddFile(level, meta.number, meta.file_size,\n+                  meta.smallest, meta.largest);\n+  }\n+\n+  CompactionStats stats;\n+  stats.micros = env_->NowMicros() - start_micros;\n+  stats.bytes_written = meta.file_size;\n+  stats_[level].Add(stats);\n+  return s;\n+}\n+\n+Status DBImpl::CompactMemTable() {\n+  mutex_.AssertHeld();\n+  assert(imm_ != NULL);\n+\n+  // Save the contents of the memtable as a new Table\n+  VersionEdit edit;\n+  Version* base = versions_->current();\n+  base->Ref();\n+  Status s = WriteLevel0Table(imm_, &edit, base);\n+  base->Unref();\n+\n+  if (s.ok() && shutting_down_.Acquire_Load()) {\n+    s = Status::IOError(\"Deleting DB during memtable compaction\");\n+  }\n+\n+  // Replace immutable memtable with the generated Table\n+  if (s.ok()) {\n+    edit.SetPrevLogNumber(0);\n+    edit.SetLogNumber(logfile_number_);  // Earlier logs no longer needed\n+    s = versions_->LogAndApply(&edit, &mutex_);\n+  }\n+\n+  if (s.ok()) {\n+    // Commit to the new state\n+    imm_->Unref();\n+    imm_ = NULL;\n+    has_imm_.Release_Store(NULL);\n+    DeleteObsoleteFiles();\n+  }\n+\n+  return s;\n+}\n+\n+void DBImpl::CompactRange(const Slice* begin, const Slice* end) {\n+  int max_level_with_files = 1;\n+  {\n+    MutexLock l(&mutex_);\n+    Version* base = versions_->current();\n+    for (int level = 1; level < config::kNumLevels; level++) {\n+      if (base->OverlapInLevel(level, begin, end)) {\n+        max_level_with_files = level;\n+      }\n+    }\n+  }\n+  TEST_CompactMemTable(); // TODO(sanjay): Skip if memtable does not overlap\n+  for (int level = 0; level < max_level_with_files; level++) {\n+    TEST_CompactRange(level, begin, end);\n+  }\n+}\n+\n+void DBImpl::TEST_CompactRange(int level, const Slice* begin,const Slice* end) {\n+  assert(level >= 0);\n+  assert(level + 1 < config::kNumLevels);\n+\n+  InternalKey begin_storage, end_storage;\n+\n+  ManualCompaction manual;\n+  manual.level = level;\n+  manual.done = false;\n+  if (begin == NULL) {\n+    manual.begin = NULL;\n+  } else {\n+    begin_storage = InternalKey(*begin, kMaxSequenceNumber, kValueTypeForSeek);\n+    manual.begin = &begin_storage;\n+  }\n+  if (end == NULL) {\n+    manual.end = NULL;\n+  } else {\n+    end_storage = InternalKey(*end, 0, static_cast<ValueType>(0));\n+    manual.end = &end_storage;\n+  }\n+\n+  MutexLock l(&mutex_);\n+  while (!manual.done) {\n+    while (manual_compaction_ != NULL) {\n+      bg_cv_.Wait();\n+    }\n+    manual_compaction_ = &manual;\n+    MaybeScheduleCompaction();\n+    while (manual_compaction_ == &manual) {\n+      bg_cv_.Wait();\n+    }\n+  }\n+}\n+\n+Status DBImpl::TEST_CompactMemTable() {\n+  // NULL batch means just wait for earlier writes to be done\n+  Status s = Write(WriteOptions(), NULL);\n+  if (s.ok()) {\n+    // Wait until the compaction completes\n+    MutexLock l(&mutex_);\n+    while (imm_ != NULL && bg_error_.ok()) {\n+      bg_cv_.Wait();\n+    }\n+    if (imm_ != NULL) {\n+      s = bg_error_;\n+    }\n+  }\n+  return s;\n+}\n+\n+void DBImpl::MaybeScheduleCompaction() {\n+  mutex_.AssertHeld();\n+  if (bg_compaction_scheduled_) {\n+    // Already scheduled\n+  } else if (shutting_down_.Acquire_Load()) {\n+    // DB is being deleted; no more background compactions\n+  } else if (imm_ == NULL &&\n+             manual_compaction_ == NULL &&\n+             !versions_->NeedsCompaction()) {\n+    // No work to be done\n+  } else {\n+    bg_compaction_scheduled_ = true;\n+    env_->Schedule(&DBImpl::BGWork, this);\n+  }\n+}\n+\n+void DBImpl::BGWork(void* db) {\n+  reinterpret_cast<DBImpl*>(db)->BackgroundCall();\n+}\n+\n+void DBImpl::BackgroundCall() {\n+  MutexLock l(&mutex_);\n+  assert(bg_compaction_scheduled_);\n+  if (!shutting_down_.Acquire_Load()) {\n+    Status s = BackgroundCompaction();\n+    if (s.ok()) {\n+      // Success\n+      consecutive_compaction_errors_ = 0;\n+    } else if (shutting_down_.Acquire_Load()) {\n+      // Error most likely due to shutdown; do not wait\n+    } else {\n+      // Wait a little bit before retrying background compaction in\n+      // case this is an environmental problem and we do not want to\n+      // chew up resources for failed compactions for the duration of\n+      // the problem.\n+      bg_cv_.SignalAll();  // In case a waiter can proceed despite the error\n+      Log(options_.info_log, \"Waiting after background compaction error: %s\",\n+          s.ToString().c_str());\n+      mutex_.Unlock();\n+      ++consecutive_compaction_errors_;\n+      int seconds_to_sleep = 1;\n+      for (int i = 0; i < 3 && i < consecutive_compaction_errors_ - 1; ++i) {\n+        seconds_to_sleep *= 2;\n+      }\n+      env_->SleepForMicroseconds(seconds_to_sleep * 1000000);\n+      mutex_.Lock();\n+    }\n+  }\n+\n+  bg_compaction_scheduled_ = false;\n+\n+  // Previous compaction may have produced too many files in a level,\n+  // so reschedule another compaction if needed.\n+  MaybeScheduleCompaction();\n+  bg_cv_.SignalAll();\n+}\n+\n+Status DBImpl::BackgroundCompaction() {\n+  mutex_.AssertHeld();\n+\n+  if (imm_ != NULL) {\n+    return CompactMemTable();\n+  }\n+\n+  Compaction* c;\n+  bool is_manual = (manual_compaction_ != NULL);\n+  InternalKey manual_end;\n+  if (is_manual) {\n+    ManualCompaction* m = manual_compaction_;\n+    c = versions_->CompactRange(m->level, m->begin, m->end);\n+    m->done = (c == NULL);\n+    if (c != NULL) {\n+      manual_end = c->input(0, c->num_input_files(0) - 1)->largest;\n+    }\n+    Log(options_.info_log,\n+        \"Manual compaction at level-%d from %s .. %s; will stop at %s\\n\",\n+        m->level,\n+        (m->begin ? m->begin->DebugString().c_str() : \"(begin)\"),\n+        (m->end ? m->end->DebugString().c_str() : \"(end)\"),\n+        (m->done ? \"(end)\" : manual_end.DebugString().c_str()));\n+  } else {\n+    c = versions_->PickCompaction();\n+  }\n+\n+  Status status;\n+  if (c == NULL) {\n+    // Nothing to do\n+  } else if (!is_manual && c->IsTrivialMove()) {\n+    // Move file to next level\n+    assert(c->num_input_files(0) == 1);\n+    FileMetaData* f = c->input(0, 0);\n+    c->edit()->DeleteFile(c->level(), f->number);\n+    c->edit()->AddFile(c->level() + 1, f->number, f->file_size,\n+                       f->smallest, f->largest);\n+    status = versions_->LogAndApply(c->edit(), &mutex_);\n+    VersionSet::LevelSummaryStorage tmp;\n+    Log(options_.info_log, \"Moved #%lld to level-%d %lld bytes %s: %s\\n\",\n+        static_cast<unsigned long long>(f->number),\n+        c->level() + 1,\n+        static_cast<unsigned long long>(f->file_size),\n+        status.ToString().c_str(),\n+        versions_->LevelSummary(&tmp));\n+  } else {\n+    CompactionState* compact = new CompactionState(c);\n+    status = DoCompactionWork(compact);\n+    CleanupCompaction(compact);\n+    c->ReleaseInputs();\n+    DeleteObsoleteFiles();\n+  }\n+  delete c;\n+\n+  if (status.ok()) {\n+    // Done\n+  } else if (shutting_down_.Acquire_Load()) {\n+    // Ignore compaction errors found during shutting down\n+  } else {\n+    Log(options_.info_log,\n+        \"Compaction error: %s\", status.ToString().c_str());\n+    if (options_.paranoid_checks && bg_error_.ok()) {\n+      bg_error_ = status;\n+    }\n+  }\n+\n+  if (is_manual) {\n+    ManualCompaction* m = manual_compaction_;\n+    if (!status.ok()) {\n+      m->done = true;\n+    }\n+    if (!m->done) {\n+      // We only compacted part of the requested range.  Update *m\n+      // to the range that is left to be compacted.\n+      m->tmp_storage = manual_end;\n+      m->begin = &m->tmp_storage;\n+    }\n+    manual_compaction_ = NULL;\n+  }\n+  return status;\n+}\n+\n+void DBImpl::CleanupCompaction(CompactionState* compact) {\n+  mutex_.AssertHeld();\n+  if (compact->builder != NULL) {\n+    // May happen if we get a shutdown call in the middle of compaction\n+    compact->builder->Abandon();\n+    delete compact->builder;\n+  } else {\n+    assert(compact->outfile == NULL);\n+  }\n+  delete compact->outfile;\n+  for (size_t i = 0; i < compact->outputs.size(); i++) {\n+    const CompactionState::Output& out = compact->outputs[i];\n+    pending_outputs_.erase(out.number);\n+  }\n+  delete compact;\n+}\n+\n+Status DBImpl::OpenCompactionOutputFile(CompactionState* compact) {\n+  assert(compact != NULL);\n+  assert(compact->builder == NULL);\n+  uint64_t file_number;\n+  {\n+    mutex_.Lock();\n+    file_number = versions_->NewFileNumber();\n+    pending_outputs_.insert(file_number);\n+    CompactionState::Output out;\n+    out.number = file_number;\n+    out.smallest.Clear();\n+    out.largest.Clear();\n+    compact->outputs.push_back(out);\n+    mutex_.Unlock();\n+  }\n+\n+  // Make the output file\n+  std::string fname = TableFileName(dbname_, file_number);\n+  Status s = env_->NewWritableFile(fname, &compact->outfile);\n+  if (s.ok()) {\n+    compact->builder = new TableBuilder(options_, compact->outfile);\n+  }\n+  return s;\n+}\n+\n+Status DBImpl::FinishCompactionOutputFile(CompactionState* compact,\n+                                          Iterator* input) {\n+  assert(compact != NULL);\n+  assert(compact->outfile != NULL);\n+  assert(compact->builder != NULL);\n+\n+  const uint64_t output_number = compact->current_output()->number;\n+  assert(output_number != 0);\n+\n+  // Check for iterator errors\n+  Status s = input->status();\n+  const uint64_t current_entries = compact->builder->NumEntries();\n+  if (s.ok()) {\n+    s = compact->builder->Finish();\n+  } else {\n+    compact->builder->Abandon();\n+  }\n+  const uint64_t current_bytes = compact->builder->FileSize();\n+  compact->current_output()->file_size = current_bytes;\n+  compact->total_bytes += current_bytes;\n+  delete compact->builder;\n+  compact->builder = NULL;\n+\n+  // Finish and check for file errors\n+  if (s.ok()) {\n+    s = compact->outfile->Sync();\n+  }\n+  if (s.ok()) {\n+    s = compact->outfile->Close();\n+  }\n+  delete compact->outfile;\n+  compact->outfile = NULL;\n+\n+  if (s.ok() && current_entries > 0) {\n+    // Verify that the table is usable\n+    Iterator* iter = table_cache_->NewIterator(ReadOptions(),\n+                                               output_number,\n+                                               current_bytes);\n+    s = iter->status();\n+    delete iter;\n+    if (s.ok()) {\n+      Log(options_.info_log,\n+          \"Generated table #%llu: %lld keys, %lld bytes\",\n+          (unsigned long long) output_number,\n+          (unsigned long long) current_entries,\n+          (unsigned long long) current_bytes);\n+    }\n+  }\n+  return s;\n+}\n+\n+\n+Status DBImpl::InstallCompactionResults(CompactionState* compact) {\n+  mutex_.AssertHeld();\n+  Log(options_.info_log,  \"Compacted %d@%d + %d@%d files => %lld bytes\",\n+      compact->compaction->num_input_files(0),\n+      compact->compaction->level(),\n+      compact->compaction->num_input_files(1),\n+      compact->compaction->level() + 1,\n+      static_cast<long long>(compact->total_bytes));\n+\n+  // Add compaction outputs\n+  compact->compaction->AddInputDeletions(compact->compaction->edit());\n+  const int level = compact->compaction->level();\n+  for (size_t i = 0; i < compact->outputs.size(); i++) {\n+    const CompactionState::Output& out = compact->outputs[i];\n+    compact->compaction->edit()->AddFile(\n+        level + 1,\n+        out.number, out.file_size, out.smallest, out.largest);\n+  }\n+  return versions_->LogAndApply(compact->compaction->edit(), &mutex_);\n+}\n+\n+Status DBImpl::DoCompactionWork(CompactionState* compact) {\n+  const uint64_t start_micros = env_->NowMicros();\n+  int64_t imm_micros = 0;  // Micros spent doing imm_ compactions\n+\n+  Log(options_.info_log,  \"Compacting %d@%d + %d@%d files\",\n+      compact->compaction->num_input_files(0),\n+      compact->compaction->level(),\n+      compact->compaction->num_input_files(1),\n+      compact->compaction->level() + 1);\n+\n+  assert(versions_->NumLevelFiles(compact->compaction->level()) > 0);\n+  assert(compact->builder == NULL);\n+  assert(compact->outfile == NULL);\n+  if (snapshots_.empty()) {\n+    compact->smallest_snapshot = versions_->LastSequence();\n+  } else {\n+    compact->smallest_snapshot = snapshots_.oldest()->number_;\n+  }\n+\n+  // Release mutex while we're actually doing the compaction work\n+  mutex_.Unlock();\n+\n+  Iterator* input = versions_->MakeInputIterator(compact->compaction);\n+  input->SeekToFirst();\n+  Status status;\n+  ParsedInternalKey ikey;\n+  std::string current_user_key;\n+  bool has_current_user_key = false;\n+  SequenceNumber last_sequence_for_key = kMaxSequenceNumber;\n+  for (; input->Valid() && !shutting_down_.Acquire_Load(); ) {\n+    // Prioritize immutable compaction work\n+    if (has_imm_.NoBarrier_Load() != NULL) {\n+      const uint64_t imm_start = env_->NowMicros();\n+      mutex_.Lock();\n+      if (imm_ != NULL) {\n+        CompactMemTable();\n+        bg_cv_.SignalAll();  // Wakeup MakeRoomForWrite() if necessary\n+      }\n+      mutex_.Unlock();\n+      imm_micros += (env_->NowMicros() - imm_start);\n+    }\n+\n+    Slice key = input->key();\n+    if (compact->compaction->ShouldStopBefore(key) &&\n+        compact->builder != NULL) {\n+      status = FinishCompactionOutputFile(compact, input);\n+      if (!status.ok()) {\n+        break;\n+      }\n+    }\n+\n+    // Handle key/value, add to state, etc.\n+    bool drop = false;\n+    if (!ParseInternalKey(key, &ikey)) {\n+      // Do not hide error keys\n+      current_user_key.clear();\n+      has_current_user_key = false;\n+      last_sequence_for_key = kMaxSequenceNumber;\n+    } else {\n+      if (!has_current_user_key ||\n+          user_comparator()->Compare(ikey.user_key,\n+                                     Slice(current_user_key)) != 0) {\n+        // First occurrence of this user key\n+        current_user_key.assign(ikey.user_key.data(), ikey.user_key.size());\n+        has_current_user_key = true;\n+        last_sequence_for_key = kMaxSequenceNumber;\n+      }\n+\n+      if (last_sequence_for_key <= compact->smallest_snapshot) {\n+        // Hidden by an newer entry for same user key\n+        drop = true;    // (A)\n+      } else if (ikey.type == kTypeDeletion &&\n+                 ikey.sequence <= compact->smallest_snapshot &&\n+                 compact->compaction->IsBaseLevelForKey(ikey.user_key)) {\n+        // For this user key:\n+        // (1) there is no data in higher levels\n+        // (2) data in lower levels will have larger sequence numbers\n+        // (3) data in layers that are being compacted here and have\n+        //     smaller sequence numbers will be dropped in the next\n+        //     few iterations of this loop (by rule (A) above).\n+        // Therefore this deletion marker is obsolete and can be dropped.\n+        drop = true;\n+      }\n+\n+      last_sequence_for_key = ikey.sequence;\n+    }\n+#if 0\n+    Log(options_.info_log,\n+        \"  Compact: %s, seq %d, type: %d %d, drop: %d, is_base: %d, \"\n+        \"%d smallest_snapshot: %d\",\n+        ikey.user_key.ToString().c_str(),\n+        (int)ikey.sequence, ikey.type, kTypeValue, drop,\n+        compact->compaction->IsBaseLevelForKey(ikey.user_key),\n+        (int)last_sequence_for_key, (int)compact->smallest_snapshot);\n+#endif\n+\n+    if (!drop) {\n+      // Open output file if necessary\n+      if (compact->builder == NULL) {\n+        status = OpenCompactionOutputFile(compact);\n+        if (!status.ok()) {\n+          break;\n+        }\n+      }\n+      if (compact->builder->NumEntries() == 0) {\n+        compact->current_output()->smallest.DecodeFrom(key);\n+      }\n+      compact->current_output()->largest.DecodeFrom(key);\n+      compact->builder->Add(key, input->value());\n+\n+      // Close output file if it is big enough\n+      if (compact->builder->FileSize() >=\n+          compact->compaction->MaxOutputFileSize()) {\n+        status = FinishCompactionOutputFile(compact, input);\n+        if (!status.ok()) {\n+          break;\n+        }\n+      }\n+    }\n+\n+    input->Next();\n+  }\n+\n+  if (status.ok() && shutting_down_.Acquire_Load()) {\n+    status = Status::IOError(\"Deleting DB during compaction\");\n+  }\n+  if (status.ok() && compact->builder != NULL) {\n+    status = FinishCompactionOutputFile(compact, input);\n+  }\n+  if (status.ok()) {\n+    status = input->status();\n+  }\n+  delete input;\n+  input = NULL;\n+\n+  CompactionStats stats;\n+  stats.micros = env_->NowMicros() - start_micros - imm_micros;\n+  for (int which = 0; which < 2; which++) {\n+    for (int i = 0; i < compact->compaction->num_input_files(which); i++) {\n+      stats.bytes_read += compact->compaction->input(which, i)->file_size;\n+    }\n+  }\n+  for (size_t i = 0; i < compact->outputs.size(); i++) {\n+    stats.bytes_written += compact->outputs[i].file_size;\n+  }\n+\n+  mutex_.Lock();\n+  stats_[compact->compaction->level() + 1].Add(stats);\n+\n+  if (status.ok()) {\n+    status = InstallCompactionResults(compact);\n+  }\n+  VersionSet::LevelSummaryStorage tmp;\n+  Log(options_.info_log,\n+      \"compacted to: %s\", versions_->LevelSummary(&tmp));\n+  return status;\n+}\n+\n+namespace {\n+struct IterState {\n+  port::Mutex* mu;\n+  Version* version;\n+  MemTable* mem;\n+  MemTable* imm;\n+};\n+\n+static void CleanupIteratorState(void* arg1, void* arg2) {\n+  IterState* state = reinterpret_cast<IterState*>(arg1);\n+  state->mu->Lock();\n+  state->mem->Unref();\n+  if (state->imm != NULL) state->imm->Unref();\n+  state->version->Unref();\n+  state->mu->Unlock();\n+  delete state;\n+}\n+}  // namespace\n+\n+Iterator* DBImpl::NewInternalIterator(const ReadOptions& options,\n+                                      SequenceNumber* latest_snapshot) {\n+  IterState* cleanup = new IterState;\n+  mutex_.Lock();\n+  *latest_snapshot = versions_->LastSequence();\n+\n+  // Collect together all needed child iterators\n+  std::vector<Iterator*> list;\n+  list.push_back(mem_->NewIterator());\n+  mem_->Ref();\n+  if (imm_ != NULL) {\n+    list.push_back(imm_->NewIterator());\n+    imm_->Ref();\n+  }\n+  versions_->current()->AddIterators(options, &list);\n+  Iterator* internal_iter =\n+      NewMergingIterator(&internal_comparator_, &list[0], list.size());\n+  versions_->current()->Ref();\n+\n+  cleanup->mu = &mutex_;\n+  cleanup->mem = mem_;\n+  cleanup->imm = imm_;\n+  cleanup->version = versions_->current();\n+  internal_iter->RegisterCleanup(CleanupIteratorState, cleanup, NULL);\n+\n+  mutex_.Unlock();\n+  return internal_iter;\n+}\n+\n+Iterator* DBImpl::TEST_NewInternalIterator() {\n+  SequenceNumber ignored;\n+  return NewInternalIterator(ReadOptions(), &ignored);\n+}\n+\n+int64_t DBImpl::TEST_MaxNextLevelOverlappingBytes() {\n+  MutexLock l(&mutex_);\n+  return versions_->MaxNextLevelOverlappingBytes();\n+}\n+\n+Status DBImpl::Get(const ReadOptions& options,\n+                   const Slice& key,\n+                   std::string* value) {\n+  Status s;\n+  MutexLock l(&mutex_);\n+  SequenceNumber snapshot;\n+  if (options.snapshot != NULL) {\n+    snapshot = reinterpret_cast<const SnapshotImpl*>(options.snapshot)->number_;\n+  } else {\n+    snapshot = versions_->LastSequence();\n+  }\n+\n+  MemTable* mem = mem_;\n+  MemTable* imm = imm_;\n+  Version* current = versions_->current();\n+  mem->Ref();\n+  if (imm != NULL) imm->Ref();\n+  current->Ref();\n+\n+  bool have_stat_update = false;\n+  Version::GetStats stats;\n+\n+  // Unlock while reading from files and memtables\n+  {\n+    mutex_.Unlock();\n+    // First look in the memtable, then in the immutable memtable (if any).\n+    LookupKey lkey(key, snapshot);\n+    if (mem->Get(lkey, value, &s)) {\n+      // Done\n+    } else if (imm != NULL && imm->Get(lkey, value, &s)) {\n+      // Done\n+    } else {\n+      s = current->Get(options, lkey, value, &stats);\n+      have_stat_update = true;\n+    }\n+    mutex_.Lock();\n+  }\n+\n+  if (have_stat_update && current->UpdateStats(stats)) {\n+    MaybeScheduleCompaction();\n+  }\n+  mem->Unref();\n+  if (imm != NULL) imm->Unref();\n+  current->Unref();\n+  return s;\n+}\n+\n+Iterator* DBImpl::NewIterator(const ReadOptions& options) {\n+  SequenceNumber latest_snapshot;\n+  Iterator* internal_iter = NewInternalIterator(options, &latest_snapshot);\n+  return NewDBIterator(\n+      &dbname_, env_, user_comparator(), internal_iter,\n+      (options.snapshot != NULL\n+       ? reinterpret_cast<const SnapshotImpl*>(options.snapshot)->number_\n+       : latest_snapshot));\n+}\n+\n+const Snapshot* DBImpl::GetSnapshot() {\n+  MutexLock l(&mutex_);\n+  return snapshots_.New(versions_->LastSequence());\n+}\n+\n+void DBImpl::ReleaseSnapshot(const Snapshot* s) {\n+  MutexLock l(&mutex_);\n+  snapshots_.Delete(reinterpret_cast<const SnapshotImpl*>(s));\n+}\n+\n+// Convenience methods\n+Status DBImpl::Put(const WriteOptions& o, const Slice& key, const Slice& val) {\n+  return DB::Put(o, key, val);\n+}\n+\n+Status DBImpl::Delete(const WriteOptions& options, const Slice& key) {\n+  return DB::Delete(options, key);\n+}\n+\n+Status DBImpl::Write(const WriteOptions& options, WriteBatch* my_batch) {\n+  Writer w(&mutex_);\n+  w.batch = my_batch;\n+  w.sync = options.sync;\n+  w.done = false;\n+\n+  MutexLock l(&mutex_);\n+  writers_.push_back(&w);\n+  while (!w.done && &w != writers_.front()) {\n+    w.cv.Wait();\n+  }\n+  if (w.done) {\n+    return w.status;\n+  }\n+\n+  // May temporarily unlock and wait.\n+  Status status = MakeRoomForWrite(my_batch == NULL);\n+  uint64_t last_sequence = versions_->LastSequence();\n+  Writer* last_writer = &w;\n+  if (status.ok() && my_batch != NULL) {  // NULL batch is for compactions\n+    WriteBatch* updates = BuildBatchGroup(&last_writer);\n+    WriteBatchInternal::SetSequence(updates, last_sequence + 1);\n+    last_sequence += WriteBatchInternal::Count(updates);\n+\n+    // Add to log and apply to memtable.  We can release the lock\n+    // during this phase since &w is currently responsible for logging\n+    // and protects against concurrent loggers and concurrent writes\n+    // into mem_.\n+    {\n+      mutex_.Unlock();\n+      status = log_->AddRecord(WriteBatchInternal::Contents(updates));\n+      if (status.ok() && options.sync) {\n+        status = logfile_->Sync();\n+      }\n+      if (status.ok()) {\n+        status = WriteBatchInternal::InsertInto(updates, mem_);\n+      }\n+      mutex_.Lock();\n+    }\n+    if (updates == tmp_batch_) tmp_batch_->Clear();\n+\n+    versions_->SetLastSequence(last_sequence);\n+  }\n+\n+  while (true) {\n+    Writer* ready = writers_.front();\n+    writers_.pop_front();\n+    if (ready != &w) {\n+      ready->status = status;\n+      ready->done = true;\n+      ready->cv.Signal();\n+    }\n+    if (ready == last_writer) break;\n+  }\n+\n+  // Notify new head of write queue\n+  if (!writers_.empty()) {\n+    writers_.front()->cv.Signal();\n+  }\n+\n+  return status;\n+}\n+\n+// REQUIRES: Writer list must be non-empty\n+// REQUIRES: First writer must have a non-NULL batch\n+WriteBatch* DBImpl::BuildBatchGroup(Writer** last_writer) {\n+  assert(!writers_.empty());\n+  Writer* first = writers_.front();\n+  WriteBatch* result = first->batch;\n+  assert(result != NULL);\n+\n+  size_t size = WriteBatchInternal::ByteSize(first->batch);\n+\n+  // Allow the group to grow up to a maximum size, but if the\n+  // original write is small, limit the growth so we do not slow\n+  // down the small write too much.\n+  size_t max_size = 1 << 20;\n+  if (size <= (128<<10)) {\n+    max_size = size + (128<<10);\n+  }\n+\n+  *last_writer = first;\n+  std::deque<Writer*>::iterator iter = writers_.begin();\n+  ++iter;  // Advance past \"first\"\n+  for (; iter != writers_.end(); ++iter) {\n+    Writer* w = *iter;\n+    if (w->sync && !first->sync) {\n+      // Do not include a sync write into a batch handled by a non-sync write.\n+      break;\n+    }\n+\n+    if (w->batch != NULL) {\n+      size += WriteBatchInternal::ByteSize(w->batch);\n+      if (size > max_size) {\n+        // Do not make batch too big\n+        break;\n+      }\n+\n+      // Append to *reuslt\n+      if (result == first->batch) {\n+        // Switch to temporary batch instead of disturbing caller's batch\n+        result = tmp_batch_;\n+        assert(WriteBatchInternal::Count(result) == 0);\n+        WriteBatchInternal::Append(result, first->batch);\n+      }\n+      WriteBatchInternal::Append(result, w->batch);\n+    }\n+    *last_writer = w;\n+  }\n+  return result;\n+}\n+\n+// REQUIRES: mutex_ is held\n+// REQUIRES: this thread is currently at the front of the writer queue\n+Status DBImpl::MakeRoomForWrite(bool force) {\n+  mutex_.AssertHeld();\n+  assert(!writers_.empty());\n+  bool allow_delay = !force;\n+  Status s;\n+  while (true) {\n+    if (!bg_error_.ok()) {\n+      // Yield previous error\n+      s = bg_error_;\n+      break;\n+    } else if (\n+        allow_delay &&\n+        versions_->NumLevelFiles(0) >= config::kL0_SlowdownWritesTrigger) {\n+      // We are getting close to hitting a hard limit on the number of\n+      // L0 files.  Rather than delaying a single write by several\n+      // seconds when we hit the hard limit, start delaying each\n+      // individual write by 1ms to reduce latency variance.  Also,\n+      // this delay hands over some CPU to the compaction thread in\n+      // case it is sharing the same core as the writer.\n+      mutex_.Unlock();\n+      env_->SleepForMicroseconds(1000);\n+      allow_delay = false;  // Do not delay a single write more than once\n+      mutex_.Lock();\n+    } else if (!force &&\n+               (mem_->ApproximateMemoryUsage() <= options_.write_buffer_size)) {\n+      // There is room in current memtable\n+      break;\n+    } else if (imm_ != NULL) {\n+      // We have filled up the current memtable, but the previous\n+      // one is still being compacted, so we wait.\n+      Log(options_.info_log, \"Current memtable full; waiting...\\n\");\n+      bg_cv_.Wait();\n+    } else if (versions_->NumLevelFiles(0) >= config::kL0_StopWritesTrigger) {\n+      // There are too many level-0 files.\n+      Log(options_.info_log, \"Too many L0 files; waiting...\\n\");\n+      bg_cv_.Wait();\n+    } else {\n+      // Attempt to switch to a new memtable and trigger compaction of old\n+      assert(versions_->PrevLogNumber() == 0);\n+      uint64_t new_log_number = versions_->NewFileNumber();\n+      WritableFile* lfile = NULL;\n+      s = env_->NewWritableFile(LogFileName(dbname_, new_log_number), &lfile);\n+      if (!s.ok()) {\n+        // Avoid chewing through file number space in a tight loop.\n+        versions_->ReuseFileNumber(new_log_number);\n+        break;\n+      }\n+      delete log_;\n+      delete logfile_;\n+      logfile_ = lfile;\n+      logfile_number_ = new_log_number;\n+      log_ = new log::Writer(lfile);\n+      imm_ = mem_;\n+      has_imm_.Release_Store(imm_);\n+      mem_ = new MemTable(internal_comparator_);\n+      mem_->Ref();\n+      force = false;   // Do not force another compaction if have room\n+      MaybeScheduleCompaction();\n+    }\n+  }\n+  return s;\n+}\n+\n+bool DBImpl::GetProperty(const Slice& property, std::string* value) {\n+  value->clear();\n+\n+  MutexLock l(&mutex_);\n+  Slice in = property;\n+  Slice prefix(\"leveldb.\");\n+  if (!in.starts_with(prefix)) return false;\n+  in.remove_prefix(prefix.size());\n+\n+  if (in.starts_with(\"num-files-at-level\")) {\n+    in.remove_prefix(strlen(\"num-files-at-level\"));\n+    uint64_t level;\n+    bool ok = ConsumeDecimalNumber(&in, &level) && in.empty();\n+    if (!ok || level >= config::kNumLevels) {\n+      return false;\n+    } else {\n+      char buf[100];\n+      snprintf(buf, sizeof(buf), \"%d\",\n+               versions_->NumLevelFiles(static_cast<int>(level)));\n+      *value = buf;\n+      return true;\n+    }\n+  } else if (in == \"stats\") {\n+    char buf[200];\n+    snprintf(buf, sizeof(buf),\n+             \"                               Compactions\\n\"\n+             \"Level  Files Size(MB) Time(sec) Read(MB) Write(MB)\\n\"\n+             \"--------------------------------------------------\\n\"\n+             );\n+    value->append(buf);\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      int files = versions_->NumLevelFiles(level);\n+      if (stats_[level].micros > 0 || files > 0) {\n+        snprintf(\n+            buf, sizeof(buf),\n+            \"%3d %8d %8.0f %9.0f %8.0f %9.0f\\n\",\n+            level,\n+            files,\n+            versions_->NumLevelBytes(level) / 1048576.0,\n+            stats_[level].micros / 1e6,\n+            stats_[level].bytes_read / 1048576.0,\n+            stats_[level].bytes_written / 1048576.0);\n+        value->append(buf);\n+      }\n+    }\n+    return true;\n+  } else if (in == \"sstables\") {\n+    *value = versions_->current()->DebugString();\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void DBImpl::GetApproximateSizes(\n+    const Range* range, int n,\n+    uint64_t* sizes) {\n+  // TODO(opt): better implementation\n+  Version* v;\n+  {\n+    MutexLock l(&mutex_);\n+    versions_->current()->Ref();\n+    v = versions_->current();\n+  }\n+\n+  for (int i = 0; i < n; i++) {\n+    // Convert user_key into a corresponding internal key.\n+    InternalKey k1(range[i].start, kMaxSequenceNumber, kValueTypeForSeek);\n+    InternalKey k2(range[i].limit, kMaxSequenceNumber, kValueTypeForSeek);\n+    uint64_t start = versions_->ApproximateOffsetOf(v, k1);\n+    uint64_t limit = versions_->ApproximateOffsetOf(v, k2);\n+    sizes[i] = (limit >= start ? limit - start : 0);\n+  }\n+\n+  {\n+    MutexLock l(&mutex_);\n+    v->Unref();\n+  }\n+}\n+\n+// Default implementations of convenience methods that subclasses of DB\n+// can call if they wish\n+Status DB::Put(const WriteOptions& opt, const Slice& key, const Slice& value) {\n+  WriteBatch batch;\n+  batch.Put(key, value);\n+  return Write(opt, &batch);\n+}\n+\n+Status DB::Delete(const WriteOptions& opt, const Slice& key) {\n+  WriteBatch batch;\n+  batch.Delete(key);\n+  return Write(opt, &batch);\n+}\n+\n+DB::~DB() { }\n+\n+Status DB::Open(const Options& options, const std::string& dbname,\n+                DB** dbptr) {\n+  *dbptr = NULL;\n+\n+  DBImpl* impl = new DBImpl(options, dbname);\n+  impl->mutex_.Lock();\n+  VersionEdit edit;\n+  Status s = impl->Recover(&edit); // Handles create_if_missing, error_if_exists\n+  if (s.ok()) {\n+    uint64_t new_log_number = impl->versions_->NewFileNumber();\n+    WritableFile* lfile;\n+    s = options.env->NewWritableFile(LogFileName(dbname, new_log_number),\n+                                     &lfile);\n+    if (s.ok()) {\n+      edit.SetLogNumber(new_log_number);\n+      impl->logfile_ = lfile;\n+      impl->logfile_number_ = new_log_number;\n+      impl->log_ = new log::Writer(lfile);\n+      s = impl->versions_->LogAndApply(&edit, &impl->mutex_);\n+    }\n+    if (s.ok()) {\n+      impl->DeleteObsoleteFiles();\n+      impl->MaybeScheduleCompaction();\n+    }\n+  }\n+  impl->mutex_.Unlock();\n+  if (s.ok()) {\n+    *dbptr = impl;\n+  } else {\n+    delete impl;\n+  }\n+  return s;\n+}\n+\n+Snapshot::~Snapshot() {\n+}\n+\n+Status DestroyDB(const std::string& dbname, const Options& options) {\n+  Env* env = options.env;\n+  std::vector<std::string> filenames;\n+  // Ignore error in case directory does not exist\n+  env->GetChildren(dbname, &filenames);\n+  if (filenames.empty()) {\n+    return Status::OK();\n+  }\n+\n+  FileLock* lock;\n+  const std::string lockname = LockFileName(dbname);\n+  Status result = env->LockFile(lockname, &lock);\n+  if (result.ok()) {\n+    uint64_t number;\n+    FileType type;\n+    for (size_t i = 0; i < filenames.size(); i++) {\n+      if (ParseFileName(filenames[i], &number, &type) &&\n+          type != kDBLockFile) {  // Lock file will be deleted at end\n+        Status del = env->DeleteFile(dbname + \"/\" + filenames[i]);\n+        if (result.ok() && !del.ok()) {\n+          result = del;\n+        }\n+      }\n+    }\n+    env->UnlockFile(lock);  // Ignore error since state is already gone\n+    env->DeleteFile(lockname);\n+    env->DeleteDir(dbname);  // Ignore error in case dir contains other files\n+  }\n+  return result;\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "3c8d711ae06de2b41dc6d9a3717808947663c09b",
        "filename": "db/db_impl.h",
        "status": "added",
        "additions": 203,
        "deletions": 0,
        "changes": 203,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/db_impl.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/db_impl.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/db_impl.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,203 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_DB_IMPL_H_\n+#define STORAGE_LEVELDB_DB_DB_IMPL_H_\n+\n+#include <deque>\n+#include <set>\n+#include \"db/dbformat.h\"\n+#include \"db/log_writer.h\"\n+#include \"db/snapshot.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+#include \"port/port.h\"\n+#include \"port/thread_annotations.h\"\n+\n+namespace leveldb {\n+\n+class MemTable;\n+class TableCache;\n+class Version;\n+class VersionEdit;\n+class VersionSet;\n+\n+class DBImpl : public DB {\n+ public:\n+  DBImpl(const Options& options, const std::string& dbname);\n+  virtual ~DBImpl();\n+\n+  // Implementations of the DB interface\n+  virtual Status Put(const WriteOptions&, const Slice& key, const Slice& value);\n+  virtual Status Delete(const WriteOptions&, const Slice& key);\n+  virtual Status Write(const WriteOptions& options, WriteBatch* updates);\n+  virtual Status Get(const ReadOptions& options,\n+                     const Slice& key,\n+                     std::string* value);\n+  virtual Iterator* NewIterator(const ReadOptions&);\n+  virtual const Snapshot* GetSnapshot();\n+  virtual void ReleaseSnapshot(const Snapshot* snapshot);\n+  virtual bool GetProperty(const Slice& property, std::string* value);\n+  virtual void GetApproximateSizes(const Range* range, int n, uint64_t* sizes);\n+  virtual void CompactRange(const Slice* begin, const Slice* end);\n+\n+  // Extra methods (for testing) that are not in the public DB interface\n+\n+  // Compact any files in the named level that overlap [*begin,*end]\n+  void TEST_CompactRange(int level, const Slice* begin, const Slice* end);\n+\n+  // Force current memtable contents to be compacted.\n+  Status TEST_CompactMemTable();\n+\n+  // Return an internal iterator over the current state of the database.\n+  // The keys of this iterator are internal keys (see format.h).\n+  // The returned iterator should be deleted when no longer needed.\n+  Iterator* TEST_NewInternalIterator();\n+\n+  // Return the maximum overlapping data (in bytes) at next level for any\n+  // file at a level >= 1.\n+  int64_t TEST_MaxNextLevelOverlappingBytes();\n+\n+ private:\n+  friend class DB;\n+  struct CompactionState;\n+  struct Writer;\n+\n+  Iterator* NewInternalIterator(const ReadOptions&,\n+                                SequenceNumber* latest_snapshot);\n+\n+  Status NewDB();\n+\n+  // Recover the descriptor from persistent storage.  May do a significant\n+  // amount of work to recover recently logged updates.  Any changes to\n+  // be made to the descriptor are added to *edit.\n+  Status Recover(VersionEdit* edit) EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+\n+  void MaybeIgnoreError(Status* s) const;\n+\n+  // Delete any unneeded files and stale in-memory entries.\n+  void DeleteObsoleteFiles();\n+\n+  // Compact the in-memory write buffer to disk.  Switches to a new\n+  // log-file/memtable and writes a new descriptor iff successful.\n+  Status CompactMemTable()\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+\n+  Status RecoverLogFile(uint64_t log_number,\n+                        VersionEdit* edit,\n+                        SequenceNumber* max_sequence)\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+\n+  Status WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base)\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+\n+  Status MakeRoomForWrite(bool force /* compact even if there is room? */)\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+  WriteBatch* BuildBatchGroup(Writer** last_writer);\n+\n+  void MaybeScheduleCompaction() EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+  static void BGWork(void* db);\n+  void BackgroundCall();\n+  Status BackgroundCompaction() EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+  void CleanupCompaction(CompactionState* compact)\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+  Status DoCompactionWork(CompactionState* compact)\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+\n+  Status OpenCompactionOutputFile(CompactionState* compact);\n+  Status FinishCompactionOutputFile(CompactionState* compact, Iterator* input);\n+  Status InstallCompactionResults(CompactionState* compact)\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+\n+  // Constant after construction\n+  Env* const env_;\n+  const InternalKeyComparator internal_comparator_;\n+  const InternalFilterPolicy internal_filter_policy_;\n+  const Options options_;  // options_.comparator == &internal_comparator_\n+  bool owns_info_log_;\n+  bool owns_cache_;\n+  const std::string dbname_;\n+\n+  // table_cache_ provides its own synchronization\n+  TableCache* table_cache_;\n+\n+  // Lock over the persistent DB state.  Non-NULL iff successfully acquired.\n+  FileLock* db_lock_;\n+\n+  // State below is protected by mutex_\n+  port::Mutex mutex_;\n+  port::AtomicPointer shutting_down_;\n+  port::CondVar bg_cv_;          // Signalled when background work finishes\n+  MemTable* mem_;\n+  MemTable* imm_;                // Memtable being compacted\n+  port::AtomicPointer has_imm_;  // So bg thread can detect non-NULL imm_\n+  WritableFile* logfile_;\n+  uint64_t logfile_number_;\n+  log::Writer* log_;\n+\n+  // Queue of writers.\n+  std::deque<Writer*> writers_;\n+  WriteBatch* tmp_batch_;\n+\n+  SnapshotList snapshots_;\n+\n+  // Set of table files to protect from deletion because they are\n+  // part of ongoing compactions.\n+  std::set<uint64_t> pending_outputs_;\n+\n+  // Has a background compaction been scheduled or is running?\n+  bool bg_compaction_scheduled_;\n+\n+  // Information for a manual compaction\n+  struct ManualCompaction {\n+    int level;\n+    bool done;\n+    const InternalKey* begin;   // NULL means beginning of key range\n+    const InternalKey* end;     // NULL means end of key range\n+    InternalKey tmp_storage;    // Used to keep track of compaction progress\n+  };\n+  ManualCompaction* manual_compaction_;\n+\n+  VersionSet* versions_;\n+\n+  // Have we encountered a background error in paranoid mode?\n+  Status bg_error_;\n+  int consecutive_compaction_errors_;\n+\n+  // Per level compaction stats.  stats_[level] stores the stats for\n+  // compactions that produced data for the specified \"level\".\n+  struct CompactionStats {\n+    int64_t micros;\n+    int64_t bytes_read;\n+    int64_t bytes_written;\n+\n+    CompactionStats() : micros(0), bytes_read(0), bytes_written(0) { }\n+\n+    void Add(const CompactionStats& c) {\n+      this->micros += c.micros;\n+      this->bytes_read += c.bytes_read;\n+      this->bytes_written += c.bytes_written;\n+    }\n+  };\n+  CompactionStats stats_[config::kNumLevels];\n+\n+  // No copying allowed\n+  DBImpl(const DBImpl&);\n+  void operator=(const DBImpl&);\n+\n+  const Comparator* user_comparator() const {\n+    return internal_comparator_.user_comparator();\n+  }\n+};\n+\n+// Sanitize db options.  The caller should delete result.info_log if\n+// it is not equal to src.info_log.\n+extern Options SanitizeOptions(const std::string& db,\n+                               const InternalKeyComparator* icmp,\n+                               const InternalFilterPolicy* ipolicy,\n+                               const Options& src);\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_DB_IMPL_H_"
      },
      {
        "sha": "87dca2ded46453dec5e04cef652d2545086af485",
        "filename": "db/db_iter.cc",
        "status": "added",
        "additions": 299,
        "deletions": 0,
        "changes": 299,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/db_iter.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/db_iter.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/db_iter.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,299 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/db_iter.h\"\n+\n+#include \"db/filename.h\"\n+#include \"db/dbformat.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/iterator.h\"\n+#include \"port/port.h\"\n+#include \"util/logging.h\"\n+#include \"util/mutexlock.h\"\n+\n+namespace leveldb {\n+\n+#if 0\n+static void DumpInternalIter(Iterator* iter) {\n+  for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+    ParsedInternalKey k;\n+    if (!ParseInternalKey(iter->key(), &k)) {\n+      fprintf(stderr, \"Corrupt '%s'\\n\", EscapeString(iter->key()).c_str());\n+    } else {\n+      fprintf(stderr, \"@ '%s'\\n\", k.DebugString().c_str());\n+    }\n+  }\n+}\n+#endif\n+\n+namespace {\n+\n+// Memtables and sstables that make the DB representation contain\n+// (userkey,seq,type) => uservalue entries.  DBIter\n+// combines multiple entries for the same userkey found in the DB\n+// representation into a single entry while accounting for sequence\n+// numbers, deletion markers, overwrites, etc.\n+class DBIter: public Iterator {\n+ public:\n+  // Which direction is the iterator currently moving?\n+  // (1) When moving forward, the internal iterator is positioned at\n+  //     the exact entry that yields this->key(), this->value()\n+  // (2) When moving backwards, the internal iterator is positioned\n+  //     just before all entries whose user key == this->key().\n+  enum Direction {\n+    kForward,\n+    kReverse\n+  };\n+\n+  DBIter(const std::string* dbname, Env* env,\n+         const Comparator* cmp, Iterator* iter, SequenceNumber s)\n+      : dbname_(dbname),\n+        env_(env),\n+        user_comparator_(cmp),\n+        iter_(iter),\n+        sequence_(s),\n+        direction_(kForward),\n+        valid_(false) {\n+  }\n+  virtual ~DBIter() {\n+    delete iter_;\n+  }\n+  virtual bool Valid() const { return valid_; }\n+  virtual Slice key() const {\n+    assert(valid_);\n+    return (direction_ == kForward) ? ExtractUserKey(iter_->key()) : saved_key_;\n+  }\n+  virtual Slice value() const {\n+    assert(valid_);\n+    return (direction_ == kForward) ? iter_->value() : saved_value_;\n+  }\n+  virtual Status status() const {\n+    if (status_.ok()) {\n+      return iter_->status();\n+    } else {\n+      return status_;\n+    }\n+  }\n+\n+  virtual void Next();\n+  virtual void Prev();\n+  virtual void Seek(const Slice& target);\n+  virtual void SeekToFirst();\n+  virtual void SeekToLast();\n+\n+ private:\n+  void FindNextUserEntry(bool skipping, std::string* skip);\n+  void FindPrevUserEntry();\n+  bool ParseKey(ParsedInternalKey* key);\n+\n+  inline void SaveKey(const Slice& k, std::string* dst) {\n+    dst->assign(k.data(), k.size());\n+  }\n+\n+  inline void ClearSavedValue() {\n+    if (saved_value_.capacity() > 1048576) {\n+      std::string empty;\n+      swap(empty, saved_value_);\n+    } else {\n+      saved_value_.clear();\n+    }\n+  }\n+\n+  const std::string* const dbname_;\n+  Env* const env_;\n+  const Comparator* const user_comparator_;\n+  Iterator* const iter_;\n+  SequenceNumber const sequence_;\n+\n+  Status status_;\n+  std::string saved_key_;     // == current key when direction_==kReverse\n+  std::string saved_value_;   // == current raw value when direction_==kReverse\n+  Direction direction_;\n+  bool valid_;\n+\n+  // No copying allowed\n+  DBIter(const DBIter&);\n+  void operator=(const DBIter&);\n+};\n+\n+inline bool DBIter::ParseKey(ParsedInternalKey* ikey) {\n+  if (!ParseInternalKey(iter_->key(), ikey)) {\n+    status_ = Status::Corruption(\"corrupted internal key in DBIter\");\n+    return false;\n+  } else {\n+    return true;\n+  }\n+}\n+\n+void DBIter::Next() {\n+  assert(valid_);\n+\n+  if (direction_ == kReverse) {  // Switch directions?\n+    direction_ = kForward;\n+    // iter_ is pointing just before the entries for this->key(),\n+    // so advance into the range of entries for this->key() and then\n+    // use the normal skipping code below.\n+    if (!iter_->Valid()) {\n+      iter_->SeekToFirst();\n+    } else {\n+      iter_->Next();\n+    }\n+    if (!iter_->Valid()) {\n+      valid_ = false;\n+      saved_key_.clear();\n+      return;\n+    }\n+  }\n+\n+  // Temporarily use saved_key_ as storage for key to skip.\n+  std::string* skip = &saved_key_;\n+  SaveKey(ExtractUserKey(iter_->key()), skip);\n+  FindNextUserEntry(true, skip);\n+}\n+\n+void DBIter::FindNextUserEntry(bool skipping, std::string* skip) {\n+  // Loop until we hit an acceptable entry to yield\n+  assert(iter_->Valid());\n+  assert(direction_ == kForward);\n+  do {\n+    ParsedInternalKey ikey;\n+    if (ParseKey(&ikey) && ikey.sequence <= sequence_) {\n+      switch (ikey.type) {\n+        case kTypeDeletion:\n+          // Arrange to skip all upcoming entries for this key since\n+          // they are hidden by this deletion.\n+          SaveKey(ikey.user_key, skip);\n+          skipping = true;\n+          break;\n+        case kTypeValue:\n+          if (skipping &&\n+              user_comparator_->Compare(ikey.user_key, *skip) <= 0) {\n+            // Entry hidden\n+          } else {\n+            valid_ = true;\n+            saved_key_.clear();\n+            return;\n+          }\n+          break;\n+      }\n+    }\n+    iter_->Next();\n+  } while (iter_->Valid());\n+  saved_key_.clear();\n+  valid_ = false;\n+}\n+\n+void DBIter::Prev() {\n+  assert(valid_);\n+\n+  if (direction_ == kForward) {  // Switch directions?\n+    // iter_ is pointing at the current entry.  Scan backwards until\n+    // the key changes so we can use the normal reverse scanning code.\n+    assert(iter_->Valid());  // Otherwise valid_ would have been false\n+    SaveKey(ExtractUserKey(iter_->key()), &saved_key_);\n+    while (true) {\n+      iter_->Prev();\n+      if (!iter_->Valid()) {\n+        valid_ = false;\n+        saved_key_.clear();\n+        ClearSavedValue();\n+        return;\n+      }\n+      if (user_comparator_->Compare(ExtractUserKey(iter_->key()),\n+                                    saved_key_) < 0) {\n+        break;\n+      }\n+    }\n+    direction_ = kReverse;\n+  }\n+\n+  FindPrevUserEntry();\n+}\n+\n+void DBIter::FindPrevUserEntry() {\n+  assert(direction_ == kReverse);\n+\n+  ValueType value_type = kTypeDeletion;\n+  if (iter_->Valid()) {\n+    do {\n+      ParsedInternalKey ikey;\n+      if (ParseKey(&ikey) && ikey.sequence <= sequence_) {\n+        if ((value_type != kTypeDeletion) &&\n+            user_comparator_->Compare(ikey.user_key, saved_key_) < 0) {\n+          // We encountered a non-deleted value in entries for previous keys,\n+          break;\n+        }\n+        value_type = ikey.type;\n+        if (value_type == kTypeDeletion) {\n+          saved_key_.clear();\n+          ClearSavedValue();\n+        } else {\n+          Slice raw_value = iter_->value();\n+          if (saved_value_.capacity() > raw_value.size() + 1048576) {\n+            std::string empty;\n+            swap(empty, saved_value_);\n+          }\n+          SaveKey(ExtractUserKey(iter_->key()), &saved_key_);\n+          saved_value_.assign(raw_value.data(), raw_value.size());\n+        }\n+      }\n+      iter_->Prev();\n+    } while (iter_->Valid());\n+  }\n+\n+  if (value_type == kTypeDeletion) {\n+    // End\n+    valid_ = false;\n+    saved_key_.clear();\n+    ClearSavedValue();\n+    direction_ = kForward;\n+  } else {\n+    valid_ = true;\n+  }\n+}\n+\n+void DBIter::Seek(const Slice& target) {\n+  direction_ = kForward;\n+  ClearSavedValue();\n+  saved_key_.clear();\n+  AppendInternalKey(\n+      &saved_key_, ParsedInternalKey(target, sequence_, kValueTypeForSeek));\n+  iter_->Seek(saved_key_);\n+  if (iter_->Valid()) {\n+    FindNextUserEntry(false, &saved_key_ /* temporary storage */);\n+  } else {\n+    valid_ = false;\n+  }\n+}\n+\n+void DBIter::SeekToFirst() {\n+  direction_ = kForward;\n+  ClearSavedValue();\n+  iter_->SeekToFirst();\n+  if (iter_->Valid()) {\n+    FindNextUserEntry(false, &saved_key_ /* temporary storage */);\n+  } else {\n+    valid_ = false;\n+  }\n+}\n+\n+void DBIter::SeekToLast() {\n+  direction_ = kReverse;\n+  ClearSavedValue();\n+  iter_->SeekToLast();\n+  FindPrevUserEntry();\n+}\n+\n+}  // anonymous namespace\n+\n+Iterator* NewDBIterator(\n+    const std::string* dbname,\n+    Env* env,\n+    const Comparator* user_key_comparator,\n+    Iterator* internal_iter,\n+    const SequenceNumber& sequence) {\n+  return new DBIter(dbname, env, user_key_comparator, internal_iter, sequence);\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "d9e1b174ab8726eeca58197bb2c36f73099d13e5",
        "filename": "db/db_iter.h",
        "status": "added",
        "additions": 26,
        "deletions": 0,
        "changes": 26,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/db_iter.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/db_iter.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/db_iter.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,26 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_DB_ITER_H_\n+#define STORAGE_LEVELDB_DB_DB_ITER_H_\n+\n+#include <stdint.h>\n+#include \"leveldb/db.h\"\n+#include \"db/dbformat.h\"\n+\n+namespace leveldb {\n+\n+// Return a new iterator that converts internal keys (yielded by\n+// \"*internal_iter\") that were live at the specified \"sequence\" number\n+// into appropriate user keys.\n+extern Iterator* NewDBIterator(\n+    const std::string* dbname,\n+    Env* env,\n+    const Comparator* user_key_comparator,\n+    Iterator* internal_iter,\n+    const SequenceNumber& sequence);\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_DB_ITER_H_"
      },
      {
        "sha": "49aae04dbd3bac4b8d057fd490751f71a2d5b294",
        "filename": "db/db_test.cc",
        "status": "added",
        "additions": 2092,
        "deletions": 0,
        "changes": 2092,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/db_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/db_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/db_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,2092 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"leveldb/db.h\"\n+#include \"leveldb/filter_policy.h\"\n+#include \"db/db_impl.h\"\n+#include \"db/filename.h\"\n+#include \"db/version_set.h\"\n+#include \"db/write_batch_internal.h\"\n+#include \"leveldb/cache.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/table.h\"\n+#include \"util/hash.h\"\n+#include \"util/logging.h\"\n+#include \"util/mutexlock.h\"\n+#include \"util/testharness.h\"\n+#include \"util/testutil.h\"\n+\n+namespace leveldb {\n+\n+static std::string RandomString(Random* rnd, int len) {\n+  std::string r;\n+  test::RandomString(rnd, len, &r);\n+  return r;\n+}\n+\n+namespace {\n+class AtomicCounter {\n+ private:\n+  port::Mutex mu_;\n+  int count_;\n+ public:\n+  AtomicCounter() : count_(0) { }\n+  void Increment() {\n+    IncrementBy(1);\n+  }\n+  void IncrementBy(int count) {\n+    MutexLock l(&mu_);\n+    count_ += count;\n+  }\n+  int Read() {\n+    MutexLock l(&mu_);\n+    return count_;\n+  }\n+  void Reset() {\n+    MutexLock l(&mu_);\n+    count_ = 0;\n+  }\n+};\n+\n+void DelayMilliseconds(int millis) {\n+  Env::Default()->SleepForMicroseconds(millis * 1000);\n+}\n+}\n+\n+// Special Env used to delay background operations\n+class SpecialEnv : public EnvWrapper {\n+ public:\n+  // sstable Sync() calls are blocked while this pointer is non-NULL.\n+  port::AtomicPointer delay_sstable_sync_;\n+\n+  // Simulate no-space errors while this pointer is non-NULL.\n+  port::AtomicPointer no_space_;\n+\n+  // Simulate non-writable file system while this pointer is non-NULL\n+  port::AtomicPointer non_writable_;\n+\n+  // Force sync of manifest files to fail while this pointer is non-NULL\n+  port::AtomicPointer manifest_sync_error_;\n+\n+  // Force write to manifest files to fail while this pointer is non-NULL\n+  port::AtomicPointer manifest_write_error_;\n+\n+  bool count_random_reads_;\n+  AtomicCounter random_read_counter_;\n+\n+  AtomicCounter sleep_counter_;\n+  AtomicCounter sleep_time_counter_;\n+\n+  explicit SpecialEnv(Env* base) : EnvWrapper(base) {\n+    delay_sstable_sync_.Release_Store(NULL);\n+    no_space_.Release_Store(NULL);\n+    non_writable_.Release_Store(NULL);\n+    count_random_reads_ = false;\n+    manifest_sync_error_.Release_Store(NULL);\n+    manifest_write_error_.Release_Store(NULL);\n+  }\n+\n+  Status NewWritableFile(const std::string& f, WritableFile** r) {\n+    class SSTableFile : public WritableFile {\n+     private:\n+      SpecialEnv* env_;\n+      WritableFile* base_;\n+\n+     public:\n+      SSTableFile(SpecialEnv* env, WritableFile* base)\n+          : env_(env),\n+            base_(base) {\n+      }\n+      ~SSTableFile() { delete base_; }\n+      Status Append(const Slice& data) {\n+        if (env_->no_space_.Acquire_Load() != NULL) {\n+          // Drop writes on the floor\n+          return Status::OK();\n+        } else {\n+          return base_->Append(data);\n+        }\n+      }\n+      Status Close() { return base_->Close(); }\n+      Status Flush() { return base_->Flush(); }\n+      Status Sync() {\n+        while (env_->delay_sstable_sync_.Acquire_Load() != NULL) {\n+          DelayMilliseconds(100);\n+        }\n+        return base_->Sync();\n+      }\n+    };\n+    class ManifestFile : public WritableFile {\n+     private:\n+      SpecialEnv* env_;\n+      WritableFile* base_;\n+     public:\n+      ManifestFile(SpecialEnv* env, WritableFile* b) : env_(env), base_(b) { }\n+      ~ManifestFile() { delete base_; }\n+      Status Append(const Slice& data) {\n+        if (env_->manifest_write_error_.Acquire_Load() != NULL) {\n+          return Status::IOError(\"simulated writer error\");\n+        } else {\n+          return base_->Append(data);\n+        }\n+      }\n+      Status Close() { return base_->Close(); }\n+      Status Flush() { return base_->Flush(); }\n+      Status Sync() {\n+        if (env_->manifest_sync_error_.Acquire_Load() != NULL) {\n+          return Status::IOError(\"simulated sync error\");\n+        } else {\n+          return base_->Sync();\n+        }\n+      }\n+    };\n+\n+    if (non_writable_.Acquire_Load() != NULL) {\n+      return Status::IOError(\"simulated write error\");\n+    }\n+\n+    Status s = target()->NewWritableFile(f, r);\n+    if (s.ok()) {\n+      if (strstr(f.c_str(), \".sst\") != NULL) {\n+        *r = new SSTableFile(this, *r);\n+      } else if (strstr(f.c_str(), \"MANIFEST\") != NULL) {\n+        *r = new ManifestFile(this, *r);\n+      }\n+    }\n+    return s;\n+  }\n+\n+  Status NewRandomAccessFile(const std::string& f, RandomAccessFile** r) {\n+    class CountingFile : public RandomAccessFile {\n+     private:\n+      RandomAccessFile* target_;\n+      AtomicCounter* counter_;\n+     public:\n+      CountingFile(RandomAccessFile* target, AtomicCounter* counter)\n+          : target_(target), counter_(counter) {\n+      }\n+      virtual ~CountingFile() { delete target_; }\n+      virtual Status Read(uint64_t offset, size_t n, Slice* result,\n+                          char* scratch) const {\n+        counter_->Increment();\n+        return target_->Read(offset, n, result, scratch);\n+      }\n+    };\n+\n+    Status s = target()->NewRandomAccessFile(f, r);\n+    if (s.ok() && count_random_reads_) {\n+      *r = new CountingFile(*r, &random_read_counter_);\n+    }\n+    return s;\n+  }\n+\n+  virtual void SleepForMicroseconds(int micros) {\n+    sleep_counter_.Increment();\n+    sleep_time_counter_.IncrementBy(micros);\n+  }\n+\n+};\n+\n+class DBTest {\n+ private:\n+  const FilterPolicy* filter_policy_;\n+\n+  // Sequence of option configurations to try\n+  enum OptionConfig {\n+    kDefault,\n+    kFilter,\n+    kUncompressed,\n+    kEnd\n+  };\n+  int option_config_;\n+\n+ public:\n+  std::string dbname_;\n+  SpecialEnv* env_;\n+  DB* db_;\n+\n+  Options last_options_;\n+\n+  DBTest() : option_config_(kDefault),\n+             env_(new SpecialEnv(Env::Default())) {\n+    filter_policy_ = NewBloomFilterPolicy(10);\n+    dbname_ = test::TmpDir() + \"/db_test\";\n+    DestroyDB(dbname_, Options());\n+    db_ = NULL;\n+    Reopen();\n+  }\n+\n+  ~DBTest() {\n+    delete db_;\n+    DestroyDB(dbname_, Options());\n+    delete env_;\n+    delete filter_policy_;\n+  }\n+\n+  // Switch to a fresh database with the next option configuration to\n+  // test.  Return false if there are no more configurations to test.\n+  bool ChangeOptions() {\n+    option_config_++;\n+    if (option_config_ >= kEnd) {\n+      return false;\n+    } else {\n+      DestroyAndReopen();\n+      return true;\n+    }\n+  }\n+\n+  // Return the current option configuration.\n+  Options CurrentOptions() {\n+    Options options;\n+    switch (option_config_) {\n+      case kFilter:\n+        options.filter_policy = filter_policy_;\n+        break;\n+      case kUncompressed:\n+        options.compression = kNoCompression;\n+        break;\n+      default:\n+        break;\n+    }\n+    return options;\n+  }\n+\n+  DBImpl* dbfull() {\n+    return reinterpret_cast<DBImpl*>(db_);\n+  }\n+\n+  void Reopen(Options* options = NULL) {\n+    ASSERT_OK(TryReopen(options));\n+  }\n+\n+  void Close() {\n+    delete db_;\n+    db_ = NULL;\n+  }\n+\n+  void DestroyAndReopen(Options* options = NULL) {\n+    delete db_;\n+    db_ = NULL;\n+    DestroyDB(dbname_, Options());\n+    ASSERT_OK(TryReopen(options));\n+  }\n+\n+  Status TryReopen(Options* options) {\n+    delete db_;\n+    db_ = NULL;\n+    Options opts;\n+    if (options != NULL) {\n+      opts = *options;\n+    } else {\n+      opts = CurrentOptions();\n+      opts.create_if_missing = true;\n+    }\n+    last_options_ = opts;\n+\n+    return DB::Open(opts, dbname_, &db_);\n+  }\n+\n+  Status Put(const std::string& k, const std::string& v) {\n+    return db_->Put(WriteOptions(), k, v);\n+  }\n+\n+  Status Delete(const std::string& k) {\n+    return db_->Delete(WriteOptions(), k);\n+  }\n+\n+  std::string Get(const std::string& k, const Snapshot* snapshot = NULL) {\n+    ReadOptions options;\n+    options.snapshot = snapshot;\n+    std::string result;\n+    Status s = db_->Get(options, k, &result);\n+    if (s.IsNotFound()) {\n+      result = \"NOT_FOUND\";\n+    } else if (!s.ok()) {\n+      result = s.ToString();\n+    }\n+    return result;\n+  }\n+\n+  // Return a string that contains all key,value pairs in order,\n+  // formatted like \"(k1->v1)(k2->v2)\".\n+  std::string Contents() {\n+    std::vector<std::string> forward;\n+    std::string result;\n+    Iterator* iter = db_->NewIterator(ReadOptions());\n+    for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+      std::string s = IterStatus(iter);\n+      result.push_back('(');\n+      result.append(s);\n+      result.push_back(')');\n+      forward.push_back(s);\n+    }\n+\n+    // Check reverse iteration results are the reverse of forward results\n+    int matched = 0;\n+    for (iter->SeekToLast(); iter->Valid(); iter->Prev()) {\n+      ASSERT_LT(matched, forward.size());\n+      ASSERT_EQ(IterStatus(iter), forward[forward.size() - matched - 1]);\n+      matched++;\n+    }\n+    ASSERT_EQ(matched, forward.size());\n+\n+    delete iter;\n+    return result;\n+  }\n+\n+  std::string AllEntriesFor(const Slice& user_key) {\n+    Iterator* iter = dbfull()->TEST_NewInternalIterator();\n+    InternalKey target(user_key, kMaxSequenceNumber, kTypeValue);\n+    iter->Seek(target.Encode());\n+    std::string result;\n+    if (!iter->status().ok()) {\n+      result = iter->status().ToString();\n+    } else {\n+      result = \"[ \";\n+      bool first = true;\n+      while (iter->Valid()) {\n+        ParsedInternalKey ikey;\n+        if (!ParseInternalKey(iter->key(), &ikey)) {\n+          result += \"CORRUPTED\";\n+        } else {\n+          if (last_options_.comparator->Compare(ikey.user_key, user_key) != 0) {\n+            break;\n+          }\n+          if (!first) {\n+            result += \", \";\n+          }\n+          first = false;\n+          switch (ikey.type) {\n+            case kTypeValue:\n+              result += iter->value().ToString();\n+              break;\n+            case kTypeDeletion:\n+              result += \"DEL\";\n+              break;\n+          }\n+        }\n+        iter->Next();\n+      }\n+      if (!first) {\n+        result += \" \";\n+      }\n+      result += \"]\";\n+    }\n+    delete iter;\n+    return result;\n+  }\n+\n+  int NumTableFilesAtLevel(int level) {\n+    std::string property;\n+    ASSERT_TRUE(\n+        db_->GetProperty(\"leveldb.num-files-at-level\" + NumberToString(level),\n+                         &property));\n+    return atoi(property.c_str());\n+  }\n+\n+  int TotalTableFiles() {\n+    int result = 0;\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      result += NumTableFilesAtLevel(level);\n+    }\n+    return result;\n+  }\n+\n+  // Return spread of files per level\n+  std::string FilesPerLevel() {\n+    std::string result;\n+    int last_non_zero_offset = 0;\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      int f = NumTableFilesAtLevel(level);\n+      char buf[100];\n+      snprintf(buf, sizeof(buf), \"%s%d\", (level ? \",\" : \"\"), f);\n+      result += buf;\n+      if (f > 0) {\n+        last_non_zero_offset = result.size();\n+      }\n+    }\n+    result.resize(last_non_zero_offset);\n+    return result;\n+  }\n+\n+  int CountFiles() {\n+    std::vector<std::string> files;\n+    env_->GetChildren(dbname_, &files);\n+    return static_cast<int>(files.size());\n+  }\n+\n+  uint64_t Size(const Slice& start, const Slice& limit) {\n+    Range r(start, limit);\n+    uint64_t size;\n+    db_->GetApproximateSizes(&r, 1, &size);\n+    return size;\n+  }\n+\n+  void Compact(const Slice& start, const Slice& limit) {\n+    db_->CompactRange(&start, &limit);\n+  }\n+\n+  // Do n memtable compactions, each of which produces an sstable\n+  // covering the range [small,large].\n+  void MakeTables(int n, const std::string& small, const std::string& large) {\n+    for (int i = 0; i < n; i++) {\n+      Put(small, \"begin\");\n+      Put(large, \"end\");\n+      dbfull()->TEST_CompactMemTable();\n+    }\n+  }\n+\n+  // Prevent pushing of new sstables into deeper levels by adding\n+  // tables that cover a specified range to all levels.\n+  void FillLevels(const std::string& smallest, const std::string& largest) {\n+    MakeTables(config::kNumLevels, smallest, largest);\n+  }\n+\n+  void DumpFileCounts(const char* label) {\n+    fprintf(stderr, \"---\\n%s:\\n\", label);\n+    fprintf(stderr, \"maxoverlap: %lld\\n\",\n+            static_cast<long long>(\n+                dbfull()->TEST_MaxNextLevelOverlappingBytes()));\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      int num = NumTableFilesAtLevel(level);\n+      if (num > 0) {\n+        fprintf(stderr, \"  level %3d : %d files\\n\", level, num);\n+      }\n+    }\n+  }\n+\n+  std::string DumpSSTableList() {\n+    std::string property;\n+    db_->GetProperty(\"leveldb.sstables\", &property);\n+    return property;\n+  }\n+\n+  std::string IterStatus(Iterator* iter) {\n+    std::string result;\n+    if (iter->Valid()) {\n+      result = iter->key().ToString() + \"->\" + iter->value().ToString();\n+    } else {\n+      result = \"(invalid)\";\n+    }\n+    return result;\n+  }\n+\n+  bool DeleteAnSSTFile() {\n+    std::vector<std::string> filenames;\n+    ASSERT_OK(env_->GetChildren(dbname_, &filenames));\n+    uint64_t number;\n+    FileType type;\n+    for (size_t i = 0; i < filenames.size(); i++) {\n+      if (ParseFileName(filenames[i], &number, &type) && type == kTableFile) {\n+        ASSERT_OK(env_->DeleteFile(TableFileName(dbname_, number)));\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+};\n+\n+TEST(DBTest, Empty) {\n+  do {\n+    ASSERT_TRUE(db_ != NULL);\n+    ASSERT_EQ(\"NOT_FOUND\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, ReadWrite) {\n+  do {\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+    ASSERT_OK(Put(\"bar\", \"v2\"));\n+    ASSERT_OK(Put(\"foo\", \"v3\"));\n+    ASSERT_EQ(\"v3\", Get(\"foo\"));\n+    ASSERT_EQ(\"v2\", Get(\"bar\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, PutDeleteGet) {\n+  do {\n+    ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v1\"));\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+    ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v2\"));\n+    ASSERT_EQ(\"v2\", Get(\"foo\"));\n+    ASSERT_OK(db_->Delete(WriteOptions(), \"foo\"));\n+    ASSERT_EQ(\"NOT_FOUND\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetFromImmutableLayer) {\n+  do {\n+    Options options = CurrentOptions();\n+    options.env = env_;\n+    options.write_buffer_size = 100000;  // Small write buffer\n+    Reopen(&options);\n+\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+\n+    env_->delay_sstable_sync_.Release_Store(env_);   // Block sync calls\n+    Put(\"k1\", std::string(100000, 'x'));             // Fill memtable\n+    Put(\"k2\", std::string(100000, 'y'));             // Trigger compaction\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+    env_->delay_sstable_sync_.Release_Store(NULL);   // Release sync calls\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetFromVersions) {\n+  do {\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetSnapshot) {\n+  do {\n+    // Try with both a short key and a long key\n+    for (int i = 0; i < 2; i++) {\n+      std::string key = (i == 0) ? std::string(\"foo\") : std::string(200, 'x');\n+      ASSERT_OK(Put(key, \"v1\"));\n+      const Snapshot* s1 = db_->GetSnapshot();\n+      ASSERT_OK(Put(key, \"v2\"));\n+      ASSERT_EQ(\"v2\", Get(key));\n+      ASSERT_EQ(\"v1\", Get(key, s1));\n+      dbfull()->TEST_CompactMemTable();\n+      ASSERT_EQ(\"v2\", Get(key));\n+      ASSERT_EQ(\"v1\", Get(key, s1));\n+      db_->ReleaseSnapshot(s1);\n+    }\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetLevel0Ordering) {\n+  do {\n+    // Check that we process level-0 files in correct order.  The code\n+    // below generates two level-0 files where the earlier one comes\n+    // before the later one in the level-0 file list since the earlier\n+    // one has a smaller \"smallest\" key.\n+    ASSERT_OK(Put(\"bar\", \"b\"));\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_OK(Put(\"foo\", \"v2\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"v2\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetOrderedByLevels) {\n+  do {\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    Compact(\"a\", \"z\");\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+    ASSERT_OK(Put(\"foo\", \"v2\"));\n+    ASSERT_EQ(\"v2\", Get(\"foo\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"v2\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetPicksCorrectFile) {\n+  do {\n+    // Arrange to have multiple files in a non-level-0 level.\n+    ASSERT_OK(Put(\"a\", \"va\"));\n+    Compact(\"a\", \"b\");\n+    ASSERT_OK(Put(\"x\", \"vx\"));\n+    Compact(\"x\", \"y\");\n+    ASSERT_OK(Put(\"f\", \"vf\"));\n+    Compact(\"f\", \"g\");\n+    ASSERT_EQ(\"va\", Get(\"a\"));\n+    ASSERT_EQ(\"vf\", Get(\"f\"));\n+    ASSERT_EQ(\"vx\", Get(\"x\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetEncountersEmptyLevel) {\n+  do {\n+    // Arrange for the following to happen:\n+    //   * sstable A in level 0\n+    //   * nothing in level 1\n+    //   * sstable B in level 2\n+    // Then do enough Get() calls to arrange for an automatic compaction\n+    // of sstable A.  A bug would cause the compaction to be marked as\n+    // occuring at level 1 (instead of the correct level 0).\n+\n+    // Step 1: First place sstables in levels 0 and 2\n+    int compaction_count = 0;\n+    while (NumTableFilesAtLevel(0) == 0 ||\n+           NumTableFilesAtLevel(2) == 0) {\n+      ASSERT_LE(compaction_count, 100) << \"could not fill levels 0 and 2\";\n+      compaction_count++;\n+      Put(\"a\", \"begin\");\n+      Put(\"z\", \"end\");\n+      dbfull()->TEST_CompactMemTable();\n+    }\n+\n+    // Step 2: clear level 1 if necessary.\n+    dbfull()->TEST_CompactRange(1, NULL, NULL);\n+    ASSERT_EQ(NumTableFilesAtLevel(0), 1);\n+    ASSERT_EQ(NumTableFilesAtLevel(1), 0);\n+    ASSERT_EQ(NumTableFilesAtLevel(2), 1);\n+\n+    // Step 3: read a bunch of times\n+    for (int i = 0; i < 1000; i++) {\n+      ASSERT_EQ(\"NOT_FOUND\", Get(\"missing\"));\n+    }\n+\n+    // Step 4: Wait for compaction to finish\n+    DelayMilliseconds(1000);\n+\n+    ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, IterEmpty) {\n+  Iterator* iter = db_->NewIterator(ReadOptions());\n+\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->Seek(\"foo\");\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  delete iter;\n+}\n+\n+TEST(DBTest, IterSingle) {\n+  ASSERT_OK(Put(\"a\", \"va\"));\n+  Iterator* iter = db_->NewIterator(ReadOptions());\n+\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->Seek(\"\");\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->Seek(\"a\");\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->Seek(\"b\");\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  delete iter;\n+}\n+\n+TEST(DBTest, IterMulti) {\n+  ASSERT_OK(Put(\"a\", \"va\"));\n+  ASSERT_OK(Put(\"b\", \"vb\"));\n+  ASSERT_OK(Put(\"c\", \"vc\"));\n+  Iterator* iter = db_->NewIterator(ReadOptions());\n+\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->Seek(\"\");\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Seek(\"a\");\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Seek(\"ax\");\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+  iter->Seek(\"b\");\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+  iter->Seek(\"z\");\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  // Switch from reverse to forward\n+  iter->SeekToLast();\n+  iter->Prev();\n+  iter->Prev();\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+\n+  // Switch from forward to reverse\n+  iter->SeekToFirst();\n+  iter->Next();\n+  iter->Next();\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+\n+  // Make sure iter stays at snapshot\n+  ASSERT_OK(Put(\"a\",  \"va2\"));\n+  ASSERT_OK(Put(\"a2\", \"va3\"));\n+  ASSERT_OK(Put(\"b\",  \"vb2\"));\n+  ASSERT_OK(Put(\"c\",  \"vc2\"));\n+  ASSERT_OK(Delete(\"b\"));\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  delete iter;\n+}\n+\n+TEST(DBTest, IterSmallAndLargeMix) {\n+  ASSERT_OK(Put(\"a\", \"va\"));\n+  ASSERT_OK(Put(\"b\", std::string(100000, 'b')));\n+  ASSERT_OK(Put(\"c\", \"vc\"));\n+  ASSERT_OK(Put(\"d\", std::string(100000, 'd')));\n+  ASSERT_OK(Put(\"e\", std::string(100000, 'e')));\n+\n+  Iterator* iter = db_->NewIterator(ReadOptions());\n+\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"b->\" + std::string(100000, 'b'));\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"d->\" + std::string(100000, 'd'));\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"e->\" + std::string(100000, 'e'));\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"e->\" + std::string(100000, 'e'));\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"d->\" + std::string(100000, 'd'));\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"b->\" + std::string(100000, 'b'));\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  delete iter;\n+}\n+\n+TEST(DBTest, IterMultiWithDelete) {\n+  do {\n+    ASSERT_OK(Put(\"a\", \"va\"));\n+    ASSERT_OK(Put(\"b\", \"vb\"));\n+    ASSERT_OK(Put(\"c\", \"vc\"));\n+    ASSERT_OK(Delete(\"b\"));\n+    ASSERT_EQ(\"NOT_FOUND\", Get(\"b\"));\n+\n+    Iterator* iter = db_->NewIterator(ReadOptions());\n+    iter->Seek(\"c\");\n+    ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+    iter->Prev();\n+    ASSERT_EQ(IterStatus(iter), \"a->va\");\n+    delete iter;\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, Recover) {\n+  do {\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    ASSERT_OK(Put(\"baz\", \"v5\"));\n+\n+    Reopen();\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+    ASSERT_EQ(\"v5\", Get(\"baz\"));\n+    ASSERT_OK(Put(\"bar\", \"v2\"));\n+    ASSERT_OK(Put(\"foo\", \"v3\"));\n+\n+    Reopen();\n+    ASSERT_EQ(\"v3\", Get(\"foo\"));\n+    ASSERT_OK(Put(\"foo\", \"v4\"));\n+    ASSERT_EQ(\"v4\", Get(\"foo\"));\n+    ASSERT_EQ(\"v2\", Get(\"bar\"));\n+    ASSERT_EQ(\"v5\", Get(\"baz\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, RecoveryWithEmptyLog) {\n+  do {\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    ASSERT_OK(Put(\"foo\", \"v2\"));\n+    Reopen();\n+    Reopen();\n+    ASSERT_OK(Put(\"foo\", \"v3\"));\n+    Reopen();\n+    ASSERT_EQ(\"v3\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+// Check that writes done during a memtable compaction are recovered\n+// if the database is shutdown during the memtable compaction.\n+TEST(DBTest, RecoverDuringMemtableCompaction) {\n+  do {\n+    Options options = CurrentOptions();\n+    options.env = env_;\n+    options.write_buffer_size = 1000000;\n+    Reopen(&options);\n+\n+    // Trigger a long memtable compaction and reopen the database during it\n+    ASSERT_OK(Put(\"foo\", \"v1\"));                         // Goes to 1st log file\n+    ASSERT_OK(Put(\"big1\", std::string(10000000, 'x')));  // Fills memtable\n+    ASSERT_OK(Put(\"big2\", std::string(1000, 'y')));      // Triggers compaction\n+    ASSERT_OK(Put(\"bar\", \"v2\"));                         // Goes to new log file\n+\n+    Reopen(&options);\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+    ASSERT_EQ(\"v2\", Get(\"bar\"));\n+    ASSERT_EQ(std::string(10000000, 'x'), Get(\"big1\"));\n+    ASSERT_EQ(std::string(1000, 'y'), Get(\"big2\"));\n+  } while (ChangeOptions());\n+}\n+\n+static std::string Key(int i) {\n+  char buf[100];\n+  snprintf(buf, sizeof(buf), \"key%06d\", i);\n+  return std::string(buf);\n+}\n+\n+TEST(DBTest, MinorCompactionsHappen) {\n+  Options options = CurrentOptions();\n+  options.write_buffer_size = 10000;\n+  Reopen(&options);\n+\n+  const int N = 500;\n+\n+  int starting_num_tables = TotalTableFiles();\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_OK(Put(Key(i), Key(i) + std::string(1000, 'v')));\n+  }\n+  int ending_num_tables = TotalTableFiles();\n+  ASSERT_GT(ending_num_tables, starting_num_tables);\n+\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_EQ(Key(i) + std::string(1000, 'v'), Get(Key(i)));\n+  }\n+\n+  Reopen();\n+\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_EQ(Key(i) + std::string(1000, 'v'), Get(Key(i)));\n+  }\n+}\n+\n+TEST(DBTest, RecoverWithLargeLog) {\n+  {\n+    Options options = CurrentOptions();\n+    Reopen(&options);\n+    ASSERT_OK(Put(\"big1\", std::string(200000, '1')));\n+    ASSERT_OK(Put(\"big2\", std::string(200000, '2')));\n+    ASSERT_OK(Put(\"small3\", std::string(10, '3')));\n+    ASSERT_OK(Put(\"small4\", std::string(10, '4')));\n+    ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+  }\n+\n+  // Make sure that if we re-open with a small write buffer size that\n+  // we flush table files in the middle of a large log file.\n+  Options options = CurrentOptions();\n+  options.write_buffer_size = 100000;\n+  Reopen(&options);\n+  ASSERT_EQ(NumTableFilesAtLevel(0), 3);\n+  ASSERT_EQ(std::string(200000, '1'), Get(\"big1\"));\n+  ASSERT_EQ(std::string(200000, '2'), Get(\"big2\"));\n+  ASSERT_EQ(std::string(10, '3'), Get(\"small3\"));\n+  ASSERT_EQ(std::string(10, '4'), Get(\"small4\"));\n+  ASSERT_GT(NumTableFilesAtLevel(0), 1);\n+}\n+\n+TEST(DBTest, CompactionsGenerateMultipleFiles) {\n+  Options options = CurrentOptions();\n+  options.write_buffer_size = 100000000;        // Large write buffer\n+  Reopen(&options);\n+\n+  Random rnd(301);\n+\n+  // Write 8MB (80 values, each 100K)\n+  ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+  std::vector<std::string> values;\n+  for (int i = 0; i < 80; i++) {\n+    values.push_back(RandomString(&rnd, 100000));\n+    ASSERT_OK(Put(Key(i), values[i]));\n+  }\n+\n+  // Reopening moves updates to level-0\n+  Reopen(&options);\n+  dbfull()->TEST_CompactRange(0, NULL, NULL);\n+\n+  ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+  ASSERT_GT(NumTableFilesAtLevel(1), 1);\n+  for (int i = 0; i < 80; i++) {\n+    ASSERT_EQ(Get(Key(i)), values[i]);\n+  }\n+}\n+\n+TEST(DBTest, RepeatedWritesToSameKey) {\n+  Options options = CurrentOptions();\n+  options.env = env_;\n+  options.write_buffer_size = 100000;  // Small write buffer\n+  Reopen(&options);\n+\n+  // We must have at most one file per level except for level-0,\n+  // which may have up to kL0_StopWritesTrigger files.\n+  const int kMaxFiles = config::kNumLevels + config::kL0_StopWritesTrigger;\n+\n+  Random rnd(301);\n+  std::string value = RandomString(&rnd, 2 * options.write_buffer_size);\n+  for (int i = 0; i < 5 * kMaxFiles; i++) {\n+    Put(\"key\", value);\n+    ASSERT_LE(TotalTableFiles(), kMaxFiles);\n+    fprintf(stderr, \"after %d: %d files\\n\", int(i+1), TotalTableFiles());\n+  }\n+}\n+\n+TEST(DBTest, SparseMerge) {\n+  Options options = CurrentOptions();\n+  options.compression = kNoCompression;\n+  Reopen(&options);\n+\n+  FillLevels(\"A\", \"Z\");\n+\n+  // Suppose there is:\n+  //    small amount of data with prefix A\n+  //    large amount of data with prefix B\n+  //    small amount of data with prefix C\n+  // and that recent updates have made small changes to all three prefixes.\n+  // Check that we do not do a compaction that merges all of B in one shot.\n+  const std::string value(1000, 'x');\n+  Put(\"A\", \"va\");\n+  // Write approximately 100MB of \"B\" values\n+  for (int i = 0; i < 100000; i++) {\n+    char key[100];\n+    snprintf(key, sizeof(key), \"B%010d\", i);\n+    Put(key, value);\n+  }\n+  Put(\"C\", \"vc\");\n+  dbfull()->TEST_CompactMemTable();\n+  dbfull()->TEST_CompactRange(0, NULL, NULL);\n+\n+  // Make sparse update\n+  Put(\"A\",    \"va2\");\n+  Put(\"B100\", \"bvalue2\");\n+  Put(\"C\",    \"vc2\");\n+  dbfull()->TEST_CompactMemTable();\n+\n+  // Compactions should not cause us to create a situation where\n+  // a file overlaps too much data at the next level.\n+  ASSERT_LE(dbfull()->TEST_MaxNextLevelOverlappingBytes(), 20*1048576);\n+  dbfull()->TEST_CompactRange(0, NULL, NULL);\n+  ASSERT_LE(dbfull()->TEST_MaxNextLevelOverlappingBytes(), 20*1048576);\n+  dbfull()->TEST_CompactRange(1, NULL, NULL);\n+  ASSERT_LE(dbfull()->TEST_MaxNextLevelOverlappingBytes(), 20*1048576);\n+}\n+\n+static bool Between(uint64_t val, uint64_t low, uint64_t high) {\n+  bool result = (val >= low) && (val <= high);\n+  if (!result) {\n+    fprintf(stderr, \"Value %llu is not in range [%llu, %llu]\\n\",\n+            (unsigned long long)(val),\n+            (unsigned long long)(low),\n+            (unsigned long long)(high));\n+  }\n+  return result;\n+}\n+\n+TEST(DBTest, ApproximateSizes) {\n+  do {\n+    Options options = CurrentOptions();\n+    options.write_buffer_size = 100000000;        // Large write buffer\n+    options.compression = kNoCompression;\n+    DestroyAndReopen();\n+\n+    ASSERT_TRUE(Between(Size(\"\", \"xyz\"), 0, 0));\n+    Reopen(&options);\n+    ASSERT_TRUE(Between(Size(\"\", \"xyz\"), 0, 0));\n+\n+    // Write 8MB (80 values, each 100K)\n+    ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+    const int N = 80;\n+    static const int S1 = 100000;\n+    static const int S2 = 105000;  // Allow some expansion from metadata\n+    Random rnd(301);\n+    for (int i = 0; i < N; i++) {\n+      ASSERT_OK(Put(Key(i), RandomString(&rnd, S1)));\n+    }\n+\n+    // 0 because GetApproximateSizes() does not account for memtable space\n+    ASSERT_TRUE(Between(Size(\"\", Key(50)), 0, 0));\n+\n+    // Check sizes across recovery by reopening a few times\n+    for (int run = 0; run < 3; run++) {\n+      Reopen(&options);\n+\n+      for (int compact_start = 0; compact_start < N; compact_start += 10) {\n+        for (int i = 0; i < N; i += 10) {\n+          ASSERT_TRUE(Between(Size(\"\", Key(i)), S1*i, S2*i));\n+          ASSERT_TRUE(Between(Size(\"\", Key(i)+\".suffix\"), S1*(i+1), S2*(i+1)));\n+          ASSERT_TRUE(Between(Size(Key(i), Key(i+10)), S1*10, S2*10));\n+        }\n+        ASSERT_TRUE(Between(Size(\"\", Key(50)), S1*50, S2*50));\n+        ASSERT_TRUE(Between(Size(\"\", Key(50)+\".suffix\"), S1*50, S2*50));\n+\n+        std::string cstart_str = Key(compact_start);\n+        std::string cend_str = Key(compact_start + 9);\n+        Slice cstart = cstart_str;\n+        Slice cend = cend_str;\n+        dbfull()->TEST_CompactRange(0, &cstart, &cend);\n+      }\n+\n+      ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+      ASSERT_GT(NumTableFilesAtLevel(1), 0);\n+    }\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, ApproximateSizes_MixOfSmallAndLarge) {\n+  do {\n+    Options options = CurrentOptions();\n+    options.compression = kNoCompression;\n+    Reopen();\n+\n+    Random rnd(301);\n+    std::string big1 = RandomString(&rnd, 100000);\n+    ASSERT_OK(Put(Key(0), RandomString(&rnd, 10000)));\n+    ASSERT_OK(Put(Key(1), RandomString(&rnd, 10000)));\n+    ASSERT_OK(Put(Key(2), big1));\n+    ASSERT_OK(Put(Key(3), RandomString(&rnd, 10000)));\n+    ASSERT_OK(Put(Key(4), big1));\n+    ASSERT_OK(Put(Key(5), RandomString(&rnd, 10000)));\n+    ASSERT_OK(Put(Key(6), RandomString(&rnd, 300000)));\n+    ASSERT_OK(Put(Key(7), RandomString(&rnd, 10000)));\n+\n+    // Check sizes across recovery by reopening a few times\n+    for (int run = 0; run < 3; run++) {\n+      Reopen(&options);\n+\n+      ASSERT_TRUE(Between(Size(\"\", Key(0)), 0, 0));\n+      ASSERT_TRUE(Between(Size(\"\", Key(1)), 10000, 11000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(2)), 20000, 21000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(3)), 120000, 121000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(4)), 130000, 131000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(5)), 230000, 231000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(6)), 240000, 241000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(7)), 540000, 541000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(8)), 550000, 560000));\n+\n+      ASSERT_TRUE(Between(Size(Key(3), Key(5)), 110000, 111000));\n+\n+      dbfull()->TEST_CompactRange(0, NULL, NULL);\n+    }\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, IteratorPinsRef) {\n+  Put(\"foo\", \"hello\");\n+\n+  // Get iterator that will yield the current contents of the DB.\n+  Iterator* iter = db_->NewIterator(ReadOptions());\n+\n+  // Write to force compactions\n+  Put(\"foo\", \"newvalue1\");\n+  for (int i = 0; i < 100; i++) {\n+    ASSERT_OK(Put(Key(i), Key(i) + std::string(100000, 'v'))); // 100K values\n+  }\n+  Put(\"foo\", \"newvalue2\");\n+\n+  iter->SeekToFirst();\n+  ASSERT_TRUE(iter->Valid());\n+  ASSERT_EQ(\"foo\", iter->key().ToString());\n+  ASSERT_EQ(\"hello\", iter->value().ToString());\n+  iter->Next();\n+  ASSERT_TRUE(!iter->Valid());\n+  delete iter;\n+}\n+\n+TEST(DBTest, Snapshot) {\n+  do {\n+    Put(\"foo\", \"v1\");\n+    const Snapshot* s1 = db_->GetSnapshot();\n+    Put(\"foo\", \"v2\");\n+    const Snapshot* s2 = db_->GetSnapshot();\n+    Put(\"foo\", \"v3\");\n+    const Snapshot* s3 = db_->GetSnapshot();\n+\n+    Put(\"foo\", \"v4\");\n+    ASSERT_EQ(\"v1\", Get(\"foo\", s1));\n+    ASSERT_EQ(\"v2\", Get(\"foo\", s2));\n+    ASSERT_EQ(\"v3\", Get(\"foo\", s3));\n+    ASSERT_EQ(\"v4\", Get(\"foo\"));\n+\n+    db_->ReleaseSnapshot(s3);\n+    ASSERT_EQ(\"v1\", Get(\"foo\", s1));\n+    ASSERT_EQ(\"v2\", Get(\"foo\", s2));\n+    ASSERT_EQ(\"v4\", Get(\"foo\"));\n+\n+    db_->ReleaseSnapshot(s1);\n+    ASSERT_EQ(\"v2\", Get(\"foo\", s2));\n+    ASSERT_EQ(\"v4\", Get(\"foo\"));\n+\n+    db_->ReleaseSnapshot(s2);\n+    ASSERT_EQ(\"v4\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, HiddenValuesAreRemoved) {\n+  do {\n+    Random rnd(301);\n+    FillLevels(\"a\", \"z\");\n+\n+    std::string big = RandomString(&rnd, 50000);\n+    Put(\"foo\", big);\n+    Put(\"pastfoo\", \"v\");\n+    const Snapshot* snapshot = db_->GetSnapshot();\n+    Put(\"foo\", \"tiny\");\n+    Put(\"pastfoo2\", \"v2\");        // Advance sequence number one more\n+\n+    ASSERT_OK(dbfull()->TEST_CompactMemTable());\n+    ASSERT_GT(NumTableFilesAtLevel(0), 0);\n+\n+    ASSERT_EQ(big, Get(\"foo\", snapshot));\n+    ASSERT_TRUE(Between(Size(\"\", \"pastfoo\"), 50000, 60000));\n+    db_->ReleaseSnapshot(snapshot);\n+    ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ tiny, \" + big + \" ]\");\n+    Slice x(\"x\");\n+    dbfull()->TEST_CompactRange(0, NULL, &x);\n+    ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ tiny ]\");\n+    ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+    ASSERT_GE(NumTableFilesAtLevel(1), 1);\n+    dbfull()->TEST_CompactRange(1, NULL, &x);\n+    ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ tiny ]\");\n+\n+    ASSERT_TRUE(Between(Size(\"\", \"pastfoo\"), 0, 1000));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, DeletionMarkers1) {\n+  Put(\"foo\", \"v1\");\n+  ASSERT_OK(dbfull()->TEST_CompactMemTable());\n+  const int last = config::kMaxMemCompactLevel;\n+  ASSERT_EQ(NumTableFilesAtLevel(last), 1);   // foo => v1 is now in last level\n+\n+  // Place a table at level last-1 to prevent merging with preceding mutation\n+  Put(\"a\", \"begin\");\n+  Put(\"z\", \"end\");\n+  dbfull()->TEST_CompactMemTable();\n+  ASSERT_EQ(NumTableFilesAtLevel(last), 1);\n+  ASSERT_EQ(NumTableFilesAtLevel(last-1), 1);\n+\n+  Delete(\"foo\");\n+  Put(\"foo\", \"v2\");\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ v2, DEL, v1 ]\");\n+  ASSERT_OK(dbfull()->TEST_CompactMemTable());  // Moves to level last-2\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ v2, DEL, v1 ]\");\n+  Slice z(\"z\");\n+  dbfull()->TEST_CompactRange(last-2, NULL, &z);\n+  // DEL eliminated, but v1 remains because we aren't compacting that level\n+  // (DEL can be eliminated because v2 hides v1).\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ v2, v1 ]\");\n+  dbfull()->TEST_CompactRange(last-1, NULL, NULL);\n+  // Merging last-1 w/ last, so we are the base level for \"foo\", so\n+  // DEL is removed.  (as is v1).\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ v2 ]\");\n+}\n+\n+TEST(DBTest, DeletionMarkers2) {\n+  Put(\"foo\", \"v1\");\n+  ASSERT_OK(dbfull()->TEST_CompactMemTable());\n+  const int last = config::kMaxMemCompactLevel;\n+  ASSERT_EQ(NumTableFilesAtLevel(last), 1);   // foo => v1 is now in last level\n+\n+  // Place a table at level last-1 to prevent merging with preceding mutation\n+  Put(\"a\", \"begin\");\n+  Put(\"z\", \"end\");\n+  dbfull()->TEST_CompactMemTable();\n+  ASSERT_EQ(NumTableFilesAtLevel(last), 1);\n+  ASSERT_EQ(NumTableFilesAtLevel(last-1), 1);\n+\n+  Delete(\"foo\");\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ DEL, v1 ]\");\n+  ASSERT_OK(dbfull()->TEST_CompactMemTable());  // Moves to level last-2\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ DEL, v1 ]\");\n+  dbfull()->TEST_CompactRange(last-2, NULL, NULL);\n+  // DEL kept: \"last\" file overlaps\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ DEL, v1 ]\");\n+  dbfull()->TEST_CompactRange(last-1, NULL, NULL);\n+  // Merging last-1 w/ last, so we are the base level for \"foo\", so\n+  // DEL is removed.  (as is v1).\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ ]\");\n+}\n+\n+TEST(DBTest, OverlapInLevel0) {\n+  do {\n+    ASSERT_EQ(config::kMaxMemCompactLevel, 2) << \"Fix test to match config\";\n+\n+    // Fill levels 1 and 2 to disable the pushing of new memtables to levels > 0.\n+    ASSERT_OK(Put(\"100\", \"v100\"));\n+    ASSERT_OK(Put(\"999\", \"v999\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_OK(Delete(\"100\"));\n+    ASSERT_OK(Delete(\"999\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"0,1,1\", FilesPerLevel());\n+\n+    // Make files spanning the following ranges in level-0:\n+    //  files[0]  200 .. 900\n+    //  files[1]  300 .. 500\n+    // Note that files are sorted by smallest key.\n+    ASSERT_OK(Put(\"300\", \"v300\"));\n+    ASSERT_OK(Put(\"500\", \"v500\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_OK(Put(\"200\", \"v200\"));\n+    ASSERT_OK(Put(\"600\", \"v600\"));\n+    ASSERT_OK(Put(\"900\", \"v900\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"2,1,1\", FilesPerLevel());\n+\n+    // Compact away the placeholder files we created initially\n+    dbfull()->TEST_CompactRange(1, NULL, NULL);\n+    dbfull()->TEST_CompactRange(2, NULL, NULL);\n+    ASSERT_EQ(\"2\", FilesPerLevel());\n+\n+    // Do a memtable compaction.  Before bug-fix, the compaction would\n+    // not detect the overlap with level-0 files and would incorrectly place\n+    // the deletion in a deeper level.\n+    ASSERT_OK(Delete(\"600\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"3\", FilesPerLevel());\n+    ASSERT_EQ(\"NOT_FOUND\", Get(\"600\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, L0_CompactionBug_Issue44_a) {\n+  Reopen();\n+  ASSERT_OK(Put(\"b\", \"v\"));\n+  Reopen();\n+  ASSERT_OK(Delete(\"b\"));\n+  ASSERT_OK(Delete(\"a\"));\n+  Reopen();\n+  ASSERT_OK(Delete(\"a\"));\n+  Reopen();\n+  ASSERT_OK(Put(\"a\", \"v\"));\n+  Reopen();\n+  Reopen();\n+  ASSERT_EQ(\"(a->v)\", Contents());\n+  DelayMilliseconds(1000);  // Wait for compaction to finish\n+  ASSERT_EQ(\"(a->v)\", Contents());\n+}\n+\n+TEST(DBTest, L0_CompactionBug_Issue44_b) {\n+  Reopen();\n+  Put(\"\",\"\");\n+  Reopen();\n+  Delete(\"e\");\n+  Put(\"\",\"\");\n+  Reopen();\n+  Put(\"c\", \"cv\");\n+  Reopen();\n+  Put(\"\",\"\");\n+  Reopen();\n+  Put(\"\",\"\");\n+  DelayMilliseconds(1000);  // Wait for compaction to finish\n+  Reopen();\n+  Put(\"d\",\"dv\");\n+  Reopen();\n+  Put(\"\",\"\");\n+  Reopen();\n+  Delete(\"d\");\n+  Delete(\"b\");\n+  Reopen();\n+  ASSERT_EQ(\"(->)(c->cv)\", Contents());\n+  DelayMilliseconds(1000);  // Wait for compaction to finish\n+  ASSERT_EQ(\"(->)(c->cv)\", Contents());\n+}\n+\n+TEST(DBTest, ComparatorCheck) {\n+  class NewComparator : public Comparator {\n+   public:\n+    virtual const char* Name() const { return \"leveldb.NewComparator\"; }\n+    virtual int Compare(const Slice& a, const Slice& b) const {\n+      return BytewiseComparator()->Compare(a, b);\n+    }\n+    virtual void FindShortestSeparator(std::string* s, const Slice& l) const {\n+      BytewiseComparator()->FindShortestSeparator(s, l);\n+    }\n+    virtual void FindShortSuccessor(std::string* key) const {\n+      BytewiseComparator()->FindShortSuccessor(key);\n+    }\n+  };\n+  NewComparator cmp;\n+  Options new_options = CurrentOptions();\n+  new_options.comparator = &cmp;\n+  Status s = TryReopen(&new_options);\n+  ASSERT_TRUE(!s.ok());\n+  ASSERT_TRUE(s.ToString().find(\"comparator\") != std::string::npos)\n+      << s.ToString();\n+}\n+\n+TEST(DBTest, CustomComparator) {\n+  class NumberComparator : public Comparator {\n+   public:\n+    virtual const char* Name() const { return \"test.NumberComparator\"; }\n+    virtual int Compare(const Slice& a, const Slice& b) const {\n+      return ToNumber(a) - ToNumber(b);\n+    }\n+    virtual void FindShortestSeparator(std::string* s, const Slice& l) const {\n+      ToNumber(*s);     // Check format\n+      ToNumber(l);      // Check format\n+    }\n+    virtual void FindShortSuccessor(std::string* key) const {\n+      ToNumber(*key);   // Check format\n+    }\n+   private:\n+    static int ToNumber(const Slice& x) {\n+      // Check that there are no extra characters.\n+      ASSERT_TRUE(x.size() >= 2 && x[0] == '[' && x[x.size()-1] == ']')\n+          << EscapeString(x);\n+      int val;\n+      char ignored;\n+      ASSERT_TRUE(sscanf(x.ToString().c_str(), \"[%i]%c\", &val, &ignored) == 1)\n+          << EscapeString(x);\n+      return val;\n+    }\n+  };\n+  NumberComparator cmp;\n+  Options new_options = CurrentOptions();\n+  new_options.create_if_missing = true;\n+  new_options.comparator = &cmp;\n+  new_options.filter_policy = NULL;     // Cannot use bloom filters\n+  new_options.write_buffer_size = 1000;  // Compact more often\n+  DestroyAndReopen(&new_options);\n+  ASSERT_OK(Put(\"[10]\", \"ten\"));\n+  ASSERT_OK(Put(\"[0x14]\", \"twenty\"));\n+  for (int i = 0; i < 2; i++) {\n+    ASSERT_EQ(\"ten\", Get(\"[10]\"));\n+    ASSERT_EQ(\"ten\", Get(\"[0xa]\"));\n+    ASSERT_EQ(\"twenty\", Get(\"[20]\"));\n+    ASSERT_EQ(\"twenty\", Get(\"[0x14]\"));\n+    ASSERT_EQ(\"NOT_FOUND\", Get(\"[15]\"));\n+    ASSERT_EQ(\"NOT_FOUND\", Get(\"[0xf]\"));\n+    Compact(\"[0]\", \"[9999]\");\n+  }\n+\n+  for (int run = 0; run < 2; run++) {\n+    for (int i = 0; i < 1000; i++) {\n+      char buf[100];\n+      snprintf(buf, sizeof(buf), \"[%d]\", i*10);\n+      ASSERT_OK(Put(buf, buf));\n+    }\n+    Compact(\"[0]\", \"[1000000]\");\n+  }\n+}\n+\n+TEST(DBTest, ManualCompaction) {\n+  ASSERT_EQ(config::kMaxMemCompactLevel, 2)\n+      << \"Need to update this test to match kMaxMemCompactLevel\";\n+\n+  MakeTables(3, \"p\", \"q\");\n+  ASSERT_EQ(\"1,1,1\", FilesPerLevel());\n+\n+  // Compaction range falls before files\n+  Compact(\"\", \"c\");\n+  ASSERT_EQ(\"1,1,1\", FilesPerLevel());\n+\n+  // Compaction range falls after files\n+  Compact(\"r\", \"z\");\n+  ASSERT_EQ(\"1,1,1\", FilesPerLevel());\n+\n+  // Compaction range overlaps files\n+  Compact(\"p1\", \"p9\");\n+  ASSERT_EQ(\"0,0,1\", FilesPerLevel());\n+\n+  // Populate a different range\n+  MakeTables(3, \"c\", \"e\");\n+  ASSERT_EQ(\"1,1,2\", FilesPerLevel());\n+\n+  // Compact just the new range\n+  Compact(\"b\", \"f\");\n+  ASSERT_EQ(\"0,0,2\", FilesPerLevel());\n+\n+  // Compact all\n+  MakeTables(1, \"a\", \"z\");\n+  ASSERT_EQ(\"0,1,2\", FilesPerLevel());\n+  db_->CompactRange(NULL, NULL);\n+  ASSERT_EQ(\"0,0,1\", FilesPerLevel());\n+}\n+\n+TEST(DBTest, DBOpen_Options) {\n+  std::string dbname = test::TmpDir() + \"/db_options_test\";\n+  DestroyDB(dbname, Options());\n+\n+  // Does not exist, and create_if_missing == false: error\n+  DB* db = NULL;\n+  Options opts;\n+  opts.create_if_missing = false;\n+  Status s = DB::Open(opts, dbname, &db);\n+  ASSERT_TRUE(strstr(s.ToString().c_str(), \"does not exist\") != NULL);\n+  ASSERT_TRUE(db == NULL);\n+\n+  // Does not exist, and create_if_missing == true: OK\n+  opts.create_if_missing = true;\n+  s = DB::Open(opts, dbname, &db);\n+  ASSERT_OK(s);\n+  ASSERT_TRUE(db != NULL);\n+\n+  delete db;\n+  db = NULL;\n+\n+  // Does exist, and error_if_exists == true: error\n+  opts.create_if_missing = false;\n+  opts.error_if_exists = true;\n+  s = DB::Open(opts, dbname, &db);\n+  ASSERT_TRUE(strstr(s.ToString().c_str(), \"exists\") != NULL);\n+  ASSERT_TRUE(db == NULL);\n+\n+  // Does exist, and error_if_exists == false: OK\n+  opts.create_if_missing = true;\n+  opts.error_if_exists = false;\n+  s = DB::Open(opts, dbname, &db);\n+  ASSERT_OK(s);\n+  ASSERT_TRUE(db != NULL);\n+\n+  delete db;\n+  db = NULL;\n+}\n+\n+TEST(DBTest, Locking) {\n+  DB* db2 = NULL;\n+  Status s = DB::Open(CurrentOptions(), dbname_, &db2);\n+  ASSERT_TRUE(!s.ok()) << \"Locking did not prevent re-opening db\";\n+}\n+\n+// Check that number of files does not grow when we are out of space\n+TEST(DBTest, NoSpace) {\n+  Options options = CurrentOptions();\n+  options.env = env_;\n+  Reopen(&options);\n+\n+  ASSERT_OK(Put(\"foo\", \"v1\"));\n+  ASSERT_EQ(\"v1\", Get(\"foo\"));\n+  Compact(\"a\", \"z\");\n+  const int num_files = CountFiles();\n+  env_->no_space_.Release_Store(env_);   // Force out-of-space errors\n+  env_->sleep_counter_.Reset();\n+  for (int i = 0; i < 5; i++) {\n+    for (int level = 0; level < config::kNumLevels-1; level++) {\n+      dbfull()->TEST_CompactRange(level, NULL, NULL);\n+    }\n+  }\n+  env_->no_space_.Release_Store(NULL);\n+  ASSERT_LT(CountFiles(), num_files + 3);\n+\n+  // Check that compaction attempts slept after errors\n+  ASSERT_GE(env_->sleep_counter_.Read(), 5);\n+}\n+\n+TEST(DBTest, ExponentialBackoff) {\n+  Options options = CurrentOptions();\n+  options.env = env_;\n+  Reopen(&options);\n+\n+  ASSERT_OK(Put(\"foo\", \"v1\"));\n+  ASSERT_EQ(\"v1\", Get(\"foo\"));\n+  Compact(\"a\", \"z\");\n+  env_->non_writable_.Release_Store(env_);  // Force errors for new files\n+  env_->sleep_counter_.Reset();\n+  env_->sleep_time_counter_.Reset();\n+  for (int i = 0; i < 5; i++) {\n+    dbfull()->TEST_CompactRange(2, NULL, NULL);\n+  }\n+  env_->non_writable_.Release_Store(NULL);\n+\n+  // Wait for compaction to finish\n+  DelayMilliseconds(1000);\n+\n+  ASSERT_GE(env_->sleep_counter_.Read(), 5);\n+  ASSERT_LT(env_->sleep_counter_.Read(), 10);\n+  ASSERT_GE(env_->sleep_time_counter_.Read(), 10e6);\n+}\n+\n+TEST(DBTest, NonWritableFileSystem) {\n+  Options options = CurrentOptions();\n+  options.write_buffer_size = 1000;\n+  options.env = env_;\n+  Reopen(&options);\n+  ASSERT_OK(Put(\"foo\", \"v1\"));\n+  env_->non_writable_.Release_Store(env_);  // Force errors for new files\n+  std::string big(100000, 'x');\n+  int errors = 0;\n+  for (int i = 0; i < 20; i++) {\n+    fprintf(stderr, \"iter %d; errors %d\\n\", i, errors);\n+    if (!Put(\"foo\", big).ok()) {\n+      errors++;\n+      DelayMilliseconds(100);\n+    }\n+  }\n+  ASSERT_GT(errors, 0);\n+  env_->non_writable_.Release_Store(NULL);\n+}\n+\n+TEST(DBTest, ManifestWriteError) {\n+  // Test for the following problem:\n+  // (a) Compaction produces file F\n+  // (b) Log record containing F is written to MANIFEST file, but Sync() fails\n+  // (c) GC deletes F\n+  // (d) After reopening DB, reads fail since deleted F is named in log record\n+\n+  // We iterate twice.  In the second iteration, everything is the\n+  // same except the log record never makes it to the MANIFEST file.\n+  for (int iter = 0; iter < 2; iter++) {\n+    port::AtomicPointer* error_type = (iter == 0)\n+        ? &env_->manifest_sync_error_\n+        : &env_->manifest_write_error_;\n+\n+    // Insert foo=>bar mapping\n+    Options options = CurrentOptions();\n+    options.env = env_;\n+    options.create_if_missing = true;\n+    options.error_if_exists = false;\n+    DestroyAndReopen(&options);\n+    ASSERT_OK(Put(\"foo\", \"bar\"));\n+    ASSERT_EQ(\"bar\", Get(\"foo\"));\n+\n+    // Memtable compaction (will succeed)\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"bar\", Get(\"foo\"));\n+    const int last = config::kMaxMemCompactLevel;\n+    ASSERT_EQ(NumTableFilesAtLevel(last), 1);   // foo=>bar is now in last level\n+\n+    // Merging compaction (will fail)\n+    error_type->Release_Store(env_);\n+    dbfull()->TEST_CompactRange(last, NULL, NULL);  // Should fail\n+    ASSERT_EQ(\"bar\", Get(\"foo\"));\n+\n+    // Recovery: should not lose data\n+    error_type->Release_Store(NULL);\n+    Reopen(&options);\n+    ASSERT_EQ(\"bar\", Get(\"foo\"));\n+  }\n+}\n+\n+TEST(DBTest, MissingSSTFile) {\n+  ASSERT_OK(Put(\"foo\", \"bar\"));\n+  ASSERT_EQ(\"bar\", Get(\"foo\"));\n+\n+  // Dump the memtable to disk.\n+  dbfull()->TEST_CompactMemTable();\n+  ASSERT_EQ(\"bar\", Get(\"foo\"));\n+\n+  Close();\n+  ASSERT_TRUE(DeleteAnSSTFile());\n+  Options options = CurrentOptions();\n+  options.paranoid_checks = true;\n+  Status s = TryReopen(&options);\n+  ASSERT_TRUE(!s.ok());\n+  ASSERT_TRUE(s.ToString().find(\"issing\") != std::string::npos)\n+      << s.ToString();\n+}\n+\n+TEST(DBTest, FilesDeletedAfterCompaction) {\n+  ASSERT_OK(Put(\"foo\", \"v2\"));\n+  Compact(\"a\", \"z\");\n+  const int num_files = CountFiles();\n+  for (int i = 0; i < 10; i++) {\n+    ASSERT_OK(Put(\"foo\", \"v2\"));\n+    Compact(\"a\", \"z\");\n+  }\n+  ASSERT_EQ(CountFiles(), num_files);\n+}\n+\n+TEST(DBTest, BloomFilter) {\n+  env_->count_random_reads_ = true;\n+  Options options = CurrentOptions();\n+  options.env = env_;\n+  options.block_cache = NewLRUCache(0);  // Prevent cache hits\n+  options.filter_policy = NewBloomFilterPolicy(10);\n+  Reopen(&options);\n+\n+  // Populate multiple layers\n+  const int N = 10000;\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_OK(Put(Key(i), Key(i)));\n+  }\n+  Compact(\"a\", \"z\");\n+  for (int i = 0; i < N; i += 100) {\n+    ASSERT_OK(Put(Key(i), Key(i)));\n+  }\n+  dbfull()->TEST_CompactMemTable();\n+\n+  // Prevent auto compactions triggered by seeks\n+  env_->delay_sstable_sync_.Release_Store(env_);\n+\n+  // Lookup present keys.  Should rarely read from small sstable.\n+  env_->random_read_counter_.Reset();\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_EQ(Key(i), Get(Key(i)));\n+  }\n+  int reads = env_->random_read_counter_.Read();\n+  fprintf(stderr, \"%d present => %d reads\\n\", N, reads);\n+  ASSERT_GE(reads, N);\n+  ASSERT_LE(reads, N + 2*N/100);\n+\n+  // Lookup present keys.  Should rarely read from either sstable.\n+  env_->random_read_counter_.Reset();\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_EQ(\"NOT_FOUND\", Get(Key(i) + \".missing\"));\n+  }\n+  reads = env_->random_read_counter_.Read();\n+  fprintf(stderr, \"%d missing => %d reads\\n\", N, reads);\n+  ASSERT_LE(reads, 3*N/100);\n+\n+  env_->delay_sstable_sync_.Release_Store(NULL);\n+  Close();\n+  delete options.block_cache;\n+  delete options.filter_policy;\n+}\n+\n+// Multi-threaded test:\n+namespace {\n+\n+static const int kNumThreads = 4;\n+static const int kTestSeconds = 10;\n+static const int kNumKeys = 1000;\n+\n+struct MTState {\n+  DBTest* test;\n+  port::AtomicPointer stop;\n+  port::AtomicPointer counter[kNumThreads];\n+  port::AtomicPointer thread_done[kNumThreads];\n+};\n+\n+struct MTThread {\n+  MTState* state;\n+  int id;\n+};\n+\n+static void MTThreadBody(void* arg) {\n+  MTThread* t = reinterpret_cast<MTThread*>(arg);\n+  int id = t->id;\n+  DB* db = t->state->test->db_;\n+  uintptr_t counter = 0;\n+  fprintf(stderr, \"... starting thread %d\\n\", id);\n+  Random rnd(1000 + id);\n+  std::string value;\n+  char valbuf[1500];\n+  while (t->state->stop.Acquire_Load() == NULL) {\n+    t->state->counter[id].Release_Store(reinterpret_cast<void*>(counter));\n+\n+    int key = rnd.Uniform(kNumKeys);\n+    char keybuf[20];\n+    snprintf(keybuf, sizeof(keybuf), \"%016d\", key);\n+\n+    if (rnd.OneIn(2)) {\n+      // Write values of the form <key, my id, counter>.\n+      // We add some padding for force compactions.\n+      snprintf(valbuf, sizeof(valbuf), \"%d.%d.%-1000d\",\n+               key, id, static_cast<int>(counter));\n+      ASSERT_OK(db->Put(WriteOptions(), Slice(keybuf), Slice(valbuf)));\n+    } else {\n+      // Read a value and verify that it matches the pattern written above.\n+      Status s = db->Get(ReadOptions(), Slice(keybuf), &value);\n+      if (s.IsNotFound()) {\n+        // Key has not yet been written\n+      } else {\n+        // Check that the writer thread counter is >= the counter in the value\n+        ASSERT_OK(s);\n+        int k, w, c;\n+        ASSERT_EQ(3, sscanf(value.c_str(), \"%d.%d.%d\", &k, &w, &c)) << value;\n+        ASSERT_EQ(k, key);\n+        ASSERT_GE(w, 0);\n+        ASSERT_LT(w, kNumThreads);\n+        ASSERT_LE(c, reinterpret_cast<uintptr_t>(\n+            t->state->counter[w].Acquire_Load()));\n+      }\n+    }\n+    counter++;\n+  }\n+  t->state->thread_done[id].Release_Store(t);\n+  fprintf(stderr, \"... stopping thread %d after %d ops\\n\", id, int(counter));\n+}\n+\n+}  // namespace\n+\n+TEST(DBTest, MultiThreaded) {\n+  do {\n+    // Initialize state\n+    MTState mt;\n+    mt.test = this;\n+    mt.stop.Release_Store(0);\n+    for (int id = 0; id < kNumThreads; id++) {\n+      mt.counter[id].Release_Store(0);\n+      mt.thread_done[id].Release_Store(0);\n+    }\n+\n+    // Start threads\n+    MTThread thread[kNumThreads];\n+    for (int id = 0; id < kNumThreads; id++) {\n+      thread[id].state = &mt;\n+      thread[id].id = id;\n+      env_->StartThread(MTThreadBody, &thread[id]);\n+    }\n+\n+    // Let them run for a while\n+    DelayMilliseconds(kTestSeconds * 1000);\n+\n+    // Stop the threads and wait for them to finish\n+    mt.stop.Release_Store(&mt);\n+    for (int id = 0; id < kNumThreads; id++) {\n+      while (mt.thread_done[id].Acquire_Load() == NULL) {\n+        DelayMilliseconds(100);\n+      }\n+    }\n+  } while (ChangeOptions());\n+}\n+\n+namespace {\n+typedef std::map<std::string, std::string> KVMap;\n+}\n+\n+class ModelDB: public DB {\n+ public:\n+  class ModelSnapshot : public Snapshot {\n+   public:\n+    KVMap map_;\n+  };\n+\n+  explicit ModelDB(const Options& options): options_(options) { }\n+  ~ModelDB() { }\n+  virtual Status Put(const WriteOptions& o, const Slice& k, const Slice& v) {\n+    return DB::Put(o, k, v);\n+  }\n+  virtual Status Delete(const WriteOptions& o, const Slice& key) {\n+    return DB::Delete(o, key);\n+  }\n+  virtual Status Get(const ReadOptions& options,\n+                     const Slice& key, std::string* value) {\n+    assert(false);      // Not implemented\n+    return Status::NotFound(key);\n+  }\n+  virtual Iterator* NewIterator(const ReadOptions& options) {\n+    if (options.snapshot == NULL) {\n+      KVMap* saved = new KVMap;\n+      *saved = map_;\n+      return new ModelIter(saved, true);\n+    } else {\n+      const KVMap* snapshot_state =\n+          &(reinterpret_cast<const ModelSnapshot*>(options.snapshot)->map_);\n+      return new ModelIter(snapshot_state, false);\n+    }\n+  }\n+  virtual const Snapshot* GetSnapshot() {\n+    ModelSnapshot* snapshot = new ModelSnapshot;\n+    snapshot->map_ = map_;\n+    return snapshot;\n+  }\n+\n+  virtual void ReleaseSnapshot(const Snapshot* snapshot) {\n+    delete reinterpret_cast<const ModelSnapshot*>(snapshot);\n+  }\n+  virtual Status Write(const WriteOptions& options, WriteBatch* batch) {\n+    class Handler : public WriteBatch::Handler {\n+     public:\n+      KVMap* map_;\n+      virtual void Put(const Slice& key, const Slice& value) {\n+        (*map_)[key.ToString()] = value.ToString();\n+      }\n+      virtual void Delete(const Slice& key) {\n+        map_->erase(key.ToString());\n+      }\n+    };\n+    Handler handler;\n+    handler.map_ = &map_;\n+    return batch->Iterate(&handler);\n+  }\n+\n+  virtual bool GetProperty(const Slice& property, std::string* value) {\n+    return false;\n+  }\n+  virtual void GetApproximateSizes(const Range* r, int n, uint64_t* sizes) {\n+    for (int i = 0; i < n; i++) {\n+      sizes[i] = 0;\n+    }\n+  }\n+  virtual void CompactRange(const Slice* start, const Slice* end) {\n+  }\n+\n+ private:\n+  class ModelIter: public Iterator {\n+   public:\n+    ModelIter(const KVMap* map, bool owned)\n+        : map_(map), owned_(owned), iter_(map_->end()) {\n+    }\n+    ~ModelIter() {\n+      if (owned_) delete map_;\n+    }\n+    virtual bool Valid() const { return iter_ != map_->end(); }\n+    virtual void SeekToFirst() { iter_ = map_->begin(); }\n+    virtual void SeekToLast() {\n+      if (map_->empty()) {\n+        iter_ = map_->end();\n+      } else {\n+        iter_ = map_->find(map_->rbegin()->first);\n+      }\n+    }\n+    virtual void Seek(const Slice& k) {\n+      iter_ = map_->lower_bound(k.ToString());\n+    }\n+    virtual void Next() { ++iter_; }\n+    virtual void Prev() { --iter_; }\n+    virtual Slice key() const { return iter_->first; }\n+    virtual Slice value() const { return iter_->second; }\n+    virtual Status status() const { return Status::OK(); }\n+   private:\n+    const KVMap* const map_;\n+    const bool owned_;  // Do we own map_\n+    KVMap::const_iterator iter_;\n+  };\n+  const Options options_;\n+  KVMap map_;\n+};\n+\n+static std::string RandomKey(Random* rnd) {\n+  int len = (rnd->OneIn(3)\n+             ? 1                // Short sometimes to encourage collisions\n+             : (rnd->OneIn(100) ? rnd->Skewed(10) : rnd->Uniform(10)));\n+  return test::RandomKey(rnd, len);\n+}\n+\n+static bool CompareIterators(int step,\n+                             DB* model,\n+                             DB* db,\n+                             const Snapshot* model_snap,\n+                             const Snapshot* db_snap) {\n+  ReadOptions options;\n+  options.snapshot = model_snap;\n+  Iterator* miter = model->NewIterator(options);\n+  options.snapshot = db_snap;\n+  Iterator* dbiter = db->NewIterator(options);\n+  bool ok = true;\n+  int count = 0;\n+  for (miter->SeekToFirst(), dbiter->SeekToFirst();\n+       ok && miter->Valid() && dbiter->Valid();\n+       miter->Next(), dbiter->Next()) {\n+    count++;\n+    if (miter->key().compare(dbiter->key()) != 0) {\n+      fprintf(stderr, \"step %d: Key mismatch: '%s' vs. '%s'\\n\",\n+              step,\n+              EscapeString(miter->key()).c_str(),\n+              EscapeString(dbiter->key()).c_str());\n+      ok = false;\n+      break;\n+    }\n+\n+    if (miter->value().compare(dbiter->value()) != 0) {\n+      fprintf(stderr, \"step %d: Value mismatch for key '%s': '%s' vs. '%s'\\n\",\n+              step,\n+              EscapeString(miter->key()).c_str(),\n+              EscapeString(miter->value()).c_str(),\n+              EscapeString(miter->value()).c_str());\n+      ok = false;\n+    }\n+  }\n+\n+  if (ok) {\n+    if (miter->Valid() != dbiter->Valid()) {\n+      fprintf(stderr, \"step %d: Mismatch at end of iterators: %d vs. %d\\n\",\n+              step, miter->Valid(), dbiter->Valid());\n+      ok = false;\n+    }\n+  }\n+  fprintf(stderr, \"%d entries compared: ok=%d\\n\", count, ok);\n+  delete miter;\n+  delete dbiter;\n+  return ok;\n+}\n+\n+TEST(DBTest, Randomized) {\n+  Random rnd(test::RandomSeed());\n+  do {\n+    ModelDB model(CurrentOptions());\n+    const int N = 10000;\n+    const Snapshot* model_snap = NULL;\n+    const Snapshot* db_snap = NULL;\n+    std::string k, v;\n+    for (int step = 0; step < N; step++) {\n+      if (step % 100 == 0) {\n+        fprintf(stderr, \"Step %d of %d\\n\", step, N);\n+      }\n+      // TODO(sanjay): Test Get() works\n+      int p = rnd.Uniform(100);\n+      if (p < 45) {                               // Put\n+        k = RandomKey(&rnd);\n+        v = RandomString(&rnd,\n+                         rnd.OneIn(20)\n+                         ? 100 + rnd.Uniform(100)\n+                         : rnd.Uniform(8));\n+        ASSERT_OK(model.Put(WriteOptions(), k, v));\n+        ASSERT_OK(db_->Put(WriteOptions(), k, v));\n+\n+      } else if (p < 90) {                        // Delete\n+        k = RandomKey(&rnd);\n+        ASSERT_OK(model.Delete(WriteOptions(), k));\n+        ASSERT_OK(db_->Delete(WriteOptions(), k));\n+\n+\n+      } else {                                    // Multi-element batch\n+        WriteBatch b;\n+        const int num = rnd.Uniform(8);\n+        for (int i = 0; i < num; i++) {\n+          if (i == 0 || !rnd.OneIn(10)) {\n+            k = RandomKey(&rnd);\n+          } else {\n+            // Periodically re-use the same key from the previous iter, so\n+            // we have multiple entries in the write batch for the same key\n+          }\n+          if (rnd.OneIn(2)) {\n+            v = RandomString(&rnd, rnd.Uniform(10));\n+            b.Put(k, v);\n+          } else {\n+            b.Delete(k);\n+          }\n+        }\n+        ASSERT_OK(model.Write(WriteOptions(), &b));\n+        ASSERT_OK(db_->Write(WriteOptions(), &b));\n+      }\n+\n+      if ((step % 100) == 0) {\n+        ASSERT_TRUE(CompareIterators(step, &model, db_, NULL, NULL));\n+        ASSERT_TRUE(CompareIterators(step, &model, db_, model_snap, db_snap));\n+        // Save a snapshot from each DB this time that we'll use next\n+        // time we compare things, to make sure the current state is\n+        // preserved with the snapshot\n+        if (model_snap != NULL) model.ReleaseSnapshot(model_snap);\n+        if (db_snap != NULL) db_->ReleaseSnapshot(db_snap);\n+\n+        Reopen();\n+        ASSERT_TRUE(CompareIterators(step, &model, db_, NULL, NULL));\n+\n+        model_snap = model.GetSnapshot();\n+        db_snap = db_->GetSnapshot();\n+      }\n+    }\n+    if (model_snap != NULL) model.ReleaseSnapshot(model_snap);\n+    if (db_snap != NULL) db_->ReleaseSnapshot(db_snap);\n+  } while (ChangeOptions());\n+}\n+\n+std::string MakeKey(unsigned int num) {\n+  char buf[30];\n+  snprintf(buf, sizeof(buf), \"%016u\", num);\n+  return std::string(buf);\n+}\n+\n+void BM_LogAndApply(int iters, int num_base_files) {\n+  std::string dbname = test::TmpDir() + \"/leveldb_test_benchmark\";\n+  DestroyDB(dbname, Options());\n+\n+  DB* db = NULL;\n+  Options opts;\n+  opts.create_if_missing = true;\n+  Status s = DB::Open(opts, dbname, &db);\n+  ASSERT_OK(s);\n+  ASSERT_TRUE(db != NULL);\n+\n+  delete db;\n+  db = NULL;\n+\n+  Env* env = Env::Default();\n+\n+  port::Mutex mu;\n+  MutexLock l(&mu);\n+\n+  InternalKeyComparator cmp(BytewiseComparator());\n+  Options options;\n+  VersionSet vset(dbname, &options, NULL, &cmp);\n+  ASSERT_OK(vset.Recover());\n+  VersionEdit vbase;\n+  uint64_t fnum = 1;\n+  for (int i = 0; i < num_base_files; i++) {\n+    InternalKey start(MakeKey(2*fnum), 1, kTypeValue);\n+    InternalKey limit(MakeKey(2*fnum+1), 1, kTypeDeletion);\n+    vbase.AddFile(2, fnum++, 1 /* file size */, start, limit);\n+  }\n+  ASSERT_OK(vset.LogAndApply(&vbase, &mu));\n+\n+  uint64_t start_micros = env->NowMicros();\n+\n+  for (int i = 0; i < iters; i++) {\n+    VersionEdit vedit;\n+    vedit.DeleteFile(2, fnum);\n+    InternalKey start(MakeKey(2*fnum), 1, kTypeValue);\n+    InternalKey limit(MakeKey(2*fnum+1), 1, kTypeDeletion);\n+    vedit.AddFile(2, fnum++, 1 /* file size */, start, limit);\n+    vset.LogAndApply(&vedit, &mu);\n+  }\n+  uint64_t stop_micros = env->NowMicros();\n+  unsigned int us = stop_micros - start_micros;\n+  char buf[16];\n+  snprintf(buf, sizeof(buf), \"%d\", num_base_files);\n+  fprintf(stderr,\n+          \"BM_LogAndApply/%-6s   %8d iters : %9u us (%7.0f us / iter)\\n\",\n+          buf, iters, us, ((float)us) / iters);\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  if (argc > 1 && std::string(argv[1]) == \"--benchmark\") {\n+    leveldb::BM_LogAndApply(1000, 1);\n+    leveldb::BM_LogAndApply(1000, 100);\n+    leveldb::BM_LogAndApply(1000, 10000);\n+    leveldb::BM_LogAndApply(100, 100000);\n+    return 0;\n+  }\n+\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "20a7ca44626050c01b7bde718581c663d38f42a9",
        "filename": "db/dbformat.cc",
        "status": "added",
        "additions": 140,
        "deletions": 0,
        "changes": 140,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/dbformat.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/dbformat.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/dbformat.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,140 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include <stdio.h>\n+#include \"db/dbformat.h\"\n+#include \"port/port.h\"\n+#include \"util/coding.h\"\n+\n+namespace leveldb {\n+\n+static uint64_t PackSequenceAndType(uint64_t seq, ValueType t) {\n+  assert(seq <= kMaxSequenceNumber);\n+  assert(t <= kValueTypeForSeek);\n+  return (seq << 8) | t;\n+}\n+\n+void AppendInternalKey(std::string* result, const ParsedInternalKey& key) {\n+  result->append(key.user_key.data(), key.user_key.size());\n+  PutFixed64(result, PackSequenceAndType(key.sequence, key.type));\n+}\n+\n+std::string ParsedInternalKey::DebugString() const {\n+  char buf[50];\n+  snprintf(buf, sizeof(buf), \"' @ %llu : %d\",\n+           (unsigned long long) sequence,\n+           int(type));\n+  std::string result = \"'\";\n+  result += EscapeString(user_key.ToString());\n+  result += buf;\n+  return result;\n+}\n+\n+std::string InternalKey::DebugString() const {\n+  std::string result;\n+  ParsedInternalKey parsed;\n+  if (ParseInternalKey(rep_, &parsed)) {\n+    result = parsed.DebugString();\n+  } else {\n+    result = \"(bad)\";\n+    result.append(EscapeString(rep_));\n+  }\n+  return result;\n+}\n+\n+const char* InternalKeyComparator::Name() const {\n+  return \"leveldb.InternalKeyComparator\";\n+}\n+\n+int InternalKeyComparator::Compare(const Slice& akey, const Slice& bkey) const {\n+  // Order by:\n+  //    increasing user key (according to user-supplied comparator)\n+  //    decreasing sequence number\n+  //    decreasing type (though sequence# should be enough to disambiguate)\n+  int r = user_comparator_->Compare(ExtractUserKey(akey), ExtractUserKey(bkey));\n+  if (r == 0) {\n+    const uint64_t anum = DecodeFixed64(akey.data() + akey.size() - 8);\n+    const uint64_t bnum = DecodeFixed64(bkey.data() + bkey.size() - 8);\n+    if (anum > bnum) {\n+      r = -1;\n+    } else if (anum < bnum) {\n+      r = +1;\n+    }\n+  }\n+  return r;\n+}\n+\n+void InternalKeyComparator::FindShortestSeparator(\n+      std::string* start,\n+      const Slice& limit) const {\n+  // Attempt to shorten the user portion of the key\n+  Slice user_start = ExtractUserKey(*start);\n+  Slice user_limit = ExtractUserKey(limit);\n+  std::string tmp(user_start.data(), user_start.size());\n+  user_comparator_->FindShortestSeparator(&tmp, user_limit);\n+  if (tmp.size() < user_start.size() &&\n+      user_comparator_->Compare(user_start, tmp) < 0) {\n+    // User key has become shorter physically, but larger logically.\n+    // Tack on the earliest possible number to the shortened user key.\n+    PutFixed64(&tmp, PackSequenceAndType(kMaxSequenceNumber,kValueTypeForSeek));\n+    assert(this->Compare(*start, tmp) < 0);\n+    assert(this->Compare(tmp, limit) < 0);\n+    start->swap(tmp);\n+  }\n+}\n+\n+void InternalKeyComparator::FindShortSuccessor(std::string* key) const {\n+  Slice user_key = ExtractUserKey(*key);\n+  std::string tmp(user_key.data(), user_key.size());\n+  user_comparator_->FindShortSuccessor(&tmp);\n+  if (tmp.size() < user_key.size() &&\n+      user_comparator_->Compare(user_key, tmp) < 0) {\n+    // User key has become shorter physically, but larger logically.\n+    // Tack on the earliest possible number to the shortened user key.\n+    PutFixed64(&tmp, PackSequenceAndType(kMaxSequenceNumber,kValueTypeForSeek));\n+    assert(this->Compare(*key, tmp) < 0);\n+    key->swap(tmp);\n+  }\n+}\n+\n+const char* InternalFilterPolicy::Name() const {\n+  return user_policy_->Name();\n+}\n+\n+void InternalFilterPolicy::CreateFilter(const Slice* keys, int n,\n+                                        std::string* dst) const {\n+  // We rely on the fact that the code in table.cc does not mind us\n+  // adjusting keys[].\n+  Slice* mkey = const_cast<Slice*>(keys);\n+  for (int i = 0; i < n; i++) {\n+    mkey[i] = ExtractUserKey(keys[i]);\n+    // TODO(sanjay): Suppress dups?\n+  }\n+  user_policy_->CreateFilter(keys, n, dst);\n+}\n+\n+bool InternalFilterPolicy::KeyMayMatch(const Slice& key, const Slice& f) const {\n+  return user_policy_->KeyMayMatch(ExtractUserKey(key), f);\n+}\n+\n+LookupKey::LookupKey(const Slice& user_key, SequenceNumber s) {\n+  size_t usize = user_key.size();\n+  size_t needed = usize + 13;  // A conservative estimate\n+  char* dst;\n+  if (needed <= sizeof(space_)) {\n+    dst = space_;\n+  } else {\n+    dst = new char[needed];\n+  }\n+  start_ = dst;\n+  dst = EncodeVarint32(dst, usize + 8);\n+  kstart_ = dst;\n+  memcpy(dst, user_key.data(), usize);\n+  dst += usize;\n+  EncodeFixed64(dst, PackSequenceAndType(s, kValueTypeForSeek));\n+  dst += 8;\n+  end_ = dst;\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "f7f64dafb6d5b3d676ecde9d0fc86ab2942b2494",
        "filename": "db/dbformat.h",
        "status": "added",
        "additions": 227,
        "deletions": 0,
        "changes": 227,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/dbformat.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/dbformat.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/dbformat.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,227 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_FORMAT_H_\n+#define STORAGE_LEVELDB_DB_FORMAT_H_\n+\n+#include <stdio.h>\n+#include \"leveldb/comparator.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/filter_policy.h\"\n+#include \"leveldb/slice.h\"\n+#include \"leveldb/table_builder.h\"\n+#include \"util/coding.h\"\n+#include \"util/logging.h\"\n+\n+namespace leveldb {\n+\n+// Grouping of constants.  We may want to make some of these\n+// parameters set via options.\n+namespace config {\n+static const int kNumLevels = 7;\n+\n+// Level-0 compaction is started when we hit this many files.\n+static const int kL0_CompactionTrigger = 4;\n+\n+// Soft limit on number of level-0 files.  We slow down writes at this point.\n+static const int kL0_SlowdownWritesTrigger = 8;\n+\n+// Maximum number of level-0 files.  We stop writes at this point.\n+static const int kL0_StopWritesTrigger = 12;\n+\n+// Maximum level to which a new compacted memtable is pushed if it\n+// does not create overlap.  We try to push to level 2 to avoid the\n+// relatively expensive level 0=>1 compactions and to avoid some\n+// expensive manifest file operations.  We do not push all the way to\n+// the largest level since that can generate a lot of wasted disk\n+// space if the same key space is being repeatedly overwritten.\n+static const int kMaxMemCompactLevel = 2;\n+\n+}  // namespace config\n+\n+class InternalKey;\n+\n+// Value types encoded as the last component of internal keys.\n+// DO NOT CHANGE THESE ENUM VALUES: they are embedded in the on-disk\n+// data structures.\n+enum ValueType {\n+  kTypeDeletion = 0x0,\n+  kTypeValue = 0x1\n+};\n+// kValueTypeForSeek defines the ValueType that should be passed when\n+// constructing a ParsedInternalKey object for seeking to a particular\n+// sequence number (since we sort sequence numbers in decreasing order\n+// and the value type is embedded as the low 8 bits in the sequence\n+// number in internal keys, we need to use the highest-numbered\n+// ValueType, not the lowest).\n+static const ValueType kValueTypeForSeek = kTypeValue;\n+\n+typedef uint64_t SequenceNumber;\n+\n+// We leave eight bits empty at the bottom so a type and sequence#\n+// can be packed together into 64-bits.\n+static const SequenceNumber kMaxSequenceNumber =\n+    ((0x1ull << 56) - 1);\n+\n+struct ParsedInternalKey {\n+  Slice user_key;\n+  SequenceNumber sequence;\n+  ValueType type;\n+\n+  ParsedInternalKey() { }  // Intentionally left uninitialized (for speed)\n+  ParsedInternalKey(const Slice& u, const SequenceNumber& seq, ValueType t)\n+      : user_key(u), sequence(seq), type(t) { }\n+  std::string DebugString() const;\n+};\n+\n+// Return the length of the encoding of \"key\".\n+inline size_t InternalKeyEncodingLength(const ParsedInternalKey& key) {\n+  return key.user_key.size() + 8;\n+}\n+\n+// Append the serialization of \"key\" to *result.\n+extern void AppendInternalKey(std::string* result,\n+                              const ParsedInternalKey& key);\n+\n+// Attempt to parse an internal key from \"internal_key\".  On success,\n+// stores the parsed data in \"*result\", and returns true.\n+//\n+// On error, returns false, leaves \"*result\" in an undefined state.\n+extern bool ParseInternalKey(const Slice& internal_key,\n+                             ParsedInternalKey* result);\n+\n+// Returns the user key portion of an internal key.\n+inline Slice ExtractUserKey(const Slice& internal_key) {\n+  assert(internal_key.size() >= 8);\n+  return Slice(internal_key.data(), internal_key.size() - 8);\n+}\n+\n+inline ValueType ExtractValueType(const Slice& internal_key) {\n+  assert(internal_key.size() >= 8);\n+  const size_t n = internal_key.size();\n+  uint64_t num = DecodeFixed64(internal_key.data() + n - 8);\n+  unsigned char c = num & 0xff;\n+  return static_cast<ValueType>(c);\n+}\n+\n+// A comparator for internal keys that uses a specified comparator for\n+// the user key portion and breaks ties by decreasing sequence number.\n+class InternalKeyComparator : public Comparator {\n+ private:\n+  const Comparator* user_comparator_;\n+ public:\n+  explicit InternalKeyComparator(const Comparator* c) : user_comparator_(c) { }\n+  virtual const char* Name() const;\n+  virtual int Compare(const Slice& a, const Slice& b) const;\n+  virtual void FindShortestSeparator(\n+      std::string* start,\n+      const Slice& limit) const;\n+  virtual void FindShortSuccessor(std::string* key) const;\n+\n+  const Comparator* user_comparator() const { return user_comparator_; }\n+\n+  int Compare(const InternalKey& a, const InternalKey& b) const;\n+};\n+\n+// Filter policy wrapper that converts from internal keys to user keys\n+class InternalFilterPolicy : public FilterPolicy {\n+ private:\n+  const FilterPolicy* const user_policy_;\n+ public:\n+  explicit InternalFilterPolicy(const FilterPolicy* p) : user_policy_(p) { }\n+  virtual const char* Name() const;\n+  virtual void CreateFilter(const Slice* keys, int n, std::string* dst) const;\n+  virtual bool KeyMayMatch(const Slice& key, const Slice& filter) const;\n+};\n+\n+// Modules in this directory should keep internal keys wrapped inside\n+// the following class instead of plain strings so that we do not\n+// incorrectly use string comparisons instead of an InternalKeyComparator.\n+class InternalKey {\n+ private:\n+  std::string rep_;\n+ public:\n+  InternalKey() { }   // Leave rep_ as empty to indicate it is invalid\n+  InternalKey(const Slice& user_key, SequenceNumber s, ValueType t) {\n+    AppendInternalKey(&rep_, ParsedInternalKey(user_key, s, t));\n+  }\n+\n+  void DecodeFrom(const Slice& s) { rep_.assign(s.data(), s.size()); }\n+  Slice Encode() const {\n+    assert(!rep_.empty());\n+    return rep_;\n+  }\n+\n+  Slice user_key() const { return ExtractUserKey(rep_); }\n+\n+  void SetFrom(const ParsedInternalKey& p) {\n+    rep_.clear();\n+    AppendInternalKey(&rep_, p);\n+  }\n+\n+  void Clear() { rep_.clear(); }\n+\n+  std::string DebugString() const;\n+};\n+\n+inline int InternalKeyComparator::Compare(\n+    const InternalKey& a, const InternalKey& b) const {\n+  return Compare(a.Encode(), b.Encode());\n+}\n+\n+inline bool ParseInternalKey(const Slice& internal_key,\n+                             ParsedInternalKey* result) {\n+  const size_t n = internal_key.size();\n+  if (n < 8) return false;\n+  uint64_t num = DecodeFixed64(internal_key.data() + n - 8);\n+  unsigned char c = num & 0xff;\n+  result->sequence = num >> 8;\n+  result->type = static_cast<ValueType>(c);\n+  result->user_key = Slice(internal_key.data(), n - 8);\n+  return (c <= static_cast<unsigned char>(kTypeValue));\n+}\n+\n+// A helper class useful for DBImpl::Get()\n+class LookupKey {\n+ public:\n+  // Initialize *this for looking up user_key at a snapshot with\n+  // the specified sequence number.\n+  LookupKey(const Slice& user_key, SequenceNumber sequence);\n+\n+  ~LookupKey();\n+\n+  // Return a key suitable for lookup in a MemTable.\n+  Slice memtable_key() const { return Slice(start_, end_ - start_); }\n+\n+  // Return an internal key (suitable for passing to an internal iterator)\n+  Slice internal_key() const { return Slice(kstart_, end_ - kstart_); }\n+\n+  // Return the user key\n+  Slice user_key() const { return Slice(kstart_, end_ - kstart_ - 8); }\n+\n+ private:\n+  // We construct a char array of the form:\n+  //    klength  varint32               <-- start_\n+  //    userkey  char[klength]          <-- kstart_\n+  //    tag      uint64\n+  //                                    <-- end_\n+  // The array is a suitable MemTable key.\n+  // The suffix starting with \"userkey\" can be used as an InternalKey.\n+  const char* start_;\n+  const char* kstart_;\n+  const char* end_;\n+  char space_[200];      // Avoid allocation for short keys\n+\n+  // No copying allowed\n+  LookupKey(const LookupKey&);\n+  void operator=(const LookupKey&);\n+};\n+\n+inline LookupKey::~LookupKey() {\n+  if (start_ != space_) delete[] start_;\n+}\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_FORMAT_H_"
      },
      {
        "sha": "5d82f5d313fad88ea4e1d079427bba13df667cfe",
        "filename": "db/dbformat_test.cc",
        "status": "added",
        "additions": 112,
        "deletions": 0,
        "changes": 112,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/dbformat_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/dbformat_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/dbformat_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,112 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/dbformat.h\"\n+#include \"util/logging.h\"\n+#include \"util/testharness.h\"\n+\n+namespace leveldb {\n+\n+static std::string IKey(const std::string& user_key,\n+                        uint64_t seq,\n+                        ValueType vt) {\n+  std::string encoded;\n+  AppendInternalKey(&encoded, ParsedInternalKey(user_key, seq, vt));\n+  return encoded;\n+}\n+\n+static std::string Shorten(const std::string& s, const std::string& l) {\n+  std::string result = s;\n+  InternalKeyComparator(BytewiseComparator()).FindShortestSeparator(&result, l);\n+  return result;\n+}\n+\n+static std::string ShortSuccessor(const std::string& s) {\n+  std::string result = s;\n+  InternalKeyComparator(BytewiseComparator()).FindShortSuccessor(&result);\n+  return result;\n+}\n+\n+static void TestKey(const std::string& key,\n+                    uint64_t seq,\n+                    ValueType vt) {\n+  std::string encoded = IKey(key, seq, vt);\n+\n+  Slice in(encoded);\n+  ParsedInternalKey decoded(\"\", 0, kTypeValue);\n+\n+  ASSERT_TRUE(ParseInternalKey(in, &decoded));\n+  ASSERT_EQ(key, decoded.user_key.ToString());\n+  ASSERT_EQ(seq, decoded.sequence);\n+  ASSERT_EQ(vt, decoded.type);\n+\n+  ASSERT_TRUE(!ParseInternalKey(Slice(\"bar\"), &decoded));\n+}\n+\n+class FormatTest { };\n+\n+TEST(FormatTest, InternalKey_EncodeDecode) {\n+  const char* keys[] = { \"\", \"k\", \"hello\", \"longggggggggggggggggggggg\" };\n+  const uint64_t seq[] = {\n+    1, 2, 3,\n+    (1ull << 8) - 1, 1ull << 8, (1ull << 8) + 1,\n+    (1ull << 16) - 1, 1ull << 16, (1ull << 16) + 1,\n+    (1ull << 32) - 1, 1ull << 32, (1ull << 32) + 1\n+  };\n+  for (int k = 0; k < sizeof(keys) / sizeof(keys[0]); k++) {\n+    for (int s = 0; s < sizeof(seq) / sizeof(seq[0]); s++) {\n+      TestKey(keys[k], seq[s], kTypeValue);\n+      TestKey(\"hello\", 1, kTypeDeletion);\n+    }\n+  }\n+}\n+\n+TEST(FormatTest, InternalKeyShortSeparator) {\n+  // When user keys are same\n+  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"foo\", 99, kTypeValue)));\n+  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"foo\", 101, kTypeValue)));\n+  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"foo\", 100, kTypeValue)));\n+  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"foo\", 100, kTypeDeletion)));\n+\n+  // When user keys are misordered\n+  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"bar\", 99, kTypeValue)));\n+\n+  // When user keys are different, but correctly ordered\n+  ASSERT_EQ(IKey(\"g\", kMaxSequenceNumber, kValueTypeForSeek),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"hello\", 200, kTypeValue)));\n+\n+  // When start user key is prefix of limit user key\n+  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"foobar\", 200, kTypeValue)));\n+\n+  // When limit user key is prefix of start user key\n+  ASSERT_EQ(IKey(\"foobar\", 100, kTypeValue),\n+            Shorten(IKey(\"foobar\", 100, kTypeValue),\n+                    IKey(\"foo\", 200, kTypeValue)));\n+}\n+\n+TEST(FormatTest, InternalKeyShortestSuccessor) {\n+  ASSERT_EQ(IKey(\"g\", kMaxSequenceNumber, kValueTypeForSeek),\n+            ShortSuccessor(IKey(\"foo\", 100, kTypeValue)));\n+  ASSERT_EQ(IKey(\"\\xff\\xff\", 100, kTypeValue),\n+            ShortSuccessor(IKey(\"\\xff\\xff\", 100, kTypeValue)));\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "3c4d49f64eb6dfbf9d3740cbfda3f6ad218a4f52",
        "filename": "db/filename.cc",
        "status": "added",
        "additions": 139,
        "deletions": 0,
        "changes": 139,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/filename.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/filename.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/filename.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,139 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include <ctype.h>\n+#include <stdio.h>\n+#include \"db/filename.h\"\n+#include \"db/dbformat.h\"\n+#include \"leveldb/env.h\"\n+#include \"util/logging.h\"\n+\n+namespace leveldb {\n+\n+// A utility routine: write \"data\" to the named file and Sync() it.\n+extern Status WriteStringToFileSync(Env* env, const Slice& data,\n+                                    const std::string& fname);\n+\n+static std::string MakeFileName(const std::string& name, uint64_t number,\n+                                const char* suffix) {\n+  char buf[100];\n+  snprintf(buf, sizeof(buf), \"/%06llu.%s\",\n+           static_cast<unsigned long long>(number),\n+           suffix);\n+  return name + buf;\n+}\n+\n+std::string LogFileName(const std::string& name, uint64_t number) {\n+  assert(number > 0);\n+  return MakeFileName(name, number, \"log\");\n+}\n+\n+std::string TableFileName(const std::string& name, uint64_t number) {\n+  assert(number > 0);\n+  return MakeFileName(name, number, \"sst\");\n+}\n+\n+std::string DescriptorFileName(const std::string& dbname, uint64_t number) {\n+  assert(number > 0);\n+  char buf[100];\n+  snprintf(buf, sizeof(buf), \"/MANIFEST-%06llu\",\n+           static_cast<unsigned long long>(number));\n+  return dbname + buf;\n+}\n+\n+std::string CurrentFileName(const std::string& dbname) {\n+  return dbname + \"/CURRENT\";\n+}\n+\n+std::string LockFileName(const std::string& dbname) {\n+  return dbname + \"/LOCK\";\n+}\n+\n+std::string TempFileName(const std::string& dbname, uint64_t number) {\n+  assert(number > 0);\n+  return MakeFileName(dbname, number, \"dbtmp\");\n+}\n+\n+std::string InfoLogFileName(const std::string& dbname) {\n+  return dbname + \"/LOG\";\n+}\n+\n+// Return the name of the old info log file for \"dbname\".\n+std::string OldInfoLogFileName(const std::string& dbname) {\n+  return dbname + \"/LOG.old\";\n+}\n+\n+\n+// Owned filenames have the form:\n+//    dbname/CURRENT\n+//    dbname/LOCK\n+//    dbname/LOG\n+//    dbname/LOG.old\n+//    dbname/MANIFEST-[0-9]+\n+//    dbname/[0-9]+.(log|sst)\n+bool ParseFileName(const std::string& fname,\n+                   uint64_t* number,\n+                   FileType* type) {\n+  Slice rest(fname);\n+  if (rest == \"CURRENT\") {\n+    *number = 0;\n+    *type = kCurrentFile;\n+  } else if (rest == \"LOCK\") {\n+    *number = 0;\n+    *type = kDBLockFile;\n+  } else if (rest == \"LOG\" || rest == \"LOG.old\") {\n+    *number = 0;\n+    *type = kInfoLogFile;\n+  } else if (rest.starts_with(\"MANIFEST-\")) {\n+    rest.remove_prefix(strlen(\"MANIFEST-\"));\n+    uint64_t num;\n+    if (!ConsumeDecimalNumber(&rest, &num)) {\n+      return false;\n+    }\n+    if (!rest.empty()) {\n+      return false;\n+    }\n+    *type = kDescriptorFile;\n+    *number = num;\n+  } else {\n+    // Avoid strtoull() to keep filename format independent of the\n+    // current locale\n+    uint64_t num;\n+    if (!ConsumeDecimalNumber(&rest, &num)) {\n+      return false;\n+    }\n+    Slice suffix = rest;\n+    if (suffix == Slice(\".log\")) {\n+      *type = kLogFile;\n+    } else if (suffix == Slice(\".sst\")) {\n+      *type = kTableFile;\n+    } else if (suffix == Slice(\".dbtmp\")) {\n+      *type = kTempFile;\n+    } else {\n+      return false;\n+    }\n+    *number = num;\n+  }\n+  return true;\n+}\n+\n+Status SetCurrentFile(Env* env, const std::string& dbname,\n+                      uint64_t descriptor_number) {\n+  // Remove leading \"dbname/\" and add newline to manifest file name\n+  std::string manifest = DescriptorFileName(dbname, descriptor_number);\n+  Slice contents = manifest;\n+  assert(contents.starts_with(dbname + \"/\"));\n+  contents.remove_prefix(dbname.size() + 1);\n+  std::string tmp = TempFileName(dbname, descriptor_number);\n+  Status s = WriteStringToFileSync(env, contents.ToString() + \"\\n\", tmp);\n+  if (s.ok()) {\n+    s = env->RenameFile(tmp, CurrentFileName(dbname));\n+  }\n+  if (!s.ok()) {\n+    env->DeleteFile(tmp);\n+  }\n+  return s;\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "d5d09b11468105324761761665bb8db72abf9540",
        "filename": "db/filename.h",
        "status": "added",
        "additions": 80,
        "deletions": 0,
        "changes": 80,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/filename.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/filename.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/filename.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,80 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// File names used by DB code\n+\n+#ifndef STORAGE_LEVELDB_DB_FILENAME_H_\n+#define STORAGE_LEVELDB_DB_FILENAME_H_\n+\n+#include <stdint.h>\n+#include <string>\n+#include \"leveldb/slice.h\"\n+#include \"leveldb/status.h\"\n+#include \"port/port.h\"\n+\n+namespace leveldb {\n+\n+class Env;\n+\n+enum FileType {\n+  kLogFile,\n+  kDBLockFile,\n+  kTableFile,\n+  kDescriptorFile,\n+  kCurrentFile,\n+  kTempFile,\n+  kInfoLogFile  // Either the current one, or an old one\n+};\n+\n+// Return the name of the log file with the specified number\n+// in the db named by \"dbname\".  The result will be prefixed with\n+// \"dbname\".\n+extern std::string LogFileName(const std::string& dbname, uint64_t number);\n+\n+// Return the name of the sstable with the specified number\n+// in the db named by \"dbname\".  The result will be prefixed with\n+// \"dbname\".\n+extern std::string TableFileName(const std::string& dbname, uint64_t number);\n+\n+// Return the name of the descriptor file for the db named by\n+// \"dbname\" and the specified incarnation number.  The result will be\n+// prefixed with \"dbname\".\n+extern std::string DescriptorFileName(const std::string& dbname,\n+                                      uint64_t number);\n+\n+// Return the name of the current file.  This file contains the name\n+// of the current manifest file.  The result will be prefixed with\n+// \"dbname\".\n+extern std::string CurrentFileName(const std::string& dbname);\n+\n+// Return the name of the lock file for the db named by\n+// \"dbname\".  The result will be prefixed with \"dbname\".\n+extern std::string LockFileName(const std::string& dbname);\n+\n+// Return the name of a temporary file owned by the db named \"dbname\".\n+// The result will be prefixed with \"dbname\".\n+extern std::string TempFileName(const std::string& dbname, uint64_t number);\n+\n+// Return the name of the info log file for \"dbname\".\n+extern std::string InfoLogFileName(const std::string& dbname);\n+\n+// Return the name of the old info log file for \"dbname\".\n+extern std::string OldInfoLogFileName(const std::string& dbname);\n+\n+// If filename is a leveldb file, store the type of the file in *type.\n+// The number encoded in the filename is stored in *number.  If the\n+// filename was successfully parsed, returns true.  Else return false.\n+extern bool ParseFileName(const std::string& filename,\n+                          uint64_t* number,\n+                          FileType* type);\n+\n+// Make the CURRENT file point to the descriptor file with the\n+// specified number.\n+extern Status SetCurrentFile(Env* env, const std::string& dbname,\n+                             uint64_t descriptor_number);\n+\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_FILENAME_H_"
      },
      {
        "sha": "5a26da4728f6a0934f12d37d84e845ef6a00e8af",
        "filename": "db/filename_test.cc",
        "status": "added",
        "additions": 122,
        "deletions": 0,
        "changes": 122,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/filename_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/filename_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/filename_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,122 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/filename.h\"\n+\n+#include \"db/dbformat.h\"\n+#include \"port/port.h\"\n+#include \"util/logging.h\"\n+#include \"util/testharness.h\"\n+\n+namespace leveldb {\n+\n+class FileNameTest { };\n+\n+TEST(FileNameTest, Parse) {\n+  Slice db;\n+  FileType type;\n+  uint64_t number;\n+\n+  // Successful parses\n+  static struct {\n+    const char* fname;\n+    uint64_t number;\n+    FileType type;\n+  } cases[] = {\n+    { \"100.log\",            100,   kLogFile },\n+    { \"0.log\",              0,     kLogFile },\n+    { \"0.sst\",              0,     kTableFile },\n+    { \"CURRENT\",            0,     kCurrentFile },\n+    { \"LOCK\",               0,     kDBLockFile },\n+    { \"MANIFEST-2\",         2,     kDescriptorFile },\n+    { \"MANIFEST-7\",         7,     kDescriptorFile },\n+    { \"LOG\",                0,     kInfoLogFile },\n+    { \"LOG.old\",            0,     kInfoLogFile },\n+    { \"18446744073709551615.log\", 18446744073709551615ull, kLogFile },\n+  };\n+  for (int i = 0; i < sizeof(cases) / sizeof(cases[0]); i++) {\n+    std::string f = cases[i].fname;\n+    ASSERT_TRUE(ParseFileName(f, &number, &type)) << f;\n+    ASSERT_EQ(cases[i].type, type) << f;\n+    ASSERT_EQ(cases[i].number, number) << f;\n+  }\n+\n+  // Errors\n+  static const char* errors[] = {\n+    \"\",\n+    \"foo\",\n+    \"foo-dx-100.log\",\n+    \".log\",\n+    \"\",\n+    \"manifest\",\n+    \"CURREN\",\n+    \"CURRENTX\",\n+    \"MANIFES\",\n+    \"MANIFEST\",\n+    \"MANIFEST-\",\n+    \"XMANIFEST-3\",\n+    \"MANIFEST-3x\",\n+    \"LOC\",\n+    \"LOCKx\",\n+    \"LO\",\n+    \"LOGx\",\n+    \"18446744073709551616.log\",\n+    \"184467440737095516150.log\",\n+    \"100\",\n+    \"100.\",\n+    \"100.lop\"\n+  };\n+  for (int i = 0; i < sizeof(errors) / sizeof(errors[0]); i++) {\n+    std::string f = errors[i];\n+    ASSERT_TRUE(!ParseFileName(f, &number, &type)) << f;\n+  }\n+}\n+\n+TEST(FileNameTest, Construction) {\n+  uint64_t number;\n+  FileType type;\n+  std::string fname;\n+\n+  fname = CurrentFileName(\"foo\");\n+  ASSERT_EQ(\"foo/\", std::string(fname.data(), 4));\n+  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n+  ASSERT_EQ(0, number);\n+  ASSERT_EQ(kCurrentFile, type);\n+\n+  fname = LockFileName(\"foo\");\n+  ASSERT_EQ(\"foo/\", std::string(fname.data(), 4));\n+  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n+  ASSERT_EQ(0, number);\n+  ASSERT_EQ(kDBLockFile, type);\n+\n+  fname = LogFileName(\"foo\", 192);\n+  ASSERT_EQ(\"foo/\", std::string(fname.data(), 4));\n+  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n+  ASSERT_EQ(192, number);\n+  ASSERT_EQ(kLogFile, type);\n+\n+  fname = TableFileName(\"bar\", 200);\n+  ASSERT_EQ(\"bar/\", std::string(fname.data(), 4));\n+  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n+  ASSERT_EQ(200, number);\n+  ASSERT_EQ(kTableFile, type);\n+\n+  fname = DescriptorFileName(\"bar\", 100);\n+  ASSERT_EQ(\"bar/\", std::string(fname.data(), 4));\n+  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n+  ASSERT_EQ(100, number);\n+  ASSERT_EQ(kDescriptorFile, type);\n+\n+  fname = TempFileName(\"tmp\", 999);\n+  ASSERT_EQ(\"tmp/\", std::string(fname.data(), 4));\n+  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n+  ASSERT_EQ(999, number);\n+  ASSERT_EQ(kTempFile, type);\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "995d76107a16efd9677ec6d2c125c04502fcec3d",
        "filename": "db/leveldb_main.cc",
        "status": "added",
        "additions": 238,
        "deletions": 0,
        "changes": 238,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/leveldb_main.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/leveldb_main.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/leveldb_main.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,238 @@\n+// Copyright (c) 2012 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include <stdio.h>\n+#include \"db/dbformat.h\"\n+#include \"db/filename.h\"\n+#include \"db/log_reader.h\"\n+#include \"db/version_edit.h\"\n+#include \"db/write_batch_internal.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/iterator.h\"\n+#include \"leveldb/options.h\"\n+#include \"leveldb/status.h\"\n+#include \"leveldb/table.h\"\n+#include \"leveldb/write_batch.h\"\n+#include \"util/logging.h\"\n+\n+namespace leveldb {\n+\n+namespace {\n+\n+bool GuessType(const std::string& fname, FileType* type) {\n+  size_t pos = fname.rfind('/');\n+  std::string basename;\n+  if (pos == std::string::npos) {\n+    basename = fname;\n+  } else {\n+    basename = std::string(fname.data() + pos + 1, fname.size() - pos - 1);\n+  }\n+  uint64_t ignored;\n+  return ParseFileName(basename, &ignored, type);\n+}\n+\n+// Notified when log reader encounters corruption.\n+class CorruptionReporter : public log::Reader::Reporter {\n+ public:\n+  virtual void Corruption(size_t bytes, const Status& status) {\n+    printf(\"corruption: %d bytes; %s\\n\",\n+            static_cast<int>(bytes),\n+            status.ToString().c_str());\n+  }\n+};\n+\n+// Print contents of a log file. (*func)() is called on every record.\n+bool PrintLogContents(Env* env, const std::string& fname,\n+                      void (*func)(Slice)) {\n+  SequentialFile* file;\n+  Status s = env->NewSequentialFile(fname, &file);\n+  if (!s.ok()) {\n+    fprintf(stderr, \"%s\\n\", s.ToString().c_str());\n+    return false;\n+  }\n+  CorruptionReporter reporter;\n+  log::Reader reader(file, &reporter, true, 0);\n+  Slice record;\n+  std::string scratch;\n+  while (reader.ReadRecord(&record, &scratch)) {\n+    printf(\"--- offset %llu; \",\n+           static_cast<unsigned long long>(reader.LastRecordOffset()));\n+    (*func)(record);\n+  }\n+  delete file;\n+  return true;\n+}\n+\n+// Called on every item found in a WriteBatch.\n+class WriteBatchItemPrinter : public WriteBatch::Handler {\n+ public:\n+  uint64_t offset_;\n+  uint64_t sequence_;\n+\n+  virtual void Put(const Slice& key, const Slice& value) {\n+    printf(\"  put '%s' '%s'\\n\",\n+           EscapeString(key).c_str(),\n+           EscapeString(value).c_str());\n+  }\n+  virtual void Delete(const Slice& key) {\n+    printf(\"  del '%s'\\n\",\n+           EscapeString(key).c_str());\n+  }\n+};\n+\n+\n+// Called on every log record (each one of which is a WriteBatch)\n+// found in a kLogFile.\n+static void WriteBatchPrinter(Slice record) {\n+  if (record.size() < 12) {\n+    printf(\"log record length %d is too small\\n\",\n+           static_cast<int>(record.size()));\n+    return;\n+  }\n+  WriteBatch batch;\n+  WriteBatchInternal::SetContents(&batch, record);\n+  printf(\"sequence %llu\\n\",\n+         static_cast<unsigned long long>(WriteBatchInternal::Sequence(&batch)));\n+  WriteBatchItemPrinter batch_item_printer;\n+  Status s = batch.Iterate(&batch_item_printer);\n+  if (!s.ok()) {\n+    printf(\"  error: %s\\n\", s.ToString().c_str());\n+  }\n+}\n+\n+bool DumpLog(Env* env, const std::string& fname) {\n+  return PrintLogContents(env, fname, WriteBatchPrinter);\n+}\n+\n+// Called on every log record (each one of which is a WriteBatch)\n+// found in a kDescriptorFile.\n+static void VersionEditPrinter(Slice record) {\n+  VersionEdit edit;\n+  Status s = edit.DecodeFrom(record);\n+  if (!s.ok()) {\n+    printf(\"%s\\n\", s.ToString().c_str());\n+    return;\n+  }\n+  printf(\"%s\", edit.DebugString().c_str());\n+}\n+\n+bool DumpDescriptor(Env* env, const std::string& fname) {\n+  return PrintLogContents(env, fname, VersionEditPrinter);\n+}\n+\n+bool DumpTable(Env* env, const std::string& fname) {\n+  uint64_t file_size;\n+  RandomAccessFile* file = NULL;\n+  Table* table = NULL;\n+  Status s = env->GetFileSize(fname, &file_size);\n+  if (s.ok()) {\n+    s = env->NewRandomAccessFile(fname, &file);\n+  }\n+  if (s.ok()) {\n+    // We use the default comparator, which may or may not match the\n+    // comparator used in this database. However this should not cause\n+    // problems since we only use Table operations that do not require\n+    // any comparisons.  In particular, we do not call Seek or Prev.\n+    s = Table::Open(Options(), file, file_size, &table);\n+  }\n+  if (!s.ok()) {\n+    fprintf(stderr, \"%s\\n\", s.ToString().c_str());\n+    delete table;\n+    delete file;\n+    return false;\n+  }\n+\n+  ReadOptions ro;\n+  ro.fill_cache = false;\n+  Iterator* iter = table->NewIterator(ro);\n+  for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+    ParsedInternalKey key;\n+    if (!ParseInternalKey(iter->key(), &key)) {\n+      printf(\"badkey '%s' => '%s'\\n\",\n+             EscapeString(iter->key()).c_str(),\n+             EscapeString(iter->value()).c_str());\n+    } else {\n+      char kbuf[20];\n+      const char* type;\n+      if (key.type == kTypeDeletion) {\n+        type = \"del\";\n+      } else if (key.type == kTypeValue) {\n+        type = \"val\";\n+      } else {\n+        snprintf(kbuf, sizeof(kbuf), \"%d\", static_cast<int>(key.type));\n+        type = kbuf;\n+      }\n+      printf(\"'%s' @ %8llu : %s => '%s'\\n\",\n+             EscapeString(key.user_key).c_str(),\n+             static_cast<unsigned long long>(key.sequence),\n+             type,\n+             EscapeString(iter->value()).c_str());\n+    }\n+  }\n+  s = iter->status();\n+  if (!s.ok()) {\n+    printf(\"iterator error: %s\\n\", s.ToString().c_str());\n+  }\n+\n+  delete iter;\n+  delete table;\n+  delete file;\n+  return true;\n+}\n+\n+bool DumpFile(Env* env, const std::string& fname) {\n+  FileType ftype;\n+  if (!GuessType(fname, &ftype)) {\n+    fprintf(stderr, \"%s: unknown file type\\n\", fname.c_str());\n+    return false;\n+  }\n+  switch (ftype) {\n+    case kLogFile:         return DumpLog(env, fname);\n+    case kDescriptorFile:  return DumpDescriptor(env, fname);\n+    case kTableFile:       return DumpTable(env, fname);\n+\n+    default: {\n+      fprintf(stderr, \"%s: not a dump-able file type\\n\", fname.c_str());\n+      break;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool HandleDumpCommand(Env* env, char** files, int num) {\n+  bool ok = true;\n+  for (int i = 0; i < num; i++) {\n+    ok &= DumpFile(env, files[i]);\n+  }\n+  return ok;\n+}\n+\n+}\n+}  // namespace leveldb\n+\n+static void Usage() {\n+  fprintf(\n+      stderr,\n+      \"Usage: leveldbutil command...\\n\"\n+      \"   dump files...         -- dump contents of specified files\\n\"\n+      );\n+}\n+\n+int main(int argc, char** argv) {\n+  leveldb::Env* env = leveldb::Env::Default();\n+  bool ok = true;\n+  if (argc < 2) {\n+    Usage();\n+    ok = false;\n+  } else {\n+    std::string command = argv[1];\n+    if (command == \"dump\") {\n+      ok = leveldb::HandleDumpCommand(env, argv+2, argc-2);\n+    } else {\n+      Usage();\n+      ok = false;\n+    }\n+  }\n+  return (ok ? 0 : 1);\n+}"
      },
      {
        "sha": "2690cb9789ee63e85260500a6736ec302d22f9b7",
        "filename": "db/log_format.h",
        "status": "added",
        "additions": 35,
        "deletions": 0,
        "changes": 35,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/log_format.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/log_format.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/log_format.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,35 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// Log format information shared by reader and writer.\n+// See ../doc/log_format.txt for more detail.\n+\n+#ifndef STORAGE_LEVELDB_DB_LOG_FORMAT_H_\n+#define STORAGE_LEVELDB_DB_LOG_FORMAT_H_\n+\n+namespace leveldb {\n+namespace log {\n+\n+enum RecordType {\n+  // Zero is reserved for preallocated files\n+  kZeroType = 0,\n+\n+  kFullType = 1,\n+\n+  // For fragments\n+  kFirstType = 2,\n+  kMiddleType = 3,\n+  kLastType = 4\n+};\n+static const int kMaxRecordType = kLastType;\n+\n+static const int kBlockSize = 32768;\n+\n+// Header is checksum (4 bytes), type (1 byte), length (2 bytes).\n+static const int kHeaderSize = 4 + 1 + 2;\n+\n+}  // namespace log\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_LOG_FORMAT_H_"
      },
      {
        "sha": "b35f115aadac28055b16fce6f133bd5148ecec16",
        "filename": "db/log_reader.cc",
        "status": "added",
        "additions": 259,
        "deletions": 0,
        "changes": 259,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/log_reader.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/log_reader.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/log_reader.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,259 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/log_reader.h\"\n+\n+#include <stdio.h>\n+#include \"leveldb/env.h\"\n+#include \"util/coding.h\"\n+#include \"util/crc32c.h\"\n+\n+namespace leveldb {\n+namespace log {\n+\n+Reader::Reporter::~Reporter() {\n+}\n+\n+Reader::Reader(SequentialFile* file, Reporter* reporter, bool checksum,\n+               uint64_t initial_offset)\n+    : file_(file),\n+      reporter_(reporter),\n+      checksum_(checksum),\n+      backing_store_(new char[kBlockSize]),\n+      buffer_(),\n+      eof_(false),\n+      last_record_offset_(0),\n+      end_of_buffer_offset_(0),\n+      initial_offset_(initial_offset) {\n+}\n+\n+Reader::~Reader() {\n+  delete[] backing_store_;\n+}\n+\n+bool Reader::SkipToInitialBlock() {\n+  size_t offset_in_block = initial_offset_ % kBlockSize;\n+  uint64_t block_start_location = initial_offset_ - offset_in_block;\n+\n+  // Don't search a block if we'd be in the trailer\n+  if (offset_in_block > kBlockSize - 6) {\n+    offset_in_block = 0;\n+    block_start_location += kBlockSize;\n+  }\n+\n+  end_of_buffer_offset_ = block_start_location;\n+\n+  // Skip to start of first block that can contain the initial record\n+  if (block_start_location > 0) {\n+    Status skip_status = file_->Skip(block_start_location);\n+    if (!skip_status.ok()) {\n+      ReportDrop(block_start_location, skip_status);\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+bool Reader::ReadRecord(Slice* record, std::string* scratch) {\n+  if (last_record_offset_ < initial_offset_) {\n+    if (!SkipToInitialBlock()) {\n+      return false;\n+    }\n+  }\n+\n+  scratch->clear();\n+  record->clear();\n+  bool in_fragmented_record = false;\n+  // Record offset of the logical record that we're reading\n+  // 0 is a dummy value to make compilers happy\n+  uint64_t prospective_record_offset = 0;\n+\n+  Slice fragment;\n+  while (true) {\n+    uint64_t physical_record_offset = end_of_buffer_offset_ - buffer_.size();\n+    const unsigned int record_type = ReadPhysicalRecord(&fragment);\n+    switch (record_type) {\n+      case kFullType:\n+        if (in_fragmented_record) {\n+          // Handle bug in earlier versions of log::Writer where\n+          // it could emit an empty kFirstType record at the tail end\n+          // of a block followed by a kFullType or kFirstType record\n+          // at the beginning of the next block.\n+          if (scratch->empty()) {\n+            in_fragmented_record = false;\n+          } else {\n+            ReportCorruption(scratch->size(), \"partial record without end(1)\");\n+          }\n+        }\n+        prospective_record_offset = physical_record_offset;\n+        scratch->clear();\n+        *record = fragment;\n+        last_record_offset_ = prospective_record_offset;\n+        return true;\n+\n+      case kFirstType:\n+        if (in_fragmented_record) {\n+          // Handle bug in earlier versions of log::Writer where\n+          // it could emit an empty kFirstType record at the tail end\n+          // of a block followed by a kFullType or kFirstType record\n+          // at the beginning of the next block.\n+          if (scratch->empty()) {\n+            in_fragmented_record = false;\n+          } else {\n+            ReportCorruption(scratch->size(), \"partial record without end(2)\");\n+          }\n+        }\n+        prospective_record_offset = physical_record_offset;\n+        scratch->assign(fragment.data(), fragment.size());\n+        in_fragmented_record = true;\n+        break;\n+\n+      case kMiddleType:\n+        if (!in_fragmented_record) {\n+          ReportCorruption(fragment.size(),\n+                           \"missing start of fragmented record(1)\");\n+        } else {\n+          scratch->append(fragment.data(), fragment.size());\n+        }\n+        break;\n+\n+      case kLastType:\n+        if (!in_fragmented_record) {\n+          ReportCorruption(fragment.size(),\n+                           \"missing start of fragmented record(2)\");\n+        } else {\n+          scratch->append(fragment.data(), fragment.size());\n+          *record = Slice(*scratch);\n+          last_record_offset_ = prospective_record_offset;\n+          return true;\n+        }\n+        break;\n+\n+      case kEof:\n+        if (in_fragmented_record) {\n+          ReportCorruption(scratch->size(), \"partial record without end(3)\");\n+          scratch->clear();\n+        }\n+        return false;\n+\n+      case kBadRecord:\n+        if (in_fragmented_record) {\n+          ReportCorruption(scratch->size(), \"error in middle of record\");\n+          in_fragmented_record = false;\n+          scratch->clear();\n+        }\n+        break;\n+\n+      default: {\n+        char buf[40];\n+        snprintf(buf, sizeof(buf), \"unknown record type %u\", record_type);\n+        ReportCorruption(\n+            (fragment.size() + (in_fragmented_record ? scratch->size() : 0)),\n+            buf);\n+        in_fragmented_record = false;\n+        scratch->clear();\n+        break;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+uint64_t Reader::LastRecordOffset() {\n+  return last_record_offset_;\n+}\n+\n+void Reader::ReportCorruption(size_t bytes, const char* reason) {\n+  ReportDrop(bytes, Status::Corruption(reason));\n+}\n+\n+void Reader::ReportDrop(size_t bytes, const Status& reason) {\n+  if (reporter_ != NULL &&\n+      end_of_buffer_offset_ - buffer_.size() - bytes >= initial_offset_) {\n+    reporter_->Corruption(bytes, reason);\n+  }\n+}\n+\n+unsigned int Reader::ReadPhysicalRecord(Slice* result) {\n+  while (true) {\n+    if (buffer_.size() < kHeaderSize) {\n+      if (!eof_) {\n+        // Last read was a full read, so this is a trailer to skip\n+        buffer_.clear();\n+        Status status = file_->Read(kBlockSize, &buffer_, backing_store_);\n+        end_of_buffer_offset_ += buffer_.size();\n+        if (!status.ok()) {\n+          buffer_.clear();\n+          ReportDrop(kBlockSize, status);\n+          eof_ = true;\n+          return kEof;\n+        } else if (buffer_.size() < kBlockSize) {\n+          eof_ = true;\n+        }\n+        continue;\n+      } else if (buffer_.size() == 0) {\n+        // End of file\n+        return kEof;\n+      } else {\n+        size_t drop_size = buffer_.size();\n+        buffer_.clear();\n+        ReportCorruption(drop_size, \"truncated record at end of file\");\n+        return kEof;\n+      }\n+    }\n+\n+    // Parse the header\n+    const char* header = buffer_.data();\n+    const uint32_t a = static_cast<uint32_t>(header[4]) & 0xff;\n+    const uint32_t b = static_cast<uint32_t>(header[5]) & 0xff;\n+    const unsigned int type = header[6];\n+    const uint32_t length = a | (b << 8);\n+    if (kHeaderSize + length > buffer_.size()) {\n+      size_t drop_size = buffer_.size();\n+      buffer_.clear();\n+      ReportCorruption(drop_size, \"bad record length\");\n+      return kBadRecord;\n+    }\n+\n+    if (type == kZeroType && length == 0) {\n+      // Skip zero length record without reporting any drops since\n+      // such records are produced by the mmap based writing code in\n+      // env_posix.cc that preallocates file regions.\n+      buffer_.clear();\n+      return kBadRecord;\n+    }\n+\n+    // Check crc\n+    if (checksum_) {\n+      uint32_t expected_crc = crc32c::Unmask(DecodeFixed32(header));\n+      uint32_t actual_crc = crc32c::Value(header + 6, 1 + length);\n+      if (actual_crc != expected_crc) {\n+        // Drop the rest of the buffer since \"length\" itself may have\n+        // been corrupted and if we trust it, we could find some\n+        // fragment of a real log record that just happens to look\n+        // like a valid log record.\n+        size_t drop_size = buffer_.size();\n+        buffer_.clear();\n+        ReportCorruption(drop_size, \"checksum mismatch\");\n+        return kBadRecord;\n+      }\n+    }\n+\n+    buffer_.remove_prefix(kHeaderSize + length);\n+\n+    // Skip physical record that started before initial_offset_\n+    if (end_of_buffer_offset_ - buffer_.size() - kHeaderSize - length <\n+        initial_offset_) {\n+      result->clear();\n+      return kBadRecord;\n+    }\n+\n+    *result = Slice(header + kHeaderSize, length);\n+    return type;\n+  }\n+}\n+\n+}  // namespace log\n+}  // namespace leveldb"
      },
      {
        "sha": "82d4bee68d0eea3a7fdd270e91b49545a536f45e",
        "filename": "db/log_reader.h",
        "status": "added",
        "additions": 108,
        "deletions": 0,
        "changes": 108,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/log_reader.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/log_reader.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/log_reader.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,108 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_LOG_READER_H_\n+#define STORAGE_LEVELDB_DB_LOG_READER_H_\n+\n+#include <stdint.h>\n+\n+#include \"db/log_format.h\"\n+#include \"leveldb/slice.h\"\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+class SequentialFile;\n+\n+namespace log {\n+\n+class Reader {\n+ public:\n+  // Interface for reporting errors.\n+  class Reporter {\n+   public:\n+    virtual ~Reporter();\n+\n+    // Some corruption was detected.  \"size\" is the approximate number\n+    // of bytes dropped due to the corruption.\n+    virtual void Corruption(size_t bytes, const Status& status) = 0;\n+  };\n+\n+  // Create a reader that will return log records from \"*file\".\n+  // \"*file\" must remain live while this Reader is in use.\n+  //\n+  // If \"reporter\" is non-NULL, it is notified whenever some data is\n+  // dropped due to a detected corruption.  \"*reporter\" must remain\n+  // live while this Reader is in use.\n+  //\n+  // If \"checksum\" is true, verify checksums if available.\n+  //\n+  // The Reader will start reading at the first record located at physical\n+  // position >= initial_offset within the file.\n+  Reader(SequentialFile* file, Reporter* reporter, bool checksum,\n+         uint64_t initial_offset);\n+\n+  ~Reader();\n+\n+  // Read the next record into *record.  Returns true if read\n+  // successfully, false if we hit end of the input.  May use\n+  // \"*scratch\" as temporary storage.  The contents filled in *record\n+  // will only be valid until the next mutating operation on this\n+  // reader or the next mutation to *scratch.\n+  bool ReadRecord(Slice* record, std::string* scratch);\n+\n+  // Returns the physical offset of the last record returned by ReadRecord.\n+  //\n+  // Undefined before the first call to ReadRecord.\n+  uint64_t LastRecordOffset();\n+\n+ private:\n+  SequentialFile* const file_;\n+  Reporter* const reporter_;\n+  bool const checksum_;\n+  char* const backing_store_;\n+  Slice buffer_;\n+  bool eof_;   // Last Read() indicated EOF by returning < kBlockSize\n+\n+  // Offset of the last record returned by ReadRecord.\n+  uint64_t last_record_offset_;\n+  // Offset of the first location past the end of buffer_.\n+  uint64_t end_of_buffer_offset_;\n+\n+  // Offset at which to start looking for the first record to return\n+  uint64_t const initial_offset_;\n+\n+  // Extend record types with the following special values\n+  enum {\n+    kEof = kMaxRecordType + 1,\n+    // Returned whenever we find an invalid physical record.\n+    // Currently there are three situations in which this happens:\n+    // * The record has an invalid CRC (ReadPhysicalRecord reports a drop)\n+    // * The record is a 0-length record (No drop is reported)\n+    // * The record is below constructor's initial_offset (No drop is reported)\n+    kBadRecord = kMaxRecordType + 2\n+  };\n+\n+  // Skips all blocks that are completely before \"initial_offset_\".\n+  //\n+  // Returns true on success. Handles reporting.\n+  bool SkipToInitialBlock();\n+\n+  // Return type, or one of the preceding special values\n+  unsigned int ReadPhysicalRecord(Slice* result);\n+\n+  // Reports dropped bytes to the reporter.\n+  // buffer_ must be updated to remove the dropped bytes prior to invocation.\n+  void ReportCorruption(size_t bytes, const char* reason);\n+  void ReportDrop(size_t bytes, const Status& reason);\n+\n+  // No copying allowed\n+  Reader(const Reader&);\n+  void operator=(const Reader&);\n+};\n+\n+}  // namespace log\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_LOG_READER_H_"
      },
      {
        "sha": "4c5cf875733c16175743224e642f0507a0da663f",
        "filename": "db/log_test.cc",
        "status": "added",
        "additions": 500,
        "deletions": 0,
        "changes": 500,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/log_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/log_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/log_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,500 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/log_reader.h\"\n+#include \"db/log_writer.h\"\n+#include \"leveldb/env.h\"\n+#include \"util/coding.h\"\n+#include \"util/crc32c.h\"\n+#include \"util/random.h\"\n+#include \"util/testharness.h\"\n+\n+namespace leveldb {\n+namespace log {\n+\n+// Construct a string of the specified length made out of the supplied\n+// partial string.\n+static std::string BigString(const std::string& partial_string, size_t n) {\n+  std::string result;\n+  while (result.size() < n) {\n+    result.append(partial_string);\n+  }\n+  result.resize(n);\n+  return result;\n+}\n+\n+// Construct a string from a number\n+static std::string NumberString(int n) {\n+  char buf[50];\n+  snprintf(buf, sizeof(buf), \"%d.\", n);\n+  return std::string(buf);\n+}\n+\n+// Return a skewed potentially long string\n+static std::string RandomSkewedString(int i, Random* rnd) {\n+  return BigString(NumberString(i), rnd->Skewed(17));\n+}\n+\n+class LogTest {\n+ private:\n+  class StringDest : public WritableFile {\n+   public:\n+    std::string contents_;\n+\n+    virtual Status Close() { return Status::OK(); }\n+    virtual Status Flush() { return Status::OK(); }\n+    virtual Status Sync() { return Status::OK(); }\n+    virtual Status Append(const Slice& slice) {\n+      contents_.append(slice.data(), slice.size());\n+      return Status::OK();\n+    }\n+  };\n+\n+  class StringSource : public SequentialFile {\n+   public:\n+    Slice contents_;\n+    bool force_error_;\n+    bool returned_partial_;\n+    StringSource() : force_error_(false), returned_partial_(false) { }\n+\n+    virtual Status Read(size_t n, Slice* result, char* scratch) {\n+      ASSERT_TRUE(!returned_partial_) << \"must not Read() after eof/error\";\n+\n+      if (force_error_) {\n+        force_error_ = false;\n+        returned_partial_ = true;\n+        return Status::Corruption(\"read error\");\n+      }\n+\n+      if (contents_.size() < n) {\n+        n = contents_.size();\n+        returned_partial_ = true;\n+      }\n+      *result = Slice(contents_.data(), n);\n+      contents_.remove_prefix(n);\n+      return Status::OK();\n+    }\n+\n+    virtual Status Skip(uint64_t n) {\n+      if (n > contents_.size()) {\n+        contents_.clear();\n+        return Status::NotFound(\"in-memory file skipepd past end\");\n+      }\n+\n+      contents_.remove_prefix(n);\n+\n+      return Status::OK();\n+    }\n+  };\n+\n+  class ReportCollector : public Reader::Reporter {\n+   public:\n+    size_t dropped_bytes_;\n+    std::string message_;\n+\n+    ReportCollector() : dropped_bytes_(0) { }\n+    virtual void Corruption(size_t bytes, const Status& status) {\n+      dropped_bytes_ += bytes;\n+      message_.append(status.ToString());\n+    }\n+  };\n+\n+  StringDest dest_;\n+  StringSource source_;\n+  ReportCollector report_;\n+  bool reading_;\n+  Writer writer_;\n+  Reader reader_;\n+\n+  // Record metadata for testing initial offset functionality\n+  static size_t initial_offset_record_sizes_[];\n+  static uint64_t initial_offset_last_record_offsets_[];\n+\n+ public:\n+  LogTest() : reading_(false),\n+              writer_(&dest_),\n+              reader_(&source_, &report_, true/*checksum*/,\n+                      0/*initial_offset*/) {\n+  }\n+\n+  void Write(const std::string& msg) {\n+    ASSERT_TRUE(!reading_) << \"Write() after starting to read\";\n+    writer_.AddRecord(Slice(msg));\n+  }\n+\n+  size_t WrittenBytes() const {\n+    return dest_.contents_.size();\n+  }\n+\n+  std::string Read() {\n+    if (!reading_) {\n+      reading_ = true;\n+      source_.contents_ = Slice(dest_.contents_);\n+    }\n+    std::string scratch;\n+    Slice record;\n+    if (reader_.ReadRecord(&record, &scratch)) {\n+      return record.ToString();\n+    } else {\n+      return \"EOF\";\n+    }\n+  }\n+\n+  void IncrementByte(int offset, int delta) {\n+    dest_.contents_[offset] += delta;\n+  }\n+\n+  void SetByte(int offset, char new_byte) {\n+    dest_.contents_[offset] = new_byte;\n+  }\n+\n+  void ShrinkSize(int bytes) {\n+    dest_.contents_.resize(dest_.contents_.size() - bytes);\n+  }\n+\n+  void FixChecksum(int header_offset, int len) {\n+    // Compute crc of type/len/data\n+    uint32_t crc = crc32c::Value(&dest_.contents_[header_offset+6], 1 + len);\n+    crc = crc32c::Mask(crc);\n+    EncodeFixed32(&dest_.contents_[header_offset], crc);\n+  }\n+\n+  void ForceError() {\n+    source_.force_error_ = true;\n+  }\n+\n+  size_t DroppedBytes() const {\n+    return report_.dropped_bytes_;\n+  }\n+\n+  std::string ReportMessage() const {\n+    return report_.message_;\n+  }\n+\n+  // Returns OK iff recorded error message contains \"msg\"\n+  std::string MatchError(const std::string& msg) const {\n+    if (report_.message_.find(msg) == std::string::npos) {\n+      return report_.message_;\n+    } else {\n+      return \"OK\";\n+    }\n+  }\n+\n+  void WriteInitialOffsetLog() {\n+    for (int i = 0; i < 4; i++) {\n+      std::string record(initial_offset_record_sizes_[i],\n+                         static_cast<char>('a' + i));\n+      Write(record);\n+    }\n+  }\n+\n+  void CheckOffsetPastEndReturnsNoRecords(uint64_t offset_past_end) {\n+    WriteInitialOffsetLog();\n+    reading_ = true;\n+    source_.contents_ = Slice(dest_.contents_);\n+    Reader* offset_reader = new Reader(&source_, &report_, true/*checksum*/,\n+                                       WrittenBytes() + offset_past_end);\n+    Slice record;\n+    std::string scratch;\n+    ASSERT_TRUE(!offset_reader->ReadRecord(&record, &scratch));\n+    delete offset_reader;\n+  }\n+\n+  void CheckInitialOffsetRecord(uint64_t initial_offset,\n+                                int expected_record_offset) {\n+    WriteInitialOffsetLog();\n+    reading_ = true;\n+    source_.contents_ = Slice(dest_.contents_);\n+    Reader* offset_reader = new Reader(&source_, &report_, true/*checksum*/,\n+                                       initial_offset);\n+    Slice record;\n+    std::string scratch;\n+    ASSERT_TRUE(offset_reader->ReadRecord(&record, &scratch));\n+    ASSERT_EQ(initial_offset_record_sizes_[expected_record_offset],\n+              record.size());\n+    ASSERT_EQ(initial_offset_last_record_offsets_[expected_record_offset],\n+              offset_reader->LastRecordOffset());\n+    ASSERT_EQ((char)('a' + expected_record_offset), record.data()[0]);\n+    delete offset_reader;\n+  }\n+\n+};\n+\n+size_t LogTest::initial_offset_record_sizes_[] =\n+    {10000,  // Two sizable records in first block\n+     10000,\n+     2 * log::kBlockSize - 1000,  // Span three blocks\n+     1};\n+\n+uint64_t LogTest::initial_offset_last_record_offsets_[] =\n+    {0,\n+     kHeaderSize + 10000,\n+     2 * (kHeaderSize + 10000),\n+     2 * (kHeaderSize + 10000) +\n+         (2 * log::kBlockSize - 1000) + 3 * kHeaderSize};\n+\n+\n+TEST(LogTest, Empty) {\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+TEST(LogTest, ReadWrite) {\n+  Write(\"foo\");\n+  Write(\"bar\");\n+  Write(\"\");\n+  Write(\"xxxx\");\n+  ASSERT_EQ(\"foo\", Read());\n+  ASSERT_EQ(\"bar\", Read());\n+  ASSERT_EQ(\"\", Read());\n+  ASSERT_EQ(\"xxxx\", Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(\"EOF\", Read());  // Make sure reads at eof work\n+}\n+\n+TEST(LogTest, ManyBlocks) {\n+  for (int i = 0; i < 100000; i++) {\n+    Write(NumberString(i));\n+  }\n+  for (int i = 0; i < 100000; i++) {\n+    ASSERT_EQ(NumberString(i), Read());\n+  }\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+TEST(LogTest, Fragmentation) {\n+  Write(\"small\");\n+  Write(BigString(\"medium\", 50000));\n+  Write(BigString(\"large\", 100000));\n+  ASSERT_EQ(\"small\", Read());\n+  ASSERT_EQ(BigString(\"medium\", 50000), Read());\n+  ASSERT_EQ(BigString(\"large\", 100000), Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+TEST(LogTest, MarginalTrailer) {\n+  // Make a trailer that is exactly the same length as an empty record.\n+  const int n = kBlockSize - 2*kHeaderSize;\n+  Write(BigString(\"foo\", n));\n+  ASSERT_EQ(kBlockSize - kHeaderSize, WrittenBytes());\n+  Write(\"\");\n+  Write(\"bar\");\n+  ASSERT_EQ(BigString(\"foo\", n), Read());\n+  ASSERT_EQ(\"\", Read());\n+  ASSERT_EQ(\"bar\", Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+TEST(LogTest, MarginalTrailer2) {\n+  // Make a trailer that is exactly the same length as an empty record.\n+  const int n = kBlockSize - 2*kHeaderSize;\n+  Write(BigString(\"foo\", n));\n+  ASSERT_EQ(kBlockSize - kHeaderSize, WrittenBytes());\n+  Write(\"bar\");\n+  ASSERT_EQ(BigString(\"foo\", n), Read());\n+  ASSERT_EQ(\"bar\", Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(0, DroppedBytes());\n+  ASSERT_EQ(\"\", ReportMessage());\n+}\n+\n+TEST(LogTest, ShortTrailer) {\n+  const int n = kBlockSize - 2*kHeaderSize + 4;\n+  Write(BigString(\"foo\", n));\n+  ASSERT_EQ(kBlockSize - kHeaderSize + 4, WrittenBytes());\n+  Write(\"\");\n+  Write(\"bar\");\n+  ASSERT_EQ(BigString(\"foo\", n), Read());\n+  ASSERT_EQ(\"\", Read());\n+  ASSERT_EQ(\"bar\", Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+TEST(LogTest, AlignedEof) {\n+  const int n = kBlockSize - 2*kHeaderSize + 4;\n+  Write(BigString(\"foo\", n));\n+  ASSERT_EQ(kBlockSize - kHeaderSize + 4, WrittenBytes());\n+  ASSERT_EQ(BigString(\"foo\", n), Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+TEST(LogTest, RandomRead) {\n+  const int N = 500;\n+  Random write_rnd(301);\n+  for (int i = 0; i < N; i++) {\n+    Write(RandomSkewedString(i, &write_rnd));\n+  }\n+  Random read_rnd(301);\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_EQ(RandomSkewedString(i, &read_rnd), Read());\n+  }\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+// Tests of all the error paths in log_reader.cc follow:\n+\n+TEST(LogTest, ReadError) {\n+  Write(\"foo\");\n+  ForceError();\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(kBlockSize, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"read error\"));\n+}\n+\n+TEST(LogTest, BadRecordType) {\n+  Write(\"foo\");\n+  // Type is stored in header[6]\n+  IncrementByte(6, 100);\n+  FixChecksum(0, 3);\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(3, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"unknown record type\"));\n+}\n+\n+TEST(LogTest, TruncatedTrailingRecord) {\n+  Write(\"foo\");\n+  ShrinkSize(4);   // Drop all payload as well as a header byte\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(kHeaderSize - 1, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"truncated record at end of file\"));\n+}\n+\n+TEST(LogTest, BadLength) {\n+  Write(\"foo\");\n+  ShrinkSize(1);\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(kHeaderSize + 2, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"bad record length\"));\n+}\n+\n+TEST(LogTest, ChecksumMismatch) {\n+  Write(\"foo\");\n+  IncrementByte(0, 10);\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(10, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"checksum mismatch\"));\n+}\n+\n+TEST(LogTest, UnexpectedMiddleType) {\n+  Write(\"foo\");\n+  SetByte(6, kMiddleType);\n+  FixChecksum(0, 3);\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(3, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"missing start\"));\n+}\n+\n+TEST(LogTest, UnexpectedLastType) {\n+  Write(\"foo\");\n+  SetByte(6, kLastType);\n+  FixChecksum(0, 3);\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(3, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"missing start\"));\n+}\n+\n+TEST(LogTest, UnexpectedFullType) {\n+  Write(\"foo\");\n+  Write(\"bar\");\n+  SetByte(6, kFirstType);\n+  FixChecksum(0, 3);\n+  ASSERT_EQ(\"bar\", Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(3, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"partial record without end\"));\n+}\n+\n+TEST(LogTest, UnexpectedFirstType) {\n+  Write(\"foo\");\n+  Write(BigString(\"bar\", 100000));\n+  SetByte(6, kFirstType);\n+  FixChecksum(0, 3);\n+  ASSERT_EQ(BigString(\"bar\", 100000), Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(3, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"partial record without end\"));\n+}\n+\n+TEST(LogTest, ErrorJoinsRecords) {\n+  // Consider two fragmented records:\n+  //    first(R1) last(R1) first(R2) last(R2)\n+  // where the middle two fragments disappear.  We do not want\n+  // first(R1),last(R2) to get joined and returned as a valid record.\n+\n+  // Write records that span two blocks\n+  Write(BigString(\"foo\", kBlockSize));\n+  Write(BigString(\"bar\", kBlockSize));\n+  Write(\"correct\");\n+\n+  // Wipe the middle block\n+  for (int offset = kBlockSize; offset < 2*kBlockSize; offset++) {\n+    SetByte(offset, 'x');\n+  }\n+\n+  ASSERT_EQ(\"correct\", Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+  const int dropped = DroppedBytes();\n+  ASSERT_LE(dropped, 2*kBlockSize + 100);\n+  ASSERT_GE(dropped, 2*kBlockSize);\n+}\n+\n+TEST(LogTest, ReadStart) {\n+  CheckInitialOffsetRecord(0, 0);\n+}\n+\n+TEST(LogTest, ReadSecondOneOff) {\n+  CheckInitialOffsetRecord(1, 1);\n+}\n+\n+TEST(LogTest, ReadSecondTenThousand) {\n+  CheckInitialOffsetRecord(10000, 1);\n+}\n+\n+TEST(LogTest, ReadSecondStart) {\n+  CheckInitialOffsetRecord(10007, 1);\n+}\n+\n+TEST(LogTest, ReadThirdOneOff) {\n+  CheckInitialOffsetRecord(10008, 2);\n+}\n+\n+TEST(LogTest, ReadThirdStart) {\n+  CheckInitialOffsetRecord(20014, 2);\n+}\n+\n+TEST(LogTest, ReadFourthOneOff) {\n+  CheckInitialOffsetRecord(20015, 3);\n+}\n+\n+TEST(LogTest, ReadFourthFirstBlockTrailer) {\n+  CheckInitialOffsetRecord(log::kBlockSize - 4, 3);\n+}\n+\n+TEST(LogTest, ReadFourthMiddleBlock) {\n+  CheckInitialOffsetRecord(log::kBlockSize + 1, 3);\n+}\n+\n+TEST(LogTest, ReadFourthLastBlock) {\n+  CheckInitialOffsetRecord(2 * log::kBlockSize + 1, 3);\n+}\n+\n+TEST(LogTest, ReadFourthStart) {\n+  CheckInitialOffsetRecord(\n+      2 * (kHeaderSize + 1000) + (2 * log::kBlockSize - 1000) + 3 * kHeaderSize,\n+      3);\n+}\n+\n+TEST(LogTest, ReadEnd) {\n+  CheckOffsetPastEndReturnsNoRecords(0);\n+}\n+\n+TEST(LogTest, ReadPastEnd) {\n+  CheckOffsetPastEndReturnsNoRecords(5);\n+}\n+\n+}  // namespace log\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "2da99ac08866397270663a1203075944162ba290",
        "filename": "db/log_writer.cc",
        "status": "added",
        "additions": 103,
        "deletions": 0,
        "changes": 103,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/log_writer.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/log_writer.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/log_writer.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,103 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/log_writer.h\"\n+\n+#include <stdint.h>\n+#include \"leveldb/env.h\"\n+#include \"util/coding.h\"\n+#include \"util/crc32c.h\"\n+\n+namespace leveldb {\n+namespace log {\n+\n+Writer::Writer(WritableFile* dest)\n+    : dest_(dest),\n+      block_offset_(0) {\n+  for (int i = 0; i <= kMaxRecordType; i++) {\n+    char t = static_cast<char>(i);\n+    type_crc_[i] = crc32c::Value(&t, 1);\n+  }\n+}\n+\n+Writer::~Writer() {\n+}\n+\n+Status Writer::AddRecord(const Slice& slice) {\n+  const char* ptr = slice.data();\n+  size_t left = slice.size();\n+\n+  // Fragment the record if necessary and emit it.  Note that if slice\n+  // is empty, we still want to iterate once to emit a single\n+  // zero-length record\n+  Status s;\n+  bool begin = true;\n+  do {\n+    const int leftover = kBlockSize - block_offset_;\n+    assert(leftover >= 0);\n+    if (leftover < kHeaderSize) {\n+      // Switch to a new block\n+      if (leftover > 0) {\n+        // Fill the trailer (literal below relies on kHeaderSize being 7)\n+        assert(kHeaderSize == 7);\n+        dest_->Append(Slice(\"\\x00\\x00\\x00\\x00\\x00\\x00\", leftover));\n+      }\n+      block_offset_ = 0;\n+    }\n+\n+    // Invariant: we never leave < kHeaderSize bytes in a block.\n+    assert(kBlockSize - block_offset_ - kHeaderSize >= 0);\n+\n+    const size_t avail = kBlockSize - block_offset_ - kHeaderSize;\n+    const size_t fragment_length = (left < avail) ? left : avail;\n+\n+    RecordType type;\n+    const bool end = (left == fragment_length);\n+    if (begin && end) {\n+      type = kFullType;\n+    } else if (begin) {\n+      type = kFirstType;\n+    } else if (end) {\n+      type = kLastType;\n+    } else {\n+      type = kMiddleType;\n+    }\n+\n+    s = EmitPhysicalRecord(type, ptr, fragment_length);\n+    ptr += fragment_length;\n+    left -= fragment_length;\n+    begin = false;\n+  } while (s.ok() && left > 0);\n+  return s;\n+}\n+\n+Status Writer::EmitPhysicalRecord(RecordType t, const char* ptr, size_t n) {\n+  assert(n <= 0xffff);  // Must fit in two bytes\n+  assert(block_offset_ + kHeaderSize + n <= kBlockSize);\n+\n+  // Format the header\n+  char buf[kHeaderSize];\n+  buf[4] = static_cast<char>(n & 0xff);\n+  buf[5] = static_cast<char>(n >> 8);\n+  buf[6] = static_cast<char>(t);\n+\n+  // Compute the crc of the record type and the payload.\n+  uint32_t crc = crc32c::Extend(type_crc_[t], ptr, n);\n+  crc = crc32c::Mask(crc);                 // Adjust for storage\n+  EncodeFixed32(buf, crc);\n+\n+  // Write the header and the payload\n+  Status s = dest_->Append(Slice(buf, kHeaderSize));\n+  if (s.ok()) {\n+    s = dest_->Append(Slice(ptr, n));\n+    if (s.ok()) {\n+      s = dest_->Flush();\n+    }\n+  }\n+  block_offset_ += kHeaderSize + n;\n+  return s;\n+}\n+\n+}  // namespace log\n+}  // namespace leveldb"
      },
      {
        "sha": "a3a954d96732542fac9aef1345ebd952075f737c",
        "filename": "db/log_writer.h",
        "status": "added",
        "additions": 48,
        "deletions": 0,
        "changes": 48,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/log_writer.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/log_writer.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/log_writer.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,48 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_LOG_WRITER_H_\n+#define STORAGE_LEVELDB_DB_LOG_WRITER_H_\n+\n+#include <stdint.h>\n+#include \"db/log_format.h\"\n+#include \"leveldb/slice.h\"\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+class WritableFile;\n+\n+namespace log {\n+\n+class Writer {\n+ public:\n+  // Create a writer that will append data to \"*dest\".\n+  // \"*dest\" must be initially empty.\n+  // \"*dest\" must remain live while this Writer is in use.\n+  explicit Writer(WritableFile* dest);\n+  ~Writer();\n+\n+  Status AddRecord(const Slice& slice);\n+\n+ private:\n+  WritableFile* dest_;\n+  int block_offset_;       // Current offset in block\n+\n+  // crc32c values for all supported record types.  These are\n+  // pre-computed to reduce the overhead of computing the crc of the\n+  // record type stored in the header.\n+  uint32_t type_crc_[kMaxRecordType + 1];\n+\n+  Status EmitPhysicalRecord(RecordType type, const char* ptr, size_t length);\n+\n+  // No copying allowed\n+  Writer(const Writer&);\n+  void operator=(const Writer&);\n+};\n+\n+}  // namespace log\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_LOG_WRITER_H_"
      },
      {
        "sha": "bfec0a7e7a1dc210b44dd527b9547e33e829d9bb",
        "filename": "db/memtable.cc",
        "status": "added",
        "additions": 145,
        "deletions": 0,
        "changes": 145,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/memtable.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/memtable.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/memtable.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,145 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/memtable.h\"\n+#include \"db/dbformat.h\"\n+#include \"leveldb/comparator.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/iterator.h\"\n+#include \"util/coding.h\"\n+\n+namespace leveldb {\n+\n+static Slice GetLengthPrefixedSlice(const char* data) {\n+  uint32_t len;\n+  const char* p = data;\n+  p = GetVarint32Ptr(p, p + 5, &len);  // +5: we assume \"p\" is not corrupted\n+  return Slice(p, len);\n+}\n+\n+MemTable::MemTable(const InternalKeyComparator& cmp)\n+    : comparator_(cmp),\n+      refs_(0),\n+      table_(comparator_, &arena_) {\n+}\n+\n+MemTable::~MemTable() {\n+  assert(refs_ == 0);\n+}\n+\n+size_t MemTable::ApproximateMemoryUsage() { return arena_.MemoryUsage(); }\n+\n+int MemTable::KeyComparator::operator()(const char* aptr, const char* bptr)\n+    const {\n+  // Internal keys are encoded as length-prefixed strings.\n+  Slice a = GetLengthPrefixedSlice(aptr);\n+  Slice b = GetLengthPrefixedSlice(bptr);\n+  return comparator.Compare(a, b);\n+}\n+\n+// Encode a suitable internal key target for \"target\" and return it.\n+// Uses *scratch as scratch space, and the returned pointer will point\n+// into this scratch space.\n+static const char* EncodeKey(std::string* scratch, const Slice& target) {\n+  scratch->clear();\n+  PutVarint32(scratch, target.size());\n+  scratch->append(target.data(), target.size());\n+  return scratch->data();\n+}\n+\n+class MemTableIterator: public Iterator {\n+ public:\n+  explicit MemTableIterator(MemTable::Table* table) : iter_(table) { }\n+\n+  virtual bool Valid() const { return iter_.Valid(); }\n+  virtual void Seek(const Slice& k) { iter_.Seek(EncodeKey(&tmp_, k)); }\n+  virtual void SeekToFirst() { iter_.SeekToFirst(); }\n+  virtual void SeekToLast() { iter_.SeekToLast(); }\n+  virtual void Next() { iter_.Next(); }\n+  virtual void Prev() { iter_.Prev(); }\n+  virtual Slice key() const { return GetLengthPrefixedSlice(iter_.key()); }\n+  virtual Slice value() const {\n+    Slice key_slice = GetLengthPrefixedSlice(iter_.key());\n+    return GetLengthPrefixedSlice(key_slice.data() + key_slice.size());\n+  }\n+\n+  virtual Status status() const { return Status::OK(); }\n+\n+ private:\n+  MemTable::Table::Iterator iter_;\n+  std::string tmp_;       // For passing to EncodeKey\n+\n+  // No copying allowed\n+  MemTableIterator(const MemTableIterator&);\n+  void operator=(const MemTableIterator&);\n+};\n+\n+Iterator* MemTable::NewIterator() {\n+  return new MemTableIterator(&table_);\n+}\n+\n+void MemTable::Add(SequenceNumber s, ValueType type,\n+                   const Slice& key,\n+                   const Slice& value) {\n+  // Format of an entry is concatenation of:\n+  //  key_size     : varint32 of internal_key.size()\n+  //  key bytes    : char[internal_key.size()]\n+  //  value_size   : varint32 of value.size()\n+  //  value bytes  : char[value.size()]\n+  size_t key_size = key.size();\n+  size_t val_size = value.size();\n+  size_t internal_key_size = key_size + 8;\n+  const size_t encoded_len =\n+      VarintLength(internal_key_size) + internal_key_size +\n+      VarintLength(val_size) + val_size;\n+  char* buf = arena_.Allocate(encoded_len);\n+  char* p = EncodeVarint32(buf, internal_key_size);\n+  memcpy(p, key.data(), key_size);\n+  p += key_size;\n+  EncodeFixed64(p, (s << 8) | type);\n+  p += 8;\n+  p = EncodeVarint32(p, val_size);\n+  memcpy(p, value.data(), val_size);\n+  assert((p + val_size) - buf == encoded_len);\n+  table_.Insert(buf);\n+}\n+\n+bool MemTable::Get(const LookupKey& key, std::string* value, Status* s) {\n+  Slice memkey = key.memtable_key();\n+  Table::Iterator iter(&table_);\n+  iter.Seek(memkey.data());\n+  if (iter.Valid()) {\n+    // entry format is:\n+    //    klength  varint32\n+    //    userkey  char[klength]\n+    //    tag      uint64\n+    //    vlength  varint32\n+    //    value    char[vlength]\n+    // Check that it belongs to same user key.  We do not check the\n+    // sequence number since the Seek() call above should have skipped\n+    // all entries with overly large sequence numbers.\n+    const char* entry = iter.key();\n+    uint32_t key_length;\n+    const char* key_ptr = GetVarint32Ptr(entry, entry+5, &key_length);\n+    if (comparator_.comparator.user_comparator()->Compare(\n+            Slice(key_ptr, key_length - 8),\n+            key.user_key()) == 0) {\n+      // Correct user key\n+      const uint64_t tag = DecodeFixed64(key_ptr + key_length - 8);\n+      switch (static_cast<ValueType>(tag & 0xff)) {\n+        case kTypeValue: {\n+          Slice v = GetLengthPrefixedSlice(key_ptr + key_length);\n+          value->assign(v.data(), v.size());\n+          return true;\n+        }\n+        case kTypeDeletion:\n+          *s = Status::NotFound(Slice());\n+          return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "92e90bb099f3356520c42d6bb887b2a35c6bbe39",
        "filename": "db/memtable.h",
        "status": "added",
        "additions": 91,
        "deletions": 0,
        "changes": 91,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/memtable.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/memtable.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/memtable.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,91 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_MEMTABLE_H_\n+#define STORAGE_LEVELDB_DB_MEMTABLE_H_\n+\n+#include <string>\n+#include \"leveldb/db.h\"\n+#include \"db/dbformat.h\"\n+#include \"db/skiplist.h\"\n+#include \"util/arena.h\"\n+\n+namespace leveldb {\n+\n+class InternalKeyComparator;\n+class Mutex;\n+class MemTableIterator;\n+\n+class MemTable {\n+ public:\n+  // MemTables are reference counted.  The initial reference count\n+  // is zero and the caller must call Ref() at least once.\n+  explicit MemTable(const InternalKeyComparator& comparator);\n+\n+  // Increase reference count.\n+  void Ref() { ++refs_; }\n+\n+  // Drop reference count.  Delete if no more references exist.\n+  void Unref() {\n+    --refs_;\n+    assert(refs_ >= 0);\n+    if (refs_ <= 0) {\n+      delete this;\n+    }\n+  }\n+\n+  // Returns an estimate of the number of bytes of data in use by this\n+  // data structure.\n+  //\n+  // REQUIRES: external synchronization to prevent simultaneous\n+  // operations on the same MemTable.\n+  size_t ApproximateMemoryUsage();\n+\n+  // Return an iterator that yields the contents of the memtable.\n+  //\n+  // The caller must ensure that the underlying MemTable remains live\n+  // while the returned iterator is live.  The keys returned by this\n+  // iterator are internal keys encoded by AppendInternalKey in the\n+  // db/format.{h,cc} module.\n+  Iterator* NewIterator();\n+\n+  // Add an entry into memtable that maps key to value at the\n+  // specified sequence number and with the specified type.\n+  // Typically value will be empty if type==kTypeDeletion.\n+  void Add(SequenceNumber seq, ValueType type,\n+           const Slice& key,\n+           const Slice& value);\n+\n+  // If memtable contains a value for key, store it in *value and return true.\n+  // If memtable contains a deletion for key, store a NotFound() error\n+  // in *status and return true.\n+  // Else, return false.\n+  bool Get(const LookupKey& key, std::string* value, Status* s);\n+\n+ private:\n+  ~MemTable();  // Private since only Unref() should be used to delete it\n+\n+  struct KeyComparator {\n+    const InternalKeyComparator comparator;\n+    explicit KeyComparator(const InternalKeyComparator& c) : comparator(c) { }\n+    int operator()(const char* a, const char* b) const;\n+  };\n+  friend class MemTableIterator;\n+  friend class MemTableBackwardIterator;\n+\n+  typedef SkipList<const char*, KeyComparator> Table;\n+\n+  KeyComparator comparator_;\n+  int refs_;\n+  Arena arena_;\n+  Table table_;\n+\n+  // No copying allowed\n+  MemTable(const MemTable&);\n+  void operator=(const MemTable&);\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_MEMTABLE_H_"
      },
      {
        "sha": "022d52f3debe0c0e89a6825ca3778c5894c3783c",
        "filename": "db/repair.cc",
        "status": "added",
        "additions": 389,
        "deletions": 0,
        "changes": 389,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/repair.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/repair.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/repair.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,389 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// We recover the contents of the descriptor from the other files we find.\n+// (1) Any log files are first converted to tables\n+// (2) We scan every table to compute\n+//     (a) smallest/largest for the table\n+//     (b) largest sequence number in the table\n+// (3) We generate descriptor contents:\n+//      - log number is set to zero\n+//      - next-file-number is set to 1 + largest file number we found\n+//      - last-sequence-number is set to largest sequence# found across\n+//        all tables (see 2c)\n+//      - compaction pointers are cleared\n+//      - every table file is added at level 0\n+//\n+// Possible optimization 1:\n+//   (a) Compute total size and use to pick appropriate max-level M\n+//   (b) Sort tables by largest sequence# in the table\n+//   (c) For each table: if it overlaps earlier table, place in level-0,\n+//       else place in level-M.\n+// Possible optimization 2:\n+//   Store per-table metadata (smallest, largest, largest-seq#, ...)\n+//   in the table's meta section to speed up ScanTable.\n+\n+#include \"db/builder.h\"\n+#include \"db/db_impl.h\"\n+#include \"db/dbformat.h\"\n+#include \"db/filename.h\"\n+#include \"db/log_reader.h\"\n+#include \"db/log_writer.h\"\n+#include \"db/memtable.h\"\n+#include \"db/table_cache.h\"\n+#include \"db/version_edit.h\"\n+#include \"db/write_batch_internal.h\"\n+#include \"leveldb/comparator.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+\n+namespace leveldb {\n+\n+namespace {\n+\n+class Repairer {\n+ public:\n+  Repairer(const std::string& dbname, const Options& options)\n+      : dbname_(dbname),\n+        env_(options.env),\n+        icmp_(options.comparator),\n+        ipolicy_(options.filter_policy),\n+        options_(SanitizeOptions(dbname, &icmp_, &ipolicy_, options)),\n+        owns_info_log_(options_.info_log != options.info_log),\n+        owns_cache_(options_.block_cache != options.block_cache),\n+        next_file_number_(1) {\n+    // TableCache can be small since we expect each table to be opened once.\n+    table_cache_ = new TableCache(dbname_, &options_, 10);\n+  }\n+\n+  ~Repairer() {\n+    delete table_cache_;\n+    if (owns_info_log_) {\n+      delete options_.info_log;\n+    }\n+    if (owns_cache_) {\n+      delete options_.block_cache;\n+    }\n+  }\n+\n+  Status Run() {\n+    Status status = FindFiles();\n+    if (status.ok()) {\n+      ConvertLogFilesToTables();\n+      ExtractMetaData();\n+      status = WriteDescriptor();\n+    }\n+    if (status.ok()) {\n+      unsigned long long bytes = 0;\n+      for (size_t i = 0; i < tables_.size(); i++) {\n+        bytes += tables_[i].meta.file_size;\n+      }\n+      Log(options_.info_log,\n+          \"**** Repaired leveldb %s; \"\n+          \"recovered %d files; %llu bytes. \"\n+          \"Some data may have been lost. \"\n+          \"****\",\n+          dbname_.c_str(),\n+          static_cast<int>(tables_.size()),\n+          bytes);\n+    }\n+    return status;\n+  }\n+\n+ private:\n+  struct TableInfo {\n+    FileMetaData meta;\n+    SequenceNumber max_sequence;\n+  };\n+\n+  std::string const dbname_;\n+  Env* const env_;\n+  InternalKeyComparator const icmp_;\n+  InternalFilterPolicy const ipolicy_;\n+  Options const options_;\n+  bool owns_info_log_;\n+  bool owns_cache_;\n+  TableCache* table_cache_;\n+  VersionEdit edit_;\n+\n+  std::vector<std::string> manifests_;\n+  std::vector<uint64_t> table_numbers_;\n+  std::vector<uint64_t> logs_;\n+  std::vector<TableInfo> tables_;\n+  uint64_t next_file_number_;\n+\n+  Status FindFiles() {\n+    std::vector<std::string> filenames;\n+    Status status = env_->GetChildren(dbname_, &filenames);\n+    if (!status.ok()) {\n+      return status;\n+    }\n+    if (filenames.empty()) {\n+      return Status::IOError(dbname_, \"repair found no files\");\n+    }\n+\n+    uint64_t number;\n+    FileType type;\n+    for (size_t i = 0; i < filenames.size(); i++) {\n+      if (ParseFileName(filenames[i], &number, &type)) {\n+        if (type == kDescriptorFile) {\n+          manifests_.push_back(filenames[i]);\n+        } else {\n+          if (number + 1 > next_file_number_) {\n+            next_file_number_ = number + 1;\n+          }\n+          if (type == kLogFile) {\n+            logs_.push_back(number);\n+          } else if (type == kTableFile) {\n+            table_numbers_.push_back(number);\n+          } else {\n+            // Ignore other files\n+          }\n+        }\n+      }\n+    }\n+    return status;\n+  }\n+\n+  void ConvertLogFilesToTables() {\n+    for (size_t i = 0; i < logs_.size(); i++) {\n+      std::string logname = LogFileName(dbname_, logs_[i]);\n+      Status status = ConvertLogToTable(logs_[i]);\n+      if (!status.ok()) {\n+        Log(options_.info_log, \"Log #%llu: ignoring conversion error: %s\",\n+            (unsigned long long) logs_[i],\n+            status.ToString().c_str());\n+      }\n+      ArchiveFile(logname);\n+    }\n+  }\n+\n+  Status ConvertLogToTable(uint64_t log) {\n+    struct LogReporter : public log::Reader::Reporter {\n+      Env* env;\n+      Logger* info_log;\n+      uint64_t lognum;\n+      virtual void Corruption(size_t bytes, const Status& s) {\n+        // We print error messages for corruption, but continue repairing.\n+        Log(info_log, \"Log #%llu: dropping %d bytes; %s\",\n+            (unsigned long long) lognum,\n+            static_cast<int>(bytes),\n+            s.ToString().c_str());\n+      }\n+    };\n+\n+    // Open the log file\n+    std::string logname = LogFileName(dbname_, log);\n+    SequentialFile* lfile;\n+    Status status = env_->NewSequentialFile(logname, &lfile);\n+    if (!status.ok()) {\n+      return status;\n+    }\n+\n+    // Create the log reader.\n+    LogReporter reporter;\n+    reporter.env = env_;\n+    reporter.info_log = options_.info_log;\n+    reporter.lognum = log;\n+    // We intentially make log::Reader do checksumming so that\n+    // corruptions cause entire commits to be skipped instead of\n+    // propagating bad information (like overly large sequence\n+    // numbers).\n+    log::Reader reader(lfile, &reporter, false/*do not checksum*/,\n+                       0/*initial_offset*/);\n+\n+    // Read all the records and add to a memtable\n+    std::string scratch;\n+    Slice record;\n+    WriteBatch batch;\n+    MemTable* mem = new MemTable(icmp_);\n+    mem->Ref();\n+    int counter = 0;\n+    while (reader.ReadRecord(&record, &scratch)) {\n+      if (record.size() < 12) {\n+        reporter.Corruption(\n+            record.size(), Status::Corruption(\"log record too small\"));\n+        continue;\n+      }\n+      WriteBatchInternal::SetContents(&batch, record);\n+      status = WriteBatchInternal::InsertInto(&batch, mem);\n+      if (status.ok()) {\n+        counter += WriteBatchInternal::Count(&batch);\n+      } else {\n+        Log(options_.info_log, \"Log #%llu: ignoring %s\",\n+            (unsigned long long) log,\n+            status.ToString().c_str());\n+        status = Status::OK();  // Keep going with rest of file\n+      }\n+    }\n+    delete lfile;\n+\n+    // Do not record a version edit for this conversion to a Table\n+    // since ExtractMetaData() will also generate edits.\n+    FileMetaData meta;\n+    meta.number = next_file_number_++;\n+    Iterator* iter = mem->NewIterator();\n+    status = BuildTable(dbname_, env_, options_, table_cache_, iter, &meta);\n+    delete iter;\n+    mem->Unref();\n+    mem = NULL;\n+    if (status.ok()) {\n+      if (meta.file_size > 0) {\n+        table_numbers_.push_back(meta.number);\n+      }\n+    }\n+    Log(options_.info_log, \"Log #%llu: %d ops saved to Table #%llu %s\",\n+        (unsigned long long) log,\n+        counter,\n+        (unsigned long long) meta.number,\n+        status.ToString().c_str());\n+    return status;\n+  }\n+\n+  void ExtractMetaData() {\n+    std::vector<TableInfo> kept;\n+    for (size_t i = 0; i < table_numbers_.size(); i++) {\n+      TableInfo t;\n+      t.meta.number = table_numbers_[i];\n+      Status status = ScanTable(&t);\n+      if (!status.ok()) {\n+        std::string fname = TableFileName(dbname_, table_numbers_[i]);\n+        Log(options_.info_log, \"Table #%llu: ignoring %s\",\n+            (unsigned long long) table_numbers_[i],\n+            status.ToString().c_str());\n+        ArchiveFile(fname);\n+      } else {\n+        tables_.push_back(t);\n+      }\n+    }\n+  }\n+\n+  Status ScanTable(TableInfo* t) {\n+    std::string fname = TableFileName(dbname_, t->meta.number);\n+    int counter = 0;\n+    Status status = env_->GetFileSize(fname, &t->meta.file_size);\n+    if (status.ok()) {\n+      Iterator* iter = table_cache_->NewIterator(\n+          ReadOptions(), t->meta.number, t->meta.file_size);\n+      bool empty = true;\n+      ParsedInternalKey parsed;\n+      t->max_sequence = 0;\n+      for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+        Slice key = iter->key();\n+        if (!ParseInternalKey(key, &parsed)) {\n+          Log(options_.info_log, \"Table #%llu: unparsable key %s\",\n+              (unsigned long long) t->meta.number,\n+              EscapeString(key).c_str());\n+          continue;\n+        }\n+\n+        counter++;\n+        if (empty) {\n+          empty = false;\n+          t->meta.smallest.DecodeFrom(key);\n+        }\n+        t->meta.largest.DecodeFrom(key);\n+        if (parsed.sequence > t->max_sequence) {\n+          t->max_sequence = parsed.sequence;\n+        }\n+      }\n+      if (!iter->status().ok()) {\n+        status = iter->status();\n+      }\n+      delete iter;\n+    }\n+    Log(options_.info_log, \"Table #%llu: %d entries %s\",\n+        (unsigned long long) t->meta.number,\n+        counter,\n+        status.ToString().c_str());\n+    return status;\n+  }\n+\n+  Status WriteDescriptor() {\n+    std::string tmp = TempFileName(dbname_, 1);\n+    WritableFile* file;\n+    Status status = env_->NewWritableFile(tmp, &file);\n+    if (!status.ok()) {\n+      return status;\n+    }\n+\n+    SequenceNumber max_sequence = 0;\n+    for (size_t i = 0; i < tables_.size(); i++) {\n+      if (max_sequence < tables_[i].max_sequence) {\n+        max_sequence = tables_[i].max_sequence;\n+      }\n+    }\n+\n+    edit_.SetComparatorName(icmp_.user_comparator()->Name());\n+    edit_.SetLogNumber(0);\n+    edit_.SetNextFile(next_file_number_);\n+    edit_.SetLastSequence(max_sequence);\n+\n+    for (size_t i = 0; i < tables_.size(); i++) {\n+      // TODO(opt): separate out into multiple levels\n+      const TableInfo& t = tables_[i];\n+      edit_.AddFile(0, t.meta.number, t.meta.file_size,\n+                    t.meta.smallest, t.meta.largest);\n+    }\n+\n+    //fprintf(stderr, \"NewDescriptor:\\n%s\\n\", edit_.DebugString().c_str());\n+    {\n+      log::Writer log(file);\n+      std::string record;\n+      edit_.EncodeTo(&record);\n+      status = log.AddRecord(record);\n+    }\n+    if (status.ok()) {\n+      status = file->Close();\n+    }\n+    delete file;\n+    file = NULL;\n+\n+    if (!status.ok()) {\n+      env_->DeleteFile(tmp);\n+    } else {\n+      // Discard older manifests\n+      for (size_t i = 0; i < manifests_.size(); i++) {\n+        ArchiveFile(dbname_ + \"/\" + manifests_[i]);\n+      }\n+\n+      // Install new manifest\n+      status = env_->RenameFile(tmp, DescriptorFileName(dbname_, 1));\n+      if (status.ok()) {\n+        status = SetCurrentFile(env_, dbname_, 1);\n+      } else {\n+        env_->DeleteFile(tmp);\n+      }\n+    }\n+    return status;\n+  }\n+\n+  void ArchiveFile(const std::string& fname) {\n+    // Move into another directory.  E.g., for\n+    //    dir/foo\n+    // rename to\n+    //    dir/lost/foo\n+    const char* slash = strrchr(fname.c_str(), '/');\n+    std::string new_dir;\n+    if (slash != NULL) {\n+      new_dir.assign(fname.data(), slash - fname.data());\n+    }\n+    new_dir.append(\"/lost\");\n+    env_->CreateDir(new_dir);  // Ignore error\n+    std::string new_file = new_dir;\n+    new_file.append(\"/\");\n+    new_file.append((slash == NULL) ? fname.c_str() : slash + 1);\n+    Status s = env_->RenameFile(fname, new_file);\n+    Log(options_.info_log, \"Archiving %s: %s\\n\",\n+        fname.c_str(), s.ToString().c_str());\n+  }\n+};\n+}  // namespace\n+\n+Status RepairDB(const std::string& dbname, const Options& options) {\n+  Repairer repairer(dbname, options);\n+  return repairer.Run();\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "af85be6d01621b249f1756b2af5fd6078dd2ee25",
        "filename": "db/skiplist.h",
        "status": "added",
        "additions": 379,
        "deletions": 0,
        "changes": 379,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/skiplist.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/skiplist.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/skiplist.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,379 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// Thread safety\n+// -------------\n+//\n+// Writes require external synchronization, most likely a mutex.\n+// Reads require a guarantee that the SkipList will not be destroyed\n+// while the read is in progress.  Apart from that, reads progress\n+// without any internal locking or synchronization.\n+//\n+// Invariants:\n+//\n+// (1) Allocated nodes are never deleted until the SkipList is\n+// destroyed.  This is trivially guaranteed by the code since we\n+// never delete any skip list nodes.\n+//\n+// (2) The contents of a Node except for the next/prev pointers are\n+// immutable after the Node has been linked into the SkipList.\n+// Only Insert() modifies the list, and it is careful to initialize\n+// a node and use release-stores to publish the nodes in one or\n+// more lists.\n+//\n+// ... prev vs. next pointer ordering ...\n+\n+#include <assert.h>\n+#include <stdlib.h>\n+#include \"port/port.h\"\n+#include \"util/arena.h\"\n+#include \"util/random.h\"\n+\n+namespace leveldb {\n+\n+class Arena;\n+\n+template<typename Key, class Comparator>\n+class SkipList {\n+ private:\n+  struct Node;\n+\n+ public:\n+  // Create a new SkipList object that will use \"cmp\" for comparing keys,\n+  // and will allocate memory using \"*arena\".  Objects allocated in the arena\n+  // must remain allocated for the lifetime of the skiplist object.\n+  explicit SkipList(Comparator cmp, Arena* arena);\n+\n+  // Insert key into the list.\n+  // REQUIRES: nothing that compares equal to key is currently in the list.\n+  void Insert(const Key& key);\n+\n+  // Returns true iff an entry that compares equal to key is in the list.\n+  bool Contains(const Key& key) const;\n+\n+  // Iteration over the contents of a skip list\n+  class Iterator {\n+   public:\n+    // Initialize an iterator over the specified list.\n+    // The returned iterator is not valid.\n+    explicit Iterator(const SkipList* list);\n+\n+    // Returns true iff the iterator is positioned at a valid node.\n+    bool Valid() const;\n+\n+    // Returns the key at the current position.\n+    // REQUIRES: Valid()\n+    const Key& key() const;\n+\n+    // Advances to the next position.\n+    // REQUIRES: Valid()\n+    void Next();\n+\n+    // Advances to the previous position.\n+    // REQUIRES: Valid()\n+    void Prev();\n+\n+    // Advance to the first entry with a key >= target\n+    void Seek(const Key& target);\n+\n+    // Position at the first entry in list.\n+    // Final state of iterator is Valid() iff list is not empty.\n+    void SeekToFirst();\n+\n+    // Position at the last entry in list.\n+    // Final state of iterator is Valid() iff list is not empty.\n+    void SeekToLast();\n+\n+   private:\n+    const SkipList* list_;\n+    Node* node_;\n+    // Intentionally copyable\n+  };\n+\n+ private:\n+  enum { kMaxHeight = 12 };\n+\n+  // Immutable after construction\n+  Comparator const compare_;\n+  Arena* const arena_;    // Arena used for allocations of nodes\n+\n+  Node* const head_;\n+\n+  // Modified only by Insert().  Read racily by readers, but stale\n+  // values are ok.\n+  port::AtomicPointer max_height_;   // Height of the entire list\n+\n+  inline int GetMaxHeight() const {\n+    return static_cast<int>(\n+        reinterpret_cast<intptr_t>(max_height_.NoBarrier_Load()));\n+  }\n+\n+  // Read/written only by Insert().\n+  Random rnd_;\n+\n+  Node* NewNode(const Key& key, int height);\n+  int RandomHeight();\n+  bool Equal(const Key& a, const Key& b) const { return (compare_(a, b) == 0); }\n+\n+  // Return true if key is greater than the data stored in \"n\"\n+  bool KeyIsAfterNode(const Key& key, Node* n) const;\n+\n+  // Return the earliest node that comes at or after key.\n+  // Return NULL if there is no such node.\n+  //\n+  // If prev is non-NULL, fills prev[level] with pointer to previous\n+  // node at \"level\" for every level in [0..max_height_-1].\n+  Node* FindGreaterOrEqual(const Key& key, Node** prev) const;\n+\n+  // Return the latest node with a key < key.\n+  // Return head_ if there is no such node.\n+  Node* FindLessThan(const Key& key) const;\n+\n+  // Return the last node in the list.\n+  // Return head_ if list is empty.\n+  Node* FindLast() const;\n+\n+  // No copying allowed\n+  SkipList(const SkipList&);\n+  void operator=(const SkipList&);\n+};\n+\n+// Implementation details follow\n+template<typename Key, class Comparator>\n+struct SkipList<Key,Comparator>::Node {\n+  explicit Node(const Key& k) : key(k) { }\n+\n+  Key const key;\n+\n+  // Accessors/mutators for links.  Wrapped in methods so we can\n+  // add the appropriate barriers as necessary.\n+  Node* Next(int n) {\n+    assert(n >= 0);\n+    // Use an 'acquire load' so that we observe a fully initialized\n+    // version of the returned Node.\n+    return reinterpret_cast<Node*>(next_[n].Acquire_Load());\n+  }\n+  void SetNext(int n, Node* x) {\n+    assert(n >= 0);\n+    // Use a 'release store' so that anybody who reads through this\n+    // pointer observes a fully initialized version of the inserted node.\n+    next_[n].Release_Store(x);\n+  }\n+\n+  // No-barrier variants that can be safely used in a few locations.\n+  Node* NoBarrier_Next(int n) {\n+    assert(n >= 0);\n+    return reinterpret_cast<Node*>(next_[n].NoBarrier_Load());\n+  }\n+  void NoBarrier_SetNext(int n, Node* x) {\n+    assert(n >= 0);\n+    next_[n].NoBarrier_Store(x);\n+  }\n+\n+ private:\n+  // Array of length equal to the node height.  next_[0] is lowest level link.\n+  port::AtomicPointer next_[1];\n+};\n+\n+template<typename Key, class Comparator>\n+typename SkipList<Key,Comparator>::Node*\n+SkipList<Key,Comparator>::NewNode(const Key& key, int height) {\n+  char* mem = arena_->AllocateAligned(\n+      sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1));\n+  return new (mem) Node(key);\n+}\n+\n+template<typename Key, class Comparator>\n+inline SkipList<Key,Comparator>::Iterator::Iterator(const SkipList* list) {\n+  list_ = list;\n+  node_ = NULL;\n+}\n+\n+template<typename Key, class Comparator>\n+inline bool SkipList<Key,Comparator>::Iterator::Valid() const {\n+  return node_ != NULL;\n+}\n+\n+template<typename Key, class Comparator>\n+inline const Key& SkipList<Key,Comparator>::Iterator::key() const {\n+  assert(Valid());\n+  return node_->key;\n+}\n+\n+template<typename Key, class Comparator>\n+inline void SkipList<Key,Comparator>::Iterator::Next() {\n+  assert(Valid());\n+  node_ = node_->Next(0);\n+}\n+\n+template<typename Key, class Comparator>\n+inline void SkipList<Key,Comparator>::Iterator::Prev() {\n+  // Instead of using explicit \"prev\" links, we just search for the\n+  // last node that falls before key.\n+  assert(Valid());\n+  node_ = list_->FindLessThan(node_->key);\n+  if (node_ == list_->head_) {\n+    node_ = NULL;\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+inline void SkipList<Key,Comparator>::Iterator::Seek(const Key& target) {\n+  node_ = list_->FindGreaterOrEqual(target, NULL);\n+}\n+\n+template<typename Key, class Comparator>\n+inline void SkipList<Key,Comparator>::Iterator::SeekToFirst() {\n+  node_ = list_->head_->Next(0);\n+}\n+\n+template<typename Key, class Comparator>\n+inline void SkipList<Key,Comparator>::Iterator::SeekToLast() {\n+  node_ = list_->FindLast();\n+  if (node_ == list_->head_) {\n+    node_ = NULL;\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+int SkipList<Key,Comparator>::RandomHeight() {\n+  // Increase height with probability 1 in kBranching\n+  static const unsigned int kBranching = 4;\n+  int height = 1;\n+  while (height < kMaxHeight && ((rnd_.Next() % kBranching) == 0)) {\n+    height++;\n+  }\n+  assert(height > 0);\n+  assert(height <= kMaxHeight);\n+  return height;\n+}\n+\n+template<typename Key, class Comparator>\n+bool SkipList<Key,Comparator>::KeyIsAfterNode(const Key& key, Node* n) const {\n+  // NULL n is considered infinite\n+  return (n != NULL) && (compare_(n->key, key) < 0);\n+}\n+\n+template<typename Key, class Comparator>\n+typename SkipList<Key,Comparator>::Node* SkipList<Key,Comparator>::FindGreaterOrEqual(const Key& key, Node** prev)\n+    const {\n+  Node* x = head_;\n+  int level = GetMaxHeight() - 1;\n+  while (true) {\n+    Node* next = x->Next(level);\n+    if (KeyIsAfterNode(key, next)) {\n+      // Keep searching in this list\n+      x = next;\n+    } else {\n+      if (prev != NULL) prev[level] = x;\n+      if (level == 0) {\n+        return next;\n+      } else {\n+        // Switch to next list\n+        level--;\n+      }\n+    }\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+typename SkipList<Key,Comparator>::Node*\n+SkipList<Key,Comparator>::FindLessThan(const Key& key) const {\n+  Node* x = head_;\n+  int level = GetMaxHeight() - 1;\n+  while (true) {\n+    assert(x == head_ || compare_(x->key, key) < 0);\n+    Node* next = x->Next(level);\n+    if (next == NULL || compare_(next->key, key) >= 0) {\n+      if (level == 0) {\n+        return x;\n+      } else {\n+        // Switch to next list\n+        level--;\n+      }\n+    } else {\n+      x = next;\n+    }\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+typename SkipList<Key,Comparator>::Node* SkipList<Key,Comparator>::FindLast()\n+    const {\n+  Node* x = head_;\n+  int level = GetMaxHeight() - 1;\n+  while (true) {\n+    Node* next = x->Next(level);\n+    if (next == NULL) {\n+      if (level == 0) {\n+        return x;\n+      } else {\n+        // Switch to next list\n+        level--;\n+      }\n+    } else {\n+      x = next;\n+    }\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+SkipList<Key,Comparator>::SkipList(Comparator cmp, Arena* arena)\n+    : compare_(cmp),\n+      arena_(arena),\n+      head_(NewNode(0 /* any key will do */, kMaxHeight)),\n+      max_height_(reinterpret_cast<void*>(1)),\n+      rnd_(0xdeadbeef) {\n+  for (int i = 0; i < kMaxHeight; i++) {\n+    head_->SetNext(i, NULL);\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+void SkipList<Key,Comparator>::Insert(const Key& key) {\n+  // TODO(opt): We can use a barrier-free variant of FindGreaterOrEqual()\n+  // here since Insert() is externally synchronized.\n+  Node* prev[kMaxHeight];\n+  Node* x = FindGreaterOrEqual(key, prev);\n+\n+  // Our data structure does not allow duplicate insertion\n+  assert(x == NULL || !Equal(key, x->key));\n+\n+  int height = RandomHeight();\n+  if (height > GetMaxHeight()) {\n+    for (int i = GetMaxHeight(); i < height; i++) {\n+      prev[i] = head_;\n+    }\n+    //fprintf(stderr, \"Change height from %d to %d\\n\", max_height_, height);\n+\n+    // It is ok to mutate max_height_ without any synchronization\n+    // with concurrent readers.  A concurrent reader that observes\n+    // the new value of max_height_ will see either the old value of\n+    // new level pointers from head_ (NULL), or a new value set in\n+    // the loop below.  In the former case the reader will\n+    // immediately drop to the next level since NULL sorts after all\n+    // keys.  In the latter case the reader will use the new node.\n+    max_height_.NoBarrier_Store(reinterpret_cast<void*>(height));\n+  }\n+\n+  x = NewNode(key, height);\n+  for (int i = 0; i < height; i++) {\n+    // NoBarrier_SetNext() suffices since we will add a barrier when\n+    // we publish a pointer to \"x\" in prev[i].\n+    x->NoBarrier_SetNext(i, prev[i]->NoBarrier_Next(i));\n+    prev[i]->SetNext(i, x);\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+bool SkipList<Key,Comparator>::Contains(const Key& key) const {\n+  Node* x = FindGreaterOrEqual(key, NULL);\n+  if (x != NULL && Equal(key, x->key)) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "c78f4b4fb1a0fc7fb5aadbefa0f7d19eaf06ba1b",
        "filename": "db/skiplist_test.cc",
        "status": "added",
        "additions": 378,
        "deletions": 0,
        "changes": 378,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/skiplist_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/skiplist_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/skiplist_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,378 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/skiplist.h\"\n+#include <set>\n+#include \"leveldb/env.h\"\n+#include \"util/arena.h\"\n+#include \"util/hash.h\"\n+#include \"util/random.h\"\n+#include \"util/testharness.h\"\n+\n+namespace leveldb {\n+\n+typedef uint64_t Key;\n+\n+struct Comparator {\n+  int operator()(const Key& a, const Key& b) const {\n+    if (a < b) {\n+      return -1;\n+    } else if (a > b) {\n+      return +1;\n+    } else {\n+      return 0;\n+    }\n+  }\n+};\n+\n+class SkipTest { };\n+\n+TEST(SkipTest, Empty) {\n+  Arena arena;\n+  Comparator cmp;\n+  SkipList<Key, Comparator> list(cmp, &arena);\n+  ASSERT_TRUE(!list.Contains(10));\n+\n+  SkipList<Key, Comparator>::Iterator iter(&list);\n+  ASSERT_TRUE(!iter.Valid());\n+  iter.SeekToFirst();\n+  ASSERT_TRUE(!iter.Valid());\n+  iter.Seek(100);\n+  ASSERT_TRUE(!iter.Valid());\n+  iter.SeekToLast();\n+  ASSERT_TRUE(!iter.Valid());\n+}\n+\n+TEST(SkipTest, InsertAndLookup) {\n+  const int N = 2000;\n+  const int R = 5000;\n+  Random rnd(1000);\n+  std::set<Key> keys;\n+  Arena arena;\n+  Comparator cmp;\n+  SkipList<Key, Comparator> list(cmp, &arena);\n+  for (int i = 0; i < N; i++) {\n+    Key key = rnd.Next() % R;\n+    if (keys.insert(key).second) {\n+      list.Insert(key);\n+    }\n+  }\n+\n+  for (int i = 0; i < R; i++) {\n+    if (list.Contains(i)) {\n+      ASSERT_EQ(keys.count(i), 1);\n+    } else {\n+      ASSERT_EQ(keys.count(i), 0);\n+    }\n+  }\n+\n+  // Simple iterator tests\n+  {\n+    SkipList<Key, Comparator>::Iterator iter(&list);\n+    ASSERT_TRUE(!iter.Valid());\n+\n+    iter.Seek(0);\n+    ASSERT_TRUE(iter.Valid());\n+    ASSERT_EQ(*(keys.begin()), iter.key());\n+\n+    iter.SeekToFirst();\n+    ASSERT_TRUE(iter.Valid());\n+    ASSERT_EQ(*(keys.begin()), iter.key());\n+\n+    iter.SeekToLast();\n+    ASSERT_TRUE(iter.Valid());\n+    ASSERT_EQ(*(keys.rbegin()), iter.key());\n+  }\n+\n+  // Forward iteration test\n+  for (int i = 0; i < R; i++) {\n+    SkipList<Key, Comparator>::Iterator iter(&list);\n+    iter.Seek(i);\n+\n+    // Compare against model iterator\n+    std::set<Key>::iterator model_iter = keys.lower_bound(i);\n+    for (int j = 0; j < 3; j++) {\n+      if (model_iter == keys.end()) {\n+        ASSERT_TRUE(!iter.Valid());\n+        break;\n+      } else {\n+        ASSERT_TRUE(iter.Valid());\n+        ASSERT_EQ(*model_iter, iter.key());\n+        ++model_iter;\n+        iter.Next();\n+      }\n+    }\n+  }\n+\n+  // Backward iteration test\n+  {\n+    SkipList<Key, Comparator>::Iterator iter(&list);\n+    iter.SeekToLast();\n+\n+    // Compare against model iterator\n+    for (std::set<Key>::reverse_iterator model_iter = keys.rbegin();\n+         model_iter != keys.rend();\n+         ++model_iter) {\n+      ASSERT_TRUE(iter.Valid());\n+      ASSERT_EQ(*model_iter, iter.key());\n+      iter.Prev();\n+    }\n+    ASSERT_TRUE(!iter.Valid());\n+  }\n+}\n+\n+// We want to make sure that with a single writer and multiple\n+// concurrent readers (with no synchronization other than when a\n+// reader's iterator is created), the reader always observes all the\n+// data that was present in the skip list when the iterator was\n+// constructor.  Because insertions are happening concurrently, we may\n+// also observe new values that were inserted since the iterator was\n+// constructed, but we should never miss any values that were present\n+// at iterator construction time.\n+//\n+// We generate multi-part keys:\n+//     <key,gen,hash>\n+// where:\n+//     key is in range [0..K-1]\n+//     gen is a generation number for key\n+//     hash is hash(key,gen)\n+//\n+// The insertion code picks a random key, sets gen to be 1 + the last\n+// generation number inserted for that key, and sets hash to Hash(key,gen).\n+//\n+// At the beginning of a read, we snapshot the last inserted\n+// generation number for each key.  We then iterate, including random\n+// calls to Next() and Seek().  For every key we encounter, we\n+// check that it is either expected given the initial snapshot or has\n+// been concurrently added since the iterator started.\n+class ConcurrentTest {\n+ private:\n+  static const uint32_t K = 4;\n+\n+  static uint64_t key(Key key) { return (key >> 40); }\n+  static uint64_t gen(Key key) { return (key >> 8) & 0xffffffffu; }\n+  static uint64_t hash(Key key) { return key & 0xff; }\n+\n+  static uint64_t HashNumbers(uint64_t k, uint64_t g) {\n+    uint64_t data[2] = { k, g };\n+    return Hash(reinterpret_cast<char*>(data), sizeof(data), 0);\n+  }\n+\n+  static Key MakeKey(uint64_t k, uint64_t g) {\n+    assert(sizeof(Key) == sizeof(uint64_t));\n+    assert(k <= K);  // We sometimes pass K to seek to the end of the skiplist\n+    assert(g <= 0xffffffffu);\n+    return ((k << 40) | (g << 8) | (HashNumbers(k, g) & 0xff));\n+  }\n+\n+  static bool IsValidKey(Key k) {\n+    return hash(k) == (HashNumbers(key(k), gen(k)) & 0xff);\n+  }\n+\n+  static Key RandomTarget(Random* rnd) {\n+    switch (rnd->Next() % 10) {\n+      case 0:\n+        // Seek to beginning\n+        return MakeKey(0, 0);\n+      case 1:\n+        // Seek to end\n+        return MakeKey(K, 0);\n+      default:\n+        // Seek to middle\n+        return MakeKey(rnd->Next() % K, 0);\n+    }\n+  }\n+\n+  // Per-key generation\n+  struct State {\n+    port::AtomicPointer generation[K];\n+    void Set(int k, intptr_t v) {\n+      generation[k].Release_Store(reinterpret_cast<void*>(v));\n+    }\n+    intptr_t Get(int k) {\n+      return reinterpret_cast<intptr_t>(generation[k].Acquire_Load());\n+    }\n+\n+    State() {\n+      for (int k = 0; k < K; k++) {\n+        Set(k, 0);\n+      }\n+    }\n+  };\n+\n+  // Current state of the test\n+  State current_;\n+\n+  Arena arena_;\n+\n+  // SkipList is not protected by mu_.  We just use a single writer\n+  // thread to modify it.\n+  SkipList<Key, Comparator> list_;\n+\n+ public:\n+  ConcurrentTest() : list_(Comparator(), &arena_) { }\n+\n+  // REQUIRES: External synchronization\n+  void WriteStep(Random* rnd) {\n+    const uint32_t k = rnd->Next() % K;\n+    const intptr_t g = current_.Get(k) + 1;\n+    const Key key = MakeKey(k, g);\n+    list_.Insert(key);\n+    current_.Set(k, g);\n+  }\n+\n+  void ReadStep(Random* rnd) {\n+    // Remember the initial committed state of the skiplist.\n+    State initial_state;\n+    for (int k = 0; k < K; k++) {\n+      initial_state.Set(k, current_.Get(k));\n+    }\n+\n+    Key pos = RandomTarget(rnd);\n+    SkipList<Key, Comparator>::Iterator iter(&list_);\n+    iter.Seek(pos);\n+    while (true) {\n+      Key current;\n+      if (!iter.Valid()) {\n+        current = MakeKey(K, 0);\n+      } else {\n+        current = iter.key();\n+        ASSERT_TRUE(IsValidKey(current)) << current;\n+      }\n+      ASSERT_LE(pos, current) << \"should not go backwards\";\n+\n+      // Verify that everything in [pos,current) was not present in\n+      // initial_state.\n+      while (pos < current) {\n+        ASSERT_LT(key(pos), K) << pos;\n+\n+        // Note that generation 0 is never inserted, so it is ok if\n+        // <*,0,*> is missing.\n+        ASSERT_TRUE((gen(pos) == 0) ||\n+                    (gen(pos) > initial_state.Get(key(pos)))\n+                    ) << \"key: \" << key(pos)\n+                      << \"; gen: \" << gen(pos)\n+                      << \"; initgen: \"\n+                      << initial_state.Get(key(pos));\n+\n+        // Advance to next key in the valid key space\n+        if (key(pos) < key(current)) {\n+          pos = MakeKey(key(pos) + 1, 0);\n+        } else {\n+          pos = MakeKey(key(pos), gen(pos) + 1);\n+        }\n+      }\n+\n+      if (!iter.Valid()) {\n+        break;\n+      }\n+\n+      if (rnd->Next() % 2) {\n+        iter.Next();\n+        pos = MakeKey(key(pos), gen(pos) + 1);\n+      } else {\n+        Key new_target = RandomTarget(rnd);\n+        if (new_target > pos) {\n+          pos = new_target;\n+          iter.Seek(new_target);\n+        }\n+      }\n+    }\n+  }\n+};\n+const uint32_t ConcurrentTest::K;\n+\n+// Simple test that does single-threaded testing of the ConcurrentTest\n+// scaffolding.\n+TEST(SkipTest, ConcurrentWithoutThreads) {\n+  ConcurrentTest test;\n+  Random rnd(test::RandomSeed());\n+  for (int i = 0; i < 10000; i++) {\n+    test.ReadStep(&rnd);\n+    test.WriteStep(&rnd);\n+  }\n+}\n+\n+class TestState {\n+ public:\n+  ConcurrentTest t_;\n+  int seed_;\n+  port::AtomicPointer quit_flag_;\n+\n+  enum ReaderState {\n+    STARTING,\n+    RUNNING,\n+    DONE\n+  };\n+\n+  explicit TestState(int s)\n+      : seed_(s),\n+        quit_flag_(NULL),\n+        state_(STARTING),\n+        state_cv_(&mu_) {}\n+\n+  void Wait(ReaderState s) {\n+    mu_.Lock();\n+    while (state_ != s) {\n+      state_cv_.Wait();\n+    }\n+    mu_.Unlock();\n+  }\n+\n+  void Change(ReaderState s) {\n+    mu_.Lock();\n+    state_ = s;\n+    state_cv_.Signal();\n+    mu_.Unlock();\n+  }\n+\n+ private:\n+  port::Mutex mu_;\n+  ReaderState state_;\n+  port::CondVar state_cv_;\n+};\n+\n+static void ConcurrentReader(void* arg) {\n+  TestState* state = reinterpret_cast<TestState*>(arg);\n+  Random rnd(state->seed_);\n+  int64_t reads = 0;\n+  state->Change(TestState::RUNNING);\n+  while (!state->quit_flag_.Acquire_Load()) {\n+    state->t_.ReadStep(&rnd);\n+    ++reads;\n+  }\n+  state->Change(TestState::DONE);\n+}\n+\n+static void RunConcurrent(int run) {\n+  const int seed = test::RandomSeed() + (run * 100);\n+  Random rnd(seed);\n+  const int N = 1000;\n+  const int kSize = 1000;\n+  for (int i = 0; i < N; i++) {\n+    if ((i % 100) == 0) {\n+      fprintf(stderr, \"Run %d of %d\\n\", i, N);\n+    }\n+    TestState state(seed + 1);\n+    Env::Default()->Schedule(ConcurrentReader, &state);\n+    state.Wait(TestState::RUNNING);\n+    for (int i = 0; i < kSize; i++) {\n+      state.t_.WriteStep(&rnd);\n+    }\n+    state.quit_flag_.Release_Store(&state);  // Any non-NULL arg will do\n+    state.Wait(TestState::DONE);\n+  }\n+}\n+\n+TEST(SkipTest, Concurrent1) { RunConcurrent(1); }\n+TEST(SkipTest, Concurrent2) { RunConcurrent(2); }\n+TEST(SkipTest, Concurrent3) { RunConcurrent(3); }\n+TEST(SkipTest, Concurrent4) { RunConcurrent(4); }\n+TEST(SkipTest, Concurrent5) { RunConcurrent(5); }\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "e7f8fd2c37cf8a482c45f5524aa729e74f7b3aa7",
        "filename": "db/snapshot.h",
        "status": "added",
        "additions": 66,
        "deletions": 0,
        "changes": 66,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/snapshot.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/snapshot.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/snapshot.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,66 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_SNAPSHOT_H_\n+#define STORAGE_LEVELDB_DB_SNAPSHOT_H_\n+\n+#include \"leveldb/db.h\"\n+\n+namespace leveldb {\n+\n+class SnapshotList;\n+\n+// Snapshots are kept in a doubly-linked list in the DB.\n+// Each SnapshotImpl corresponds to a particular sequence number.\n+class SnapshotImpl : public Snapshot {\n+ public:\n+  SequenceNumber number_;  // const after creation\n+\n+ private:\n+  friend class SnapshotList;\n+\n+  // SnapshotImpl is kept in a doubly-linked circular list\n+  SnapshotImpl* prev_;\n+  SnapshotImpl* next_;\n+\n+  SnapshotList* list_;                 // just for sanity checks\n+};\n+\n+class SnapshotList {\n+ public:\n+  SnapshotList() {\n+    list_.prev_ = &list_;\n+    list_.next_ = &list_;\n+  }\n+\n+  bool empty() const { return list_.next_ == &list_; }\n+  SnapshotImpl* oldest() const { assert(!empty()); return list_.next_; }\n+  SnapshotImpl* newest() const { assert(!empty()); return list_.prev_; }\n+\n+  const SnapshotImpl* New(SequenceNumber seq) {\n+    SnapshotImpl* s = new SnapshotImpl;\n+    s->number_ = seq;\n+    s->list_ = this;\n+    s->next_ = &list_;\n+    s->prev_ = list_.prev_;\n+    s->prev_->next_ = s;\n+    s->next_->prev_ = s;\n+    return s;\n+  }\n+\n+  void Delete(const SnapshotImpl* s) {\n+    assert(s->list_ == this);\n+    s->prev_->next_ = s->next_;\n+    s->next_->prev_ = s->prev_;\n+    delete s;\n+  }\n+\n+ private:\n+  // Dummy head of doubly-linked list of snapshots\n+  SnapshotImpl list_;\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_SNAPSHOT_H_"
      },
      {
        "sha": "497db270766d8857ddb355ad09ed0892e4ab2daa",
        "filename": "db/table_cache.cc",
        "status": "added",
        "additions": 121,
        "deletions": 0,
        "changes": 121,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/table_cache.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/table_cache.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/table_cache.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,121 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/table_cache.h\"\n+\n+#include \"db/filename.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/table.h\"\n+#include \"util/coding.h\"\n+\n+namespace leveldb {\n+\n+struct TableAndFile {\n+  RandomAccessFile* file;\n+  Table* table;\n+};\n+\n+static void DeleteEntry(const Slice& key, void* value) {\n+  TableAndFile* tf = reinterpret_cast<TableAndFile*>(value);\n+  delete tf->table;\n+  delete tf->file;\n+  delete tf;\n+}\n+\n+static void UnrefEntry(void* arg1, void* arg2) {\n+  Cache* cache = reinterpret_cast<Cache*>(arg1);\n+  Cache::Handle* h = reinterpret_cast<Cache::Handle*>(arg2);\n+  cache->Release(h);\n+}\n+\n+TableCache::TableCache(const std::string& dbname,\n+                       const Options* options,\n+                       int entries)\n+    : env_(options->env),\n+      dbname_(dbname),\n+      options_(options),\n+      cache_(NewLRUCache(entries)) {\n+}\n+\n+TableCache::~TableCache() {\n+  delete cache_;\n+}\n+\n+Status TableCache::FindTable(uint64_t file_number, uint64_t file_size,\n+                             Cache::Handle** handle) {\n+  Status s;\n+  char buf[sizeof(file_number)];\n+  EncodeFixed64(buf, file_number);\n+  Slice key(buf, sizeof(buf));\n+  *handle = cache_->Lookup(key);\n+  if (*handle == NULL) {\n+    std::string fname = TableFileName(dbname_, file_number);\n+    RandomAccessFile* file = NULL;\n+    Table* table = NULL;\n+    s = env_->NewRandomAccessFile(fname, &file);\n+    if (s.ok()) {\n+      s = Table::Open(*options_, file, file_size, &table);\n+    }\n+\n+    if (!s.ok()) {\n+      assert(table == NULL);\n+      delete file;\n+      // We do not cache error results so that if the error is transient,\n+      // or somebody repairs the file, we recover automatically.\n+    } else {\n+      TableAndFile* tf = new TableAndFile;\n+      tf->file = file;\n+      tf->table = table;\n+      *handle = cache_->Insert(key, tf, 1, &DeleteEntry);\n+    }\n+  }\n+  return s;\n+}\n+\n+Iterator* TableCache::NewIterator(const ReadOptions& options,\n+                                  uint64_t file_number,\n+                                  uint64_t file_size,\n+                                  Table** tableptr) {\n+  if (tableptr != NULL) {\n+    *tableptr = NULL;\n+  }\n+\n+  Cache::Handle* handle = NULL;\n+  Status s = FindTable(file_number, file_size, &handle);\n+  if (!s.ok()) {\n+    return NewErrorIterator(s);\n+  }\n+\n+  Table* table = reinterpret_cast<TableAndFile*>(cache_->Value(handle))->table;\n+  Iterator* result = table->NewIterator(options);\n+  result->RegisterCleanup(&UnrefEntry, cache_, handle);\n+  if (tableptr != NULL) {\n+    *tableptr = table;\n+  }\n+  return result;\n+}\n+\n+Status TableCache::Get(const ReadOptions& options,\n+                       uint64_t file_number,\n+                       uint64_t file_size,\n+                       const Slice& k,\n+                       void* arg,\n+                       void (*saver)(void*, const Slice&, const Slice&)) {\n+  Cache::Handle* handle = NULL;\n+  Status s = FindTable(file_number, file_size, &handle);\n+  if (s.ok()) {\n+    Table* t = reinterpret_cast<TableAndFile*>(cache_->Value(handle))->table;\n+    s = t->InternalGet(options, k, arg, saver);\n+    cache_->Release(handle);\n+  }\n+  return s;\n+}\n+\n+void TableCache::Evict(uint64_t file_number) {\n+  char buf[sizeof(file_number)];\n+  EncodeFixed64(buf, file_number);\n+  cache_->Erase(Slice(buf, sizeof(buf)));\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "8cf4aaf12d8ed1a02bd7d1962b79cc8506575b6f",
        "filename": "db/table_cache.h",
        "status": "added",
        "additions": 61,
        "deletions": 0,
        "changes": 61,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/table_cache.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/table_cache.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/table_cache.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,61 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// Thread-safe (provides internal synchronization)\n+\n+#ifndef STORAGE_LEVELDB_DB_TABLE_CACHE_H_\n+#define STORAGE_LEVELDB_DB_TABLE_CACHE_H_\n+\n+#include <string>\n+#include <stdint.h>\n+#include \"db/dbformat.h\"\n+#include \"leveldb/cache.h\"\n+#include \"leveldb/table.h\"\n+#include \"port/port.h\"\n+\n+namespace leveldb {\n+\n+class Env;\n+\n+class TableCache {\n+ public:\n+  TableCache(const std::string& dbname, const Options* options, int entries);\n+  ~TableCache();\n+\n+  // Return an iterator for the specified file number (the corresponding\n+  // file length must be exactly \"file_size\" bytes).  If \"tableptr\" is\n+  // non-NULL, also sets \"*tableptr\" to point to the Table object\n+  // underlying the returned iterator, or NULL if no Table object underlies\n+  // the returned iterator.  The returned \"*tableptr\" object is owned by\n+  // the cache and should not be deleted, and is valid for as long as the\n+  // returned iterator is live.\n+  Iterator* NewIterator(const ReadOptions& options,\n+                        uint64_t file_number,\n+                        uint64_t file_size,\n+                        Table** tableptr = NULL);\n+\n+  // If a seek to internal key \"k\" in specified file finds an entry,\n+  // call (*handle_result)(arg, found_key, found_value).\n+  Status Get(const ReadOptions& options,\n+             uint64_t file_number,\n+             uint64_t file_size,\n+             const Slice& k,\n+             void* arg,\n+             void (*handle_result)(void*, const Slice&, const Slice&));\n+\n+  // Evict any entry for the specified file number\n+  void Evict(uint64_t file_number);\n+\n+ private:\n+  Env* const env_;\n+  const std::string dbname_;\n+  const Options* options_;\n+  Cache* cache_;\n+\n+  Status FindTable(uint64_t file_number, uint64_t file_size, Cache::Handle**);\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_TABLE_CACHE_H_"
      },
      {
        "sha": "f10a2d58b211cb16becb0ac0298210f0dacbd2a5",
        "filename": "db/version_edit.cc",
        "status": "added",
        "additions": 266,
        "deletions": 0,
        "changes": 266,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/version_edit.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/version_edit.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/version_edit.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,266 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/version_edit.h\"\n+\n+#include \"db/version_set.h\"\n+#include \"util/coding.h\"\n+\n+namespace leveldb {\n+\n+// Tag numbers for serialized VersionEdit.  These numbers are written to\n+// disk and should not be changed.\n+enum Tag {\n+  kComparator           = 1,\n+  kLogNumber            = 2,\n+  kNextFileNumber       = 3,\n+  kLastSequence         = 4,\n+  kCompactPointer       = 5,\n+  kDeletedFile          = 6,\n+  kNewFile              = 7,\n+  // 8 was used for large value refs\n+  kPrevLogNumber        = 9\n+};\n+\n+void VersionEdit::Clear() {\n+  comparator_.clear();\n+  log_number_ = 0;\n+  prev_log_number_ = 0;\n+  last_sequence_ = 0;\n+  next_file_number_ = 0;\n+  has_comparator_ = false;\n+  has_log_number_ = false;\n+  has_prev_log_number_ = false;\n+  has_next_file_number_ = false;\n+  has_last_sequence_ = false;\n+  deleted_files_.clear();\n+  new_files_.clear();\n+}\n+\n+void VersionEdit::EncodeTo(std::string* dst) const {\n+  if (has_comparator_) {\n+    PutVarint32(dst, kComparator);\n+    PutLengthPrefixedSlice(dst, comparator_);\n+  }\n+  if (has_log_number_) {\n+    PutVarint32(dst, kLogNumber);\n+    PutVarint64(dst, log_number_);\n+  }\n+  if (has_prev_log_number_) {\n+    PutVarint32(dst, kPrevLogNumber);\n+    PutVarint64(dst, prev_log_number_);\n+  }\n+  if (has_next_file_number_) {\n+    PutVarint32(dst, kNextFileNumber);\n+    PutVarint64(dst, next_file_number_);\n+  }\n+  if (has_last_sequence_) {\n+    PutVarint32(dst, kLastSequence);\n+    PutVarint64(dst, last_sequence_);\n+  }\n+\n+  for (size_t i = 0; i < compact_pointers_.size(); i++) {\n+    PutVarint32(dst, kCompactPointer);\n+    PutVarint32(dst, compact_pointers_[i].first);  // level\n+    PutLengthPrefixedSlice(dst, compact_pointers_[i].second.Encode());\n+  }\n+\n+  for (DeletedFileSet::const_iterator iter = deleted_files_.begin();\n+       iter != deleted_files_.end();\n+       ++iter) {\n+    PutVarint32(dst, kDeletedFile);\n+    PutVarint32(dst, iter->first);   // level\n+    PutVarint64(dst, iter->second);  // file number\n+  }\n+\n+  for (size_t i = 0; i < new_files_.size(); i++) {\n+    const FileMetaData& f = new_files_[i].second;\n+    PutVarint32(dst, kNewFile);\n+    PutVarint32(dst, new_files_[i].first);  // level\n+    PutVarint64(dst, f.number);\n+    PutVarint64(dst, f.file_size);\n+    PutLengthPrefixedSlice(dst, f.smallest.Encode());\n+    PutLengthPrefixedSlice(dst, f.largest.Encode());\n+  }\n+}\n+\n+static bool GetInternalKey(Slice* input, InternalKey* dst) {\n+  Slice str;\n+  if (GetLengthPrefixedSlice(input, &str)) {\n+    dst->DecodeFrom(str);\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+static bool GetLevel(Slice* input, int* level) {\n+  uint32_t v;\n+  if (GetVarint32(input, &v) &&\n+      v < config::kNumLevels) {\n+    *level = v;\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+Status VersionEdit::DecodeFrom(const Slice& src) {\n+  Clear();\n+  Slice input = src;\n+  const char* msg = NULL;\n+  uint32_t tag;\n+\n+  // Temporary storage for parsing\n+  int level;\n+  uint64_t number;\n+  FileMetaData f;\n+  Slice str;\n+  InternalKey key;\n+\n+  while (msg == NULL && GetVarint32(&input, &tag)) {\n+    switch (tag) {\n+      case kComparator:\n+        if (GetLengthPrefixedSlice(&input, &str)) {\n+          comparator_ = str.ToString();\n+          has_comparator_ = true;\n+        } else {\n+          msg = \"comparator name\";\n+        }\n+        break;\n+\n+      case kLogNumber:\n+        if (GetVarint64(&input, &log_number_)) {\n+          has_log_number_ = true;\n+        } else {\n+          msg = \"log number\";\n+        }\n+        break;\n+\n+      case kPrevLogNumber:\n+        if (GetVarint64(&input, &prev_log_number_)) {\n+          has_prev_log_number_ = true;\n+        } else {\n+          msg = \"previous log number\";\n+        }\n+        break;\n+\n+      case kNextFileNumber:\n+        if (GetVarint64(&input, &next_file_number_)) {\n+          has_next_file_number_ = true;\n+        } else {\n+          msg = \"next file number\";\n+        }\n+        break;\n+\n+      case kLastSequence:\n+        if (GetVarint64(&input, &last_sequence_)) {\n+          has_last_sequence_ = true;\n+        } else {\n+          msg = \"last sequence number\";\n+        }\n+        break;\n+\n+      case kCompactPointer:\n+        if (GetLevel(&input, &level) &&\n+            GetInternalKey(&input, &key)) {\n+          compact_pointers_.push_back(std::make_pair(level, key));\n+        } else {\n+          msg = \"compaction pointer\";\n+        }\n+        break;\n+\n+      case kDeletedFile:\n+        if (GetLevel(&input, &level) &&\n+            GetVarint64(&input, &number)) {\n+          deleted_files_.insert(std::make_pair(level, number));\n+        } else {\n+          msg = \"deleted file\";\n+        }\n+        break;\n+\n+      case kNewFile:\n+        if (GetLevel(&input, &level) &&\n+            GetVarint64(&input, &f.number) &&\n+            GetVarint64(&input, &f.file_size) &&\n+            GetInternalKey(&input, &f.smallest) &&\n+            GetInternalKey(&input, &f.largest)) {\n+          new_files_.push_back(std::make_pair(level, f));\n+        } else {\n+          msg = \"new-file entry\";\n+        }\n+        break;\n+\n+      default:\n+        msg = \"unknown tag\";\n+        break;\n+    }\n+  }\n+\n+  if (msg == NULL && !input.empty()) {\n+    msg = \"invalid tag\";\n+  }\n+\n+  Status result;\n+  if (msg != NULL) {\n+    result = Status::Corruption(\"VersionEdit\", msg);\n+  }\n+  return result;\n+}\n+\n+std::string VersionEdit::DebugString() const {\n+  std::string r;\n+  r.append(\"VersionEdit {\");\n+  if (has_comparator_) {\n+    r.append(\"\\n  Comparator: \");\n+    r.append(comparator_);\n+  }\n+  if (has_log_number_) {\n+    r.append(\"\\n  LogNumber: \");\n+    AppendNumberTo(&r, log_number_);\n+  }\n+  if (has_prev_log_number_) {\n+    r.append(\"\\n  PrevLogNumber: \");\n+    AppendNumberTo(&r, prev_log_number_);\n+  }\n+  if (has_next_file_number_) {\n+    r.append(\"\\n  NextFile: \");\n+    AppendNumberTo(&r, next_file_number_);\n+  }\n+  if (has_last_sequence_) {\n+    r.append(\"\\n  LastSeq: \");\n+    AppendNumberTo(&r, last_sequence_);\n+  }\n+  for (size_t i = 0; i < compact_pointers_.size(); i++) {\n+    r.append(\"\\n  CompactPointer: \");\n+    AppendNumberTo(&r, compact_pointers_[i].first);\n+    r.append(\" \");\n+    r.append(compact_pointers_[i].second.DebugString());\n+  }\n+  for (DeletedFileSet::const_iterator iter = deleted_files_.begin();\n+       iter != deleted_files_.end();\n+       ++iter) {\n+    r.append(\"\\n  DeleteFile: \");\n+    AppendNumberTo(&r, iter->first);\n+    r.append(\" \");\n+    AppendNumberTo(&r, iter->second);\n+  }\n+  for (size_t i = 0; i < new_files_.size(); i++) {\n+    const FileMetaData& f = new_files_[i].second;\n+    r.append(\"\\n  AddFile: \");\n+    AppendNumberTo(&r, new_files_[i].first);\n+    r.append(\" \");\n+    AppendNumberTo(&r, f.number);\n+    r.append(\" \");\n+    AppendNumberTo(&r, f.file_size);\n+    r.append(\" \");\n+    r.append(f.smallest.DebugString());\n+    r.append(\" .. \");\n+    r.append(f.largest.DebugString());\n+  }\n+  r.append(\"\\n}\\n\");\n+  return r;\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "eaef77b327c64a3756a5f2512bc1786530907cbd",
        "filename": "db/version_edit.h",
        "status": "added",
        "additions": 107,
        "deletions": 0,
        "changes": 107,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/version_edit.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/version_edit.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/version_edit.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,107 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_VERSION_EDIT_H_\n+#define STORAGE_LEVELDB_DB_VERSION_EDIT_H_\n+\n+#include <set>\n+#include <utility>\n+#include <vector>\n+#include \"db/dbformat.h\"\n+\n+namespace leveldb {\n+\n+class VersionSet;\n+\n+struct FileMetaData {\n+  int refs;\n+  int allowed_seeks;          // Seeks allowed until compaction\n+  uint64_t number;\n+  uint64_t file_size;         // File size in bytes\n+  InternalKey smallest;       // Smallest internal key served by table\n+  InternalKey largest;        // Largest internal key served by table\n+\n+  FileMetaData() : refs(0), allowed_seeks(1 << 30), file_size(0) { }\n+};\n+\n+class VersionEdit {\n+ public:\n+  VersionEdit() { Clear(); }\n+  ~VersionEdit() { }\n+\n+  void Clear();\n+\n+  void SetComparatorName(const Slice& name) {\n+    has_comparator_ = true;\n+    comparator_ = name.ToString();\n+  }\n+  void SetLogNumber(uint64_t num) {\n+    has_log_number_ = true;\n+    log_number_ = num;\n+  }\n+  void SetPrevLogNumber(uint64_t num) {\n+    has_prev_log_number_ = true;\n+    prev_log_number_ = num;\n+  }\n+  void SetNextFile(uint64_t num) {\n+    has_next_file_number_ = true;\n+    next_file_number_ = num;\n+  }\n+  void SetLastSequence(SequenceNumber seq) {\n+    has_last_sequence_ = true;\n+    last_sequence_ = seq;\n+  }\n+  void SetCompactPointer(int level, const InternalKey& key) {\n+    compact_pointers_.push_back(std::make_pair(level, key));\n+  }\n+\n+  // Add the specified file at the specified number.\n+  // REQUIRES: This version has not been saved (see VersionSet::SaveTo)\n+  // REQUIRES: \"smallest\" and \"largest\" are smallest and largest keys in file\n+  void AddFile(int level, uint64_t file,\n+               uint64_t file_size,\n+               const InternalKey& smallest,\n+               const InternalKey& largest) {\n+    FileMetaData f;\n+    f.number = file;\n+    f.file_size = file_size;\n+    f.smallest = smallest;\n+    f.largest = largest;\n+    new_files_.push_back(std::make_pair(level, f));\n+  }\n+\n+  // Delete the specified \"file\" from the specified \"level\".\n+  void DeleteFile(int level, uint64_t file) {\n+    deleted_files_.insert(std::make_pair(level, file));\n+  }\n+\n+  void EncodeTo(std::string* dst) const;\n+  Status DecodeFrom(const Slice& src);\n+\n+  std::string DebugString() const;\n+\n+ private:\n+  friend class VersionSet;\n+\n+  typedef std::set< std::pair<int, uint64_t> > DeletedFileSet;\n+\n+  std::string comparator_;\n+  uint64_t log_number_;\n+  uint64_t prev_log_number_;\n+  uint64_t next_file_number_;\n+  SequenceNumber last_sequence_;\n+  bool has_comparator_;\n+  bool has_log_number_;\n+  bool has_prev_log_number_;\n+  bool has_next_file_number_;\n+  bool has_last_sequence_;\n+\n+  std::vector< std::pair<int, InternalKey> > compact_pointers_;\n+  DeletedFileSet deleted_files_;\n+  std::vector< std::pair<int, FileMetaData> > new_files_;\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_VERSION_EDIT_H_"
      },
      {
        "sha": "280310b49d846e245df0d000ca1407724582daa2",
        "filename": "db/version_edit_test.cc",
        "status": "added",
        "additions": 46,
        "deletions": 0,
        "changes": 46,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/version_edit_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/version_edit_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/version_edit_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,46 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/version_edit.h\"\n+#include \"util/testharness.h\"\n+\n+namespace leveldb {\n+\n+static void TestEncodeDecode(const VersionEdit& edit) {\n+  std::string encoded, encoded2;\n+  edit.EncodeTo(&encoded);\n+  VersionEdit parsed;\n+  Status s = parsed.DecodeFrom(encoded);\n+  ASSERT_TRUE(s.ok()) << s.ToString();\n+  parsed.EncodeTo(&encoded2);\n+  ASSERT_EQ(encoded, encoded2);\n+}\n+\n+class VersionEditTest { };\n+\n+TEST(VersionEditTest, EncodeDecode) {\n+  static const uint64_t kBig = 1ull << 50;\n+\n+  VersionEdit edit;\n+  for (int i = 0; i < 4; i++) {\n+    TestEncodeDecode(edit);\n+    edit.AddFile(3, kBig + 300 + i, kBig + 400 + i,\n+                 InternalKey(\"foo\", kBig + 500 + i, kTypeValue),\n+                 InternalKey(\"zoo\", kBig + 600 + i, kTypeDeletion));\n+    edit.DeleteFile(4, kBig + 700 + i);\n+    edit.SetCompactPointer(i, InternalKey(\"x\", kBig + 900 + i, kTypeValue));\n+  }\n+\n+  edit.SetComparatorName(\"foo\");\n+  edit.SetLogNumber(kBig + 100);\n+  edit.SetNextFile(kBig + 200);\n+  edit.SetLastSequence(kBig + 1000);\n+  TestEncodeDecode(edit);\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "4fd1ddef21011ffb379019b65ce64464ad8ca54c",
        "filename": "db/version_set.cc",
        "status": "added",
        "additions": 1443,
        "deletions": 0,
        "changes": 1443,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/version_set.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/version_set.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/version_set.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,1443 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/version_set.h\"\n+\n+#include <algorithm>\n+#include <stdio.h>\n+#include \"db/filename.h\"\n+#include \"db/log_reader.h\"\n+#include \"db/log_writer.h\"\n+#include \"db/memtable.h\"\n+#include \"db/table_cache.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/table_builder.h\"\n+#include \"table/merger.h\"\n+#include \"table/two_level_iterator.h\"\n+#include \"util/coding.h\"\n+#include \"util/logging.h\"\n+\n+namespace leveldb {\n+\n+static const int kTargetFileSize = 2 * 1048576;\n+\n+// Maximum bytes of overlaps in grandparent (i.e., level+2) before we\n+// stop building a single file in a level->level+1 compaction.\n+static const int64_t kMaxGrandParentOverlapBytes = 10 * kTargetFileSize;\n+\n+// Maximum number of bytes in all compacted files.  We avoid expanding\n+// the lower level file set of a compaction if it would make the\n+// total compaction cover more than this many bytes.\n+static const int64_t kExpandedCompactionByteSizeLimit = 25 * kTargetFileSize;\n+\n+static double MaxBytesForLevel(int level) {\n+  // Note: the result for level zero is not really used since we set\n+  // the level-0 compaction threshold based on number of files.\n+  double result = 10 * 1048576.0;  // Result for both level-0 and level-1\n+  while (level > 1) {\n+    result *= 10;\n+    level--;\n+  }\n+  return result;\n+}\n+\n+static uint64_t MaxFileSizeForLevel(int level) {\n+  return kTargetFileSize;  // We could vary per level to reduce number of files?\n+}\n+\n+static int64_t TotalFileSize(const std::vector<FileMetaData*>& files) {\n+  int64_t sum = 0;\n+  for (size_t i = 0; i < files.size(); i++) {\n+    sum += files[i]->file_size;\n+  }\n+  return sum;\n+}\n+\n+namespace {\n+std::string IntSetToString(const std::set<uint64_t>& s) {\n+  std::string result = \"{\";\n+  for (std::set<uint64_t>::const_iterator it = s.begin();\n+       it != s.end();\n+       ++it) {\n+    result += (result.size() > 1) ? \",\" : \"\";\n+    result += NumberToString(*it);\n+  }\n+  result += \"}\";\n+  return result;\n+}\n+}  // namespace\n+\n+Version::~Version() {\n+  assert(refs_ == 0);\n+\n+  // Remove from linked list\n+  prev_->next_ = next_;\n+  next_->prev_ = prev_;\n+\n+  // Drop references to files\n+  for (int level = 0; level < config::kNumLevels; level++) {\n+    for (size_t i = 0; i < files_[level].size(); i++) {\n+      FileMetaData* f = files_[level][i];\n+      assert(f->refs > 0);\n+      f->refs--;\n+      if (f->refs <= 0) {\n+        delete f;\n+      }\n+    }\n+  }\n+}\n+\n+int FindFile(const InternalKeyComparator& icmp,\n+             const std::vector<FileMetaData*>& files,\n+             const Slice& key) {\n+  uint32_t left = 0;\n+  uint32_t right = files.size();\n+  while (left < right) {\n+    uint32_t mid = (left + right) / 2;\n+    const FileMetaData* f = files[mid];\n+    if (icmp.InternalKeyComparator::Compare(f->largest.Encode(), key) < 0) {\n+      // Key at \"mid.largest\" is < \"target\".  Therefore all\n+      // files at or before \"mid\" are uninteresting.\n+      left = mid + 1;\n+    } else {\n+      // Key at \"mid.largest\" is >= \"target\".  Therefore all files\n+      // after \"mid\" are uninteresting.\n+      right = mid;\n+    }\n+  }\n+  return right;\n+}\n+\n+static bool AfterFile(const Comparator* ucmp,\n+                      const Slice* user_key, const FileMetaData* f) {\n+  // NULL user_key occurs before all keys and is therefore never after *f\n+  return (user_key != NULL &&\n+          ucmp->Compare(*user_key, f->largest.user_key()) > 0);\n+}\n+\n+static bool BeforeFile(const Comparator* ucmp,\n+                       const Slice* user_key, const FileMetaData* f) {\n+  // NULL user_key occurs after all keys and is therefore never before *f\n+  return (user_key != NULL &&\n+          ucmp->Compare(*user_key, f->smallest.user_key()) < 0);\n+}\n+\n+bool SomeFileOverlapsRange(\n+    const InternalKeyComparator& icmp,\n+    bool disjoint_sorted_files,\n+    const std::vector<FileMetaData*>& files,\n+    const Slice* smallest_user_key,\n+    const Slice* largest_user_key) {\n+  const Comparator* ucmp = icmp.user_comparator();\n+  if (!disjoint_sorted_files) {\n+    // Need to check against all files\n+    for (size_t i = 0; i < files.size(); i++) {\n+      const FileMetaData* f = files[i];\n+      if (AfterFile(ucmp, smallest_user_key, f) ||\n+          BeforeFile(ucmp, largest_user_key, f)) {\n+        // No overlap\n+      } else {\n+        return true;  // Overlap\n+      }\n+    }\n+    return false;\n+  }\n+\n+  // Binary search over file list\n+  uint32_t index = 0;\n+  if (smallest_user_key != NULL) {\n+    // Find the earliest possible internal key for smallest_user_key\n+    InternalKey small(*smallest_user_key, kMaxSequenceNumber,kValueTypeForSeek);\n+    index = FindFile(icmp, files, small.Encode());\n+  }\n+\n+  if (index >= files.size()) {\n+    // beginning of range is after all files, so no overlap.\n+    return false;\n+  }\n+\n+  return !BeforeFile(ucmp, largest_user_key, files[index]);\n+}\n+\n+// An internal iterator.  For a given version/level pair, yields\n+// information about the files in the level.  For a given entry, key()\n+// is the largest key that occurs in the file, and value() is an\n+// 16-byte value containing the file number and file size, both\n+// encoded using EncodeFixed64.\n+class Version::LevelFileNumIterator : public Iterator {\n+ public:\n+  LevelFileNumIterator(const InternalKeyComparator& icmp,\n+                       const std::vector<FileMetaData*>* flist)\n+      : icmp_(icmp),\n+        flist_(flist),\n+        index_(flist->size()) {        // Marks as invalid\n+  }\n+  virtual bool Valid() const {\n+    return index_ < flist_->size();\n+  }\n+  virtual void Seek(const Slice& target) {\n+    index_ = FindFile(icmp_, *flist_, target);\n+  }\n+  virtual void SeekToFirst() { index_ = 0; }\n+  virtual void SeekToLast() {\n+    index_ = flist_->empty() ? 0 : flist_->size() - 1;\n+  }\n+  virtual void Next() {\n+    assert(Valid());\n+    index_++;\n+  }\n+  virtual void Prev() {\n+    assert(Valid());\n+    if (index_ == 0) {\n+      index_ = flist_->size();  // Marks as invalid\n+    } else {\n+      index_--;\n+    }\n+  }\n+  Slice key() const {\n+    assert(Valid());\n+    return (*flist_)[index_]->largest.Encode();\n+  }\n+  Slice value() const {\n+    assert(Valid());\n+    EncodeFixed64(value_buf_, (*flist_)[index_]->number);\n+    EncodeFixed64(value_buf_+8, (*flist_)[index_]->file_size);\n+    return Slice(value_buf_, sizeof(value_buf_));\n+  }\n+  virtual Status status() const { return Status::OK(); }\n+ private:\n+  const InternalKeyComparator icmp_;\n+  const std::vector<FileMetaData*>* const flist_;\n+  uint32_t index_;\n+\n+  // Backing store for value().  Holds the file number and size.\n+  mutable char value_buf_[16];\n+};\n+\n+static Iterator* GetFileIterator(void* arg,\n+                                 const ReadOptions& options,\n+                                 const Slice& file_value) {\n+  TableCache* cache = reinterpret_cast<TableCache*>(arg);\n+  if (file_value.size() != 16) {\n+    return NewErrorIterator(\n+        Status::Corruption(\"FileReader invoked with unexpected value\"));\n+  } else {\n+    return cache->NewIterator(options,\n+                              DecodeFixed64(file_value.data()),\n+                              DecodeFixed64(file_value.data() + 8));\n+  }\n+}\n+\n+Iterator* Version::NewConcatenatingIterator(const ReadOptions& options,\n+                                            int level) const {\n+  return NewTwoLevelIterator(\n+      new LevelFileNumIterator(vset_->icmp_, &files_[level]),\n+      &GetFileIterator, vset_->table_cache_, options);\n+}\n+\n+void Version::AddIterators(const ReadOptions& options,\n+                           std::vector<Iterator*>* iters) {\n+  // Merge all level zero files together since they may overlap\n+  for (size_t i = 0; i < files_[0].size(); i++) {\n+    iters->push_back(\n+        vset_->table_cache_->NewIterator(\n+            options, files_[0][i]->number, files_[0][i]->file_size));\n+  }\n+\n+  // For levels > 0, we can use a concatenating iterator that sequentially\n+  // walks through the non-overlapping files in the level, opening them\n+  // lazily.\n+  for (int level = 1; level < config::kNumLevels; level++) {\n+    if (!files_[level].empty()) {\n+      iters->push_back(NewConcatenatingIterator(options, level));\n+    }\n+  }\n+}\n+\n+// Callback from TableCache::Get()\n+namespace {\n+enum SaverState {\n+  kNotFound,\n+  kFound,\n+  kDeleted,\n+  kCorrupt,\n+};\n+struct Saver {\n+  SaverState state;\n+  const Comparator* ucmp;\n+  Slice user_key;\n+  std::string* value;\n+};\n+}\n+static void SaveValue(void* arg, const Slice& ikey, const Slice& v) {\n+  Saver* s = reinterpret_cast<Saver*>(arg);\n+  ParsedInternalKey parsed_key;\n+  if (!ParseInternalKey(ikey, &parsed_key)) {\n+    s->state = kCorrupt;\n+  } else {\n+    if (s->ucmp->Compare(parsed_key.user_key, s->user_key) == 0) {\n+      s->state = (parsed_key.type == kTypeValue) ? kFound : kDeleted;\n+      if (s->state == kFound) {\n+        s->value->assign(v.data(), v.size());\n+      }\n+    }\n+  }\n+}\n+\n+static bool NewestFirst(FileMetaData* a, FileMetaData* b) {\n+  return a->number > b->number;\n+}\n+\n+Status Version::Get(const ReadOptions& options,\n+                    const LookupKey& k,\n+                    std::string* value,\n+                    GetStats* stats) {\n+  Slice ikey = k.internal_key();\n+  Slice user_key = k.user_key();\n+  const Comparator* ucmp = vset_->icmp_.user_comparator();\n+  Status s;\n+\n+  stats->seek_file = NULL;\n+  stats->seek_file_level = -1;\n+  FileMetaData* last_file_read = NULL;\n+  int last_file_read_level = -1;\n+\n+  // We can search level-by-level since entries never hop across\n+  // levels.  Therefore we are guaranteed that if we find data\n+  // in an smaller level, later levels are irrelevant.\n+  std::vector<FileMetaData*> tmp;\n+  FileMetaData* tmp2;\n+  for (int level = 0; level < config::kNumLevels; level++) {\n+    size_t num_files = files_[level].size();\n+    if (num_files == 0) continue;\n+\n+    // Get the list of files to search in this level\n+    FileMetaData* const* files = &files_[level][0];\n+    if (level == 0) {\n+      // Level-0 files may overlap each other.  Find all files that\n+      // overlap user_key and process them in order from newest to oldest.\n+      tmp.reserve(num_files);\n+      for (uint32_t i = 0; i < num_files; i++) {\n+        FileMetaData* f = files[i];\n+        if (ucmp->Compare(user_key, f->smallest.user_key()) >= 0 &&\n+            ucmp->Compare(user_key, f->largest.user_key()) <= 0) {\n+          tmp.push_back(f);\n+        }\n+      }\n+      if (tmp.empty()) continue;\n+\n+      std::sort(tmp.begin(), tmp.end(), NewestFirst);\n+      files = &tmp[0];\n+      num_files = tmp.size();\n+    } else {\n+      // Binary search to find earliest index whose largest key >= ikey.\n+      uint32_t index = FindFile(vset_->icmp_, files_[level], ikey);\n+      if (index >= num_files) {\n+        files = NULL;\n+        num_files = 0;\n+      } else {\n+        tmp2 = files[index];\n+        if (ucmp->Compare(user_key, tmp2->smallest.user_key()) < 0) {\n+          // All of \"tmp2\" is past any data for user_key\n+          files = NULL;\n+          num_files = 0;\n+        } else {\n+          files = &tmp2;\n+          num_files = 1;\n+        }\n+      }\n+    }\n+\n+    for (uint32_t i = 0; i < num_files; ++i) {\n+      if (last_file_read != NULL && stats->seek_file == NULL) {\n+        // We have had more than one seek for this read.  Charge the 1st file.\n+        stats->seek_file = last_file_read;\n+        stats->seek_file_level = last_file_read_level;\n+      }\n+\n+      FileMetaData* f = files[i];\n+      last_file_read = f;\n+      last_file_read_level = level;\n+\n+      Saver saver;\n+      saver.state = kNotFound;\n+      saver.ucmp = ucmp;\n+      saver.user_key = user_key;\n+      saver.value = value;\n+      s = vset_->table_cache_->Get(options, f->number, f->file_size,\n+                                   ikey, &saver, SaveValue);\n+      if (!s.ok()) {\n+        return s;\n+      }\n+      switch (saver.state) {\n+        case kNotFound:\n+          break;      // Keep searching in other files\n+        case kFound:\n+          return s;\n+        case kDeleted:\n+          s = Status::NotFound(Slice());  // Use empty error message for speed\n+          return s;\n+        case kCorrupt:\n+          s = Status::Corruption(\"corrupted key for \", user_key);\n+          return s;\n+      }\n+    }\n+  }\n+\n+  return Status::NotFound(Slice());  // Use an empty error message for speed\n+}\n+\n+bool Version::UpdateStats(const GetStats& stats) {\n+  FileMetaData* f = stats.seek_file;\n+  if (f != NULL) {\n+    f->allowed_seeks--;\n+    if (f->allowed_seeks <= 0 && file_to_compact_ == NULL) {\n+      file_to_compact_ = f;\n+      file_to_compact_level_ = stats.seek_file_level;\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+void Version::Ref() {\n+  ++refs_;\n+}\n+\n+void Version::Unref() {\n+  assert(this != &vset_->dummy_versions_);\n+  assert(refs_ >= 1);\n+  --refs_;\n+  if (refs_ == 0) {\n+    delete this;\n+  }\n+}\n+\n+bool Version::OverlapInLevel(int level,\n+                             const Slice* smallest_user_key,\n+                             const Slice* largest_user_key) {\n+  return SomeFileOverlapsRange(vset_->icmp_, (level > 0), files_[level],\n+                               smallest_user_key, largest_user_key);\n+}\n+\n+int Version::PickLevelForMemTableOutput(\n+    const Slice& smallest_user_key,\n+    const Slice& largest_user_key) {\n+  int level = 0;\n+  if (!OverlapInLevel(0, &smallest_user_key, &largest_user_key)) {\n+    // Push to next level if there is no overlap in next level,\n+    // and the #bytes overlapping in the level after that are limited.\n+    InternalKey start(smallest_user_key, kMaxSequenceNumber, kValueTypeForSeek);\n+    InternalKey limit(largest_user_key, 0, static_cast<ValueType>(0));\n+    std::vector<FileMetaData*> overlaps;\n+    while (level < config::kMaxMemCompactLevel) {\n+      if (OverlapInLevel(level + 1, &smallest_user_key, &largest_user_key)) {\n+        break;\n+      }\n+      GetOverlappingInputs(level + 2, &start, &limit, &overlaps);\n+      const int64_t sum = TotalFileSize(overlaps);\n+      if (sum > kMaxGrandParentOverlapBytes) {\n+        break;\n+      }\n+      level++;\n+    }\n+  }\n+  return level;\n+}\n+\n+// Store in \"*inputs\" all files in \"level\" that overlap [begin,end]\n+void Version::GetOverlappingInputs(\n+    int level,\n+    const InternalKey* begin,\n+    const InternalKey* end,\n+    std::vector<FileMetaData*>* inputs) {\n+  inputs->clear();\n+  Slice user_begin, user_end;\n+  if (begin != NULL) {\n+    user_begin = begin->user_key();\n+  }\n+  if (end != NULL) {\n+    user_end = end->user_key();\n+  }\n+  const Comparator* user_cmp = vset_->icmp_.user_comparator();\n+  for (size_t i = 0; i < files_[level].size(); ) {\n+    FileMetaData* f = files_[level][i++];\n+    const Slice file_start = f->smallest.user_key();\n+    const Slice file_limit = f->largest.user_key();\n+    if (begin != NULL && user_cmp->Compare(file_limit, user_begin) < 0) {\n+      // \"f\" is completely before specified range; skip it\n+    } else if (end != NULL && user_cmp->Compare(file_start, user_end) > 0) {\n+      // \"f\" is completely after specified range; skip it\n+    } else {\n+      inputs->push_back(f);\n+      if (level == 0) {\n+        // Level-0 files may overlap each other.  So check if the newly\n+        // added file has expanded the range.  If so, restart search.\n+        if (begin != NULL && user_cmp->Compare(file_start, user_begin) < 0) {\n+          user_begin = file_start;\n+          inputs->clear();\n+          i = 0;\n+        } else if (end != NULL && user_cmp->Compare(file_limit, user_end) > 0) {\n+          user_end = file_limit;\n+          inputs->clear();\n+          i = 0;\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+std::string Version::DebugString() const {\n+  std::string r;\n+  for (int level = 0; level < config::kNumLevels; level++) {\n+    // E.g.,\n+    //   --- level 1 ---\n+    //   17:123['a' .. 'd']\n+    //   20:43['e' .. 'g']\n+    r.append(\"--- level \");\n+    AppendNumberTo(&r, level);\n+    r.append(\" ---\\n\");\n+    const std::vector<FileMetaData*>& files = files_[level];\n+    for (size_t i = 0; i < files.size(); i++) {\n+      r.push_back(' ');\n+      AppendNumberTo(&r, files[i]->number);\n+      r.push_back(':');\n+      AppendNumberTo(&r, files[i]->file_size);\n+      r.append(\"[\");\n+      r.append(files[i]->smallest.DebugString());\n+      r.append(\" .. \");\n+      r.append(files[i]->largest.DebugString());\n+      r.append(\"]\\n\");\n+    }\n+  }\n+  return r;\n+}\n+\n+// A helper class so we can efficiently apply a whole sequence\n+// of edits to a particular state without creating intermediate\n+// Versions that contain full copies of the intermediate state.\n+class VersionSet::Builder {\n+ private:\n+  // Helper to sort by v->files_[file_number].smallest\n+  struct BySmallestKey {\n+    const InternalKeyComparator* internal_comparator;\n+\n+    bool operator()(FileMetaData* f1, FileMetaData* f2) const {\n+      int r = internal_comparator->Compare(f1->smallest, f2->smallest);\n+      if (r != 0) {\n+        return (r < 0);\n+      } else {\n+        // Break ties by file number\n+        return (f1->number < f2->number);\n+      }\n+    }\n+  };\n+\n+  typedef std::set<FileMetaData*, BySmallestKey> FileSet;\n+  struct LevelState {\n+    std::set<uint64_t> deleted_files;\n+    FileSet* added_files;\n+  };\n+\n+  VersionSet* vset_;\n+  Version* base_;\n+  LevelState levels_[config::kNumLevels];\n+\n+ public:\n+  // Initialize a builder with the files from *base and other info from *vset\n+  Builder(VersionSet* vset, Version* base)\n+      : vset_(vset),\n+        base_(base) {\n+    base_->Ref();\n+    BySmallestKey cmp;\n+    cmp.internal_comparator = &vset_->icmp_;\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      levels_[level].added_files = new FileSet(cmp);\n+    }\n+  }\n+\n+  ~Builder() {\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      const FileSet* added = levels_[level].added_files;\n+      std::vector<FileMetaData*> to_unref;\n+      to_unref.reserve(added->size());\n+      for (FileSet::const_iterator it = added->begin();\n+          it != added->end(); ++it) {\n+        to_unref.push_back(*it);\n+      }\n+      delete added;\n+      for (uint32_t i = 0; i < to_unref.size(); i++) {\n+        FileMetaData* f = to_unref[i];\n+        f->refs--;\n+        if (f->refs <= 0) {\n+          delete f;\n+        }\n+      }\n+    }\n+    base_->Unref();\n+  }\n+\n+  // Apply all of the edits in *edit to the current state.\n+  void Apply(VersionEdit* edit) {\n+    // Update compaction pointers\n+    for (size_t i = 0; i < edit->compact_pointers_.size(); i++) {\n+      const int level = edit->compact_pointers_[i].first;\n+      vset_->compact_pointer_[level] =\n+          edit->compact_pointers_[i].second.Encode().ToString();\n+    }\n+\n+    // Delete files\n+    const VersionEdit::DeletedFileSet& del = edit->deleted_files_;\n+    for (VersionEdit::DeletedFileSet::const_iterator iter = del.begin();\n+         iter != del.end();\n+         ++iter) {\n+      const int level = iter->first;\n+      const uint64_t number = iter->second;\n+      levels_[level].deleted_files.insert(number);\n+    }\n+\n+    // Add new files\n+    for (size_t i = 0; i < edit->new_files_.size(); i++) {\n+      const int level = edit->new_files_[i].first;\n+      FileMetaData* f = new FileMetaData(edit->new_files_[i].second);\n+      f->refs = 1;\n+\n+      // We arrange to automatically compact this file after\n+      // a certain number of seeks.  Let's assume:\n+      //   (1) One seek costs 10ms\n+      //   (2) Writing or reading 1MB costs 10ms (100MB/s)\n+      //   (3) A compaction of 1MB does 25MB of IO:\n+      //         1MB read from this level\n+      //         10-12MB read from next level (boundaries may be misaligned)\n+      //         10-12MB written to next level\n+      // This implies that 25 seeks cost the same as the compaction\n+      // of 1MB of data.  I.e., one seek costs approximately the\n+      // same as the compaction of 40KB of data.  We are a little\n+      // conservative and allow approximately one seek for every 16KB\n+      // of data before triggering a compaction.\n+      f->allowed_seeks = (f->file_size / 16384);\n+      if (f->allowed_seeks < 100) f->allowed_seeks = 100;\n+\n+      levels_[level].deleted_files.erase(f->number);\n+      levels_[level].added_files->insert(f);\n+    }\n+  }\n+\n+  // Save the current state in *v.\n+  void SaveTo(Version* v) {\n+    BySmallestKey cmp;\n+    cmp.internal_comparator = &vset_->icmp_;\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      // Merge the set of added files with the set of pre-existing files.\n+      // Drop any deleted files.  Store the result in *v.\n+      const std::vector<FileMetaData*>& base_files = base_->files_[level];\n+      std::vector<FileMetaData*>::const_iterator base_iter = base_files.begin();\n+      std::vector<FileMetaData*>::const_iterator base_end = base_files.end();\n+      const FileSet* added = levels_[level].added_files;\n+      v->files_[level].reserve(base_files.size() + added->size());\n+      for (FileSet::const_iterator added_iter = added->begin();\n+           added_iter != added->end();\n+           ++added_iter) {\n+        // Add all smaller files listed in base_\n+        for (std::vector<FileMetaData*>::const_iterator bpos\n+                 = std::upper_bound(base_iter, base_end, *added_iter, cmp);\n+             base_iter != bpos;\n+             ++base_iter) {\n+          MaybeAddFile(v, level, *base_iter);\n+        }\n+\n+        MaybeAddFile(v, level, *added_iter);\n+      }\n+\n+      // Add remaining base files\n+      for (; base_iter != base_end; ++base_iter) {\n+        MaybeAddFile(v, level, *base_iter);\n+      }\n+\n+#ifndef NDEBUG\n+      // Make sure there is no overlap in levels > 0\n+      if (level > 0) {\n+        for (uint32_t i = 1; i < v->files_[level].size(); i++) {\n+          const InternalKey& prev_end = v->files_[level][i-1]->largest;\n+          const InternalKey& this_begin = v->files_[level][i]->smallest;\n+          if (vset_->icmp_.Compare(prev_end, this_begin) >= 0) {\n+            fprintf(stderr, \"overlapping ranges in same level %s vs. %s\\n\",\n+                    prev_end.DebugString().c_str(),\n+                    this_begin.DebugString().c_str());\n+            abort();\n+          }\n+        }\n+      }\n+#endif\n+    }\n+  }\n+\n+  void MaybeAddFile(Version* v, int level, FileMetaData* f) {\n+    if (levels_[level].deleted_files.count(f->number) > 0) {\n+      // File is deleted: do nothing\n+    } else {\n+      std::vector<FileMetaData*>* files = &v->files_[level];\n+      if (level > 0 && !files->empty()) {\n+        // Must not overlap\n+        assert(vset_->icmp_.Compare((*files)[files->size()-1]->largest,\n+                                    f->smallest) < 0);\n+      }\n+      f->refs++;\n+      files->push_back(f);\n+    }\n+  }\n+};\n+\n+VersionSet::VersionSet(const std::string& dbname,\n+                       const Options* options,\n+                       TableCache* table_cache,\n+                       const InternalKeyComparator* cmp)\n+    : env_(options->env),\n+      dbname_(dbname),\n+      options_(options),\n+      table_cache_(table_cache),\n+      icmp_(*cmp),\n+      next_file_number_(2),\n+      manifest_file_number_(0),  // Filled by Recover()\n+      last_sequence_(0),\n+      log_number_(0),\n+      prev_log_number_(0),\n+      descriptor_file_(NULL),\n+      descriptor_log_(NULL),\n+      dummy_versions_(this),\n+      current_(NULL) {\n+  AppendVersion(new Version(this));\n+}\n+\n+VersionSet::~VersionSet() {\n+  current_->Unref();\n+  assert(dummy_versions_.next_ == &dummy_versions_);  // List must be empty\n+  delete descriptor_log_;\n+  delete descriptor_file_;\n+}\n+\n+void VersionSet::AppendVersion(Version* v) {\n+  // Make \"v\" current\n+  assert(v->refs_ == 0);\n+  assert(v != current_);\n+  if (current_ != NULL) {\n+    current_->Unref();\n+  }\n+  current_ = v;\n+  v->Ref();\n+\n+  // Append to linked list\n+  v->prev_ = dummy_versions_.prev_;\n+  v->next_ = &dummy_versions_;\n+  v->prev_->next_ = v;\n+  v->next_->prev_ = v;\n+}\n+\n+Status VersionSet::LogAndApply(VersionEdit* edit, port::Mutex* mu) {\n+  if (edit->has_log_number_) {\n+    assert(edit->log_number_ >= log_number_);\n+    assert(edit->log_number_ < next_file_number_);\n+  } else {\n+    edit->SetLogNumber(log_number_);\n+  }\n+\n+  if (!edit->has_prev_log_number_) {\n+    edit->SetPrevLogNumber(prev_log_number_);\n+  }\n+\n+  edit->SetNextFile(next_file_number_);\n+  edit->SetLastSequence(last_sequence_);\n+\n+  Version* v = new Version(this);\n+  {\n+    Builder builder(this, current_);\n+    builder.Apply(edit);\n+    builder.SaveTo(v);\n+  }\n+  Finalize(v);\n+\n+  // Initialize new descriptor log file if necessary by creating\n+  // a temporary file that contains a snapshot of the current version.\n+  std::string new_manifest_file;\n+  Status s;\n+  if (descriptor_log_ == NULL) {\n+    // No reason to unlock *mu here since we only hit this path in the\n+    // first call to LogAndApply (when opening the database).\n+    assert(descriptor_file_ == NULL);\n+    new_manifest_file = DescriptorFileName(dbname_, manifest_file_number_);\n+    edit->SetNextFile(next_file_number_);\n+    s = env_->NewWritableFile(new_manifest_file, &descriptor_file_);\n+    if (s.ok()) {\n+      descriptor_log_ = new log::Writer(descriptor_file_);\n+      s = WriteSnapshot(descriptor_log_);\n+    }\n+  }\n+\n+  // Unlock during expensive MANIFEST log write\n+  {\n+    mu->Unlock();\n+\n+    // Write new record to MANIFEST log\n+    if (s.ok()) {\n+      std::string record;\n+      edit->EncodeTo(&record);\n+      s = descriptor_log_->AddRecord(record);\n+      if (s.ok()) {\n+        s = descriptor_file_->Sync();\n+      }\n+      if (!s.ok()) {\n+        Log(options_->info_log, \"MANIFEST write: %s\\n\", s.ToString().c_str());\n+        if (ManifestContains(record)) {\n+          Log(options_->info_log,\n+              \"MANIFEST contains log record despite error; advancing to new \"\n+              \"version to prevent mismatch between in-memory and logged state\");\n+          s = Status::OK();\n+        }\n+      }\n+    }\n+\n+    // If we just created a new descriptor file, install it by writing a\n+    // new CURRENT file that points to it.\n+    if (s.ok() && !new_manifest_file.empty()) {\n+      s = SetCurrentFile(env_, dbname_, manifest_file_number_);\n+      // No need to double-check MANIFEST in case of error since it\n+      // will be discarded below.\n+    }\n+\n+    mu->Lock();\n+  }\n+\n+  // Install the new version\n+  if (s.ok()) {\n+    AppendVersion(v);\n+    log_number_ = edit->log_number_;\n+    prev_log_number_ = edit->prev_log_number_;\n+  } else {\n+    delete v;\n+    if (!new_manifest_file.empty()) {\n+      delete descriptor_log_;\n+      delete descriptor_file_;\n+      descriptor_log_ = NULL;\n+      descriptor_file_ = NULL;\n+      env_->DeleteFile(new_manifest_file);\n+    }\n+  }\n+\n+  return s;\n+}\n+\n+Status VersionSet::Recover() {\n+  struct LogReporter : public log::Reader::Reporter {\n+    Status* status;\n+    virtual void Corruption(size_t bytes, const Status& s) {\n+      if (this->status->ok()) *this->status = s;\n+    }\n+  };\n+\n+  // Read \"CURRENT\" file, which contains a pointer to the current manifest file\n+  std::string current;\n+  Status s = ReadFileToString(env_, CurrentFileName(dbname_), &current);\n+  if (!s.ok()) {\n+    return s;\n+  }\n+  if (current.empty() || current[current.size()-1] != '\\n') {\n+    return Status::Corruption(\"CURRENT file does not end with newline\");\n+  }\n+  current.resize(current.size() - 1);\n+\n+  std::string dscname = dbname_ + \"/\" + current;\n+  SequentialFile* file;\n+  s = env_->NewSequentialFile(dscname, &file);\n+  if (!s.ok()) {\n+    return s;\n+  }\n+\n+  bool have_log_number = false;\n+  bool have_prev_log_number = false;\n+  bool have_next_file = false;\n+  bool have_last_sequence = false;\n+  uint64_t next_file = 0;\n+  uint64_t last_sequence = 0;\n+  uint64_t log_number = 0;\n+  uint64_t prev_log_number = 0;\n+  Builder builder(this, current_);\n+\n+  {\n+    LogReporter reporter;\n+    reporter.status = &s;\n+    log::Reader reader(file, &reporter, true/*checksum*/, 0/*initial_offset*/);\n+    Slice record;\n+    std::string scratch;\n+    while (reader.ReadRecord(&record, &scratch) && s.ok()) {\n+      VersionEdit edit;\n+      s = edit.DecodeFrom(record);\n+      if (s.ok()) {\n+        if (edit.has_comparator_ &&\n+            edit.comparator_ != icmp_.user_comparator()->Name()) {\n+          s = Status::InvalidArgument(\n+              edit.comparator_ + \" does not match existing comparator \",\n+              icmp_.user_comparator()->Name());\n+        }\n+      }\n+\n+      if (s.ok()) {\n+        builder.Apply(&edit);\n+      }\n+\n+      if (edit.has_log_number_) {\n+        log_number = edit.log_number_;\n+        have_log_number = true;\n+      }\n+\n+      if (edit.has_prev_log_number_) {\n+        prev_log_number = edit.prev_log_number_;\n+        have_prev_log_number = true;\n+      }\n+\n+      if (edit.has_next_file_number_) {\n+        next_file = edit.next_file_number_;\n+        have_next_file = true;\n+      }\n+\n+      if (edit.has_last_sequence_) {\n+        last_sequence = edit.last_sequence_;\n+        have_last_sequence = true;\n+      }\n+    }\n+  }\n+  delete file;\n+  file = NULL;\n+\n+  if (s.ok()) {\n+    if (!have_next_file) {\n+      s = Status::Corruption(\"no meta-nextfile entry in descriptor\");\n+    } else if (!have_log_number) {\n+      s = Status::Corruption(\"no meta-lognumber entry in descriptor\");\n+    } else if (!have_last_sequence) {\n+      s = Status::Corruption(\"no last-sequence-number entry in descriptor\");\n+    }\n+\n+    if (!have_prev_log_number) {\n+      prev_log_number = 0;\n+    }\n+\n+    MarkFileNumberUsed(prev_log_number);\n+    MarkFileNumberUsed(log_number);\n+  }\n+\n+  if (s.ok()) {\n+    Version* v = new Version(this);\n+    builder.SaveTo(v);\n+    // Install recovered version\n+    Finalize(v);\n+    AppendVersion(v);\n+    manifest_file_number_ = next_file;\n+    next_file_number_ = next_file + 1;\n+    last_sequence_ = last_sequence;\n+    log_number_ = log_number;\n+    prev_log_number_ = prev_log_number;\n+  }\n+\n+  return s;\n+}\n+\n+void VersionSet::MarkFileNumberUsed(uint64_t number) {\n+  if (next_file_number_ <= number) {\n+    next_file_number_ = number + 1;\n+  }\n+}\n+\n+void VersionSet::Finalize(Version* v) {\n+  // Precomputed best level for next compaction\n+  int best_level = -1;\n+  double best_score = -1;\n+\n+  for (int level = 0; level < config::kNumLevels-1; level++) {\n+    double score;\n+    if (level == 0) {\n+      // We treat level-0 specially by bounding the number of files\n+      // instead of number of bytes for two reasons:\n+      //\n+      // (1) With larger write-buffer sizes, it is nice not to do too\n+      // many level-0 compactions.\n+      //\n+      // (2) The files in level-0 are merged on every read and\n+      // therefore we wish to avoid too many files when the individual\n+      // file size is small (perhaps because of a small write-buffer\n+      // setting, or very high compression ratios, or lots of\n+      // overwrites/deletions).\n+      score = v->files_[level].size() /\n+          static_cast<double>(config::kL0_CompactionTrigger);\n+    } else {\n+      // Compute the ratio of current size to size limit.\n+      const uint64_t level_bytes = TotalFileSize(v->files_[level]);\n+      score = static_cast<double>(level_bytes) / MaxBytesForLevel(level);\n+    }\n+\n+    if (score > best_score) {\n+      best_level = level;\n+      best_score = score;\n+    }\n+  }\n+\n+  v->compaction_level_ = best_level;\n+  v->compaction_score_ = best_score;\n+}\n+\n+Status VersionSet::WriteSnapshot(log::Writer* log) {\n+  // TODO: Break up into multiple records to reduce memory usage on recovery?\n+\n+  // Save metadata\n+  VersionEdit edit;\n+  edit.SetComparatorName(icmp_.user_comparator()->Name());\n+\n+  // Save compaction pointers\n+  for (int level = 0; level < config::kNumLevels; level++) {\n+    if (!compact_pointer_[level].empty()) {\n+      InternalKey key;\n+      key.DecodeFrom(compact_pointer_[level]);\n+      edit.SetCompactPointer(level, key);\n+    }\n+  }\n+\n+  // Save files\n+  for (int level = 0; level < config::kNumLevels; level++) {\n+    const std::vector<FileMetaData*>& files = current_->files_[level];\n+    for (size_t i = 0; i < files.size(); i++) {\n+      const FileMetaData* f = files[i];\n+      edit.AddFile(level, f->number, f->file_size, f->smallest, f->largest);\n+    }\n+  }\n+\n+  std::string record;\n+  edit.EncodeTo(&record);\n+  return log->AddRecord(record);\n+}\n+\n+int VersionSet::NumLevelFiles(int level) const {\n+  assert(level >= 0);\n+  assert(level < config::kNumLevels);\n+  return current_->files_[level].size();\n+}\n+\n+const char* VersionSet::LevelSummary(LevelSummaryStorage* scratch) const {\n+  // Update code if kNumLevels changes\n+  assert(config::kNumLevels == 7);\n+  snprintf(scratch->buffer, sizeof(scratch->buffer),\n+           \"files[ %d %d %d %d %d %d %d ]\",\n+           int(current_->files_[0].size()),\n+           int(current_->files_[1].size()),\n+           int(current_->files_[2].size()),\n+           int(current_->files_[3].size()),\n+           int(current_->files_[4].size()),\n+           int(current_->files_[5].size()),\n+           int(current_->files_[6].size()));\n+  return scratch->buffer;\n+}\n+\n+// Return true iff the manifest contains the specified record.\n+bool VersionSet::ManifestContains(const std::string& record) const {\n+  std::string fname = DescriptorFileName(dbname_, manifest_file_number_);\n+  Log(options_->info_log, \"ManifestContains: checking %s\\n\", fname.c_str());\n+  SequentialFile* file = NULL;\n+  Status s = env_->NewSequentialFile(fname, &file);\n+  if (!s.ok()) {\n+    Log(options_->info_log, \"ManifestContains: %s\\n\", s.ToString().c_str());\n+    return false;\n+  }\n+  log::Reader reader(file, NULL, true/*checksum*/, 0);\n+  Slice r;\n+  std::string scratch;\n+  bool result = false;\n+  while (reader.ReadRecord(&r, &scratch)) {\n+    if (r == Slice(record)) {\n+      result = true;\n+      break;\n+    }\n+  }\n+  delete file;\n+  Log(options_->info_log, \"ManifestContains: result = %d\\n\", result ? 1 : 0);\n+  return result;\n+}\n+\n+uint64_t VersionSet::ApproximateOffsetOf(Version* v, const InternalKey& ikey) {\n+  uint64_t result = 0;\n+  for (int level = 0; level < config::kNumLevels; level++) {\n+    const std::vector<FileMetaData*>& files = v->files_[level];\n+    for (size_t i = 0; i < files.size(); i++) {\n+      if (icmp_.Compare(files[i]->largest, ikey) <= 0) {\n+        // Entire file is before \"ikey\", so just add the file size\n+        result += files[i]->file_size;\n+      } else if (icmp_.Compare(files[i]->smallest, ikey) > 0) {\n+        // Entire file is after \"ikey\", so ignore\n+        if (level > 0) {\n+          // Files other than level 0 are sorted by meta->smallest, so\n+          // no further files in this level will contain data for\n+          // \"ikey\".\n+          break;\n+        }\n+      } else {\n+        // \"ikey\" falls in the range for this table.  Add the\n+        // approximate offset of \"ikey\" within the table.\n+        Table* tableptr;\n+        Iterator* iter = table_cache_->NewIterator(\n+            ReadOptions(), files[i]->number, files[i]->file_size, &tableptr);\n+        if (tableptr != NULL) {\n+          result += tableptr->ApproximateOffsetOf(ikey.Encode());\n+        }\n+        delete iter;\n+      }\n+    }\n+  }\n+  return result;\n+}\n+\n+void VersionSet::AddLiveFiles(std::set<uint64_t>* live) {\n+  for (Version* v = dummy_versions_.next_;\n+       v != &dummy_versions_;\n+       v = v->next_) {\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      const std::vector<FileMetaData*>& files = v->files_[level];\n+      for (size_t i = 0; i < files.size(); i++) {\n+        live->insert(files[i]->number);\n+      }\n+    }\n+  }\n+}\n+\n+int64_t VersionSet::NumLevelBytes(int level) const {\n+  assert(level >= 0);\n+  assert(level < config::kNumLevels);\n+  return TotalFileSize(current_->files_[level]);\n+}\n+\n+int64_t VersionSet::MaxNextLevelOverlappingBytes() {\n+  int64_t result = 0;\n+  std::vector<FileMetaData*> overlaps;\n+  for (int level = 1; level < config::kNumLevels - 1; level++) {\n+    for (size_t i = 0; i < current_->files_[level].size(); i++) {\n+      const FileMetaData* f = current_->files_[level][i];\n+      current_->GetOverlappingInputs(level+1, &f->smallest, &f->largest,\n+                                     &overlaps);\n+      const int64_t sum = TotalFileSize(overlaps);\n+      if (sum > result) {\n+        result = sum;\n+      }\n+    }\n+  }\n+  return result;\n+}\n+\n+// Stores the minimal range that covers all entries in inputs in\n+// *smallest, *largest.\n+// REQUIRES: inputs is not empty\n+void VersionSet::GetRange(const std::vector<FileMetaData*>& inputs,\n+                          InternalKey* smallest,\n+                          InternalKey* largest) {\n+  assert(!inputs.empty());\n+  smallest->Clear();\n+  largest->Clear();\n+  for (size_t i = 0; i < inputs.size(); i++) {\n+    FileMetaData* f = inputs[i];\n+    if (i == 0) {\n+      *smallest = f->smallest;\n+      *largest = f->largest;\n+    } else {\n+      if (icmp_.Compare(f->smallest, *smallest) < 0) {\n+        *smallest = f->smallest;\n+      }\n+      if (icmp_.Compare(f->largest, *largest) > 0) {\n+        *largest = f->largest;\n+      }\n+    }\n+  }\n+}\n+\n+// Stores the minimal range that covers all entries in inputs1 and inputs2\n+// in *smallest, *largest.\n+// REQUIRES: inputs is not empty\n+void VersionSet::GetRange2(const std::vector<FileMetaData*>& inputs1,\n+                           const std::vector<FileMetaData*>& inputs2,\n+                           InternalKey* smallest,\n+                           InternalKey* largest) {\n+  std::vector<FileMetaData*> all = inputs1;\n+  all.insert(all.end(), inputs2.begin(), inputs2.end());\n+  GetRange(all, smallest, largest);\n+}\n+\n+Iterator* VersionSet::MakeInputIterator(Compaction* c) {\n+  ReadOptions options;\n+  options.verify_checksums = options_->paranoid_checks;\n+  options.fill_cache = false;\n+\n+  // Level-0 files have to be merged together.  For other levels,\n+  // we will make a concatenating iterator per level.\n+  // TODO(opt): use concatenating iterator for level-0 if there is no overlap\n+  const int space = (c->level() == 0 ? c->inputs_[0].size() + 1 : 2);\n+  Iterator** list = new Iterator*[space];\n+  int num = 0;\n+  for (int which = 0; which < 2; which++) {\n+    if (!c->inputs_[which].empty()) {\n+      if (c->level() + which == 0) {\n+        const std::vector<FileMetaData*>& files = c->inputs_[which];\n+        for (size_t i = 0; i < files.size(); i++) {\n+          list[num++] = table_cache_->NewIterator(\n+              options, files[i]->number, files[i]->file_size);\n+        }\n+      } else {\n+        // Create concatenating iterator for the files from this level\n+        list[num++] = NewTwoLevelIterator(\n+            new Version::LevelFileNumIterator(icmp_, &c->inputs_[which]),\n+            &GetFileIterator, table_cache_, options);\n+      }\n+    }\n+  }\n+  assert(num <= space);\n+  Iterator* result = NewMergingIterator(&icmp_, list, num);\n+  delete[] list;\n+  return result;\n+}\n+\n+Compaction* VersionSet::PickCompaction() {\n+  Compaction* c;\n+  int level;\n+\n+  // We prefer compactions triggered by too much data in a level over\n+  // the compactions triggered by seeks.\n+  const bool size_compaction = (current_->compaction_score_ >= 1);\n+  const bool seek_compaction = (current_->file_to_compact_ != NULL);\n+  if (size_compaction) {\n+    level = current_->compaction_level_;\n+    assert(level >= 0);\n+    assert(level+1 < config::kNumLevels);\n+    c = new Compaction(level);\n+\n+    // Pick the first file that comes after compact_pointer_[level]\n+    for (size_t i = 0; i < current_->files_[level].size(); i++) {\n+      FileMetaData* f = current_->files_[level][i];\n+      if (compact_pointer_[level].empty() ||\n+          icmp_.Compare(f->largest.Encode(), compact_pointer_[level]) > 0) {\n+        c->inputs_[0].push_back(f);\n+        break;\n+      }\n+    }\n+    if (c->inputs_[0].empty()) {\n+      // Wrap-around to the beginning of the key space\n+      c->inputs_[0].push_back(current_->files_[level][0]);\n+    }\n+  } else if (seek_compaction) {\n+    level = current_->file_to_compact_level_;\n+    c = new Compaction(level);\n+    c->inputs_[0].push_back(current_->file_to_compact_);\n+  } else {\n+    return NULL;\n+  }\n+\n+  c->input_version_ = current_;\n+  c->input_version_->Ref();\n+\n+  // Files in level 0 may overlap each other, so pick up all overlapping ones\n+  if (level == 0) {\n+    InternalKey smallest, largest;\n+    GetRange(c->inputs_[0], &smallest, &largest);\n+    // Note that the next call will discard the file we placed in\n+    // c->inputs_[0] earlier and replace it with an overlapping set\n+    // which will include the picked file.\n+    current_->GetOverlappingInputs(0, &smallest, &largest, &c->inputs_[0]);\n+    assert(!c->inputs_[0].empty());\n+  }\n+\n+  SetupOtherInputs(c);\n+\n+  return c;\n+}\n+\n+void VersionSet::SetupOtherInputs(Compaction* c) {\n+  const int level = c->level();\n+  InternalKey smallest, largest;\n+  GetRange(c->inputs_[0], &smallest, &largest);\n+\n+  current_->GetOverlappingInputs(level+1, &smallest, &largest, &c->inputs_[1]);\n+\n+  // Get entire range covered by compaction\n+  InternalKey all_start, all_limit;\n+  GetRange2(c->inputs_[0], c->inputs_[1], &all_start, &all_limit);\n+\n+  // See if we can grow the number of inputs in \"level\" without\n+  // changing the number of \"level+1\" files we pick up.\n+  if (!c->inputs_[1].empty()) {\n+    std::vector<FileMetaData*> expanded0;\n+    current_->GetOverlappingInputs(level, &all_start, &all_limit, &expanded0);\n+    const int64_t inputs0_size = TotalFileSize(c->inputs_[0]);\n+    const int64_t inputs1_size = TotalFileSize(c->inputs_[1]);\n+    const int64_t expanded0_size = TotalFileSize(expanded0);\n+    if (expanded0.size() > c->inputs_[0].size() &&\n+        inputs1_size + expanded0_size < kExpandedCompactionByteSizeLimit) {\n+      InternalKey new_start, new_limit;\n+      GetRange(expanded0, &new_start, &new_limit);\n+      std::vector<FileMetaData*> expanded1;\n+      current_->GetOverlappingInputs(level+1, &new_start, &new_limit,\n+                                     &expanded1);\n+      if (expanded1.size() == c->inputs_[1].size()) {\n+        Log(options_->info_log,\n+            \"Expanding@%d %d+%d (%ld+%ld bytes) to %d+%d (%ld+%ld bytes)\\n\",\n+            level,\n+            int(c->inputs_[0].size()),\n+            int(c->inputs_[1].size()),\n+            long(inputs0_size), long(inputs1_size),\n+            int(expanded0.size()),\n+            int(expanded1.size()),\n+            long(expanded0_size), long(inputs1_size));\n+        smallest = new_start;\n+        largest = new_limit;\n+        c->inputs_[0] = expanded0;\n+        c->inputs_[1] = expanded1;\n+        GetRange2(c->inputs_[0], c->inputs_[1], &all_start, &all_limit);\n+      }\n+    }\n+  }\n+\n+  // Compute the set of grandparent files that overlap this compaction\n+  // (parent == level+1; grandparent == level+2)\n+  if (level + 2 < config::kNumLevels) {\n+    current_->GetOverlappingInputs(level + 2, &all_start, &all_limit,\n+                                   &c->grandparents_);\n+  }\n+\n+  if (false) {\n+    Log(options_->info_log, \"Compacting %d '%s' .. '%s'\",\n+        level,\n+        smallest.DebugString().c_str(),\n+        largest.DebugString().c_str());\n+  }\n+\n+  // Update the place where we will do the next compaction for this level.\n+  // We update this immediately instead of waiting for the VersionEdit\n+  // to be applied so that if the compaction fails, we will try a different\n+  // key range next time.\n+  compact_pointer_[level] = largest.Encode().ToString();\n+  c->edit_.SetCompactPointer(level, largest);\n+}\n+\n+Compaction* VersionSet::CompactRange(\n+    int level,\n+    const InternalKey* begin,\n+    const InternalKey* end) {\n+  std::vector<FileMetaData*> inputs;\n+  current_->GetOverlappingInputs(level, begin, end, &inputs);\n+  if (inputs.empty()) {\n+    return NULL;\n+  }\n+\n+  // Avoid compacting too much in one shot in case the range is large.\n+  // But we cannot do this for level-0 since level-0 files can overlap\n+  // and we must not pick one file and drop another older file if the\n+  // two files overlap.\n+  if (level > 0) {\n+    const uint64_t limit = MaxFileSizeForLevel(level);\n+    uint64_t total = 0;\n+    for (size_t i = 0; i < inputs.size(); i++) {\n+      uint64_t s = inputs[i]->file_size;\n+      total += s;\n+      if (total >= limit) {\n+        inputs.resize(i + 1);\n+        break;\n+      }\n+    }\n+  }\n+\n+  Compaction* c = new Compaction(level);\n+  c->input_version_ = current_;\n+  c->input_version_->Ref();\n+  c->inputs_[0] = inputs;\n+  SetupOtherInputs(c);\n+  return c;\n+}\n+\n+Compaction::Compaction(int level)\n+    : level_(level),\n+      max_output_file_size_(MaxFileSizeForLevel(level)),\n+      input_version_(NULL),\n+      grandparent_index_(0),\n+      seen_key_(false),\n+      overlapped_bytes_(0) {\n+  for (int i = 0; i < config::kNumLevels; i++) {\n+    level_ptrs_[i] = 0;\n+  }\n+}\n+\n+Compaction::~Compaction() {\n+  if (input_version_ != NULL) {\n+    input_version_->Unref();\n+  }\n+}\n+\n+bool Compaction::IsTrivialMove() const {\n+  // Avoid a move if there is lots of overlapping grandparent data.\n+  // Otherwise, the move could create a parent file that will require\n+  // a very expensive merge later on.\n+  return (num_input_files(0) == 1 &&\n+          num_input_files(1) == 0 &&\n+          TotalFileSize(grandparents_) <= kMaxGrandParentOverlapBytes);\n+}\n+\n+void Compaction::AddInputDeletions(VersionEdit* edit) {\n+  for (int which = 0; which < 2; which++) {\n+    for (size_t i = 0; i < inputs_[which].size(); i++) {\n+      edit->DeleteFile(level_ + which, inputs_[which][i]->number);\n+    }\n+  }\n+}\n+\n+bool Compaction::IsBaseLevelForKey(const Slice& user_key) {\n+  // Maybe use binary search to find right entry instead of linear search?\n+  const Comparator* user_cmp = input_version_->vset_->icmp_.user_comparator();\n+  for (int lvl = level_ + 2; lvl < config::kNumLevels; lvl++) {\n+    const std::vector<FileMetaData*>& files = input_version_->files_[lvl];\n+    for (; level_ptrs_[lvl] < files.size(); ) {\n+      FileMetaData* f = files[level_ptrs_[lvl]];\n+      if (user_cmp->Compare(user_key, f->largest.user_key()) <= 0) {\n+        // We've advanced far enough\n+        if (user_cmp->Compare(user_key, f->smallest.user_key()) >= 0) {\n+          // Key falls in this file's range, so definitely not base level\n+          return false;\n+        }\n+        break;\n+      }\n+      level_ptrs_[lvl]++;\n+    }\n+  }\n+  return true;\n+}\n+\n+bool Compaction::ShouldStopBefore(const Slice& internal_key) {\n+  // Scan to find earliest grandparent file that contains key.\n+  const InternalKeyComparator* icmp = &input_version_->vset_->icmp_;\n+  while (grandparent_index_ < grandparents_.size() &&\n+      icmp->Compare(internal_key,\n+                    grandparents_[grandparent_index_]->largest.Encode()) > 0) {\n+    if (seen_key_) {\n+      overlapped_bytes_ += grandparents_[grandparent_index_]->file_size;\n+    }\n+    grandparent_index_++;\n+  }\n+  seen_key_ = true;\n+\n+  if (overlapped_bytes_ > kMaxGrandParentOverlapBytes) {\n+    // Too much overlap for current output; start new output\n+    overlapped_bytes_ = 0;\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+void Compaction::ReleaseInputs() {\n+  if (input_version_ != NULL) {\n+    input_version_->Unref();\n+    input_version_ = NULL;\n+  }\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "9d084fdb7d00574c627291b1a48e059a8093d5b4",
        "filename": "db/version_set.h",
        "status": "added",
        "additions": 383,
        "deletions": 0,
        "changes": 383,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/version_set.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/version_set.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/version_set.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,383 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// The representation of a DBImpl consists of a set of Versions.  The\n+// newest version is called \"current\".  Older versions may be kept\n+// around to provide a consistent view to live iterators.\n+//\n+// Each Version keeps track of a set of Table files per level.  The\n+// entire set of versions is maintained in a VersionSet.\n+//\n+// Version,VersionSet are thread-compatible, but require external\n+// synchronization on all accesses.\n+\n+#ifndef STORAGE_LEVELDB_DB_VERSION_SET_H_\n+#define STORAGE_LEVELDB_DB_VERSION_SET_H_\n+\n+#include <map>\n+#include <set>\n+#include <vector>\n+#include \"db/dbformat.h\"\n+#include \"db/version_edit.h\"\n+#include \"port/port.h\"\n+#include \"port/thread_annotations.h\"\n+\n+namespace leveldb {\n+\n+namespace log { class Writer; }\n+\n+class Compaction;\n+class Iterator;\n+class MemTable;\n+class TableBuilder;\n+class TableCache;\n+class Version;\n+class VersionSet;\n+class WritableFile;\n+\n+// Return the smallest index i such that files[i]->largest >= key.\n+// Return files.size() if there is no such file.\n+// REQUIRES: \"files\" contains a sorted list of non-overlapping files.\n+extern int FindFile(const InternalKeyComparator& icmp,\n+                    const std::vector<FileMetaData*>& files,\n+                    const Slice& key);\n+\n+// Returns true iff some file in \"files\" overlaps the user key range\n+// [*smallest,*largest].\n+// smallest==NULL represents a key smaller than all keys in the DB.\n+// largest==NULL represents a key largest than all keys in the DB.\n+// REQUIRES: If disjoint_sorted_files, files[] contains disjoint ranges\n+//           in sorted order.\n+extern bool SomeFileOverlapsRange(\n+    const InternalKeyComparator& icmp,\n+    bool disjoint_sorted_files,\n+    const std::vector<FileMetaData*>& files,\n+    const Slice* smallest_user_key,\n+    const Slice* largest_user_key);\n+\n+class Version {\n+ public:\n+  // Append to *iters a sequence of iterators that will\n+  // yield the contents of this Version when merged together.\n+  // REQUIRES: This version has been saved (see VersionSet::SaveTo)\n+  void AddIterators(const ReadOptions&, std::vector<Iterator*>* iters);\n+\n+  // Lookup the value for key.  If found, store it in *val and\n+  // return OK.  Else return a non-OK status.  Fills *stats.\n+  // REQUIRES: lock is not held\n+  struct GetStats {\n+    FileMetaData* seek_file;\n+    int seek_file_level;\n+  };\n+  Status Get(const ReadOptions&, const LookupKey& key, std::string* val,\n+             GetStats* stats);\n+\n+  // Adds \"stats\" into the current state.  Returns true if a new\n+  // compaction may need to be triggered, false otherwise.\n+  // REQUIRES: lock is held\n+  bool UpdateStats(const GetStats& stats);\n+\n+  // Reference count management (so Versions do not disappear out from\n+  // under live iterators)\n+  void Ref();\n+  void Unref();\n+\n+  void GetOverlappingInputs(\n+      int level,\n+      const InternalKey* begin,         // NULL means before all keys\n+      const InternalKey* end,           // NULL means after all keys\n+      std::vector<FileMetaData*>* inputs);\n+\n+  // Returns true iff some file in the specified level overlaps\n+  // some part of [*smallest_user_key,*largest_user_key].\n+  // smallest_user_key==NULL represents a key smaller than all keys in the DB.\n+  // largest_user_key==NULL represents a key largest than all keys in the DB.\n+  bool OverlapInLevel(int level,\n+                      const Slice* smallest_user_key,\n+                      const Slice* largest_user_key);\n+\n+  // Return the level at which we should place a new memtable compaction\n+  // result that covers the range [smallest_user_key,largest_user_key].\n+  int PickLevelForMemTableOutput(const Slice& smallest_user_key,\n+                                 const Slice& largest_user_key);\n+\n+  int NumFiles(int level) const { return files_[level].size(); }\n+\n+  // Return a human readable string that describes this version's contents.\n+  std::string DebugString() const;\n+\n+ private:\n+  friend class Compaction;\n+  friend class VersionSet;\n+\n+  class LevelFileNumIterator;\n+  Iterator* NewConcatenatingIterator(const ReadOptions&, int level) const;\n+\n+  VersionSet* vset_;            // VersionSet to which this Version belongs\n+  Version* next_;               // Next version in linked list\n+  Version* prev_;               // Previous version in linked list\n+  int refs_;                    // Number of live refs to this version\n+\n+  // List of files per level\n+  std::vector<FileMetaData*> files_[config::kNumLevels];\n+\n+  // Next file to compact based on seek stats.\n+  FileMetaData* file_to_compact_;\n+  int file_to_compact_level_;\n+\n+  // Level that should be compacted next and its compaction score.\n+  // Score < 1 means compaction is not strictly needed.  These fields\n+  // are initialized by Finalize().\n+  double compaction_score_;\n+  int compaction_level_;\n+\n+  explicit Version(VersionSet* vset)\n+      : vset_(vset), next_(this), prev_(this), refs_(0),\n+        file_to_compact_(NULL),\n+        file_to_compact_level_(-1),\n+        compaction_score_(-1),\n+        compaction_level_(-1) {\n+  }\n+\n+  ~Version();\n+\n+  // No copying allowed\n+  Version(const Version&);\n+  void operator=(const Version&);\n+};\n+\n+class VersionSet {\n+ public:\n+  VersionSet(const std::string& dbname,\n+             const Options* options,\n+             TableCache* table_cache,\n+             const InternalKeyComparator*);\n+  ~VersionSet();\n+\n+  // Apply *edit to the current version to form a new descriptor that\n+  // is both saved to persistent state and installed as the new\n+  // current version.  Will release *mu while actually writing to the file.\n+  // REQUIRES: *mu is held on entry.\n+  // REQUIRES: no other thread concurrently calls LogAndApply()\n+  Status LogAndApply(VersionEdit* edit, port::Mutex* mu)\n+      EXCLUSIVE_LOCKS_REQUIRED(mu);\n+\n+  // Recover the last saved descriptor from persistent storage.\n+  Status Recover();\n+\n+  // Return the current version.\n+  Version* current() const { return current_; }\n+\n+  // Return the current manifest file number\n+  uint64_t ManifestFileNumber() const { return manifest_file_number_; }\n+\n+  // Allocate and return a new file number\n+  uint64_t NewFileNumber() { return next_file_number_++; }\n+\n+  // Arrange to reuse \"file_number\" unless a newer file number has\n+  // already been allocated.\n+  // REQUIRES: \"file_number\" was returned by a call to NewFileNumber().\n+  void ReuseFileNumber(uint64_t file_number) {\n+    if (next_file_number_ == file_number + 1) {\n+      next_file_number_ = file_number;\n+    }\n+  }\n+\n+  // Return the number of Table files at the specified level.\n+  int NumLevelFiles(int level) const;\n+\n+  // Return the combined file size of all files at the specified level.\n+  int64_t NumLevelBytes(int level) const;\n+\n+  // Return the last sequence number.\n+  uint64_t LastSequence() const { return last_sequence_; }\n+\n+  // Set the last sequence number to s.\n+  void SetLastSequence(uint64_t s) {\n+    assert(s >= last_sequence_);\n+    last_sequence_ = s;\n+  }\n+\n+  // Mark the specified file number as used.\n+  void MarkFileNumberUsed(uint64_t number);\n+\n+  // Return the current log file number.\n+  uint64_t LogNumber() const { return log_number_; }\n+\n+  // Return the log file number for the log file that is currently\n+  // being compacted, or zero if there is no such log file.\n+  uint64_t PrevLogNumber() const { return prev_log_number_; }\n+\n+  // Pick level and inputs for a new compaction.\n+  // Returns NULL if there is no compaction to be done.\n+  // Otherwise returns a pointer to a heap-allocated object that\n+  // describes the compaction.  Caller should delete the result.\n+  Compaction* PickCompaction();\n+\n+  // Return a compaction object for compacting the range [begin,end] in\n+  // the specified level.  Returns NULL if there is nothing in that\n+  // level that overlaps the specified range.  Caller should delete\n+  // the result.\n+  Compaction* CompactRange(\n+      int level,\n+      const InternalKey* begin,\n+      const InternalKey* end);\n+\n+  // Return the maximum overlapping data (in bytes) at next level for any\n+  // file at a level >= 1.\n+  int64_t MaxNextLevelOverlappingBytes();\n+\n+  // Create an iterator that reads over the compaction inputs for \"*c\".\n+  // The caller should delete the iterator when no longer needed.\n+  Iterator* MakeInputIterator(Compaction* c);\n+\n+  // Returns true iff some level needs a compaction.\n+  bool NeedsCompaction() const {\n+    Version* v = current_;\n+    return (v->compaction_score_ >= 1) || (v->file_to_compact_ != NULL);\n+  }\n+\n+  // Add all files listed in any live version to *live.\n+  // May also mutate some internal state.\n+  void AddLiveFiles(std::set<uint64_t>* live);\n+\n+  // Return the approximate offset in the database of the data for\n+  // \"key\" as of version \"v\".\n+  uint64_t ApproximateOffsetOf(Version* v, const InternalKey& key);\n+\n+  // Return a human-readable short (single-line) summary of the number\n+  // of files per level.  Uses *scratch as backing store.\n+  struct LevelSummaryStorage {\n+    char buffer[100];\n+  };\n+  const char* LevelSummary(LevelSummaryStorage* scratch) const;\n+\n+ private:\n+  class Builder;\n+\n+  friend class Compaction;\n+  friend class Version;\n+\n+  void Finalize(Version* v);\n+\n+  void GetRange(const std::vector<FileMetaData*>& inputs,\n+                InternalKey* smallest,\n+                InternalKey* largest);\n+\n+  void GetRange2(const std::vector<FileMetaData*>& inputs1,\n+                 const std::vector<FileMetaData*>& inputs2,\n+                 InternalKey* smallest,\n+                 InternalKey* largest);\n+\n+  void SetupOtherInputs(Compaction* c);\n+\n+  // Save current contents to *log\n+  Status WriteSnapshot(log::Writer* log);\n+\n+  void AppendVersion(Version* v);\n+\n+  bool ManifestContains(const std::string& record) const;\n+\n+  Env* const env_;\n+  const std::string dbname_;\n+  const Options* const options_;\n+  TableCache* const table_cache_;\n+  const InternalKeyComparator icmp_;\n+  uint64_t next_file_number_;\n+  uint64_t manifest_file_number_;\n+  uint64_t last_sequence_;\n+  uint64_t log_number_;\n+  uint64_t prev_log_number_;  // 0 or backing store for memtable being compacted\n+\n+  // Opened lazily\n+  WritableFile* descriptor_file_;\n+  log::Writer* descriptor_log_;\n+  Version dummy_versions_;  // Head of circular doubly-linked list of versions.\n+  Version* current_;        // == dummy_versions_.prev_\n+\n+  // Per-level key at which the next compaction at that level should start.\n+  // Either an empty string, or a valid InternalKey.\n+  std::string compact_pointer_[config::kNumLevels];\n+\n+  // No copying allowed\n+  VersionSet(const VersionSet&);\n+  void operator=(const VersionSet&);\n+};\n+\n+// A Compaction encapsulates information about a compaction.\n+class Compaction {\n+ public:\n+  ~Compaction();\n+\n+  // Return the level that is being compacted.  Inputs from \"level\"\n+  // and \"level+1\" will be merged to produce a set of \"level+1\" files.\n+  int level() const { return level_; }\n+\n+  // Return the object that holds the edits to the descriptor done\n+  // by this compaction.\n+  VersionEdit* edit() { return &edit_; }\n+\n+  // \"which\" must be either 0 or 1\n+  int num_input_files(int which) const { return inputs_[which].size(); }\n+\n+  // Return the ith input file at \"level()+which\" (\"which\" must be 0 or 1).\n+  FileMetaData* input(int which, int i) const { return inputs_[which][i]; }\n+\n+  // Maximum size of files to build during this compaction.\n+  uint64_t MaxOutputFileSize() const { return max_output_file_size_; }\n+\n+  // Is this a trivial compaction that can be implemented by just\n+  // moving a single input file to the next level (no merging or splitting)\n+  bool IsTrivialMove() const;\n+\n+  // Add all inputs to this compaction as delete operations to *edit.\n+  void AddInputDeletions(VersionEdit* edit);\n+\n+  // Returns true if the information we have available guarantees that\n+  // the compaction is producing data in \"level+1\" for which no data exists\n+  // in levels greater than \"level+1\".\n+  bool IsBaseLevelForKey(const Slice& user_key);\n+\n+  // Returns true iff we should stop building the current output\n+  // before processing \"internal_key\".\n+  bool ShouldStopBefore(const Slice& internal_key);\n+\n+  // Release the input version for the compaction, once the compaction\n+  // is successful.\n+  void ReleaseInputs();\n+\n+ private:\n+  friend class Version;\n+  friend class VersionSet;\n+\n+  explicit Compaction(int level);\n+\n+  int level_;\n+  uint64_t max_output_file_size_;\n+  Version* input_version_;\n+  VersionEdit edit_;\n+\n+  // Each compaction reads inputs from \"level_\" and \"level_+1\"\n+  std::vector<FileMetaData*> inputs_[2];      // The two sets of inputs\n+\n+  // State used to check for number of of overlapping grandparent files\n+  // (parent == level_ + 1, grandparent == level_ + 2)\n+  std::vector<FileMetaData*> grandparents_;\n+  size_t grandparent_index_;  // Index in grandparent_starts_\n+  bool seen_key_;             // Some output key has been seen\n+  int64_t overlapped_bytes_;  // Bytes of overlap between current output\n+                              // and grandparent files\n+\n+  // State for implementing IsBaseLevelForKey\n+\n+  // level_ptrs_ holds indices into input_version_->levels_: our state\n+  // is that we are positioned at one of the file ranges for each\n+  // higher level than the ones involved in this compaction (i.e. for\n+  // all L >= level_ + 2).\n+  size_t level_ptrs_[config::kNumLevels];\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_VERSION_SET_H_"
      },
      {
        "sha": "501e34d1337d3917185b70369d9982db54e787aa",
        "filename": "db/version_set_test.cc",
        "status": "added",
        "additions": 179,
        "deletions": 0,
        "changes": 179,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/version_set_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/version_set_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/version_set_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,179 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/version_set.h\"\n+#include \"util/logging.h\"\n+#include \"util/testharness.h\"\n+#include \"util/testutil.h\"\n+\n+namespace leveldb {\n+\n+class FindFileTest {\n+ public:\n+  std::vector<FileMetaData*> files_;\n+  bool disjoint_sorted_files_;\n+\n+  FindFileTest() : disjoint_sorted_files_(true) { }\n+\n+  ~FindFileTest() {\n+    for (int i = 0; i < files_.size(); i++) {\n+      delete files_[i];\n+    }\n+  }\n+\n+  void Add(const char* smallest, const char* largest,\n+           SequenceNumber smallest_seq = 100,\n+           SequenceNumber largest_seq = 100) {\n+    FileMetaData* f = new FileMetaData;\n+    f->number = files_.size() + 1;\n+    f->smallest = InternalKey(smallest, smallest_seq, kTypeValue);\n+    f->largest = InternalKey(largest, largest_seq, kTypeValue);\n+    files_.push_back(f);\n+  }\n+\n+  int Find(const char* key) {\n+    InternalKey target(key, 100, kTypeValue);\n+    InternalKeyComparator cmp(BytewiseComparator());\n+    return FindFile(cmp, files_, target.Encode());\n+  }\n+\n+  bool Overlaps(const char* smallest, const char* largest) {\n+    InternalKeyComparator cmp(BytewiseComparator());\n+    Slice s(smallest != NULL ? smallest : \"\");\n+    Slice l(largest != NULL ? largest : \"\");\n+    return SomeFileOverlapsRange(cmp, disjoint_sorted_files_, files_,\n+                                 (smallest != NULL ? &s : NULL),\n+                                 (largest != NULL ? &l : NULL));\n+  }\n+};\n+\n+TEST(FindFileTest, Empty) {\n+  ASSERT_EQ(0, Find(\"foo\"));\n+  ASSERT_TRUE(! Overlaps(\"a\", \"z\"));\n+  ASSERT_TRUE(! Overlaps(NULL, \"z\"));\n+  ASSERT_TRUE(! Overlaps(\"a\", NULL));\n+  ASSERT_TRUE(! Overlaps(NULL, NULL));\n+}\n+\n+TEST(FindFileTest, Single) {\n+  Add(\"p\", \"q\");\n+  ASSERT_EQ(0, Find(\"a\"));\n+  ASSERT_EQ(0, Find(\"p\"));\n+  ASSERT_EQ(0, Find(\"p1\"));\n+  ASSERT_EQ(0, Find(\"q\"));\n+  ASSERT_EQ(1, Find(\"q1\"));\n+  ASSERT_EQ(1, Find(\"z\"));\n+\n+  ASSERT_TRUE(! Overlaps(\"a\", \"b\"));\n+  ASSERT_TRUE(! Overlaps(\"z1\", \"z2\"));\n+  ASSERT_TRUE(Overlaps(\"a\", \"p\"));\n+  ASSERT_TRUE(Overlaps(\"a\", \"q\"));\n+  ASSERT_TRUE(Overlaps(\"a\", \"z\"));\n+  ASSERT_TRUE(Overlaps(\"p\", \"p1\"));\n+  ASSERT_TRUE(Overlaps(\"p\", \"q\"));\n+  ASSERT_TRUE(Overlaps(\"p\", \"z\"));\n+  ASSERT_TRUE(Overlaps(\"p1\", \"p2\"));\n+  ASSERT_TRUE(Overlaps(\"p1\", \"z\"));\n+  ASSERT_TRUE(Overlaps(\"q\", \"q\"));\n+  ASSERT_TRUE(Overlaps(\"q\", \"q1\"));\n+\n+  ASSERT_TRUE(! Overlaps(NULL, \"j\"));\n+  ASSERT_TRUE(! Overlaps(\"r\", NULL));\n+  ASSERT_TRUE(Overlaps(NULL, \"p\"));\n+  ASSERT_TRUE(Overlaps(NULL, \"p1\"));\n+  ASSERT_TRUE(Overlaps(\"q\", NULL));\n+  ASSERT_TRUE(Overlaps(NULL, NULL));\n+}\n+\n+\n+TEST(FindFileTest, Multiple) {\n+  Add(\"150\", \"200\");\n+  Add(\"200\", \"250\");\n+  Add(\"300\", \"350\");\n+  Add(\"400\", \"450\");\n+  ASSERT_EQ(0, Find(\"100\"));\n+  ASSERT_EQ(0, Find(\"150\"));\n+  ASSERT_EQ(0, Find(\"151\"));\n+  ASSERT_EQ(0, Find(\"199\"));\n+  ASSERT_EQ(0, Find(\"200\"));\n+  ASSERT_EQ(1, Find(\"201\"));\n+  ASSERT_EQ(1, Find(\"249\"));\n+  ASSERT_EQ(1, Find(\"250\"));\n+  ASSERT_EQ(2, Find(\"251\"));\n+  ASSERT_EQ(2, Find(\"299\"));\n+  ASSERT_EQ(2, Find(\"300\"));\n+  ASSERT_EQ(2, Find(\"349\"));\n+  ASSERT_EQ(2, Find(\"350\"));\n+  ASSERT_EQ(3, Find(\"351\"));\n+  ASSERT_EQ(3, Find(\"400\"));\n+  ASSERT_EQ(3, Find(\"450\"));\n+  ASSERT_EQ(4, Find(\"451\"));\n+\n+  ASSERT_TRUE(! Overlaps(\"100\", \"149\"));\n+  ASSERT_TRUE(! Overlaps(\"251\", \"299\"));\n+  ASSERT_TRUE(! Overlaps(\"451\", \"500\"));\n+  ASSERT_TRUE(! Overlaps(\"351\", \"399\"));\n+\n+  ASSERT_TRUE(Overlaps(\"100\", \"150\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"200\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"300\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"400\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"500\"));\n+  ASSERT_TRUE(Overlaps(\"375\", \"400\"));\n+  ASSERT_TRUE(Overlaps(\"450\", \"450\"));\n+  ASSERT_TRUE(Overlaps(\"450\", \"500\"));\n+}\n+\n+TEST(FindFileTest, MultipleNullBoundaries) {\n+  Add(\"150\", \"200\");\n+  Add(\"200\", \"250\");\n+  Add(\"300\", \"350\");\n+  Add(\"400\", \"450\");\n+  ASSERT_TRUE(! Overlaps(NULL, \"149\"));\n+  ASSERT_TRUE(! Overlaps(\"451\", NULL));\n+  ASSERT_TRUE(Overlaps(NULL, NULL));\n+  ASSERT_TRUE(Overlaps(NULL, \"150\"));\n+  ASSERT_TRUE(Overlaps(NULL, \"199\"));\n+  ASSERT_TRUE(Overlaps(NULL, \"200\"));\n+  ASSERT_TRUE(Overlaps(NULL, \"201\"));\n+  ASSERT_TRUE(Overlaps(NULL, \"400\"));\n+  ASSERT_TRUE(Overlaps(NULL, \"800\"));\n+  ASSERT_TRUE(Overlaps(\"100\", NULL));\n+  ASSERT_TRUE(Overlaps(\"200\", NULL));\n+  ASSERT_TRUE(Overlaps(\"449\", NULL));\n+  ASSERT_TRUE(Overlaps(\"450\", NULL));\n+}\n+\n+TEST(FindFileTest, OverlapSequenceChecks) {\n+  Add(\"200\", \"200\", 5000, 3000);\n+  ASSERT_TRUE(! Overlaps(\"199\", \"199\"));\n+  ASSERT_TRUE(! Overlaps(\"201\", \"300\"));\n+  ASSERT_TRUE(Overlaps(\"200\", \"200\"));\n+  ASSERT_TRUE(Overlaps(\"190\", \"200\"));\n+  ASSERT_TRUE(Overlaps(\"200\", \"210\"));\n+}\n+\n+TEST(FindFileTest, OverlappingFiles) {\n+  Add(\"150\", \"600\");\n+  Add(\"400\", \"500\");\n+  disjoint_sorted_files_ = false;\n+  ASSERT_TRUE(! Overlaps(\"100\", \"149\"));\n+  ASSERT_TRUE(! Overlaps(\"601\", \"700\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"150\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"200\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"300\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"400\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"500\"));\n+  ASSERT_TRUE(Overlaps(\"375\", \"400\"));\n+  ASSERT_TRUE(Overlaps(\"450\", \"450\"));\n+  ASSERT_TRUE(Overlaps(\"450\", \"500\"));\n+  ASSERT_TRUE(Overlaps(\"450\", \"700\"));\n+  ASSERT_TRUE(Overlaps(\"600\", \"700\"));\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "33f4a4257ea94e0105a9368de79d761ac7bf979a",
        "filename": "db/write_batch.cc",
        "status": "added",
        "additions": 147,
        "deletions": 0,
        "changes": 147,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/write_batch.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/write_batch.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/write_batch.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,147 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// WriteBatch::rep_ :=\n+//    sequence: fixed64\n+//    count: fixed32\n+//    data: record[count]\n+// record :=\n+//    kTypeValue varstring varstring         |\n+//    kTypeDeletion varstring\n+// varstring :=\n+//    len: varint32\n+//    data: uint8[len]\n+\n+#include \"leveldb/write_batch.h\"\n+\n+#include \"leveldb/db.h\"\n+#include \"db/dbformat.h\"\n+#include \"db/memtable.h\"\n+#include \"db/write_batch_internal.h\"\n+#include \"util/coding.h\"\n+\n+namespace leveldb {\n+\n+// WriteBatch header has an 8-byte sequence number followed by a 4-byte count.\n+static const size_t kHeader = 12;\n+\n+WriteBatch::WriteBatch() {\n+  Clear();\n+}\n+\n+WriteBatch::~WriteBatch() { }\n+\n+WriteBatch::Handler::~Handler() { }\n+\n+void WriteBatch::Clear() {\n+  rep_.clear();\n+  rep_.resize(kHeader);\n+}\n+\n+Status WriteBatch::Iterate(Handler* handler) const {\n+  Slice input(rep_);\n+  if (input.size() < kHeader) {\n+    return Status::Corruption(\"malformed WriteBatch (too small)\");\n+  }\n+\n+  input.remove_prefix(kHeader);\n+  Slice key, value;\n+  int found = 0;\n+  while (!input.empty()) {\n+    found++;\n+    char tag = input[0];\n+    input.remove_prefix(1);\n+    switch (tag) {\n+      case kTypeValue:\n+        if (GetLengthPrefixedSlice(&input, &key) &&\n+            GetLengthPrefixedSlice(&input, &value)) {\n+          handler->Put(key, value);\n+        } else {\n+          return Status::Corruption(\"bad WriteBatch Put\");\n+        }\n+        break;\n+      case kTypeDeletion:\n+        if (GetLengthPrefixedSlice(&input, &key)) {\n+          handler->Delete(key);\n+        } else {\n+          return Status::Corruption(\"bad WriteBatch Delete\");\n+        }\n+        break;\n+      default:\n+        return Status::Corruption(\"unknown WriteBatch tag\");\n+    }\n+  }\n+  if (found != WriteBatchInternal::Count(this)) {\n+    return Status::Corruption(\"WriteBatch has wrong count\");\n+  } else {\n+    return Status::OK();\n+  }\n+}\n+\n+int WriteBatchInternal::Count(const WriteBatch* b) {\n+  return DecodeFixed32(b->rep_.data() + 8);\n+}\n+\n+void WriteBatchInternal::SetCount(WriteBatch* b, int n) {\n+  EncodeFixed32(&b->rep_[8], n);\n+}\n+\n+SequenceNumber WriteBatchInternal::Sequence(const WriteBatch* b) {\n+  return SequenceNumber(DecodeFixed64(b->rep_.data()));\n+}\n+\n+void WriteBatchInternal::SetSequence(WriteBatch* b, SequenceNumber seq) {\n+  EncodeFixed64(&b->rep_[0], seq);\n+}\n+\n+void WriteBatch::Put(const Slice& key, const Slice& value) {\n+  WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1);\n+  rep_.push_back(static_cast<char>(kTypeValue));\n+  PutLengthPrefixedSlice(&rep_, key);\n+  PutLengthPrefixedSlice(&rep_, value);\n+}\n+\n+void WriteBatch::Delete(const Slice& key) {\n+  WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1);\n+  rep_.push_back(static_cast<char>(kTypeDeletion));\n+  PutLengthPrefixedSlice(&rep_, key);\n+}\n+\n+namespace {\n+class MemTableInserter : public WriteBatch::Handler {\n+ public:\n+  SequenceNumber sequence_;\n+  MemTable* mem_;\n+\n+  virtual void Put(const Slice& key, const Slice& value) {\n+    mem_->Add(sequence_, kTypeValue, key, value);\n+    sequence_++;\n+  }\n+  virtual void Delete(const Slice& key) {\n+    mem_->Add(sequence_, kTypeDeletion, key, Slice());\n+    sequence_++;\n+  }\n+};\n+}  // namespace\n+\n+Status WriteBatchInternal::InsertInto(const WriteBatch* b,\n+                                      MemTable* memtable) {\n+  MemTableInserter inserter;\n+  inserter.sequence_ = WriteBatchInternal::Sequence(b);\n+  inserter.mem_ = memtable;\n+  return b->Iterate(&inserter);\n+}\n+\n+void WriteBatchInternal::SetContents(WriteBatch* b, const Slice& contents) {\n+  assert(contents.size() >= kHeader);\n+  b->rep_.assign(contents.data(), contents.size());\n+}\n+\n+void WriteBatchInternal::Append(WriteBatch* dst, const WriteBatch* src) {\n+  SetCount(dst, Count(dst) + Count(src));\n+  assert(src->rep_.size() >= kHeader);\n+  dst->rep_.append(src->rep_.data() + kHeader, src->rep_.size() - kHeader);\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "4423a7f31842457dea62d43547616b2f5ec852f8",
        "filename": "db/write_batch_internal.h",
        "status": "added",
        "additions": 49,
        "deletions": 0,
        "changes": 49,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/write_batch_internal.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/write_batch_internal.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/write_batch_internal.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,49 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_WRITE_BATCH_INTERNAL_H_\n+#define STORAGE_LEVELDB_DB_WRITE_BATCH_INTERNAL_H_\n+\n+#include \"leveldb/write_batch.h\"\n+\n+namespace leveldb {\n+\n+class MemTable;\n+\n+// WriteBatchInternal provides static methods for manipulating a\n+// WriteBatch that we don't want in the public WriteBatch interface.\n+class WriteBatchInternal {\n+ public:\n+  // Return the number of entries in the batch.\n+  static int Count(const WriteBatch* batch);\n+\n+  // Set the count for the number of entries in the batch.\n+  static void SetCount(WriteBatch* batch, int n);\n+\n+  // Return the seqeunce number for the start of this batch.\n+  static SequenceNumber Sequence(const WriteBatch* batch);\n+\n+  // Store the specified number as the seqeunce number for the start of\n+  // this batch.\n+  static void SetSequence(WriteBatch* batch, SequenceNumber seq);\n+\n+  static Slice Contents(const WriteBatch* batch) {\n+    return Slice(batch->rep_);\n+  }\n+\n+  static size_t ByteSize(const WriteBatch* batch) {\n+    return batch->rep_.size();\n+  }\n+\n+  static void SetContents(WriteBatch* batch, const Slice& contents);\n+\n+  static Status InsertInto(const WriteBatch* batch, MemTable* memtable);\n+\n+  static void Append(WriteBatch* dst, const WriteBatch* src);\n+};\n+\n+}  // namespace leveldb\n+\n+\n+#endif  // STORAGE_LEVELDB_DB_WRITE_BATCH_INTERNAL_H_"
      },
      {
        "sha": "9064e3d85eb35f32d20ef4c7456b0866d525aee8",
        "filename": "db/write_batch_test.cc",
        "status": "added",
        "additions": 120,
        "deletions": 0,
        "changes": 120,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/write_batch_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/db/write_batch_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/write_batch_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,120 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"leveldb/db.h\"\n+\n+#include \"db/memtable.h\"\n+#include \"db/write_batch_internal.h\"\n+#include \"leveldb/env.h\"\n+#include \"util/logging.h\"\n+#include \"util/testharness.h\"\n+\n+namespace leveldb {\n+\n+static std::string PrintContents(WriteBatch* b) {\n+  InternalKeyComparator cmp(BytewiseComparator());\n+  MemTable* mem = new MemTable(cmp);\n+  mem->Ref();\n+  std::string state;\n+  Status s = WriteBatchInternal::InsertInto(b, mem);\n+  int count = 0;\n+  Iterator* iter = mem->NewIterator();\n+  for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+    ParsedInternalKey ikey;\n+    ASSERT_TRUE(ParseInternalKey(iter->key(), &ikey));\n+    switch (ikey.type) {\n+      case kTypeValue:\n+        state.append(\"Put(\");\n+        state.append(ikey.user_key.ToString());\n+        state.append(\", \");\n+        state.append(iter->value().ToString());\n+        state.append(\")\");\n+        count++;\n+        break;\n+      case kTypeDeletion:\n+        state.append(\"Delete(\");\n+        state.append(ikey.user_key.ToString());\n+        state.append(\")\");\n+        count++;\n+        break;\n+    }\n+    state.append(\"@\");\n+    state.append(NumberToString(ikey.sequence));\n+  }\n+  delete iter;\n+  if (!s.ok()) {\n+    state.append(\"ParseError()\");\n+  } else if (count != WriteBatchInternal::Count(b)) {\n+    state.append(\"CountMismatch()\");\n+  }\n+  mem->Unref();\n+  return state;\n+}\n+\n+class WriteBatchTest { };\n+\n+TEST(WriteBatchTest, Empty) {\n+  WriteBatch batch;\n+  ASSERT_EQ(\"\", PrintContents(&batch));\n+  ASSERT_EQ(0, WriteBatchInternal::Count(&batch));\n+}\n+\n+TEST(WriteBatchTest, Multiple) {\n+  WriteBatch batch;\n+  batch.Put(Slice(\"foo\"), Slice(\"bar\"));\n+  batch.Delete(Slice(\"box\"));\n+  batch.Put(Slice(\"baz\"), Slice(\"boo\"));\n+  WriteBatchInternal::SetSequence(&batch, 100);\n+  ASSERT_EQ(100, WriteBatchInternal::Sequence(&batch));\n+  ASSERT_EQ(3, WriteBatchInternal::Count(&batch));\n+  ASSERT_EQ(\"Put(baz, boo)@102\"\n+            \"Delete(box)@101\"\n+            \"Put(foo, bar)@100\",\n+            PrintContents(&batch));\n+}\n+\n+TEST(WriteBatchTest, Corruption) {\n+  WriteBatch batch;\n+  batch.Put(Slice(\"foo\"), Slice(\"bar\"));\n+  batch.Delete(Slice(\"box\"));\n+  WriteBatchInternal::SetSequence(&batch, 200);\n+  Slice contents = WriteBatchInternal::Contents(&batch);\n+  WriteBatchInternal::SetContents(&batch,\n+                                  Slice(contents.data(),contents.size()-1));\n+  ASSERT_EQ(\"Put(foo, bar)@200\"\n+            \"ParseError()\",\n+            PrintContents(&batch));\n+}\n+\n+TEST(WriteBatchTest, Append) {\n+  WriteBatch b1, b2;\n+  WriteBatchInternal::SetSequence(&b1, 200);\n+  WriteBatchInternal::SetSequence(&b2, 300);\n+  WriteBatchInternal::Append(&b1, &b2);\n+  ASSERT_EQ(\"\",\n+            PrintContents(&b1));\n+  b2.Put(\"a\", \"va\");\n+  WriteBatchInternal::Append(&b1, &b2);\n+  ASSERT_EQ(\"Put(a, va)@200\",\n+            PrintContents(&b1));\n+  b2.Clear();\n+  b2.Put(\"b\", \"vb\");\n+  WriteBatchInternal::Append(&b1, &b2);\n+  ASSERT_EQ(\"Put(a, va)@200\"\n+            \"Put(b, vb)@201\",\n+            PrintContents(&b1));\n+  b2.Delete(\"foo\");\n+  WriteBatchInternal::Append(&b1, &b2);\n+  ASSERT_EQ(\"Put(a, va)@200\"\n+            \"Put(b, vb)@202\"\n+            \"Put(b, vb)@201\"\n+            \"Delete(foo)@203\",\n+            PrintContents(&b1));\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "e63aaa8dcc289915176b12749dd9ec21b50aba02",
        "filename": "doc/bench/db_bench_sqlite3.cc",
        "status": "added",
        "additions": 718,
        "deletions": 0,
        "changes": 718,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/bench/db_bench_sqlite3.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/bench/db_bench_sqlite3.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/bench/db_bench_sqlite3.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,718 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include <stdio.h>\n+#include <stdlib.h>\n+#include <sqlite3.h>\n+#include \"util/histogram.h\"\n+#include \"util/random.h\"\n+#include \"util/testutil.h\"\n+\n+// Comma-separated list of operations to run in the specified order\n+//   Actual benchmarks:\n+//\n+//   fillseq       -- write N values in sequential key order in async mode\n+//   fillseqsync   -- write N/100 values in sequential key order in sync mode\n+//   fillseqbatch  -- batch write N values in sequential key order in async mode\n+//   fillrandom    -- write N values in random key order in async mode\n+//   fillrandsync  -- write N/100 values in random key order in sync mode\n+//   fillrandbatch -- batch write N values in sequential key order in async mode\n+//   overwrite     -- overwrite N values in random key order in async mode\n+//   fillrand100K  -- write N/1000 100K values in random order in async mode\n+//   fillseq100K   -- write N/1000 100K values in sequential order in async mode\n+//   readseq       -- read N times sequentially\n+//   readrandom    -- read N times in random order\n+//   readrand100K  -- read N/1000 100K values in sequential order in async mode\n+static const char* FLAGS_benchmarks =\n+    \"fillseq,\"\n+    \"fillseqsync,\"\n+    \"fillseqbatch,\"\n+    \"fillrandom,\"\n+    \"fillrandsync,\"\n+    \"fillrandbatch,\"\n+    \"overwrite,\"\n+    \"overwritebatch,\"\n+    \"readrandom,\"\n+    \"readseq,\"\n+    \"fillrand100K,\"\n+    \"fillseq100K,\"\n+    \"readseq,\"\n+    \"readrand100K,\"\n+    ;\n+\n+// Number of key/values to place in database\n+static int FLAGS_num = 1000000;\n+\n+// Number of read operations to do.  If negative, do FLAGS_num reads.\n+static int FLAGS_reads = -1;\n+\n+// Size of each value\n+static int FLAGS_value_size = 100;\n+\n+// Print histogram of operation timings\n+static bool FLAGS_histogram = false;\n+\n+// Arrange to generate values that shrink to this fraction of\n+// their original size after compression\n+static double FLAGS_compression_ratio = 0.5;\n+\n+// Page size. Default 1 KB.\n+static int FLAGS_page_size = 1024;\n+\n+// Number of pages.\n+// Default cache size = FLAGS_page_size * FLAGS_num_pages = 4 MB.\n+static int FLAGS_num_pages = 4096;\n+\n+// If true, do not destroy the existing database.  If you set this\n+// flag and also specify a benchmark that wants a fresh database, that\n+// benchmark will fail.\n+static bool FLAGS_use_existing_db = false;\n+\n+// If true, we allow batch writes to occur\n+static bool FLAGS_transaction = true;\n+\n+// If true, we enable Write-Ahead Logging\n+static bool FLAGS_WAL_enabled = true;\n+\n+// Use the db with the following name.\n+static const char* FLAGS_db = NULL;\n+\n+inline\n+static void ExecErrorCheck(int status, char *err_msg) {\n+  if (status != SQLITE_OK) {\n+    fprintf(stderr, \"SQL error: %s\\n\", err_msg);\n+    sqlite3_free(err_msg);\n+    exit(1);\n+  }\n+}\n+\n+inline\n+static void StepErrorCheck(int status) {\n+  if (status != SQLITE_DONE) {\n+    fprintf(stderr, \"SQL step error: status = %d\\n\", status);\n+    exit(1);\n+  }\n+}\n+\n+inline\n+static void ErrorCheck(int status) {\n+  if (status != SQLITE_OK) {\n+    fprintf(stderr, \"sqlite3 error: status = %d\\n\", status);\n+    exit(1);\n+  }\n+}\n+\n+inline\n+static void WalCheckpoint(sqlite3* db_) {\n+  // Flush all writes to disk\n+  if (FLAGS_WAL_enabled) {\n+    sqlite3_wal_checkpoint_v2(db_, NULL, SQLITE_CHECKPOINT_FULL, NULL, NULL);\n+  }\n+}\n+\n+namespace leveldb {\n+\n+// Helper for quickly generating random data.\n+namespace {\n+class RandomGenerator {\n+ private:\n+  std::string data_;\n+  int pos_;\n+\n+ public:\n+  RandomGenerator() {\n+    // We use a limited amount of data over and over again and ensure\n+    // that it is larger than the compression window (32KB), and also\n+    // large enough to serve all typical value sizes we want to write.\n+    Random rnd(301);\n+    std::string piece;\n+    while (data_.size() < 1048576) {\n+      // Add a short fragment that is as compressible as specified\n+      // by FLAGS_compression_ratio.\n+      test::CompressibleString(&rnd, FLAGS_compression_ratio, 100, &piece);\n+      data_.append(piece);\n+    }\n+    pos_ = 0;\n+  }\n+\n+  Slice Generate(int len) {\n+    if (pos_ + len > data_.size()) {\n+      pos_ = 0;\n+      assert(len < data_.size());\n+    }\n+    pos_ += len;\n+    return Slice(data_.data() + pos_ - len, len);\n+  }\n+};\n+\n+static Slice TrimSpace(Slice s) {\n+  int start = 0;\n+  while (start < s.size() && isspace(s[start])) {\n+    start++;\n+  }\n+  int limit = s.size();\n+  while (limit > start && isspace(s[limit-1])) {\n+    limit--;\n+  }\n+  return Slice(s.data() + start, limit - start);\n+}\n+\n+}  // namespace\n+\n+class Benchmark {\n+ private:\n+  sqlite3* db_;\n+  int db_num_;\n+  int num_;\n+  int reads_;\n+  double start_;\n+  double last_op_finish_;\n+  int64_t bytes_;\n+  std::string message_;\n+  Histogram hist_;\n+  RandomGenerator gen_;\n+  Random rand_;\n+\n+  // State kept for progress messages\n+  int done_;\n+  int next_report_;     // When to report next\n+\n+  void PrintHeader() {\n+    const int kKeySize = 16;\n+    PrintEnvironment();\n+    fprintf(stdout, \"Keys:       %d bytes each\\n\", kKeySize);\n+    fprintf(stdout, \"Values:     %d bytes each\\n\", FLAGS_value_size);\n+    fprintf(stdout, \"Entries:    %d\\n\", num_);\n+    fprintf(stdout, \"RawSize:    %.1f MB (estimated)\\n\",\n+            ((static_cast<int64_t>(kKeySize + FLAGS_value_size) * num_)\n+             / 1048576.0));\n+    PrintWarnings();\n+    fprintf(stdout, \"------------------------------------------------\\n\");\n+  }\n+\n+  void PrintWarnings() {\n+#if defined(__GNUC__) && !defined(__OPTIMIZE__)\n+    fprintf(stdout,\n+            \"WARNING: Optimization is disabled: benchmarks unnecessarily slow\\n\"\n+            );\n+#endif\n+#ifndef NDEBUG\n+    fprintf(stdout,\n+            \"WARNING: Assertions are enabled; benchmarks unnecessarily slow\\n\");\n+#endif\n+  }\n+\n+  void PrintEnvironment() {\n+    fprintf(stderr, \"SQLite:     version %s\\n\", SQLITE_VERSION);\n+\n+#if defined(__linux)\n+    time_t now = time(NULL);\n+    fprintf(stderr, \"Date:       %s\", ctime(&now));  // ctime() adds newline\n+\n+    FILE* cpuinfo = fopen(\"/proc/cpuinfo\", \"r\");\n+    if (cpuinfo != NULL) {\n+      char line[1000];\n+      int num_cpus = 0;\n+      std::string cpu_type;\n+      std::string cache_size;\n+      while (fgets(line, sizeof(line), cpuinfo) != NULL) {\n+        const char* sep = strchr(line, ':');\n+        if (sep == NULL) {\n+          continue;\n+        }\n+        Slice key = TrimSpace(Slice(line, sep - 1 - line));\n+        Slice val = TrimSpace(Slice(sep + 1));\n+        if (key == \"model name\") {\n+          ++num_cpus;\n+          cpu_type = val.ToString();\n+        } else if (key == \"cache size\") {\n+          cache_size = val.ToString();\n+        }\n+      }\n+      fclose(cpuinfo);\n+      fprintf(stderr, \"CPU:        %d * %s\\n\", num_cpus, cpu_type.c_str());\n+      fprintf(stderr, \"CPUCache:   %s\\n\", cache_size.c_str());\n+    }\n+#endif\n+  }\n+\n+  void Start() {\n+    start_ = Env::Default()->NowMicros() * 1e-6;\n+    bytes_ = 0;\n+    message_.clear();\n+    last_op_finish_ = start_;\n+    hist_.Clear();\n+    done_ = 0;\n+    next_report_ = 100;\n+  }\n+\n+  void FinishedSingleOp() {\n+    if (FLAGS_histogram) {\n+      double now = Env::Default()->NowMicros() * 1e-6;\n+      double micros = (now - last_op_finish_) * 1e6;\n+      hist_.Add(micros);\n+      if (micros > 20000) {\n+        fprintf(stderr, \"long op: %.1f micros%30s\\r\", micros, \"\");\n+        fflush(stderr);\n+      }\n+      last_op_finish_ = now;\n+    }\n+\n+    done_++;\n+    if (done_ >= next_report_) {\n+      if      (next_report_ < 1000)   next_report_ += 100;\n+      else if (next_report_ < 5000)   next_report_ += 500;\n+      else if (next_report_ < 10000)  next_report_ += 1000;\n+      else if (next_report_ < 50000)  next_report_ += 5000;\n+      else if (next_report_ < 100000) next_report_ += 10000;\n+      else if (next_report_ < 500000) next_report_ += 50000;\n+      else                            next_report_ += 100000;\n+      fprintf(stderr, \"... finished %d ops%30s\\r\", done_, \"\");\n+      fflush(stderr);\n+    }\n+  }\n+\n+  void Stop(const Slice& name) {\n+    double finish = Env::Default()->NowMicros() * 1e-6;\n+\n+    // Pretend at least one op was done in case we are running a benchmark\n+    // that does not call FinishedSingleOp().\n+    if (done_ < 1) done_ = 1;\n+\n+    if (bytes_ > 0) {\n+      char rate[100];\n+      snprintf(rate, sizeof(rate), \"%6.1f MB/s\",\n+               (bytes_ / 1048576.0) / (finish - start_));\n+      if (!message_.empty()) {\n+        message_  = std::string(rate) + \" \" + message_;\n+      } else {\n+        message_ = rate;\n+      }\n+    }\n+\n+    fprintf(stdout, \"%-12s : %11.3f micros/op;%s%s\\n\",\n+            name.ToString().c_str(),\n+            (finish - start_) * 1e6 / done_,\n+            (message_.empty() ? \"\" : \" \"),\n+            message_.c_str());\n+    if (FLAGS_histogram) {\n+      fprintf(stdout, \"Microseconds per op:\\n%s\\n\", hist_.ToString().c_str());\n+    }\n+    fflush(stdout);\n+  }\n+\n+ public:\n+  enum Order {\n+    SEQUENTIAL,\n+    RANDOM\n+  };\n+  enum DBState {\n+    FRESH,\n+    EXISTING\n+  };\n+\n+  Benchmark()\n+  : db_(NULL),\n+    db_num_(0),\n+    num_(FLAGS_num),\n+    reads_(FLAGS_reads < 0 ? FLAGS_num : FLAGS_reads),\n+    bytes_(0),\n+    rand_(301) {\n+    std::vector<std::string> files;\n+    std::string test_dir;\n+    Env::Default()->GetTestDirectory(&test_dir);\n+    Env::Default()->GetChildren(test_dir, &files);\n+    if (!FLAGS_use_existing_db) {\n+      for (int i = 0; i < files.size(); i++) {\n+        if (Slice(files[i]).starts_with(\"dbbench_sqlite3\")) {\n+          std::string file_name(test_dir);\n+          file_name += \"/\";\n+          file_name += files[i];\n+          Env::Default()->DeleteFile(file_name.c_str());\n+        }\n+      }\n+    }\n+  }\n+\n+  ~Benchmark() {\n+    int status = sqlite3_close(db_);\n+    ErrorCheck(status);\n+  }\n+\n+  void Run() {\n+    PrintHeader();\n+    Open();\n+\n+    const char* benchmarks = FLAGS_benchmarks;\n+    while (benchmarks != NULL) {\n+      const char* sep = strchr(benchmarks, ',');\n+      Slice name;\n+      if (sep == NULL) {\n+        name = benchmarks;\n+        benchmarks = NULL;\n+      } else {\n+        name = Slice(benchmarks, sep - benchmarks);\n+        benchmarks = sep + 1;\n+      }\n+\n+      bytes_ = 0;\n+      Start();\n+\n+      bool known = true;\n+      bool write_sync = false;\n+      if (name == Slice(\"fillseq\")) {\n+        Write(write_sync, SEQUENTIAL, FRESH, num_, FLAGS_value_size, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillseqbatch\")) {\n+        Write(write_sync, SEQUENTIAL, FRESH, num_, FLAGS_value_size, 1000);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillrandom\")) {\n+        Write(write_sync, RANDOM, FRESH, num_, FLAGS_value_size, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillrandbatch\")) {\n+        Write(write_sync, RANDOM, FRESH, num_, FLAGS_value_size, 1000);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"overwrite\")) {\n+        Write(write_sync, RANDOM, EXISTING, num_, FLAGS_value_size, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"overwritebatch\")) {\n+        Write(write_sync, RANDOM, EXISTING, num_, FLAGS_value_size, 1000);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillrandsync\")) {\n+        write_sync = true;\n+        Write(write_sync, RANDOM, FRESH, num_ / 100, FLAGS_value_size, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillseqsync\")) {\n+        write_sync = true;\n+        Write(write_sync, SEQUENTIAL, FRESH, num_ / 100, FLAGS_value_size, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillrand100K\")) {\n+        Write(write_sync, RANDOM, FRESH, num_ / 1000, 100 * 1000, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillseq100K\")) {\n+        Write(write_sync, SEQUENTIAL, FRESH, num_ / 1000, 100 * 1000, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"readseq\")) {\n+        ReadSequential();\n+      } else if (name == Slice(\"readrandom\")) {\n+        Read(RANDOM, 1);\n+      } else if (name == Slice(\"readrand100K\")) {\n+        int n = reads_;\n+        reads_ /= 1000;\n+        Read(RANDOM, 1);\n+        reads_ = n;\n+      } else {\n+        known = false;\n+        if (name != Slice()) {  // No error message for empty name\n+          fprintf(stderr, \"unknown benchmark '%s'\\n\", name.ToString().c_str());\n+        }\n+      }\n+      if (known) {\n+        Stop(name);\n+      }\n+    }\n+  }\n+\n+  void Open() {\n+    assert(db_ == NULL);\n+\n+    int status;\n+    char file_name[100];\n+    char* err_msg = NULL;\n+    db_num_++;\n+\n+    // Open database\n+    std::string tmp_dir;\n+    Env::Default()->GetTestDirectory(&tmp_dir);\n+    snprintf(file_name, sizeof(file_name),\n+             \"%s/dbbench_sqlite3-%d.db\",\n+             tmp_dir.c_str(),\n+             db_num_);\n+    status = sqlite3_open(file_name, &db_);\n+    if (status) {\n+      fprintf(stderr, \"open error: %s\\n\", sqlite3_errmsg(db_));\n+      exit(1);\n+    }\n+\n+    // Change SQLite cache size\n+    char cache_size[100];\n+    snprintf(cache_size, sizeof(cache_size), \"PRAGMA cache_size = %d\",\n+             FLAGS_num_pages);\n+    status = sqlite3_exec(db_, cache_size, NULL, NULL, &err_msg);\n+    ExecErrorCheck(status, err_msg);\n+\n+    // FLAGS_page_size is defaulted to 1024\n+    if (FLAGS_page_size != 1024) {\n+      char page_size[100];\n+      snprintf(page_size, sizeof(page_size), \"PRAGMA page_size = %d\",\n+               FLAGS_page_size);\n+      status = sqlite3_exec(db_, page_size, NULL, NULL, &err_msg);\n+      ExecErrorCheck(status, err_msg);\n+    }\n+\n+    // Change journal mode to WAL if WAL enabled flag is on\n+    if (FLAGS_WAL_enabled) {\n+      std::string WAL_stmt = \"PRAGMA journal_mode = WAL\";\n+\n+      // LevelDB's default cache size is a combined 4 MB\n+      std::string WAL_checkpoint = \"PRAGMA wal_autocheckpoint = 4096\";\n+      status = sqlite3_exec(db_, WAL_stmt.c_str(), NULL, NULL, &err_msg);\n+      ExecErrorCheck(status, err_msg);\n+      status = sqlite3_exec(db_, WAL_checkpoint.c_str(), NULL, NULL, &err_msg);\n+      ExecErrorCheck(status, err_msg);\n+    }\n+\n+    // Change locking mode to exclusive and create tables/index for database\n+    std::string locking_stmt = \"PRAGMA locking_mode = EXCLUSIVE\";\n+    std::string create_stmt =\n+          \"CREATE TABLE test (key blob, value blob, PRIMARY KEY(key))\";\n+    std::string stmt_array[] = { locking_stmt, create_stmt };\n+    int stmt_array_length = sizeof(stmt_array) / sizeof(std::string);\n+    for (int i = 0; i < stmt_array_length; i++) {\n+      status = sqlite3_exec(db_, stmt_array[i].c_str(), NULL, NULL, &err_msg);\n+      ExecErrorCheck(status, err_msg);\n+    }\n+  }\n+\n+  void Write(bool write_sync, Order order, DBState state,\n+             int num_entries, int value_size, int entries_per_batch) {\n+    // Create new database if state == FRESH\n+    if (state == FRESH) {\n+      if (FLAGS_use_existing_db) {\n+        message_ = \"skipping (--use_existing_db is true)\";\n+        return;\n+      }\n+      sqlite3_close(db_);\n+      db_ = NULL;\n+      Open();\n+      Start();\n+    }\n+\n+    if (num_entries != num_) {\n+      char msg[100];\n+      snprintf(msg, sizeof(msg), \"(%d ops)\", num_entries);\n+      message_ = msg;\n+    }\n+\n+    char* err_msg = NULL;\n+    int status;\n+\n+    sqlite3_stmt *replace_stmt, *begin_trans_stmt, *end_trans_stmt;\n+    std::string replace_str = \"REPLACE INTO test (key, value) VALUES (?, ?)\";\n+    std::string begin_trans_str = \"BEGIN TRANSACTION;\";\n+    std::string end_trans_str = \"END TRANSACTION;\";\n+\n+    // Check for synchronous flag in options\n+    std::string sync_stmt = (write_sync) ? \"PRAGMA synchronous = FULL\" :\n+                                           \"PRAGMA synchronous = OFF\";\n+    status = sqlite3_exec(db_, sync_stmt.c_str(), NULL, NULL, &err_msg);\n+    ExecErrorCheck(status, err_msg);\n+\n+    // Preparing sqlite3 statements\n+    status = sqlite3_prepare_v2(db_, replace_str.c_str(), -1,\n+                                &replace_stmt, NULL);\n+    ErrorCheck(status);\n+    status = sqlite3_prepare_v2(db_, begin_trans_str.c_str(), -1,\n+                                &begin_trans_stmt, NULL);\n+    ErrorCheck(status);\n+    status = sqlite3_prepare_v2(db_, end_trans_str.c_str(), -1,\n+                                &end_trans_stmt, NULL);\n+    ErrorCheck(status);\n+\n+    bool transaction = (entries_per_batch > 1);\n+    for (int i = 0; i < num_entries; i += entries_per_batch) {\n+      // Begin write transaction\n+      if (FLAGS_transaction && transaction) {\n+        status = sqlite3_step(begin_trans_stmt);\n+        StepErrorCheck(status);\n+        status = sqlite3_reset(begin_trans_stmt);\n+        ErrorCheck(status);\n+      }\n+\n+      // Create and execute SQL statements\n+      for (int j = 0; j < entries_per_batch; j++) {\n+        const char* value = gen_.Generate(value_size).data();\n+\n+        // Create values for key-value pair\n+        const int k = (order == SEQUENTIAL) ? i + j :\n+                      (rand_.Next() % num_entries);\n+        char key[100];\n+        snprintf(key, sizeof(key), \"%016d\", k);\n+\n+        // Bind KV values into replace_stmt\n+        status = sqlite3_bind_blob(replace_stmt, 1, key, 16, SQLITE_STATIC);\n+        ErrorCheck(status);\n+        status = sqlite3_bind_blob(replace_stmt, 2, value,\n+                                   value_size, SQLITE_STATIC);\n+        ErrorCheck(status);\n+\n+        // Execute replace_stmt\n+        bytes_ += value_size + strlen(key);\n+        status = sqlite3_step(replace_stmt);\n+        StepErrorCheck(status);\n+\n+        // Reset SQLite statement for another use\n+        status = sqlite3_clear_bindings(replace_stmt);\n+        ErrorCheck(status);\n+        status = sqlite3_reset(replace_stmt);\n+        ErrorCheck(status);\n+\n+        FinishedSingleOp();\n+      }\n+\n+      // End write transaction\n+      if (FLAGS_transaction && transaction) {\n+        status = sqlite3_step(end_trans_stmt);\n+        StepErrorCheck(status);\n+        status = sqlite3_reset(end_trans_stmt);\n+        ErrorCheck(status);\n+      }\n+    }\n+\n+    status = sqlite3_finalize(replace_stmt);\n+    ErrorCheck(status);\n+    status = sqlite3_finalize(begin_trans_stmt);\n+    ErrorCheck(status);\n+    status = sqlite3_finalize(end_trans_stmt);\n+    ErrorCheck(status);\n+  }\n+\n+  void Read(Order order, int entries_per_batch) {\n+    int status;\n+    sqlite3_stmt *read_stmt, *begin_trans_stmt, *end_trans_stmt;\n+\n+    std::string read_str = \"SELECT * FROM test WHERE key = ?\";\n+    std::string begin_trans_str = \"BEGIN TRANSACTION;\";\n+    std::string end_trans_str = \"END TRANSACTION;\";\n+\n+    // Preparing sqlite3 statements\n+    status = sqlite3_prepare_v2(db_, begin_trans_str.c_str(), -1,\n+                                &begin_trans_stmt, NULL);\n+    ErrorCheck(status);\n+    status = sqlite3_prepare_v2(db_, end_trans_str.c_str(), -1,\n+                                &end_trans_stmt, NULL);\n+    ErrorCheck(status);\n+    status = sqlite3_prepare_v2(db_, read_str.c_str(), -1, &read_stmt, NULL);\n+    ErrorCheck(status);\n+\n+    bool transaction = (entries_per_batch > 1);\n+    for (int i = 0; i < reads_; i += entries_per_batch) {\n+      // Begin read transaction\n+      if (FLAGS_transaction && transaction) {\n+        status = sqlite3_step(begin_trans_stmt);\n+        StepErrorCheck(status);\n+        status = sqlite3_reset(begin_trans_stmt);\n+        ErrorCheck(status);\n+      }\n+\n+      // Create and execute SQL statements\n+      for (int j = 0; j < entries_per_batch; j++) {\n+        // Create key value\n+        char key[100];\n+        int k = (order == SEQUENTIAL) ? i + j : (rand_.Next() % reads_);\n+        snprintf(key, sizeof(key), \"%016d\", k);\n+\n+        // Bind key value into read_stmt\n+        status = sqlite3_bind_blob(read_stmt, 1, key, 16, SQLITE_STATIC);\n+        ErrorCheck(status);\n+\n+        // Execute read statement\n+        while ((status = sqlite3_step(read_stmt)) == SQLITE_ROW) {}\n+        StepErrorCheck(status);\n+\n+        // Reset SQLite statement for another use\n+        status = sqlite3_clear_bindings(read_stmt);\n+        ErrorCheck(status);\n+        status = sqlite3_reset(read_stmt);\n+        ErrorCheck(status);\n+        FinishedSingleOp();\n+      }\n+\n+      // End read transaction\n+      if (FLAGS_transaction && transaction) {\n+        status = sqlite3_step(end_trans_stmt);\n+        StepErrorCheck(status);\n+        status = sqlite3_reset(end_trans_stmt);\n+        ErrorCheck(status);\n+      }\n+    }\n+\n+    status = sqlite3_finalize(read_stmt);\n+    ErrorCheck(status);\n+    status = sqlite3_finalize(begin_trans_stmt);\n+    ErrorCheck(status);\n+    status = sqlite3_finalize(end_trans_stmt);\n+    ErrorCheck(status);\n+  }\n+\n+  void ReadSequential() {\n+    int status;\n+    sqlite3_stmt *pStmt;\n+    std::string read_str = \"SELECT * FROM test ORDER BY key\";\n+\n+    status = sqlite3_prepare_v2(db_, read_str.c_str(), -1, &pStmt, NULL);\n+    ErrorCheck(status);\n+    for (int i = 0; i < reads_ && SQLITE_ROW == sqlite3_step(pStmt); i++) {\n+      bytes_ += sqlite3_column_bytes(pStmt, 1) + sqlite3_column_bytes(pStmt, 2);\n+      FinishedSingleOp();\n+    }\n+\n+    status = sqlite3_finalize(pStmt);\n+    ErrorCheck(status);\n+  }\n+\n+};\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  std::string default_db_path;\n+  for (int i = 1; i < argc; i++) {\n+    double d;\n+    int n;\n+    char junk;\n+    if (leveldb::Slice(argv[i]).starts_with(\"--benchmarks=\")) {\n+      FLAGS_benchmarks = argv[i] + strlen(\"--benchmarks=\");\n+    } else if (sscanf(argv[i], \"--histogram=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_histogram = n;\n+    } else if (sscanf(argv[i], \"--compression_ratio=%lf%c\", &d, &junk) == 1) {\n+      FLAGS_compression_ratio = d;\n+    } else if (sscanf(argv[i], \"--use_existing_db=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_use_existing_db = n;\n+    } else if (sscanf(argv[i], \"--num=%d%c\", &n, &junk) == 1) {\n+      FLAGS_num = n;\n+    } else if (sscanf(argv[i], \"--reads=%d%c\", &n, &junk) == 1) {\n+      FLAGS_reads = n;\n+    } else if (sscanf(argv[i], \"--value_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_value_size = n;\n+    } else if (leveldb::Slice(argv[i]) == leveldb::Slice(\"--no_transaction\")) {\n+      FLAGS_transaction = false;\n+    } else if (sscanf(argv[i], \"--page_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_page_size = n;\n+    } else if (sscanf(argv[i], \"--num_pages=%d%c\", &n, &junk) == 1) {\n+      FLAGS_num_pages = n;\n+    } else if (sscanf(argv[i], \"--WAL_enabled=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_WAL_enabled = n;\n+    } else if (strncmp(argv[i], \"--db=\", 5) == 0) {\n+      FLAGS_db = argv[i] + 5;\n+    } else {\n+      fprintf(stderr, \"Invalid flag '%s'\\n\", argv[i]);\n+      exit(1);\n+    }\n+  }\n+\n+  // Choose a location for the test database if none given with --db=<path>\n+  if (FLAGS_db == NULL) {\n+      leveldb::Env::Default()->GetTestDirectory(&default_db_path);\n+      default_db_path += \"/dbbench\";\n+      FLAGS_db = default_db_path.c_str();\n+  }\n+\n+  leveldb::Benchmark benchmark;\n+  benchmark.Run();\n+  return 0;\n+}"
      },
      {
        "sha": "ed86f031c25fe931b0e3a05f4501269afd233f02",
        "filename": "doc/bench/db_bench_tree_db.cc",
        "status": "added",
        "additions": 528,
        "deletions": 0,
        "changes": 528,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/bench/db_bench_tree_db.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/bench/db_bench_tree_db.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/bench/db_bench_tree_db.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,528 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include <stdio.h>\n+#include <stdlib.h>\n+#include <kcpolydb.h>\n+#include \"util/histogram.h\"\n+#include \"util/random.h\"\n+#include \"util/testutil.h\"\n+\n+// Comma-separated list of operations to run in the specified order\n+//   Actual benchmarks:\n+//\n+//   fillseq       -- write N values in sequential key order in async mode\n+//   fillrandom    -- write N values in random key order in async mode\n+//   overwrite     -- overwrite N values in random key order in async mode\n+//   fillseqsync   -- write N/100 values in sequential key order in sync mode\n+//   fillrandsync  -- write N/100 values in random key order in sync mode\n+//   fillrand100K  -- write N/1000 100K values in random order in async mode\n+//   fillseq100K   -- write N/1000 100K values in seq order in async mode\n+//   readseq       -- read N times sequentially\n+//   readseq100K   -- read N/1000 100K values in sequential order in async mode\n+//   readrand100K  -- read N/1000 100K values in sequential order in async mode\n+//   readrandom    -- read N times in random order\n+static const char* FLAGS_benchmarks =\n+    \"fillseq,\"\n+    \"fillseqsync,\"\n+    \"fillrandsync,\"\n+    \"fillrandom,\"\n+    \"overwrite,\"\n+    \"readrandom,\"\n+    \"readseq,\"\n+    \"fillrand100K,\"\n+    \"fillseq100K,\"\n+    \"readseq100K,\"\n+    \"readrand100K,\"\n+    ;\n+\n+// Number of key/values to place in database\n+static int FLAGS_num = 1000000;\n+\n+// Number of read operations to do.  If negative, do FLAGS_num reads.\n+static int FLAGS_reads = -1;\n+\n+// Size of each value\n+static int FLAGS_value_size = 100;\n+\n+// Arrange to generate values that shrink to this fraction of\n+// their original size after compression\n+static double FLAGS_compression_ratio = 0.5;\n+\n+// Print histogram of operation timings\n+static bool FLAGS_histogram = false;\n+\n+// Cache size. Default 4 MB\n+static int FLAGS_cache_size = 4194304;\n+\n+// Page size. Default 1 KB\n+static int FLAGS_page_size = 1024;\n+\n+// If true, do not destroy the existing database.  If you set this\n+// flag and also specify a benchmark that wants a fresh database, that\n+// benchmark will fail.\n+static bool FLAGS_use_existing_db = false;\n+\n+// Compression flag. If true, compression is on. If false, compression\n+// is off.\n+static bool FLAGS_compression = true;\n+\n+// Use the db with the following name.\n+static const char* FLAGS_db = NULL;\n+\n+inline\n+static void DBSynchronize(kyotocabinet::TreeDB* db_)\n+{\n+  // Synchronize will flush writes to disk\n+  if (!db_->synchronize()) {\n+    fprintf(stderr, \"synchronize error: %s\\n\", db_->error().name());\n+  }\n+}\n+\n+namespace leveldb {\n+\n+// Helper for quickly generating random data.\n+namespace {\n+class RandomGenerator {\n+ private:\n+  std::string data_;\n+  int pos_;\n+\n+ public:\n+  RandomGenerator() {\n+    // We use a limited amount of data over and over again and ensure\n+    // that it is larger than the compression window (32KB), and also\n+    // large enough to serve all typical value sizes we want to write.\n+    Random rnd(301);\n+    std::string piece;\n+    while (data_.size() < 1048576) {\n+      // Add a short fragment that is as compressible as specified\n+      // by FLAGS_compression_ratio.\n+      test::CompressibleString(&rnd, FLAGS_compression_ratio, 100, &piece);\n+      data_.append(piece);\n+    }\n+    pos_ = 0;\n+  }\n+\n+  Slice Generate(int len) {\n+    if (pos_ + len > data_.size()) {\n+      pos_ = 0;\n+      assert(len < data_.size());\n+    }\n+    pos_ += len;\n+    return Slice(data_.data() + pos_ - len, len);\n+  }\n+};\n+\n+static Slice TrimSpace(Slice s) {\n+  int start = 0;\n+  while (start < s.size() && isspace(s[start])) {\n+    start++;\n+  }\n+  int limit = s.size();\n+  while (limit > start && isspace(s[limit-1])) {\n+    limit--;\n+  }\n+  return Slice(s.data() + start, limit - start);\n+}\n+\n+}  // namespace\n+\n+class Benchmark {\n+ private:\n+  kyotocabinet::TreeDB* db_;\n+  int db_num_;\n+  int num_;\n+  int reads_;\n+  double start_;\n+  double last_op_finish_;\n+  int64_t bytes_;\n+  std::string message_;\n+  Histogram hist_;\n+  RandomGenerator gen_;\n+  Random rand_;\n+  kyotocabinet::LZOCompressor<kyotocabinet::LZO::RAW> comp_;\n+\n+  // State kept for progress messages\n+  int done_;\n+  int next_report_;     // When to report next\n+\n+  void PrintHeader() {\n+    const int kKeySize = 16;\n+    PrintEnvironment();\n+    fprintf(stdout, \"Keys:       %d bytes each\\n\", kKeySize);\n+    fprintf(stdout, \"Values:     %d bytes each (%d bytes after compression)\\n\",\n+            FLAGS_value_size,\n+            static_cast<int>(FLAGS_value_size * FLAGS_compression_ratio + 0.5));\n+    fprintf(stdout, \"Entries:    %d\\n\", num_);\n+    fprintf(stdout, \"RawSize:    %.1f MB (estimated)\\n\",\n+            ((static_cast<int64_t>(kKeySize + FLAGS_value_size) * num_)\n+             / 1048576.0));\n+    fprintf(stdout, \"FileSize:   %.1f MB (estimated)\\n\",\n+            (((kKeySize + FLAGS_value_size * FLAGS_compression_ratio) * num_)\n+             / 1048576.0));\n+    PrintWarnings();\n+    fprintf(stdout, \"------------------------------------------------\\n\");\n+  }\n+\n+  void PrintWarnings() {\n+#if defined(__GNUC__) && !defined(__OPTIMIZE__)\n+    fprintf(stdout,\n+            \"WARNING: Optimization is disabled: benchmarks unnecessarily slow\\n\"\n+            );\n+#endif\n+#ifndef NDEBUG\n+    fprintf(stdout,\n+            \"WARNING: Assertions are enabled; benchmarks unnecessarily slow\\n\");\n+#endif\n+  }\n+\n+  void PrintEnvironment() {\n+    fprintf(stderr, \"Kyoto Cabinet:    version %s, lib ver %d, lib rev %d\\n\",\n+            kyotocabinet::VERSION, kyotocabinet::LIBVER, kyotocabinet::LIBREV);\n+\n+#if defined(__linux)\n+    time_t now = time(NULL);\n+    fprintf(stderr, \"Date:           %s\", ctime(&now));  // ctime() adds newline\n+\n+    FILE* cpuinfo = fopen(\"/proc/cpuinfo\", \"r\");\n+    if (cpuinfo != NULL) {\n+      char line[1000];\n+      int num_cpus = 0;\n+      std::string cpu_type;\n+      std::string cache_size;\n+      while (fgets(line, sizeof(line), cpuinfo) != NULL) {\n+        const char* sep = strchr(line, ':');\n+        if (sep == NULL) {\n+          continue;\n+        }\n+        Slice key = TrimSpace(Slice(line, sep - 1 - line));\n+        Slice val = TrimSpace(Slice(sep + 1));\n+        if (key == \"model name\") {\n+          ++num_cpus;\n+          cpu_type = val.ToString();\n+        } else if (key == \"cache size\") {\n+          cache_size = val.ToString();\n+        }\n+      }\n+      fclose(cpuinfo);\n+      fprintf(stderr, \"CPU:            %d * %s\\n\", num_cpus, cpu_type.c_str());\n+      fprintf(stderr, \"CPUCache:       %s\\n\", cache_size.c_str());\n+    }\n+#endif\n+  }\n+\n+  void Start() {\n+    start_ = Env::Default()->NowMicros() * 1e-6;\n+    bytes_ = 0;\n+    message_.clear();\n+    last_op_finish_ = start_;\n+    hist_.Clear();\n+    done_ = 0;\n+    next_report_ = 100;\n+  }\n+\n+  void FinishedSingleOp() {\n+    if (FLAGS_histogram) {\n+      double now = Env::Default()->NowMicros() * 1e-6;\n+      double micros = (now - last_op_finish_) * 1e6;\n+      hist_.Add(micros);\n+      if (micros > 20000) {\n+        fprintf(stderr, \"long op: %.1f micros%30s\\r\", micros, \"\");\n+        fflush(stderr);\n+      }\n+      last_op_finish_ = now;\n+    }\n+\n+    done_++;\n+    if (done_ >= next_report_) {\n+      if      (next_report_ < 1000)   next_report_ += 100;\n+      else if (next_report_ < 5000)   next_report_ += 500;\n+      else if (next_report_ < 10000)  next_report_ += 1000;\n+      else if (next_report_ < 50000)  next_report_ += 5000;\n+      else if (next_report_ < 100000) next_report_ += 10000;\n+      else if (next_report_ < 500000) next_report_ += 50000;\n+      else                            next_report_ += 100000;\n+      fprintf(stderr, \"... finished %d ops%30s\\r\", done_, \"\");\n+      fflush(stderr);\n+    }\n+  }\n+\n+  void Stop(const Slice& name) {\n+    double finish = Env::Default()->NowMicros() * 1e-6;\n+\n+    // Pretend at least one op was done in case we are running a benchmark\n+    // that does not call FinishedSingleOp().\n+    if (done_ < 1) done_ = 1;\n+\n+    if (bytes_ > 0) {\n+      char rate[100];\n+      snprintf(rate, sizeof(rate), \"%6.1f MB/s\",\n+               (bytes_ / 1048576.0) / (finish - start_));\n+      if (!message_.empty()) {\n+        message_  = std::string(rate) + \" \" + message_;\n+      } else {\n+        message_ = rate;\n+      }\n+    }\n+\n+    fprintf(stdout, \"%-12s : %11.3f micros/op;%s%s\\n\",\n+            name.ToString().c_str(),\n+            (finish - start_) * 1e6 / done_,\n+            (message_.empty() ? \"\" : \" \"),\n+            message_.c_str());\n+    if (FLAGS_histogram) {\n+      fprintf(stdout, \"Microseconds per op:\\n%s\\n\", hist_.ToString().c_str());\n+    }\n+    fflush(stdout);\n+  }\n+\n+ public:\n+  enum Order {\n+    SEQUENTIAL,\n+    RANDOM\n+  };\n+  enum DBState {\n+    FRESH,\n+    EXISTING\n+  };\n+\n+  Benchmark()\n+  : db_(NULL),\n+    num_(FLAGS_num),\n+    reads_(FLAGS_reads < 0 ? FLAGS_num : FLAGS_reads),\n+    bytes_(0),\n+    rand_(301) {\n+    std::vector<std::string> files;\n+    std::string test_dir;\n+    Env::Default()->GetTestDirectory(&test_dir);\n+    Env::Default()->GetChildren(test_dir.c_str(), &files);\n+    if (!FLAGS_use_existing_db) {\n+      for (int i = 0; i < files.size(); i++) {\n+        if (Slice(files[i]).starts_with(\"dbbench_polyDB\")) {\n+          std::string file_name(test_dir);\n+          file_name += \"/\";\n+          file_name += files[i];\n+          Env::Default()->DeleteFile(file_name.c_str());\n+        }\n+      }\n+    }\n+  }\n+\n+  ~Benchmark() {\n+    if (!db_->close()) {\n+      fprintf(stderr, \"close error: %s\\n\", db_->error().name());\n+    }\n+  }\n+\n+  void Run() {\n+    PrintHeader();\n+    Open(false);\n+\n+    const char* benchmarks = FLAGS_benchmarks;\n+    while (benchmarks != NULL) {\n+      const char* sep = strchr(benchmarks, ',');\n+      Slice name;\n+      if (sep == NULL) {\n+        name = benchmarks;\n+        benchmarks = NULL;\n+      } else {\n+        name = Slice(benchmarks, sep - benchmarks);\n+        benchmarks = sep + 1;\n+      }\n+\n+      Start();\n+\n+      bool known = true;\n+      bool write_sync = false;\n+      if (name == Slice(\"fillseq\")) {\n+        Write(write_sync, SEQUENTIAL, FRESH, num_, FLAGS_value_size, 1);\n+        \n+      } else if (name == Slice(\"fillrandom\")) {\n+        Write(write_sync, RANDOM, FRESH, num_, FLAGS_value_size, 1);\n+        DBSynchronize(db_);\n+      } else if (name == Slice(\"overwrite\")) {\n+        Write(write_sync, RANDOM, EXISTING, num_, FLAGS_value_size, 1);\n+        DBSynchronize(db_);\n+      } else if (name == Slice(\"fillrandsync\")) {\n+        write_sync = true;\n+        Write(write_sync, RANDOM, FRESH, num_ / 100, FLAGS_value_size, 1);\n+        DBSynchronize(db_);\n+      } else if (name == Slice(\"fillseqsync\")) {\n+        write_sync = true;\n+        Write(write_sync, SEQUENTIAL, FRESH, num_ / 100, FLAGS_value_size, 1);\n+        DBSynchronize(db_);\n+      } else if (name == Slice(\"fillrand100K\")) {\n+        Write(write_sync, RANDOM, FRESH, num_ / 1000, 100 * 1000, 1);\n+        DBSynchronize(db_);\n+      } else if (name == Slice(\"fillseq100K\")) {\n+        Write(write_sync, SEQUENTIAL, FRESH, num_ / 1000, 100 * 1000, 1);\n+        DBSynchronize(db_);\n+      } else if (name == Slice(\"readseq\")) {\n+        ReadSequential();\n+      } else if (name == Slice(\"readrandom\")) {\n+        ReadRandom();\n+      } else if (name == Slice(\"readrand100K\")) {\n+        int n = reads_;\n+        reads_ /= 1000;\n+        ReadRandom();\n+        reads_ = n;\n+      } else if (name == Slice(\"readseq100K\")) {\n+        int n = reads_;\n+        reads_ /= 1000;\n+        ReadSequential();\n+        reads_ = n;\n+      } else {\n+        known = false;\n+        if (name != Slice()) {  // No error message for empty name\n+          fprintf(stderr, \"unknown benchmark '%s'\\n\", name.ToString().c_str());\n+        }\n+      }\n+      if (known) {\n+        Stop(name);\n+      }\n+    }\n+  }\n+\n+ private:\n+    void Open(bool sync) {\n+    assert(db_ == NULL);\n+\n+    // Initialize db_\n+    db_ = new kyotocabinet::TreeDB();\n+    char file_name[100];\n+    db_num_++;\n+    std::string test_dir;\n+    Env::Default()->GetTestDirectory(&test_dir);\n+    snprintf(file_name, sizeof(file_name),\n+             \"%s/dbbench_polyDB-%d.kct\",\n+             test_dir.c_str(),\n+             db_num_);\n+\n+    // Create tuning options and open the database\n+    int open_options = kyotocabinet::PolyDB::OWRITER |\n+                       kyotocabinet::PolyDB::OCREATE;\n+    int tune_options = kyotocabinet::TreeDB::TSMALL |\n+        kyotocabinet::TreeDB::TLINEAR;\n+    if (FLAGS_compression) {\n+      tune_options |= kyotocabinet::TreeDB::TCOMPRESS;\n+      db_->tune_compressor(&comp_);\n+    }\n+    db_->tune_options(tune_options);\n+    db_->tune_page_cache(FLAGS_cache_size);\n+    db_->tune_page(FLAGS_page_size);\n+    db_->tune_map(256LL<<20);\n+    if (sync) {\n+      open_options |= kyotocabinet::PolyDB::OAUTOSYNC;\n+    }\n+    if (!db_->open(file_name, open_options)) {\n+      fprintf(stderr, \"open error: %s\\n\", db_->error().name());\n+    }\n+  }\n+\n+  void Write(bool sync, Order order, DBState state,\n+             int num_entries, int value_size, int entries_per_batch) {\n+    // Create new database if state == FRESH\n+    if (state == FRESH) {\n+      if (FLAGS_use_existing_db) {\n+        message_ = \"skipping (--use_existing_db is true)\";\n+        return;\n+      }\n+      delete db_;\n+      db_ = NULL;\n+      Open(sync);\n+      Start();  // Do not count time taken to destroy/open\n+    }\n+\n+    if (num_entries != num_) {\n+      char msg[100];\n+      snprintf(msg, sizeof(msg), \"(%d ops)\", num_entries);\n+      message_ = msg;\n+    }\n+\n+    // Write to database\n+    for (int i = 0; i < num_entries; i++)\n+    {\n+      const int k = (order == SEQUENTIAL) ? i : (rand_.Next() % num_entries);\n+      char key[100];\n+      snprintf(key, sizeof(key), \"%016d\", k);\n+      bytes_ += value_size + strlen(key);\n+      std::string cpp_key = key;\n+      if (!db_->set(cpp_key, gen_.Generate(value_size).ToString())) {\n+        fprintf(stderr, \"set error: %s\\n\", db_->error().name());\n+      }\n+      FinishedSingleOp();\n+    }\n+  }\n+\n+  void ReadSequential() {\n+    kyotocabinet::DB::Cursor* cur = db_->cursor();\n+    cur->jump();\n+    std::string ckey, cvalue;\n+    while (cur->get(&ckey, &cvalue, true)) {\n+      bytes_ += ckey.size() + cvalue.size();\n+      FinishedSingleOp();\n+    }\n+    delete cur;\n+  }\n+\n+  void ReadRandom() {\n+    std::string value;\n+    for (int i = 0; i < reads_; i++) {\n+      char key[100];\n+      const int k = rand_.Next() % reads_;\n+      snprintf(key, sizeof(key), \"%016d\", k);\n+      db_->get(key, &value);\n+      FinishedSingleOp();\n+    }\n+  }\n+};\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  std::string default_db_path;\n+  for (int i = 1; i < argc; i++) {\n+    double d;\n+    int n;\n+    char junk;\n+    if (leveldb::Slice(argv[i]).starts_with(\"--benchmarks=\")) {\n+      FLAGS_benchmarks = argv[i] + strlen(\"--benchmarks=\");\n+    } else if (sscanf(argv[i], \"--compression_ratio=%lf%c\", &d, &junk) == 1) {\n+      FLAGS_compression_ratio = d;\n+    } else if (sscanf(argv[i], \"--histogram=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_histogram = n;\n+    } else if (sscanf(argv[i], \"--num=%d%c\", &n, &junk) == 1) {\n+      FLAGS_num = n;\n+    } else if (sscanf(argv[i], \"--reads=%d%c\", &n, &junk) == 1) {\n+      FLAGS_reads = n;\n+    } else if (sscanf(argv[i], \"--value_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_value_size = n;\n+    } else if (sscanf(argv[i], \"--cache_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_cache_size = n;\n+    } else if (sscanf(argv[i], \"--page_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_page_size = n;\n+    } else if (sscanf(argv[i], \"--compression=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_compression = (n == 1) ? true : false;\n+    } else if (strncmp(argv[i], \"--db=\", 5) == 0) {\n+      FLAGS_db = argv[i] + 5;\n+    } else {\n+      fprintf(stderr, \"Invalid flag '%s'\\n\", argv[i]);\n+      exit(1);\n+    }\n+  }\n+\n+  // Choose a location for the test database if none given with --db=<path>\n+  if (FLAGS_db == NULL) {\n+      leveldb::Env::Default()->GetTestDirectory(&default_db_path);\n+      default_db_path += \"/dbbench\";\n+      FLAGS_db = default_db_path.c_str();\n+  }\n+\n+  leveldb::Benchmark benchmark;\n+  benchmark.Run();\n+  return 0;\n+}"
      },
      {
        "sha": "c4639772c175b463f6f41eeb0cd0c73fe7c16d68",
        "filename": "doc/benchmark.html",
        "status": "added",
        "additions": 459,
        "deletions": 0,
        "changes": 459,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/benchmark.html",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/benchmark.html",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/benchmark.html?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,459 @@\n+<!DOCTYPE html>\n+<html>\n+<head>\n+<title>LevelDB Benchmarks</title>\n+<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n+<style>\n+body {\n+  font-family:Helvetica,sans-serif;\n+  padding:20px;\n+}\n+\n+h2 {\n+  padding-top:30px;\n+}\n+\n+table.bn {\n+  width:800px;\n+  border-collapse:collapse;\n+  border:0;\n+  padding:0;\n+}\n+\n+table.bnbase {\n+  width:650px;\n+}\n+\n+table.bn td {\n+  padding:2px 0;\n+}\n+\n+table.bn td.c1 {\n+  font-weight:bold;\n+  width:150px;\n+}\n+\n+table.bn td.c1 div.e {\n+  float:right;\n+  font-weight:normal;\n+}\n+\n+table.bn td.c2 {\n+  width:150px;\n+  text-align:right;\n+  padding:2px;\n+}\n+\n+table.bn td.c3 {\n+  width:350px;\n+}\n+\n+table.bn td.c4 {\n+  width:150px;\n+  font-size:small;\n+  padding-left:4px;\n+}\n+\n+/* chart bars */\n+div.bldb {\n+  background-color:#0255df;\n+}\n+\n+div.bkct {\n+  background-color:#df5555;\n+}\n+\n+div.bsql {\n+  background-color:#aadf55;\n+}\n+\n+.code {\n+  font-family:monospace;\n+  font-size:large;\n+}\n+\n+.todo {\n+  color: red;\n+}\n+\n+</style>\n+</head>\n+<body>\n+<h1>LevelDB Benchmarks</h1>\n+<p>Google, July 2011</p>\n+<hr>\n+\n+<p>In order to test LevelDB's performance, we benchmark it against other well-established database implementations. We compare LevelDB (revision 39) against <a href=\"http://www.sqlite.org/\">SQLite3</a> (version 3.7.6.3) and <a href=\"http://fallabs.com/kyotocabinet/spex.html\">Kyoto Cabinet's</a> (version 1.2.67) TreeDB (a B+Tree based key-value store). We would like to acknowledge Scott Hess and Mikio Hirabayashi for their suggestions and contributions to the SQLite3 and Kyoto Cabinet benchmarks, respectively.</p>\n+\n+<p>Benchmarks were all performed on a six-core Intel(R) Xeon(R) CPU X5650 @ 2.67GHz, with 12288 KB of total L3 cache and 12 GB of DDR3 RAM at 1333 MHz. (Note that LevelDB uses at most two CPUs since the benchmarks are single threaded: one to run the benchmark, and one for background compactions.) We ran the benchmarks on two machines (with identical processors), one with an Ext3 file system and one with an Ext4 file system. The machine with the Ext3 file system has a SATA Hitachi HDS721050CLA362 hard drive. The machine with the Ext4 file system has a SATA Samsung HD502HJ hard drive. Both hard drives spin at 7200 RPM and have hard drive write-caching enabled (using `hdparm -W 1 [device]`). The numbers reported below are the median of three measurements.</p>\n+\n+<h4>Benchmark Source Code</h4>\n+<p>We wrote benchmark tools for SQLite and Kyoto TreeDB based on LevelDB's <span class=\"code\">db_bench</span>. The code for each of the benchmarks resides here:</p>\n+<ul>\n+\t<li> <b>LevelDB:</b> <a href=\"http://code.google.com/p/leveldb/source/browse/trunk/db/db_bench.cc\">db/db_bench.cc</a>.</li>\n+\t<li> <b>SQLite:</b> <a href=\"http://code.google.com/p/leveldb/source/browse/#svn%2Ftrunk%2Fdoc%2Fbench%2Fdb_bench_sqlite3.cc\">doc/bench/db_bench_sqlite3.cc</a>.</li>\n+\t<li> <b>Kyoto TreeDB:</b> <a href=\"http://code.google.com/p/leveldb/source/browse/#svn%2Ftrunk%2Fdoc%2Fbench%2Fdb_bench_tree_db.cc\">doc/bench/db_bench_tree_db.cc</a>.</li>\n+</ul>\n+\n+<h4>Custom Build Specifications</h4>\n+<ul>\n+<li>LevelDB: LevelDB was compiled with the <a href=\"http://code.google.com/p/google-perftools\">tcmalloc</a> library and the <a href=\"http://code.google.com/p/snappy/\">Snappy</a> compression library (revision 33).  Assertions were disabled.</li>\n+<li>TreeDB: TreeDB was compiled using the <a href=\"http://www.oberhumer.com/opensource/lzo/\">LZO</a> compression library (version 2.03). Furthermore, we enabled the TSMALL and TLINEAR options when opening the database in order to reduce the footprint of each record.</li>\n+<li>SQLite: We tuned SQLite's performance, by setting its locking mode to exclusive.  We also enabled SQLite's <a href=\"http://www.sqlite.org/draft/wal.html\">write-ahead logging</a>.</li>\n+</ul>\n+\n+<h2>1. Baseline Performance</h2>\n+<p>This section gives the baseline performance of all the\n+databases.  Following sections show how performance changes as various\n+parameters are varied.  For the baseline:</p>\n+<ul>\n+\t<li> Each database is allowed 4 MB of cache memory.</li>\n+        <li> Databases are opened in <em>asynchronous</em> write mode.\n+             (LevelDB's sync option, TreeDB's OAUTOSYNC option, and\n+             SQLite3's synchronous options are all turned off).  I.e.,\n+             every write is pushed to the operating system, but the\n+             benchmark does not wait for the write to reach the disk.</li>\n+\t<li> Keys are 16 bytes each.</li>\n+        <li> Value are 100 bytes each (with enough redundancy so that\n+             a simple compressor shrinks them to 50% of their original\n+             size).</li>\n+\t<li> Sequential reads/writes traverse the key space in increasing order.</li>\n+\t<li> Random reads/writes traverse the key space in random order.</li>\n+</ul>\n+\n+<h3>A. Sequential Reads</h3>\n+<table class=\"bn bnbase\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">4,030,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">1,010,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:95px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">383,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:33px\">&nbsp;</div></td>\n+</table>\n+<h3>B. Random Reads</h3>\n+<table class=\"bn bnbase\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">129,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:298px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">151,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:350px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">134,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:310px\">&nbsp;</div></td>\n+</table>\n+<h3>C. Sequential Writes</h3>\n+<table class=\"bn bnbase\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">779,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">342,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:154px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">48,600 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:22px\">&nbsp;</div></td>\n+</table>\n+<h3>D. Random Writes</h3>\n+<table class=\"bn bnbase\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">164,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">88,500 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:188px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">9,860 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:21px\">&nbsp;</div></td>\n+</table>\n+\n+<p>LevelDB outperforms both SQLite3 and TreeDB in sequential and random write operations and sequential read operations. Kyoto Cabinet has the fastest random read operations.</p>\n+\n+<h2>2. Write Performance under Different Configurations</h2>\n+<h3>A. Large Values </h3>\n+<p>For this benchmark, we start with an empty database, and write 100,000 byte values (~50% compressible). To keep the benchmark running time reasonable, we stop after writing 1000 values.</p>\n+<h4>Sequential Writes</h4>\n+<table class=\"bn bnbase\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">1,100 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:234px\">&nbsp;</div></td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">1,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:224px\">&nbsp;</div></td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">1,600 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:350px\">&nbsp;</div></td></tr>\n+</table>\n+<h4>Random Writes</h4>\n+<table class=\"bn bnbase\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">480 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:105px\">&nbsp;</div></td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">1,100 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:240px\">&nbsp;</div></td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">1,600 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:350px\">&nbsp;</div></td></tr>\n+</table>\n+<p>LevelDB doesn't perform as well with large values of 100,000 bytes each. This is because LevelDB writes keys and values at least twice: first time to the transaction log, and second time (during a compaction) to a sorted file.\n+With larger values, LevelDB's per-operation efficiency is swamped by the\n+cost of extra copies of large values.</p>\n+<h3>B. Batch Writes</h3>\n+<p>A batch write is a set of writes that are applied atomically to the underlying database. A single batch of N writes may be significantly faster than N individual writes. The following benchmark writes one thousand batches where each batch contains one thousand 100-byte values. TreeDB does not support batch writes and is omitted from this benchmark.</p>\n+<h4>Sequential Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">840,000 entries/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.08x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">124,000 entries/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:52px\">&nbsp;</div></td>\n+    <td class=\"c4\">(2.55x baseline)</td></tr>\n+</table>\n+<h4>Random Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">221,000 entries/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.35x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">22,000 entries/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:34px\">&nbsp;</div></td>\n+    <td class=\"c4\">(2.23x baseline)</td></tr>\n+</table>\n+\n+<p>Because of the way LevelDB persistent storage is organized, batches of\n+random writes are not much slower (only a factor of 4x) than batches\n+of sequential writes.</p>\n+\n+<h3>C. Synchronous Writes</h3>\n+<p>In the following benchmark, we enable the synchronous writing modes\n+of all of the databases.  Since this change significantly slows down the\n+benchmark, we stop after 10,000 writes. For synchronous write tests, we've\n+disabled hard drive write-caching (using `hdparm -W 0 [device]`).</p>\n+<ul>\n+    <li>For LevelDB, we set WriteOptions.sync = true.</li>\n+    <li>In TreeDB, we enabled TreeDB's OAUTOSYNC option.</li>\n+    <li>For SQLite3, we set \"PRAGMA synchronous = FULL\".</li>\n+</ul>\n+<h4>Sequential Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">100 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.003x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">7 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:27px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.0004x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">88 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:315px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.002x baseline)</td></tr>\n+</table>\n+<h4>Random Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">100 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.015x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">8 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:29px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.001x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">88 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:314px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.009x baseline)</td></tr>\n+</table>\n+\n+<p>Also see the <code>ext4</code> performance numbers below\n+since synchronous writes behave significantly differently\n+on <code>ext3</code> and <code>ext4</code>.</p>\n+\n+<h3>D. Turning Compression Off</h3>\n+\n+<p>In the baseline measurements, LevelDB and TreeDB were using\n+light-weight compression\n+(<a href=\"http://code.google.com/p/snappy/\">Snappy</a> for LevelDB,\n+and <a href=\"http://www.oberhumer.com/opensource/lzo/\">LZO</a> for\n+TreeDB). SQLite3, by default does not use compression.  The\n+experiments below show what happens when compression is disabled in\n+all of the databases (the SQLite3 numbers are just a copy of\n+its baseline measurements):</p>\n+\n+<h4>Sequential Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">594,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.76x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">485,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:239px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.42x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">48,600 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:29px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.00x baseline)</td></tr>\n+</table>\n+<h4>Random Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">135,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:296px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.82x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">159,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.80x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">9,860 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:22px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.00x baseline)</td></tr>\n+</table>\n+\n+<p>LevelDB's write performance is better with compression than without\n+since compression decreases the amount of data that has to be written\n+to disk.  Therefore LevelDB users can leave compression enabled in\n+most scenarios without having worry about a tradeoff between space\n+usage and performance.  TreeDB's performance on the other hand is\n+better without compression than with compression.  Presumably this is\n+because TreeDB's compression library (LZO) is more expensive than\n+LevelDB's compression library (Snappy).<p>\n+\n+<h3>E. Using More Memory</h3>\n+<p>We increased the overall cache size for each database to 128 MB. For LevelDB, we partitioned 128 MB into a 120 MB write buffer and 8 MB of cache (up from 2 MB of write buffer and 2 MB of cache). For SQLite3, we kept the page size at 1024 bytes, but increased the number of pages to 131,072 (up from 4096). For TreeDB, we also kept the page size at 1024 bytes, but increased the cache size to 128 MB (up from 4 MB).</p>\n+<h4>Sequential Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">812,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.04x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">321,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:138px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.94x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">48,500 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:21px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.00x baseline)</td></tr>\n+</table>\n+<h4>Random Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">355,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(2.16x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">284,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:280px\">&nbsp;</div></td>\n+    <td class=\"c4\">(3.21x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">9,670 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:10px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.98x baseline)</td></tr>\n+</table>\n+\n+<p>SQLite's performance does not change substantially when compared to\n+the baseline, but the random write performance for both LevelDB and\n+TreeDB increases significantly.  LevelDB's performance improves\n+because a larger write buffer reduces the need to merge sorted files\n+(since it creates a smaller number of larger sorted files).  TreeDB's\n+performance goes up because the entire database is available in memory\n+for fast in-place updates.</p>\n+\n+  <h2>3. Read Performance under Different Configurations</h2>\n+<h3>A. Larger Caches</h3>\n+<p>We increased the overall memory usage to 128 MB for each database.\n+For LevelDB, we allocated 8 MB to LevelDB's write buffer and 120 MB\n+to LevelDB's cache. The other databases don't differentiate between a\n+write buffer and a cache, so we simply set their cache size to 128\n+MB.</p>\n+<h4>Sequential Reads</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">5,210,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.29x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">1,070,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:72px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.06x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">609,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:41px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.59x baseline)</td></tr>\n+</table>\n+\n+<h4>Random Reads</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">190,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:144px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.47x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">463,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(3.07x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">186,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:141px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.39x baseline)</td></tr>\n+</table>\n+\n+<p>As expected, the read performance of all of the databases increases\n+when the caches are enlarged.  In particular, TreeDB seems to make\n+very effective use of a cache that is large enough to hold the entire\n+database.</p>\n+\n+<h3>B. No Compression Reads </h3>\n+<p>For this benchmark, we populated a database with 1 million entries consisting of 16 byte keys and 100 byte values. We compiled LevelDB and Kyoto Cabinet without compression support, so results that are read out from the database are already uncompressed. We've listed the SQLite3 baseline read performance as a point of comparison.</p>\n+<h4>Sequential Reads</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">4,880,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.21x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">1,230,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:88px\">&nbsp;</div></td>\n+    <td class=\"c4\">(3.60x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">383,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:27px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.00x baseline)</td></tr>\n+</table>\n+<h4>Random Reads</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">149,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:300px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.16x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">175,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.16x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">134,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:268px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.00x baseline)</td></tr>\n+</table>\n+\n+<p>Performance of both LevelDB and TreeDB improves a small amount when\n+compression is disabled.  Note however that under different workloads,\n+performance may very well be better with compression if it allows more\n+of the working set to fit in memory.</p>\n+\n+<h2>Note about Ext4 Filesystems</h2>\n+<p>The preceding numbers are for an ext3 file system. Synchronous writes are much slower under <a href=\"http://en.wikipedia.org/wiki/Ext4\">ext4</a> (LevelDB drops to ~31 writes / second and TreeDB drops to ~5 writes / second; SQLite3's synchronous writes do not noticeably drop) due to ext4's different handling of <span class=\"code\">fsync</span> / <span class=\"code\">msync</span> calls. Even LevelDB's asynchronous write performance drops somewhat since it spreads its storage across multiple files and issues <span class=\"code\">fsync</span> calls when switching to a new file.</p>\n+\n+<h2>Acknowledgements</h2>\n+<p>Jeff Dean and Sanjay Ghemawat wrote LevelDB. Kevin Tseng wrote and compiled these benchmarks. Mikio Hirabayashi, Scott Hess, and Gabor Cselle provided help and advice.</p>\n+</body>\n+</html>"
      },
      {
        "sha": "700c564e433827c311cd8a26896ceaa7c8260818",
        "filename": "doc/doc.css",
        "status": "added",
        "additions": 89,
        "deletions": 0,
        "changes": 89,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/doc.css",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/doc.css",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/doc.css?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,89 @@\n+body {\n+  margin-left: 0.5in;\n+  margin-right: 0.5in;\n+  background: white;\n+  color: black;\n+}\n+\n+h1 {\n+  margin-left: -0.2in;\n+  font-size: 14pt;\n+}\n+h2 {\n+  margin-left: -0in;\n+  font-size: 12pt;\n+}\n+h3 {\n+  margin-left: -0in;\n+}\n+h4 {\n+  margin-left: -0in;\n+}\n+hr {\n+  margin-left: -0in;\n+}\n+\n+/* Definition lists: definition term bold */\n+dt {\n+  font-weight: bold;\n+}\n+\n+address {\n+  text-align: center;\n+}\n+code,samp,var {\n+  color: blue;\n+}\n+kbd {\n+  color: #600000;\n+}\n+div.note p {\n+  float: right;\n+  width: 3in;\n+  margin-right: 0%;\n+  padding: 1px;\n+  border: 2px solid #6060a0;\n+  background-color: #fffff0;\n+}\n+\n+ul {\n+  margin-top: -0em;\n+  margin-bottom: -0em;\n+}\n+\n+ol {\n+  margin-top: -0em;\n+  margin-bottom: -0em;\n+}\n+\n+UL.nobullets {\n+  list-style-type: none;\n+  list-style-image: none;\n+  margin-left: -1em;\n+}\n+\n+p {\n+  margin: 1em 0 1em 0;\n+  padding: 0 0 0 0;\n+}\n+\n+pre {\n+  line-height: 1.3em;\n+  padding: 0.4em 0 0.8em 0;\n+  margin:  0 0 0 0;\n+  border:  0 0 0 0;\n+  color: blue;\n+}\n+\n+.datatable {\n+  margin-left: auto;\n+  margin-right: auto;\n+  margin-top: 2em;\n+  margin-bottom: 2em;\n+  border: 1px solid;\n+}\n+\n+.datatable td,th {\n+  padding: 0 0.5em 0 0.5em;\n+  text-align: right;\n+}"
      },
      {
        "sha": "e870795d231463b167d5b79efdc16b80107de93e",
        "filename": "doc/impl.html",
        "status": "added",
        "additions": 213,
        "deletions": 0,
        "changes": 213,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/impl.html",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/impl.html",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/impl.html?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,213 @@\n+<!DOCTYPE html>\n+<html>\n+<head>\n+<link rel=\"stylesheet\" type=\"text/css\" href=\"doc.css\" />\n+<title>Leveldb file layout and compactions</title>\n+</head>\n+\n+<body>\n+\n+<h1>Files</h1>\n+\n+The implementation of leveldb is similar in spirit to the\n+representation of a single\n+<a href=\"http://labs.google.com/papers/bigtable.html\">\n+Bigtable tablet (section 5.3)</a>.\n+However the organization of the files that make up the representation\n+is somewhat different and is explained below.\n+\n+<p>\n+Each database is represented by a set of files stored in a directory.\n+There are several different types of files as documented below:\n+<p>\n+<h2>Log files</h2>\n+<p>\n+A log file (*.log) stores a sequence of recent updates.  Each update\n+is appended to the current log file.  When the log file reaches a\n+pre-determined size (approximately 4MB by default), it is converted\n+to a sorted table (see below) and a new log file is created for future\n+updates.\n+<p>\n+A copy of the current log file is kept in an in-memory structure (the\n+<code>memtable</code>).  This copy is consulted on every read so that read\n+operations reflect all logged updates.\n+<p>\n+<h2>Sorted tables</h2>\n+<p>\n+A sorted table (*.sst) stores a sequence of entries sorted by key.\n+Each entry is either a value for the key, or a deletion marker for the\n+key.  (Deletion markers are kept around to hide obsolete values\n+present in older sorted tables).\n+<p>\n+The set of sorted tables are organized into a sequence of levels.  The\n+sorted table generated from a log file is placed in a special <code>young</code>\n+level (also called level-0).  When the number of young files exceeds a\n+certain threshold (currently four), all of the young files are merged\n+together with all of the overlapping level-1 files to produce a\n+sequence of new level-1 files (we create a new level-1 file for every\n+2MB of data.)\n+<p>\n+Files in the young level may contain overlapping keys.  However files\n+in other levels have distinct non-overlapping key ranges.  Consider\n+level number L where L >= 1.  When the combined size of files in\n+level-L exceeds (10^L) MB (i.e., 10MB for level-1, 100MB for level-2,\n+...), one file in level-L, and all of the overlapping files in\n+level-(L+1) are merged to form a set of new files for level-(L+1).\n+These merges have the effect of gradually migrating new updates from\n+the young level to the largest level using only bulk reads and writes\n+(i.e., minimizing expensive seeks).\n+\n+<h2>Manifest</h2>\n+<p>\n+A MANIFEST file lists the set of sorted tables that make up each\n+level, the corresponding key ranges, and other important metadata.\n+A new MANIFEST file (with a new number embedded in the file name)\n+is created whenever the database is reopened.  The MANIFEST file is\n+formatted as a log, and changes made to the serving state (as files\n+are added or removed) are appended to this log.\n+<p>\n+<h2>Current</h2>\n+<p>\n+CURRENT is a simple text file that contains the name of the latest\n+MANIFEST file.\n+<p>\n+<h2>Info logs</h2>\n+<p>\n+Informational messages are printed to files named LOG and LOG.old.\n+<p>\n+<h2>Others</h2>\n+<p>\n+Other files used for miscellaneous purposes may also be present\n+(LOCK, *.dbtmp).\n+\n+<h1>Level 0</h1>\n+When the log file grows above a certain size (1MB by default):\n+<ul>\n+<li>Create a brand new memtable and log file and direct future updates here\n+<li>In the background:\n+<ul>\n+<li>Write the contents of the previous memtable to an sstable\n+<li>Discard the memtable\n+<li>Delete the old log file and the old memtable\n+<li>Add the new sstable to the young (level-0) level.\n+</ul>\n+</ul>\n+\n+<h1>Compactions</h1>\n+\n+<p>\n+When the size of level L exceeds its limit, we compact it in a\n+background thread.  The compaction picks a file from level L and all\n+overlapping files from the next level L+1.  Note that if a level-L\n+file overlaps only part of a level-(L+1) file, the entire file at\n+level-(L+1) is used as an input to the compaction and will be\n+discarded after the compaction.  Aside: because level-0 is special\n+(files in it may overlap each other), we treat compactions from\n+level-0 to level-1 specially: a level-0 compaction may pick more than\n+one level-0 file in case some of these files overlap each other.\n+\n+<p>\n+A compaction merges the contents of the picked files to produce a\n+sequence of level-(L+1) files.  We switch to producing a new\n+level-(L+1) file after the current output file has reached the target\n+file size (2MB).  We also switch to a new output file when the key\n+range of the current output file has grown enough to overlap more then\n+ten level-(L+2) files.  This last rule ensures that a later compaction\n+of a level-(L+1) file will not pick up too much data from level-(L+2).\n+\n+<p>\n+The old files are discarded and the new files are added to the serving\n+state.\n+\n+<p>\n+Compactions for a particular level rotate through the key space.  In\n+more detail, for each level L, we remember the ending key of the last\n+compaction at level L.  The next compaction for level L will pick the\n+first file that starts after this key (wrapping around to the\n+beginning of the key space if there is no such file).\n+\n+<p>\n+Compactions drop overwritten values.  They also drop deletion markers\n+if there are no higher numbered levels that contain a file whose range\n+overlaps the current key.\n+\n+<h2>Timing</h2>\n+\n+Level-0 compactions will read up to four 1MB files from level-0, and\n+at worst all the level-1 files (10MB).  I.e., we will read 14MB and\n+write 14MB.\n+\n+<p>\n+Other than the special level-0 compactions, we will pick one 2MB file\n+from level L.  In the worst case, this will overlap ~ 12 files from\n+level L+1 (10 because level-(L+1) is ten times the size of level-L,\n+and another two at the boundaries since the file ranges at level-L\n+will usually not be aligned with the file ranges at level-L+1).  The\n+compaction will therefore read 26MB and write 26MB.  Assuming a disk\n+IO rate of 100MB/s (ballpark range for modern drives), the worst\n+compaction cost will be approximately 0.5 second.\n+\n+<p>\n+If we throttle the background writing to something small, say 10% of\n+the full 100MB/s speed, a compaction may take up to 5 seconds.  If the\n+user is writing at 10MB/s, we might build up lots of level-0 files\n+(~50 to hold the 5*10MB).  This may signficantly increase the cost of\n+reads due to the overhead of merging more files together on every\n+read.\n+\n+<p>\n+Solution 1: To reduce this problem, we might want to increase the log\n+switching threshold when the number of level-0 files is large.  Though\n+the downside is that the larger this threshold, the more memory we will\n+need to hold the corresponding memtable.\n+\n+<p>\n+Solution 2: We might want to decrease write rate artificially when the\n+number of level-0 files goes up.\n+\n+<p>\n+Solution 3: We work on reducing the cost of very wide merges.\n+Perhaps most of the level-0 files will have their blocks sitting\n+uncompressed in the cache and we will only need to worry about the\n+O(N) complexity in the merging iterator.\n+\n+<h2>Number of files</h2>\n+\n+Instead of always making 2MB files, we could make larger files for\n+larger levels to reduce the total file count, though at the expense of\n+more bursty compactions.  Alternatively, we could shard the set of\n+files into multiple directories.\n+\n+<p>\n+An experiment on an <code>ext3</code> filesystem on Feb 04, 2011 shows\n+the following timings to do 100K file opens in directories with\n+varying number of files:\n+<table class=\"datatable\">\n+<tr><th>Files in directory</th><th>Microseconds to open a file</th></tr>\n+<tr><td>1000</td><td>9</td>\n+<tr><td>10000</td><td>10</td>\n+<tr><td>100000</td><td>16</td>\n+</table>\n+So maybe even the sharding is not necessary on modern filesystems?\n+\n+<h1>Recovery</h1>\n+\n+<ul>\n+<li> Read CURRENT to find name of the latest committed MANIFEST\n+<li> Read the named MANIFEST file\n+<li> Clean up stale files\n+<li> We could open all sstables here, but it is probably better to be lazy...\n+<li> Convert log chunk to a new level-0 sstable\n+<li> Start directing new writes to a new log file with recovered sequence#\n+</ul>\n+\n+<h1>Garbage collection of files</h1>\n+\n+<code>DeleteObsoleteFiles()</code> is called at the end of every\n+compaction and at the end of recovery.  It finds the names of all\n+files in the database.  It deletes all log files that are not the\n+current log file.  It deletes all table files that are not referenced\n+from some level and are not the output of an active compaction.\n+\n+</body>\n+</html>"
      },
      {
        "sha": "3ed0ed9d9e305ed630f39a201e3034e00d815b43",
        "filename": "doc/index.html",
        "status": "added",
        "additions": 549,
        "deletions": 0,
        "changes": 549,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/index.html",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/index.html",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/index.html?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,549 @@\n+<!DOCTYPE html>\n+<html>\n+<head>\n+<link rel=\"stylesheet\" type=\"text/css\" href=\"doc.css\" />\n+<title>Leveldb</title>\n+</head>\n+\n+<body>\n+<h1>Leveldb</h1>\n+<address>Jeff Dean, Sanjay Ghemawat</address>\n+<p>\n+The <code>leveldb</code> library provides a persistent key value store.  Keys and\n+values are arbitrary byte arrays.  The keys are ordered within the key\n+value store according to a user-specified comparator function.\n+\n+<p>\n+<h1>Opening A Database</h1>\n+<p>\n+A <code>leveldb</code> database has a name which corresponds to a file system\n+directory.  All of the contents of database are stored in this\n+directory.  The following example shows how to open a database,\n+creating it if necessary:\n+<p>\n+<pre>\n+  #include &lt;assert&gt;\n+  #include \"leveldb/db.h\"\n+\n+  leveldb::DB* db;\n+  leveldb::Options options;\n+  options.create_if_missing = true;\n+  leveldb::Status status = leveldb::DB::Open(options, \"/tmp/testdb\", &amp;db);\n+  assert(status.ok());\n+  ...\n+</pre>\n+If you want to raise an error if the database already exists, add\n+the following line before the <code>leveldb::DB::Open</code> call:\n+<pre>\n+  options.error_if_exists = true;\n+</pre>\n+<h1>Status</h1>\n+<p>\n+You may have noticed the <code>leveldb::Status</code> type above.  Values of this\n+type are returned by most functions in <code>leveldb</code> that may encounter an\n+error.  You can check if such a result is ok, and also print an\n+associated error message:\n+<p>\n+<pre>\n+   leveldb::Status s = ...;\n+   if (!s.ok()) cerr &lt;&lt; s.ToString() &lt;&lt; endl;\n+</pre>\n+<h1>Closing A Database</h1>\n+<p>\n+When you are done with a database, just delete the database object.\n+Example:\n+<p>\n+<pre>\n+  ... open the db as described above ...\n+  ... do something with db ...\n+  delete db;\n+</pre>\n+<h1>Reads And Writes</h1>\n+<p>\n+The database provides <code>Put</code>, <code>Delete</code>, and <code>Get</code> methods to\n+modify/query the database.  For example, the following code\n+moves the value stored under key1 to key2.\n+<pre>\n+  std::string value;\n+  leveldb::Status s = db-&gt;Get(leveldb::ReadOptions(), key1, &amp;value);\n+  if (s.ok()) s = db-&gt;Put(leveldb::WriteOptions(), key2, value);\n+  if (s.ok()) s = db-&gt;Delete(leveldb::WriteOptions(), key1);\n+</pre>\n+\n+<h1>Atomic Updates</h1>\n+<p>\n+Note that if the process dies after the Put of key2 but before the\n+delete of key1, the same value may be left stored under multiple keys.\n+Such problems can be avoided by using the <code>WriteBatch</code> class to\n+atomically apply a set of updates:\n+<p>\n+<pre>\n+  #include \"leveldb/write_batch.h\"\n+  ...\n+  std::string value;\n+  leveldb::Status s = db-&gt;Get(leveldb::ReadOptions(), key1, &amp;value);\n+  if (s.ok()) {\n+    leveldb::WriteBatch batch;\n+    batch.Delete(key1);\n+    batch.Put(key2, value);\n+    s = db-&gt;Write(leveldb::WriteOptions(), &amp;batch);\n+  }\n+</pre>\n+The <code>WriteBatch</code> holds a sequence of edits to be made to the database,\n+and these edits within the batch are applied in order.  Note that we\n+called <code>Delete</code> before <code>Put</code> so that if <code>key1</code> is identical to <code>key2</code>,\n+we do not end up erroneously dropping the value entirely.\n+<p>\n+Apart from its atomicity benefits, <code>WriteBatch</code> may also be used to\n+speed up bulk updates by placing lots of individual mutations into the\n+same batch.\n+\n+<h1>Synchronous Writes</h1>\n+By default, each write to <code>leveldb</code> is asynchronous: it\n+returns after pushing the write from the process into the operating\n+system.  The transfer from operating system memory to the underlying\n+persistent storage happens asynchronously.  The <code>sync</code> flag\n+can be turned on for a particular write to make the write operation\n+not return until the data being written has been pushed all the way to\n+persistent storage.  (On Posix systems, this is implemented by calling\n+either <code>fsync(...)</code> or <code>fdatasync(...)</code> or\n+<code>msync(..., MS_SYNC)</code> before the write operation returns.)\n+<pre>\n+  leveldb::WriteOptions write_options;\n+  write_options.sync = true;\n+  db-&gt;Put(write_options, ...);\n+</pre>\n+Asynchronous writes are often more than a thousand times as fast as\n+synchronous writes.  The downside of asynchronous writes is that a\n+crash of the machine may cause the last few updates to be lost.  Note\n+that a crash of just the writing process (i.e., not a reboot) will not\n+cause any loss since even when <code>sync</code> is false, an update\n+is pushed from the process memory into the operating system before it\n+is considered done.\n+\n+<p>\n+Asynchronous writes can often be used safely.  For example, when\n+loading a large amount of data into the database you can handle lost\n+updates by restarting the bulk load after a crash.  A hybrid scheme is\n+also possible where every Nth write is synchronous, and in the event\n+of a crash, the bulk load is restarted just after the last synchronous\n+write finished by the previous run.  (The synchronous write can update\n+a marker that describes where to restart on a crash.)\n+\n+<p>\n+<code>WriteBatch</code> provides an alternative to asynchronous writes.\n+Multiple updates may be placed in the same <code>WriteBatch</code> and\n+applied together using a synchronous write (i.e.,\n+<code>write_options.sync</code> is set to true).  The extra cost of\n+the synchronous write will be amortized across all of the writes in\n+the batch.\n+\n+<p>\n+<h1>Concurrency</h1>\n+<p>\n+A database may only be opened by one process at a time.\n+The <code>leveldb</code> implementation acquires a lock from the\n+operating system to prevent misuse.  Within a single process, the\n+same <code>leveldb::DB</code> object may be safely shared by multiple\n+concurrent threads.  I.e., different threads may write into or fetch\n+iterators or call <code>Get</code> on the same database without any\n+external synchronization (the leveldb implementation will\n+automatically do the required synchronization).  However other objects\n+(like Iterator and WriteBatch) may require external synchronization.\n+If two threads share such an object, they must protect access to it\n+using their own locking protocol.  More details are available in\n+the public header files.\n+<p>\n+<h1>Iteration</h1>\n+<p>\n+The following example demonstrates how to print all key,value pairs\n+in a database.\n+<p>\n+<pre>\n+  leveldb::Iterator* it = db-&gt;NewIterator(leveldb::ReadOptions());\n+  for (it-&gt;SeekToFirst(); it-&gt;Valid(); it-&gt;Next()) {\n+    cout &lt;&lt; it-&gt;key().ToString() &lt;&lt; \": \"  &lt;&lt; it-&gt;value().ToString() &lt;&lt; endl;\n+  }\n+  assert(it-&gt;status().ok());  // Check for any errors found during the scan\n+  delete it;\n+</pre>\n+The following variation shows how to process just the keys in the\n+range <code>[start,limit)</code>:\n+<p>\n+<pre>\n+  for (it-&gt;Seek(start);\n+       it-&gt;Valid() &amp;&amp; it-&gt;key().ToString() &lt; limit;\n+       it-&gt;Next()) {\n+    ...\n+  }\n+</pre>\n+You can also process entries in reverse order.  (Caveat: reverse\n+iteration may be somewhat slower than forward iteration.)\n+<p>\n+<pre>\n+  for (it-&gt;SeekToLast(); it-&gt;Valid(); it-&gt;Prev()) {\n+    ...\n+  }\n+</pre>\n+<h1>Snapshots</h1>\n+<p>\n+Snapshots provide consistent read-only views over the entire state of\n+the key-value store.  <code>ReadOptions::snapshot</code> may be non-NULL to indicate\n+that a read should operate on a particular version of the DB state.\n+If <code>ReadOptions::snapshot</code> is NULL, the read will operate on an\n+implicit snapshot of the current state.\n+<p>\n+Snapshots are created by the DB::GetSnapshot() method:\n+<p>\n+<pre>\n+  leveldb::ReadOptions options;\n+  options.snapshot = db-&gt;GetSnapshot();\n+  ... apply some updates to db ...\n+  leveldb::Iterator* iter = db-&gt;NewIterator(options);\n+  ... read using iter to view the state when the snapshot was created ...\n+  delete iter;\n+  db-&gt;ReleaseSnapshot(options.snapshot);\n+</pre>\n+Note that when a snapshot is no longer needed, it should be released\n+using the DB::ReleaseSnapshot interface.  This allows the\n+implementation to get rid of state that was being maintained just to\n+support reading as of that snapshot.\n+<h1>Slice</h1>\n+<p>\n+The return value of the <code>it->key()</code> and <code>it->value()</code> calls above\n+are instances of the <code>leveldb::Slice</code> type.  <code>Slice</code> is a simple\n+structure that contains a length and a pointer to an external byte\n+array.  Returning a <code>Slice</code> is a cheaper alternative to returning a\n+<code>std::string</code> since we do not need to copy potentially large keys and\n+values.  In addition, <code>leveldb</code> methods do not return null-terminated\n+C-style strings since <code>leveldb</code> keys and values are allowed to\n+contain '\\0' bytes.\n+<p>\n+C++ strings and null-terminated C-style strings can be easily converted\n+to a Slice:\n+<p>\n+<pre>\n+   leveldb::Slice s1 = \"hello\";\n+\n+   std::string str(\"world\");\n+   leveldb::Slice s2 = str;\n+</pre>\n+A Slice can be easily converted back to a C++ string:\n+<pre>\n+   std::string str = s1.ToString();\n+   assert(str == std::string(\"hello\"));\n+</pre>\n+Be careful when using Slices since it is up to the caller to ensure that\n+the external byte array into which the Slice points remains live while\n+the Slice is in use.  For example, the following is buggy:\n+<p>\n+<pre>\n+   leveldb::Slice slice;\n+   if (...) {\n+     std::string str = ...;\n+     slice = str;\n+   }\n+   Use(slice);\n+</pre>\n+When the <code>if</code> statement goes out of scope, <code>str</code> will be destroyed and the\n+backing storage for <code>slice</code> will disappear.\n+<p>\n+<h1>Comparators</h1>\n+<p>\n+The preceding examples used the default ordering function for key,\n+which orders bytes lexicographically.  You can however supply a custom\n+comparator when opening a database.  For example, suppose each\n+database key consists of two numbers and we should sort by the first\n+number, breaking ties by the second number.  First, define a proper\n+subclass of <code>leveldb::Comparator</code> that expresses these rules:\n+<p>\n+<pre>\n+  class TwoPartComparator : public leveldb::Comparator {\n+   public:\n+    // Three-way comparison function:\n+    //   if a &lt; b: negative result\n+    //   if a &gt; b: positive result\n+    //   else: zero result\n+    int Compare(const leveldb::Slice&amp; a, const leveldb::Slice&amp; b) const {\n+      int a1, a2, b1, b2;\n+      ParseKey(a, &amp;a1, &amp;a2);\n+      ParseKey(b, &amp;b1, &amp;b2);\n+      if (a1 &lt; b1) return -1;\n+      if (a1 &gt; b1) return +1;\n+      if (a2 &lt; b2) return -1;\n+      if (a2 &gt; b2) return +1;\n+      return 0;\n+    }\n+\n+    // Ignore the following methods for now:\n+    const char* Name() const { return \"TwoPartComparator\"; }\n+    void FindShortestSeparator(std::string*, const leveldb::Slice&amp;) const { }\n+    void FindShortSuccessor(std::string*) const { }\n+  };\n+</pre>\n+Now create a database using this custom comparator:\n+<p>\n+<pre>\n+  TwoPartComparator cmp;\n+  leveldb::DB* db;\n+  leveldb::Options options;\n+  options.create_if_missing = true;\n+  options.comparator = &amp;cmp;\n+  leveldb::Status status = leveldb::DB::Open(options, \"/tmp/testdb\", &amp;db);\n+  ...\n+</pre>\n+<h2>Backwards compatibility</h2>\n+<p>\n+The result of the comparator's <code>Name</code> method is attached to the\n+database when it is created, and is checked on every subsequent\n+database open.  If the name changes, the <code>leveldb::DB::Open</code> call will\n+fail.  Therefore, change the name if and only if the new key format\n+and comparison function are incompatible with existing databases, and\n+it is ok to discard the contents of all existing databases.\n+<p>\n+You can however still gradually evolve your key format over time with\n+a little bit of pre-planning.  For example, you could store a version\n+number at the end of each key (one byte should suffice for most uses).\n+When you wish to switch to a new key format (e.g., adding an optional\n+third part to the keys processed by <code>TwoPartComparator</code>),\n+(a) keep the same comparator name (b) increment the version number\n+for new keys (c) change the comparator function so it uses the\n+version numbers found in the keys to decide how to interpret them.\n+<p>\n+<h1>Performance</h1>\n+<p>\n+Performance can be tuned by changing the default values of the\n+types defined in <code>include/leveldb/options.h</code>.\n+\n+<p>\n+<h2>Block size</h2>\n+<p>\n+<code>leveldb</code> groups adjacent keys together into the same block and such a\n+block is the unit of transfer to and from persistent storage.  The\n+default block size is approximately 4096 uncompressed bytes.\n+Applications that mostly do bulk scans over the contents of the\n+database may wish to increase this size.  Applications that do a lot\n+of point reads of small values may wish to switch to a smaller block\n+size if performance measurements indicate an improvement.  There isn't\n+much benefit in using blocks smaller than one kilobyte, or larger than\n+a few megabytes.  Also note that compression will be more effective\n+with larger block sizes.\n+<p>\n+<h2>Compression</h2>\n+<p>\n+Each block is individually compressed before being written to\n+persistent storage.  Compression is on by default since the default\n+compression method is very fast, and is automatically disabled for\n+uncompressible data.  In rare cases, applications may want to disable\n+compression entirely, but should only do so if benchmarks show a\n+performance improvement:\n+<p>\n+<pre>\n+  leveldb::Options options;\n+  options.compression = leveldb::kNoCompression;\n+  ... leveldb::DB::Open(options, name, ...) ....\n+</pre>\n+<h2>Cache</h2>\n+<p>\n+The contents of the database are stored in a set of files in the\n+filesystem and each file stores a sequence of compressed blocks.  If\n+<code>options.cache</code> is non-NULL, it is used to cache frequently used\n+uncompressed block contents.\n+<p>\n+<pre>\n+  #include \"leveldb/cache.h\"\n+\n+  leveldb::Options options;\n+  options.cache = leveldb::NewLRUCache(100 * 1048576);  // 100MB cache\n+  leveldb::DB* db;\n+  leveldb::DB::Open(options, name, &db);\n+  ... use the db ...\n+  delete db\n+  delete options.cache;\n+</pre>\n+Note that the cache holds uncompressed data, and therefore it should\n+be sized according to application level data sizes, without any\n+reduction from compression.  (Caching of compressed blocks is left to\n+the operating system buffer cache, or any custom <code>Env</code>\n+implementation provided by the client.)\n+<p>\n+When performing a bulk read, the application may wish to disable\n+caching so that the data processed by the bulk read does not end up\n+displacing most of the cached contents.  A per-iterator option can be\n+used to achieve this:\n+<p>\n+<pre>\n+  leveldb::ReadOptions options;\n+  options.fill_cache = false;\n+  leveldb::Iterator* it = db-&gt;NewIterator(options);\n+  for (it-&gt;SeekToFirst(); it-&gt;Valid(); it-&gt;Next()) {\n+    ...\n+  }\n+</pre>\n+<h2>Key Layout</h2>\n+<p>\n+Note that the unit of disk transfer and caching is a block.  Adjacent\n+keys (according to the database sort order) will usually be placed in\n+the same block.  Therefore the application can improve its performance\n+by placing keys that are accessed together near each other and placing\n+infrequently used keys in a separate region of the key space.\n+<p>\n+For example, suppose we are implementing a simple file system on top\n+of <code>leveldb</code>.  The types of entries we might wish to store are:\n+<p>\n+<pre>\n+   filename -&gt; permission-bits, length, list of file_block_ids\n+   file_block_id -&gt; data\n+</pre>\n+We might want to prefix <code>filename</code> keys with one letter (say '/') and the\n+<code>file_block_id</code> keys with a different letter (say '0') so that scans\n+over just the metadata do not force us to fetch and cache bulky file\n+contents.\n+<p>\n+<h2>Filters</h2>\n+<p>\n+Because of the way <code>leveldb</code> data is organized on disk,\n+a single <code>Get()</code> call may involve multiple reads from disk.\n+The optional <code>FilterPolicy</code> mechanism can be used to reduce\n+the number of disk reads substantially.\n+<pre>\n+   leveldb::Options options;\n+   options.filter_policy = NewBloomFilterPolicy(10);\n+   leveldb::DB* db;\n+   leveldb::DB::Open(options, \"/tmp/testdb\", &amp;db);\n+   ... use the database ...\n+   delete db;\n+   delete options.filter_policy;\n+</pre>\n+The preceding code associates a\n+<a href=\"http://en.wikipedia.org/wiki/Bloom_filter\">Bloom filter</a>\n+based filtering policy with the database.  Bloom filter based\n+filtering relies on keeping some number of bits of data in memory per\n+key (in this case 10 bits per key since that is the argument we passed\n+to NewBloomFilterPolicy).  This filter will reduce the number of unnecessary\n+disk reads needed for <code>Get()</code> calls by a factor of\n+approximately a 100.  Increasing the bits per key will lead to a\n+larger reduction at the cost of more memory usage.  We recommend that\n+applications whose working set does not fit in memory and that do a\n+lot of random reads set a filter policy.\n+<p>\n+If you are using a custom comparator, you should ensure that the filter\n+policy you are using is compatible with your comparator.  For example,\n+consider a comparator that ignores trailing spaces when comparing keys.\n+<code>NewBloomFilterPolicy</code> must not be used with such a comparator.\n+Instead, the application should provide a custom filter policy that\n+also ignores trailing spaces.  For example:\n+<pre>\n+  class CustomFilterPolicy : public leveldb::FilterPolicy {\n+   private:\n+    FilterPolicy* builtin_policy_;\n+   public:\n+    CustomFilterPolicy() : builtin_policy_(NewBloomFilterPolicy(10)) { }\n+    ~CustomFilterPolicy() { delete builtin_policy_; }\n+\n+    const char* Name() const { return \"IgnoreTrailingSpacesFilter\"; }\n+\n+    void CreateFilter(const Slice* keys, int n, std::string* dst) const {\n+      // Use builtin bloom filter code after removing trailing spaces\n+      std::vector&lt;Slice&gt; trimmed(n);\n+      for (int i = 0; i &lt; n; i++) {\n+        trimmed[i] = RemoveTrailingSpaces(keys[i]);\n+      }\n+      return builtin_policy_-&gt;CreateFilter(&amp;trimmed[i], n, dst);\n+    }\n+\n+    bool KeyMayMatch(const Slice& key, const Slice& filter) const {\n+      // Use builtin bloom filter code after removing trailing spaces\n+      return builtin_policy_-&gt;KeyMayMatch(RemoveTrailingSpaces(key), filter);\n+    }\n+  };\n+</pre>\n+<p>\n+Advanced applications may provide a filter policy that does not use\n+a bloom filter but uses some other mechanism for summarizing a set\n+of keys.  See <code>leveldb/filter_policy.h</code> for detail.\n+<p>\n+<h1>Checksums</h1>\n+<p>\n+<code>leveldb</code> associates checksums with all data it stores in the file system.\n+There are two separate controls provided over how aggressively these\n+checksums are verified:\n+<p>\n+<ul>\n+<li> <code>ReadOptions::verify_checksums</code> may be set to true to force\n+  checksum verification of all data that is read from the file system on\n+  behalf of a particular read.  By default, no such verification is\n+  done.\n+<p>\n+<li> <code>Options::paranoid_checks</code> may be set to true before opening a\n+  database to make the database implementation raise an error as soon as\n+  it detects an internal corruption.  Depending on which portion of the\n+  database has been corrupted, the error may be raised when the database\n+  is opened, or later by another database operation.  By default,\n+  paranoid checking is off so that the database can be used even if\n+  parts of its persistent storage have been corrupted.\n+<p>\n+  If a database is corrupted (perhaps it cannot be opened when\n+  paranoid checking is turned on), the <code>leveldb::RepairDB</code> function\n+  may be used to recover as much of the data as possible\n+<p>\n+</ul>\n+<h1>Approximate Sizes</h1>\n+<p>\n+The <code>GetApproximateSizes</code> method can used to get the approximate\n+number of bytes of file system space used by one or more key ranges.\n+<p>\n+<pre>\n+   leveldb::Range ranges[2];\n+   ranges[0] = leveldb::Range(\"a\", \"c\");\n+   ranges[1] = leveldb::Range(\"x\", \"z\");\n+   uint64_t sizes[2];\n+   leveldb::Status s = db-&gt;GetApproximateSizes(ranges, 2, sizes);\n+</pre>\n+The preceding call will set <code>sizes[0]</code> to the approximate number of\n+bytes of file system space used by the key range <code>[a..c)</code> and\n+<code>sizes[1]</code> to the approximate number of bytes used by the key range\n+<code>[x..z)</code>.\n+<p>\n+<h1>Environment</h1>\n+<p>\n+All file operations (and other operating system calls) issued by the\n+<code>leveldb</code> implementation are routed through a <code>leveldb::Env</code> object.\n+Sophisticated clients may wish to provide their own <code>Env</code>\n+implementation to get better control.  For example, an application may\n+introduce artificial delays in the file IO paths to limit the impact\n+of <code>leveldb</code> on other activities in the system.\n+<p>\n+<pre>\n+  class SlowEnv : public leveldb::Env {\n+    .. implementation of the Env interface ...\n+  };\n+\n+  SlowEnv env;\n+  leveldb::Options options;\n+  options.env = &amp;env;\n+  Status s = leveldb::DB::Open(options, ...);\n+</pre>\n+<h1>Porting</h1>\n+<p>\n+<code>leveldb</code> may be ported to a new platform by providing platform\n+specific implementations of the types/methods/functions exported by\n+<code>leveldb/port/port.h</code>.  See <code>leveldb/port/port_example.h</code> for more\n+details.\n+<p>\n+In addition, the new platform may need a new default <code>leveldb::Env</code>\n+implementation.  See <code>leveldb/util/env_posix.h</code> for an example.\n+\n+<h1>Other Information</h1>\n+\n+<p>\n+Details about the <code>leveldb</code> implementation may be found in\n+the following documents:\n+<ul>\n+<li> <a href=\"impl.html\">Implementation notes</a>\n+<li> <a href=\"table_format.txt\">Format of an immutable Table file</a>\n+<li> <a href=\"log_format.txt\">Format of a log file</a>\n+</ul>\n+\n+</body>\n+</html>"
      },
      {
        "sha": "5228f624de4347c114b87520c12a1ae39425d411",
        "filename": "doc/log_format.txt",
        "status": "added",
        "additions": 75,
        "deletions": 0,
        "changes": 75,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/log_format.txt",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/log_format.txt",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/log_format.txt?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,75 @@\n+The log file contents are a sequence of 32KB blocks.  The only\n+exception is that the tail of the file may contain a partial block.\n+\n+Each block consists of a sequence of records:\n+   block := record* trailer?\n+   record :=\n+\tchecksum: uint32\t// crc32c of type and data[] ; little-endian\n+\tlength: uint16\t\t// little-endian\n+\ttype: uint8\t\t// One of FULL, FIRST, MIDDLE, LAST\n+\tdata: uint8[length]\n+\n+A record never starts within the last six bytes of a block (since it\n+won't fit).  Any leftover bytes here form the trailer, which must\n+consist entirely of zero bytes and must be skipped by readers.  \n+\n+Aside: if exactly seven bytes are left in the current block, and a new\n+non-zero length record is added, the writer must emit a FIRST record\n+(which contains zero bytes of user data) to fill up the trailing seven\n+bytes of the block and then emit all of the user data in subsequent\n+blocks.\n+\n+More types may be added in the future.  Some Readers may skip record\n+types they do not understand, others may report that some data was\n+skipped.\n+\n+FULL == 1\n+FIRST == 2\n+MIDDLE == 3\n+LAST == 4\n+\n+The FULL record contains the contents of an entire user record.\n+\n+FIRST, MIDDLE, LAST are types used for user records that have been\n+split into multiple fragments (typically because of block boundaries).\n+FIRST is the type of the first fragment of a user record, LAST is the\n+type of the last fragment of a user record, and MID is the type of all\n+interior fragments of a user record.\n+\n+Example: consider a sequence of user records:\n+   A: length 1000\n+   B: length 97270\n+   C: length 8000\n+A will be stored as a FULL record in the first block.\n+\n+B will be split into three fragments: first fragment occupies the rest\n+of the first block, second fragment occupies the entirety of the\n+second block, and the third fragment occupies a prefix of the third\n+block.  This will leave six bytes free in the third block, which will\n+be left empty as the trailer.\n+\n+C will be stored as a FULL record in the fourth block.\n+\n+===================\n+\n+Some benefits over the recordio format:\n+\n+(1) We do not need any heuristics for resyncing - just go to next\n+block boundary and scan.  If there is a corruption, skip to the next\n+block.  As a side-benefit, we do not get confused when part of the\n+contents of one log file are embedded as a record inside another log\n+file.\n+\n+(2) Splitting at approximate boundaries (e.g., for mapreduce) is\n+simple: find the next block boundary and skip records until we\n+hit a FULL or FIRST record.\n+\n+(3) We do not need extra buffering for large records.\n+\n+Some downsides compared to recordio format:\n+\n+(1) No packing of tiny records.  This could be fixed by adding a new\n+record type, so it is a shortcoming of the current implementation,\n+not necessarily the format.\n+\n+(2) No compression.  Again, this could be fixed by adding new record types."
      },
      {
        "sha": "ca8f9b4460ad85d9e09f14a959ed47bd2812edb5",
        "filename": "doc/table_format.txt",
        "status": "added",
        "additions": 104,
        "deletions": 0,
        "changes": 104,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/table_format.txt",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/doc/table_format.txt",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/table_format.txt?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,104 @@\n+File format\n+===========\n+\n+  <beginning_of_file>\n+  [data block 1]\n+  [data block 2]\n+  ...\n+  [data block N]\n+  [meta block 1]\n+  ...\n+  [meta block K]\n+  [metaindex block]\n+  [index block]\n+  [Footer]        (fixed size; starts at file_size - sizeof(Footer))\n+  <end_of_file>\n+\n+The file contains internal pointers.  Each such pointer is called\n+a BlockHandle and contains the following information:\n+  offset:\t    varint64\n+  size:\t\t    varint64\n+See https://developers.google.com/protocol-buffers/docs/encoding#varints\n+for an explanation of varint64 format.\n+\n+(1) The sequence of key/value pairs in the file are stored in sorted\n+order and partitioned into a sequence of data blocks.  These blocks\n+come one after another at the beginning of the file.  Each data block\n+is formatted according to the code in block_builder.cc, and then\n+optionally compressed.\n+\n+(2) After the data blocks we store a bunch of meta blocks.  The\n+supported meta block types are described below.  More meta block types\n+may be added in the future.  Each meta block is again formatted using\n+block_builder.cc and then optionally compressed.\n+\n+(3) A \"metaindex\" block.  It contains one entry for every other meta\n+block where the key is the name of the meta block and the value is a\n+BlockHandle pointing to that meta block.\n+\n+(4) An \"index\" block.  This block contains one entry per data block,\n+where the key is a string >= last key in that data block and before\n+the first key in the successive data block.  The value is the\n+BlockHandle for the data block.\n+\n+(6) At the very end of the file is a fixed length footer that contains\n+the BlockHandle of the metaindex and index blocks as well as a magic number.\n+       metaindex_handle: char[p];    // Block handle for metaindex\n+       index_handle:     char[q];    // Block handle for index\n+       padding:          char[40-p-q]; // zeroed bytes to make fixed length\n+                                       // (40==2*BlockHandle::kMaxEncodedLength)\n+       magic:            fixed64;    // == 0xdb4775248b80fb57 (little-endian)\n+\n+\"filter\" Meta Block\n+-------------------\n+\n+If a \"FilterPolicy\" was specified when the database was opened, a\n+filter block is stored in each table.  The \"metaindex\" block contains\n+an entry that maps from \"filter.<N>\" to the BlockHandle for the filter\n+block where \"<N>\" is the string returned by the filter policy's\n+\"Name()\" method.\n+\n+The filter block stores a sequence of filters, where filter i contains\n+the output of FilterPolicy::CreateFilter() on all keys that are stored\n+in a block whose file offset falls within the range\n+\n+    [ i*base ... (i+1)*base-1 ]\n+\n+Currently, \"base\" is 2KB.  So for example, if blocks X and Y start in\n+the range [ 0KB .. 2KB-1 ], all of the keys in X and Y will be\n+converted to a filter by calling FilterPolicy::CreateFilter(), and the\n+resulting filter will be stored as the first filter in the filter\n+block.\n+\n+The filter block is formatted as follows:\n+\n+     [filter 0]\n+     [filter 1]\n+     [filter 2]\n+     ...\n+     [filter N-1]\n+\n+     [offset of filter 0]                  : 4 bytes\n+     [offset of filter 1]                  : 4 bytes\n+     [offset of filter 2]                  : 4 bytes\n+     ...\n+     [offset of filter N-1]                : 4 bytes\n+\n+     [offset of beginning of offset array] : 4 bytes\n+     lg(base)                              : 1 byte\n+\n+The offset array at the end of the filter block allows efficient\n+mapping from a data block offset to the corresponding filter.\n+\n+\"stats\" Meta Block\n+------------------\n+\n+This meta block contains a bunch of stats.  The key is the name\n+of the statistic.  The value contains the statistic.\n+TODO(postrelease): record following stats.\n+  data size\n+  index size\n+  key size (uncompressed)\n+  value size (uncompressed)\n+  number of entries\n+  number of data blocks"
      },
      {
        "sha": "5879de121456a7c5c16457eb36d85c64ad0a1b61",
        "filename": "helpers/memenv/memenv.cc",
        "status": "added",
        "additions": 384,
        "deletions": 0,
        "changes": 384,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/helpers/memenv/memenv.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/helpers/memenv/memenv.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/helpers/memenv/memenv.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,384 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"helpers/memenv/memenv.h\"\n+\n+#include \"leveldb/env.h\"\n+#include \"leveldb/status.h\"\n+#include \"port/port.h\"\n+#include \"util/mutexlock.h\"\n+#include <map>\n+#include <string.h>\n+#include <string>\n+#include <vector>\n+\n+namespace leveldb {\n+\n+namespace {\n+\n+class FileState {\n+ public:\n+  // FileStates are reference counted. The initial reference count is zero\n+  // and the caller must call Ref() at least once.\n+  FileState() : refs_(0), size_(0) {}\n+\n+  // Increase the reference count.\n+  void Ref() {\n+    MutexLock lock(&refs_mutex_);\n+    ++refs_;\n+  }\n+\n+  // Decrease the reference count. Delete if this is the last reference.\n+  void Unref() {\n+    bool do_delete = false;\n+\n+    {\n+      MutexLock lock(&refs_mutex_);\n+      --refs_;\n+      assert(refs_ >= 0);\n+      if (refs_ <= 0) {\n+        do_delete = true;\n+      }\n+    }\n+\n+    if (do_delete) {\n+      delete this;\n+    }\n+  }\n+\n+  uint64_t Size() const { return size_; }\n+\n+  Status Read(uint64_t offset, size_t n, Slice* result, char* scratch) const {\n+    if (offset > size_) {\n+      return Status::IOError(\"Offset greater than file size.\");\n+    }\n+    const uint64_t available = size_ - offset;\n+    if (n > available) {\n+      n = available;\n+    }\n+    if (n == 0) {\n+      *result = Slice();\n+      return Status::OK();\n+    }\n+\n+    size_t block = offset / kBlockSize;\n+    size_t block_offset = offset % kBlockSize;\n+\n+    if (n <= kBlockSize - block_offset) {\n+      // The requested bytes are all in the first block.\n+      *result = Slice(blocks_[block] + block_offset, n);\n+      return Status::OK();\n+    }\n+\n+    size_t bytes_to_copy = n;\n+    char* dst = scratch;\n+\n+    while (bytes_to_copy > 0) {\n+      size_t avail = kBlockSize - block_offset;\n+      if (avail > bytes_to_copy) {\n+        avail = bytes_to_copy;\n+      }\n+      memcpy(dst, blocks_[block] + block_offset, avail);\n+\n+      bytes_to_copy -= avail;\n+      dst += avail;\n+      block++;\n+      block_offset = 0;\n+    }\n+\n+    *result = Slice(scratch, n);\n+    return Status::OK();\n+  }\n+\n+  Status Append(const Slice& data) {\n+    const char* src = data.data();\n+    size_t src_len = data.size();\n+\n+    while (src_len > 0) {\n+      size_t avail;\n+      size_t offset = size_ % kBlockSize;\n+\n+      if (offset != 0) {\n+        // There is some room in the last block.\n+        avail = kBlockSize - offset;\n+      } else {\n+        // No room in the last block; push new one.\n+        blocks_.push_back(new char[kBlockSize]);\n+        avail = kBlockSize;\n+      }\n+\n+      if (avail > src_len) {\n+        avail = src_len;\n+      }\n+      memcpy(blocks_.back() + offset, src, avail);\n+      src_len -= avail;\n+      src += avail;\n+      size_ += avail;\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+ private:\n+  // Private since only Unref() should be used to delete it.\n+  ~FileState() {\n+    for (std::vector<char*>::iterator i = blocks_.begin(); i != blocks_.end();\n+         ++i) {\n+      delete [] *i;\n+    }\n+  }\n+\n+  // No copying allowed.\n+  FileState(const FileState&);\n+  void operator=(const FileState&);\n+\n+  port::Mutex refs_mutex_;\n+  int refs_;  // Protected by refs_mutex_;\n+\n+  // The following fields are not protected by any mutex. They are only mutable\n+  // while the file is being written, and concurrent access is not allowed\n+  // to writable files.\n+  std::vector<char*> blocks_;\n+  uint64_t size_;\n+\n+  enum { kBlockSize = 8 * 1024 };\n+};\n+\n+class SequentialFileImpl : public SequentialFile {\n+ public:\n+  explicit SequentialFileImpl(FileState* file) : file_(file), pos_(0) {\n+    file_->Ref();\n+  }\n+\n+  ~SequentialFileImpl() {\n+    file_->Unref();\n+  }\n+\n+  virtual Status Read(size_t n, Slice* result, char* scratch) {\n+    Status s = file_->Read(pos_, n, result, scratch);\n+    if (s.ok()) {\n+      pos_ += result->size();\n+    }\n+    return s;\n+  }\n+\n+  virtual Status Skip(uint64_t n) {\n+    if (pos_ > file_->Size()) {\n+      return Status::IOError(\"pos_ > file_->Size()\");\n+    }\n+    const size_t available = file_->Size() - pos_;\n+    if (n > available) {\n+      n = available;\n+    }\n+    pos_ += n;\n+    return Status::OK();\n+  }\n+\n+ private:\n+  FileState* file_;\n+  size_t pos_;\n+};\n+\n+class RandomAccessFileImpl : public RandomAccessFile {\n+ public:\n+  explicit RandomAccessFileImpl(FileState* file) : file_(file) {\n+    file_->Ref();\n+  }\n+\n+  ~RandomAccessFileImpl() {\n+    file_->Unref();\n+  }\n+\n+  virtual Status Read(uint64_t offset, size_t n, Slice* result,\n+                      char* scratch) const {\n+    return file_->Read(offset, n, result, scratch);\n+  }\n+\n+ private:\n+  FileState* file_;\n+};\n+\n+class WritableFileImpl : public WritableFile {\n+ public:\n+  WritableFileImpl(FileState* file) : file_(file) {\n+    file_->Ref();\n+  }\n+\n+  ~WritableFileImpl() {\n+    file_->Unref();\n+  }\n+\n+  virtual Status Append(const Slice& data) {\n+    return file_->Append(data);\n+  }\n+\n+  virtual Status Close() { return Status::OK(); }\n+  virtual Status Flush() { return Status::OK(); }\n+  virtual Status Sync() { return Status::OK(); }\n+\n+ private:\n+  FileState* file_;\n+};\n+\n+class NoOpLogger : public Logger {\n+ public:\n+  virtual void Logv(const char* format, va_list ap) { }\n+};\n+\n+class InMemoryEnv : public EnvWrapper {\n+ public:\n+  explicit InMemoryEnv(Env* base_env) : EnvWrapper(base_env) { }\n+\n+  virtual ~InMemoryEnv() {\n+    for (FileSystem::iterator i = file_map_.begin(); i != file_map_.end(); ++i){\n+      i->second->Unref();\n+    }\n+  }\n+\n+  // Partial implementation of the Env interface.\n+  virtual Status NewSequentialFile(const std::string& fname,\n+                                   SequentialFile** result) {\n+    MutexLock lock(&mutex_);\n+    if (file_map_.find(fname) == file_map_.end()) {\n+      *result = NULL;\n+      return Status::IOError(fname, \"File not found\");\n+    }\n+\n+    *result = new SequentialFileImpl(file_map_[fname]);\n+    return Status::OK();\n+  }\n+\n+  virtual Status NewRandomAccessFile(const std::string& fname,\n+                                     RandomAccessFile** result) {\n+    MutexLock lock(&mutex_);\n+    if (file_map_.find(fname) == file_map_.end()) {\n+      *result = NULL;\n+      return Status::IOError(fname, \"File not found\");\n+    }\n+\n+    *result = new RandomAccessFileImpl(file_map_[fname]);\n+    return Status::OK();\n+  }\n+\n+  virtual Status NewWritableFile(const std::string& fname,\n+                                 WritableFile** result) {\n+    MutexLock lock(&mutex_);\n+    if (file_map_.find(fname) != file_map_.end()) {\n+      DeleteFileInternal(fname);\n+    }\n+\n+    FileState* file = new FileState();\n+    file->Ref();\n+    file_map_[fname] = file;\n+\n+    *result = new WritableFileImpl(file);\n+    return Status::OK();\n+  }\n+\n+  virtual bool FileExists(const std::string& fname) {\n+    MutexLock lock(&mutex_);\n+    return file_map_.find(fname) != file_map_.end();\n+  }\n+\n+  virtual Status GetChildren(const std::string& dir,\n+                             std::vector<std::string>* result) {\n+    MutexLock lock(&mutex_);\n+    result->clear();\n+\n+    for (FileSystem::iterator i = file_map_.begin(); i != file_map_.end(); ++i){\n+      const std::string& filename = i->first;\n+\n+      if (filename.size() >= dir.size() + 1 && filename[dir.size()] == '/' &&\n+          Slice(filename).starts_with(Slice(dir))) {\n+        result->push_back(filename.substr(dir.size() + 1));\n+      }\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  void DeleteFileInternal(const std::string& fname) {\n+    if (file_map_.find(fname) == file_map_.end()) {\n+      return;\n+    }\n+\n+    file_map_[fname]->Unref();\n+    file_map_.erase(fname);\n+  }\n+\n+  virtual Status DeleteFile(const std::string& fname) {\n+    MutexLock lock(&mutex_);\n+    if (file_map_.find(fname) == file_map_.end()) {\n+      return Status::IOError(fname, \"File not found\");\n+    }\n+\n+    DeleteFileInternal(fname);\n+    return Status::OK();\n+  }\n+\n+  virtual Status CreateDir(const std::string& dirname) {\n+    return Status::OK();\n+  }\n+\n+  virtual Status DeleteDir(const std::string& dirname) {\n+    return Status::OK();\n+  }\n+\n+  virtual Status GetFileSize(const std::string& fname, uint64_t* file_size) {\n+    MutexLock lock(&mutex_);\n+    if (file_map_.find(fname) == file_map_.end()) {\n+      return Status::IOError(fname, \"File not found\");\n+    }\n+\n+    *file_size = file_map_[fname]->Size();\n+    return Status::OK();\n+  }\n+\n+  virtual Status RenameFile(const std::string& src,\n+                            const std::string& target) {\n+    MutexLock lock(&mutex_);\n+    if (file_map_.find(src) == file_map_.end()) {\n+      return Status::IOError(src, \"File not found\");\n+    }\n+\n+    DeleteFileInternal(target);\n+    file_map_[target] = file_map_[src];\n+    file_map_.erase(src);\n+    return Status::OK();\n+  }\n+\n+  virtual Status LockFile(const std::string& fname, FileLock** lock) {\n+    *lock = new FileLock;\n+    return Status::OK();\n+  }\n+\n+  virtual Status UnlockFile(FileLock* lock) {\n+    delete lock;\n+    return Status::OK();\n+  }\n+\n+  virtual Status GetTestDirectory(std::string* path) {\n+    *path = \"/test\";\n+    return Status::OK();\n+  }\n+\n+  virtual Status NewLogger(const std::string& fname, Logger** result) {\n+    *result = new NoOpLogger;\n+    return Status::OK();\n+  }\n+\n+ private:\n+  // Map from filenames to FileState objects, representing a simple file system.\n+  typedef std::map<std::string, FileState*> FileSystem;\n+  port::Mutex mutex_;\n+  FileSystem file_map_;  // Protected by mutex_.\n+};\n+\n+}  // namespace\n+\n+Env* NewMemEnv(Env* base_env) {\n+  return new InMemoryEnv(base_env);\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "03b88de761dc732e09dec54baddd34e94ee17613",
        "filename": "helpers/memenv/memenv.h",
        "status": "added",
        "additions": 20,
        "deletions": 0,
        "changes": 20,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/helpers/memenv/memenv.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/helpers/memenv/memenv.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/helpers/memenv/memenv.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,20 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_HELPERS_MEMENV_MEMENV_H_\n+#define STORAGE_LEVELDB_HELPERS_MEMENV_MEMENV_H_\n+\n+namespace leveldb {\n+\n+class Env;\n+\n+// Returns a new environment that stores its data in memory and delegates\n+// all non-file-storage tasks to base_env. The caller must delete the result\n+// when it is no longer needed.\n+// *base_env must remain live while the result is in use.\n+Env* NewMemEnv(Env* base_env);\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_HELPERS_MEMENV_MEMENV_H_"
      },
      {
        "sha": "a44310fed80cd7f210d64b2c8e79ceb74284217a",
        "filename": "helpers/memenv/memenv_test.cc",
        "status": "added",
        "additions": 232,
        "deletions": 0,
        "changes": 232,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/helpers/memenv/memenv_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/helpers/memenv/memenv_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/helpers/memenv/memenv_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,232 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"helpers/memenv/memenv.h\"\n+\n+#include \"db/db_impl.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+#include \"util/testharness.h\"\n+#include <string>\n+#include <vector>\n+\n+namespace leveldb {\n+\n+class MemEnvTest {\n+ public:\n+  Env* env_;\n+\n+  MemEnvTest()\n+      : env_(NewMemEnv(Env::Default())) {\n+  }\n+  ~MemEnvTest() {\n+    delete env_;\n+  }\n+};\n+\n+TEST(MemEnvTest, Basics) {\n+  uint64_t file_size;\n+  WritableFile* writable_file;\n+  std::vector<std::string> children;\n+\n+  ASSERT_OK(env_->CreateDir(\"/dir\"));\n+\n+  // Check that the directory is empty.\n+  ASSERT_TRUE(!env_->FileExists(\"/dir/non_existent\"));\n+  ASSERT_TRUE(!env_->GetFileSize(\"/dir/non_existent\", &file_size).ok());\n+  ASSERT_OK(env_->GetChildren(\"/dir\", &children));\n+  ASSERT_EQ(0, children.size());\n+\n+  // Create a file.\n+  ASSERT_OK(env_->NewWritableFile(\"/dir/f\", &writable_file));\n+  delete writable_file;\n+\n+  // Check that the file exists.\n+  ASSERT_TRUE(env_->FileExists(\"/dir/f\"));\n+  ASSERT_OK(env_->GetFileSize(\"/dir/f\", &file_size));\n+  ASSERT_EQ(0, file_size);\n+  ASSERT_OK(env_->GetChildren(\"/dir\", &children));\n+  ASSERT_EQ(1, children.size());\n+  ASSERT_EQ(\"f\", children[0]);\n+\n+  // Write to the file.\n+  ASSERT_OK(env_->NewWritableFile(\"/dir/f\", &writable_file));\n+  ASSERT_OK(writable_file->Append(\"abc\"));\n+  delete writable_file;\n+\n+  // Check for expected size.\n+  ASSERT_OK(env_->GetFileSize(\"/dir/f\", &file_size));\n+  ASSERT_EQ(3, file_size);\n+\n+  // Check that renaming works.\n+  ASSERT_TRUE(!env_->RenameFile(\"/dir/non_existent\", \"/dir/g\").ok());\n+  ASSERT_OK(env_->RenameFile(\"/dir/f\", \"/dir/g\"));\n+  ASSERT_TRUE(!env_->FileExists(\"/dir/f\"));\n+  ASSERT_TRUE(env_->FileExists(\"/dir/g\"));\n+  ASSERT_OK(env_->GetFileSize(\"/dir/g\", &file_size));\n+  ASSERT_EQ(3, file_size);\n+\n+  // Check that opening non-existent file fails.\n+  SequentialFile* seq_file;\n+  RandomAccessFile* rand_file;\n+  ASSERT_TRUE(!env_->NewSequentialFile(\"/dir/non_existent\", &seq_file).ok());\n+  ASSERT_TRUE(!seq_file);\n+  ASSERT_TRUE(!env_->NewRandomAccessFile(\"/dir/non_existent\", &rand_file).ok());\n+  ASSERT_TRUE(!rand_file);\n+\n+  // Check that deleting works.\n+  ASSERT_TRUE(!env_->DeleteFile(\"/dir/non_existent\").ok());\n+  ASSERT_OK(env_->DeleteFile(\"/dir/g\"));\n+  ASSERT_TRUE(!env_->FileExists(\"/dir/g\"));\n+  ASSERT_OK(env_->GetChildren(\"/dir\", &children));\n+  ASSERT_EQ(0, children.size());\n+  ASSERT_OK(env_->DeleteDir(\"/dir\"));\n+}\n+\n+TEST(MemEnvTest, ReadWrite) {\n+  WritableFile* writable_file;\n+  SequentialFile* seq_file;\n+  RandomAccessFile* rand_file;\n+  Slice result;\n+  char scratch[100];\n+\n+  ASSERT_OK(env_->CreateDir(\"/dir\"));\n+\n+  ASSERT_OK(env_->NewWritableFile(\"/dir/f\", &writable_file));\n+  ASSERT_OK(writable_file->Append(\"hello \"));\n+  ASSERT_OK(writable_file->Append(\"world\"));\n+  delete writable_file;\n+\n+  // Read sequentially.\n+  ASSERT_OK(env_->NewSequentialFile(\"/dir/f\", &seq_file));\n+  ASSERT_OK(seq_file->Read(5, &result, scratch)); // Read \"hello\".\n+  ASSERT_EQ(0, result.compare(\"hello\"));\n+  ASSERT_OK(seq_file->Skip(1));\n+  ASSERT_OK(seq_file->Read(1000, &result, scratch)); // Read \"world\".\n+  ASSERT_EQ(0, result.compare(\"world\"));\n+  ASSERT_OK(seq_file->Read(1000, &result, scratch)); // Try reading past EOF.\n+  ASSERT_EQ(0, result.size());\n+  ASSERT_OK(seq_file->Skip(100)); // Try to skip past end of file.\n+  ASSERT_OK(seq_file->Read(1000, &result, scratch));\n+  ASSERT_EQ(0, result.size());\n+  delete seq_file;\n+\n+  // Random reads.\n+  ASSERT_OK(env_->NewRandomAccessFile(\"/dir/f\", &rand_file));\n+  ASSERT_OK(rand_file->Read(6, 5, &result, scratch)); // Read \"world\".\n+  ASSERT_EQ(0, result.compare(\"world\"));\n+  ASSERT_OK(rand_file->Read(0, 5, &result, scratch)); // Read \"hello\".\n+  ASSERT_EQ(0, result.compare(\"hello\"));\n+  ASSERT_OK(rand_file->Read(10, 100, &result, scratch)); // Read \"d\".\n+  ASSERT_EQ(0, result.compare(\"d\"));\n+\n+  // Too high offset.\n+  ASSERT_TRUE(!rand_file->Read(1000, 5, &result, scratch).ok());\n+  delete rand_file;\n+}\n+\n+TEST(MemEnvTest, Locks) {\n+  FileLock* lock;\n+\n+  // These are no-ops, but we test they return success.\n+  ASSERT_OK(env_->LockFile(\"some file\", &lock));\n+  ASSERT_OK(env_->UnlockFile(lock));\n+}\n+\n+TEST(MemEnvTest, Misc) {\n+  std::string test_dir;\n+  ASSERT_OK(env_->GetTestDirectory(&test_dir));\n+  ASSERT_TRUE(!test_dir.empty());\n+\n+  WritableFile* writable_file;\n+  ASSERT_OK(env_->NewWritableFile(\"/a/b\", &writable_file));\n+\n+  // These are no-ops, but we test they return success.\n+  ASSERT_OK(writable_file->Sync());\n+  ASSERT_OK(writable_file->Flush());\n+  ASSERT_OK(writable_file->Close());\n+  delete writable_file;\n+}\n+\n+TEST(MemEnvTest, LargeWrite) {\n+  const size_t kWriteSize = 300 * 1024;\n+  char* scratch = new char[kWriteSize * 2];\n+\n+  std::string write_data;\n+  for (size_t i = 0; i < kWriteSize; ++i) {\n+    write_data.append(1, static_cast<char>(i));\n+  }\n+\n+  WritableFile* writable_file;\n+  ASSERT_OK(env_->NewWritableFile(\"/dir/f\", &writable_file));\n+  ASSERT_OK(writable_file->Append(\"foo\"));\n+  ASSERT_OK(writable_file->Append(write_data));\n+  delete writable_file;\n+\n+  SequentialFile* seq_file;\n+  Slice result;\n+  ASSERT_OK(env_->NewSequentialFile(\"/dir/f\", &seq_file));\n+  ASSERT_OK(seq_file->Read(3, &result, scratch)); // Read \"foo\".\n+  ASSERT_EQ(0, result.compare(\"foo\"));\n+\n+  size_t read = 0;\n+  std::string read_data;\n+  while (read < kWriteSize) {\n+    ASSERT_OK(seq_file->Read(kWriteSize - read, &result, scratch));\n+    read_data.append(result.data(), result.size());\n+    read += result.size();\n+  }\n+  ASSERT_TRUE(write_data == read_data);\n+  delete seq_file;\n+  delete [] scratch;\n+}\n+\n+TEST(MemEnvTest, DBTest) {\n+  Options options;\n+  options.create_if_missing = true;\n+  options.env = env_;\n+  DB* db;\n+\n+  const Slice keys[] = {Slice(\"aaa\"), Slice(\"bbb\"), Slice(\"ccc\")};\n+  const Slice vals[] = {Slice(\"foo\"), Slice(\"bar\"), Slice(\"baz\")};\n+\n+  ASSERT_OK(DB::Open(options, \"/dir/db\", &db));\n+  for (size_t i = 0; i < 3; ++i) {\n+    ASSERT_OK(db->Put(WriteOptions(), keys[i], vals[i]));\n+  }\n+\n+  for (size_t i = 0; i < 3; ++i) {\n+    std::string res;\n+    ASSERT_OK(db->Get(ReadOptions(), keys[i], &res));\n+    ASSERT_TRUE(res == vals[i]);\n+  }\n+\n+  Iterator* iterator = db->NewIterator(ReadOptions());\n+  iterator->SeekToFirst();\n+  for (size_t i = 0; i < 3; ++i) {\n+    ASSERT_TRUE(iterator->Valid());\n+    ASSERT_TRUE(keys[i] == iterator->key());\n+    ASSERT_TRUE(vals[i] == iterator->value());\n+    iterator->Next();\n+  }\n+  ASSERT_TRUE(!iterator->Valid());\n+  delete iterator;\n+\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db);\n+  ASSERT_OK(dbi->TEST_CompactMemTable());\n+\n+  for (size_t i = 0; i < 3; ++i) {\n+    std::string res;\n+    ASSERT_OK(db->Get(ReadOptions(), keys[i], &res));\n+    ASSERT_TRUE(res == vals[i]);\n+  }\n+\n+  delete db;\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "1fa58866c3958c66d7c68b264aa4ee4ccdf51c68",
        "filename": "include/leveldb/c.h",
        "status": "added",
        "additions": 291,
        "deletions": 0,
        "changes": 291,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/c.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/c.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/include/leveldb/c.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,291 @@\n+/* Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+  Use of this source code is governed by a BSD-style license that can be\n+  found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+  C bindings for leveldb.  May be useful as a stable ABI that can be\n+  used by programs that keep leveldb in a shared library, or for\n+  a JNI api.\n+\n+  Does not support:\n+  . getters for the option types\n+  . custom comparators that implement key shortening\n+  . capturing post-write-snapshot\n+  . custom iter, db, env, cache implementations using just the C bindings\n+\n+  Some conventions:\n+\n+  (1) We expose just opaque struct pointers and functions to clients.\n+  This allows us to change internal representations without having to\n+  recompile clients.\n+\n+  (2) For simplicity, there is no equivalent to the Slice type.  Instead,\n+  the caller has to pass the pointer and length as separate\n+  arguments.\n+\n+  (3) Errors are represented by a null-terminated c string.  NULL\n+  means no error.  All operations that can raise an error are passed\n+  a \"char** errptr\" as the last argument.  One of the following must\n+  be true on entry:\n+     *errptr == NULL\n+     *errptr points to a malloc()ed null-terminated error message\n+       (On Windows, *errptr must have been malloc()-ed by this library.)\n+  On success, a leveldb routine leaves *errptr unchanged.\n+  On failure, leveldb frees the old value of *errptr and\n+  set *errptr to a malloc()ed error message.\n+\n+  (4) Bools have the type unsigned char (0 == false; rest == true)\n+\n+  (5) All of the pointer arguments must be non-NULL.\n+*/\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_C_H_\n+#define STORAGE_LEVELDB_INCLUDE_C_H_\n+\n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n+\n+#include <stdarg.h>\n+#include <stddef.h>\n+#include <stdint.h>\n+\n+/* Exported types */\n+\n+typedef struct leveldb_t               leveldb_t;\n+typedef struct leveldb_cache_t         leveldb_cache_t;\n+typedef struct leveldb_comparator_t    leveldb_comparator_t;\n+typedef struct leveldb_env_t           leveldb_env_t;\n+typedef struct leveldb_filelock_t      leveldb_filelock_t;\n+typedef struct leveldb_filterpolicy_t  leveldb_filterpolicy_t;\n+typedef struct leveldb_iterator_t      leveldb_iterator_t;\n+typedef struct leveldb_logger_t        leveldb_logger_t;\n+typedef struct leveldb_options_t       leveldb_options_t;\n+typedef struct leveldb_randomfile_t    leveldb_randomfile_t;\n+typedef struct leveldb_readoptions_t   leveldb_readoptions_t;\n+typedef struct leveldb_seqfile_t       leveldb_seqfile_t;\n+typedef struct leveldb_snapshot_t      leveldb_snapshot_t;\n+typedef struct leveldb_writablefile_t  leveldb_writablefile_t;\n+typedef struct leveldb_writebatch_t    leveldb_writebatch_t;\n+typedef struct leveldb_writeoptions_t  leveldb_writeoptions_t;\n+\n+/* DB operations */\n+\n+extern leveldb_t* leveldb_open(\n+    const leveldb_options_t* options,\n+    const char* name,\n+    char** errptr);\n+\n+extern void leveldb_close(leveldb_t* db);\n+\n+extern void leveldb_put(\n+    leveldb_t* db,\n+    const leveldb_writeoptions_t* options,\n+    const char* key, size_t keylen,\n+    const char* val, size_t vallen,\n+    char** errptr);\n+\n+extern void leveldb_delete(\n+    leveldb_t* db,\n+    const leveldb_writeoptions_t* options,\n+    const char* key, size_t keylen,\n+    char** errptr);\n+\n+extern void leveldb_write(\n+    leveldb_t* db,\n+    const leveldb_writeoptions_t* options,\n+    leveldb_writebatch_t* batch,\n+    char** errptr);\n+\n+/* Returns NULL if not found.  A malloc()ed array otherwise.\n+   Stores the length of the array in *vallen. */\n+extern char* leveldb_get(\n+    leveldb_t* db,\n+    const leveldb_readoptions_t* options,\n+    const char* key, size_t keylen,\n+    size_t* vallen,\n+    char** errptr);\n+\n+extern leveldb_iterator_t* leveldb_create_iterator(\n+    leveldb_t* db,\n+    const leveldb_readoptions_t* options);\n+\n+extern const leveldb_snapshot_t* leveldb_create_snapshot(\n+    leveldb_t* db);\n+\n+extern void leveldb_release_snapshot(\n+    leveldb_t* db,\n+    const leveldb_snapshot_t* snapshot);\n+\n+/* Returns NULL if property name is unknown.\n+   Else returns a pointer to a malloc()-ed null-terminated value. */\n+extern char* leveldb_property_value(\n+    leveldb_t* db,\n+    const char* propname);\n+\n+extern void leveldb_approximate_sizes(\n+    leveldb_t* db,\n+    int num_ranges,\n+    const char* const* range_start_key, const size_t* range_start_key_len,\n+    const char* const* range_limit_key, const size_t* range_limit_key_len,\n+    uint64_t* sizes);\n+\n+extern void leveldb_compact_range(\n+    leveldb_t* db,\n+    const char* start_key, size_t start_key_len,\n+    const char* limit_key, size_t limit_key_len);\n+\n+/* Management operations */\n+\n+extern void leveldb_destroy_db(\n+    const leveldb_options_t* options,\n+    const char* name,\n+    char** errptr);\n+\n+extern void leveldb_repair_db(\n+    const leveldb_options_t* options,\n+    const char* name,\n+    char** errptr);\n+\n+/* Iterator */\n+\n+extern void leveldb_iter_destroy(leveldb_iterator_t*);\n+extern unsigned char leveldb_iter_valid(const leveldb_iterator_t*);\n+extern void leveldb_iter_seek_to_first(leveldb_iterator_t*);\n+extern void leveldb_iter_seek_to_last(leveldb_iterator_t*);\n+extern void leveldb_iter_seek(leveldb_iterator_t*, const char* k, size_t klen);\n+extern void leveldb_iter_next(leveldb_iterator_t*);\n+extern void leveldb_iter_prev(leveldb_iterator_t*);\n+extern const char* leveldb_iter_key(const leveldb_iterator_t*, size_t* klen);\n+extern const char* leveldb_iter_value(const leveldb_iterator_t*, size_t* vlen);\n+extern void leveldb_iter_get_error(const leveldb_iterator_t*, char** errptr);\n+\n+/* Write batch */\n+\n+extern leveldb_writebatch_t* leveldb_writebatch_create();\n+extern void leveldb_writebatch_destroy(leveldb_writebatch_t*);\n+extern void leveldb_writebatch_clear(leveldb_writebatch_t*);\n+extern void leveldb_writebatch_put(\n+    leveldb_writebatch_t*,\n+    const char* key, size_t klen,\n+    const char* val, size_t vlen);\n+extern void leveldb_writebatch_delete(\n+    leveldb_writebatch_t*,\n+    const char* key, size_t klen);\n+extern void leveldb_writebatch_iterate(\n+    leveldb_writebatch_t*,\n+    void* state,\n+    void (*put)(void*, const char* k, size_t klen, const char* v, size_t vlen),\n+    void (*deleted)(void*, const char* k, size_t klen));\n+\n+/* Options */\n+\n+extern leveldb_options_t* leveldb_options_create();\n+extern void leveldb_options_destroy(leveldb_options_t*);\n+extern void leveldb_options_set_comparator(\n+    leveldb_options_t*,\n+    leveldb_comparator_t*);\n+extern void leveldb_options_set_filter_policy(\n+    leveldb_options_t*,\n+    leveldb_filterpolicy_t*);\n+extern void leveldb_options_set_create_if_missing(\n+    leveldb_options_t*, unsigned char);\n+extern void leveldb_options_set_error_if_exists(\n+    leveldb_options_t*, unsigned char);\n+extern void leveldb_options_set_paranoid_checks(\n+    leveldb_options_t*, unsigned char);\n+extern void leveldb_options_set_env(leveldb_options_t*, leveldb_env_t*);\n+extern void leveldb_options_set_info_log(leveldb_options_t*, leveldb_logger_t*);\n+extern void leveldb_options_set_write_buffer_size(leveldb_options_t*, size_t);\n+extern void leveldb_options_set_max_open_files(leveldb_options_t*, int);\n+extern void leveldb_options_set_cache(leveldb_options_t*, leveldb_cache_t*);\n+extern void leveldb_options_set_block_size(leveldb_options_t*, size_t);\n+extern void leveldb_options_set_block_restart_interval(leveldb_options_t*, int);\n+\n+enum {\n+  leveldb_no_compression = 0,\n+  leveldb_snappy_compression = 1\n+};\n+extern void leveldb_options_set_compression(leveldb_options_t*, int);\n+\n+/* Comparator */\n+\n+extern leveldb_comparator_t* leveldb_comparator_create(\n+    void* state,\n+    void (*destructor)(void*),\n+    int (*compare)(\n+        void*,\n+        const char* a, size_t alen,\n+        const char* b, size_t blen),\n+    const char* (*name)(void*));\n+extern void leveldb_comparator_destroy(leveldb_comparator_t*);\n+\n+/* Filter policy */\n+\n+extern leveldb_filterpolicy_t* leveldb_filterpolicy_create(\n+    void* state,\n+    void (*destructor)(void*),\n+    char* (*create_filter)(\n+        void*,\n+        const char* const* key_array, const size_t* key_length_array,\n+        int num_keys,\n+        size_t* filter_length),\n+    unsigned char (*key_may_match)(\n+        void*,\n+        const char* key, size_t length,\n+        const char* filter, size_t filter_length),\n+    const char* (*name)(void*));\n+extern void leveldb_filterpolicy_destroy(leveldb_filterpolicy_t*);\n+\n+extern leveldb_filterpolicy_t* leveldb_filterpolicy_create_bloom(\n+    int bits_per_key);\n+\n+/* Read options */\n+\n+extern leveldb_readoptions_t* leveldb_readoptions_create();\n+extern void leveldb_readoptions_destroy(leveldb_readoptions_t*);\n+extern void leveldb_readoptions_set_verify_checksums(\n+    leveldb_readoptions_t*,\n+    unsigned char);\n+extern void leveldb_readoptions_set_fill_cache(\n+    leveldb_readoptions_t*, unsigned char);\n+extern void leveldb_readoptions_set_snapshot(\n+    leveldb_readoptions_t*,\n+    const leveldb_snapshot_t*);\n+\n+/* Write options */\n+\n+extern leveldb_writeoptions_t* leveldb_writeoptions_create();\n+extern void leveldb_writeoptions_destroy(leveldb_writeoptions_t*);\n+extern void leveldb_writeoptions_set_sync(\n+    leveldb_writeoptions_t*, unsigned char);\n+\n+/* Cache */\n+\n+extern leveldb_cache_t* leveldb_cache_create_lru(size_t capacity);\n+extern void leveldb_cache_destroy(leveldb_cache_t* cache);\n+\n+/* Env */\n+\n+extern leveldb_env_t* leveldb_create_default_env();\n+extern void leveldb_env_destroy(leveldb_env_t*);\n+\n+/* Utility */\n+\n+/* Calls free(ptr).\n+   REQUIRES: ptr was malloc()-ed and returned by one of the routines\n+   in this file.  Note that in certain cases (typically on Windows), you\n+   may need to call this routine instead of free(ptr) to dispose of\n+   malloc()-ed memory returned by this library. */\n+extern void leveldb_free(void* ptr);\n+\n+/* Return the major version number for this release. */\n+extern int leveldb_major_version();\n+\n+/* Return the minor version number for this release. */\n+extern int leveldb_minor_version();\n+\n+#ifdef __cplusplus\n+}  /* end extern \"C\" */\n+#endif\n+\n+#endif  /* STORAGE_LEVELDB_INCLUDE_C_H_ */"
      },
      {
        "sha": "5e3b47637d49e9f963b141dc5a011e1272615750",
        "filename": "include/leveldb/cache.h",
        "status": "added",
        "additions": 99,
        "deletions": 0,
        "changes": 99,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/cache.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/cache.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/include/leveldb/cache.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,99 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// A Cache is an interface that maps keys to values.  It has internal\n+// synchronization and may be safely accessed concurrently from\n+// multiple threads.  It may automatically evict entries to make room\n+// for new entries.  Values have a specified charge against the cache\n+// capacity.  For example, a cache where the values are variable\n+// length strings, may use the length of the string as the charge for\n+// the string.\n+//\n+// A builtin cache implementation with a least-recently-used eviction\n+// policy is provided.  Clients may use their own implementations if\n+// they want something more sophisticated (like scan-resistance, a\n+// custom eviction policy, variable cache sizing, etc.)\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_CACHE_H_\n+#define STORAGE_LEVELDB_INCLUDE_CACHE_H_\n+\n+#include <stdint.h>\n+#include \"leveldb/slice.h\"\n+\n+namespace leveldb {\n+\n+class Cache;\n+\n+// Create a new cache with a fixed size capacity.  This implementation\n+// of Cache uses a least-recently-used eviction policy.\n+extern Cache* NewLRUCache(size_t capacity);\n+\n+class Cache {\n+ public:\n+  Cache() { }\n+\n+  // Destroys all existing entries by calling the \"deleter\"\n+  // function that was passed to the constructor.\n+  virtual ~Cache();\n+\n+  // Opaque handle to an entry stored in the cache.\n+  struct Handle { };\n+\n+  // Insert a mapping from key->value into the cache and assign it\n+  // the specified charge against the total cache capacity.\n+  //\n+  // Returns a handle that corresponds to the mapping.  The caller\n+  // must call this->Release(handle) when the returned mapping is no\n+  // longer needed.\n+  //\n+  // When the inserted entry is no longer needed, the key and\n+  // value will be passed to \"deleter\".\n+  virtual Handle* Insert(const Slice& key, void* value, size_t charge,\n+                         void (*deleter)(const Slice& key, void* value)) = 0;\n+\n+  // If the cache has no mapping for \"key\", returns NULL.\n+  //\n+  // Else return a handle that corresponds to the mapping.  The caller\n+  // must call this->Release(handle) when the returned mapping is no\n+  // longer needed.\n+  virtual Handle* Lookup(const Slice& key) = 0;\n+\n+  // Release a mapping returned by a previous Lookup().\n+  // REQUIRES: handle must not have been released yet.\n+  // REQUIRES: handle must have been returned by a method on *this.\n+  virtual void Release(Handle* handle) = 0;\n+\n+  // Return the value encapsulated in a handle returned by a\n+  // successful Lookup().\n+  // REQUIRES: handle must not have been released yet.\n+  // REQUIRES: handle must have been returned by a method on *this.\n+  virtual void* Value(Handle* handle) = 0;\n+\n+  // If the cache contains entry for key, erase it.  Note that the\n+  // underlying entry will be kept around until all existing handles\n+  // to it have been released.\n+  virtual void Erase(const Slice& key) = 0;\n+\n+  // Return a new numeric id.  May be used by multiple clients who are\n+  // sharing the same cache to partition the key space.  Typically the\n+  // client will allocate a new id at startup and prepend the id to\n+  // its cache keys.\n+  virtual uint64_t NewId() = 0;\n+\n+ private:\n+  void LRU_Remove(Handle* e);\n+  void LRU_Append(Handle* e);\n+  void Unref(Handle* e);\n+\n+  struct Rep;\n+  Rep* rep_;\n+\n+  // No copying allowed\n+  Cache(const Cache&);\n+  void operator=(const Cache&);\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_UTIL_CACHE_H_"
      },
      {
        "sha": "556b984c7694f6520088754f3017bf58c7cafc9d",
        "filename": "include/leveldb/comparator.h",
        "status": "added",
        "additions": 63,
        "deletions": 0,
        "changes": 63,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/comparator.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/comparator.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/include/leveldb/comparator.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,63 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_COMPARATOR_H_\n+#define STORAGE_LEVELDB_INCLUDE_COMPARATOR_H_\n+\n+#include <string>\n+\n+namespace leveldb {\n+\n+class Slice;\n+\n+// A Comparator object provides a total order across slices that are\n+// used as keys in an sstable or a database.  A Comparator implementation\n+// must be thread-safe since leveldb may invoke its methods concurrently\n+// from multiple threads.\n+class Comparator {\n+ public:\n+  virtual ~Comparator();\n+\n+  // Three-way comparison.  Returns value:\n+  //   < 0 iff \"a\" < \"b\",\n+  //   == 0 iff \"a\" == \"b\",\n+  //   > 0 iff \"a\" > \"b\"\n+  virtual int Compare(const Slice& a, const Slice& b) const = 0;\n+\n+  // The name of the comparator.  Used to check for comparator\n+  // mismatches (i.e., a DB created with one comparator is\n+  // accessed using a different comparator.\n+  //\n+  // The client of this package should switch to a new name whenever\n+  // the comparator implementation changes in a way that will cause\n+  // the relative ordering of any two keys to change.\n+  //\n+  // Names starting with \"leveldb.\" are reserved and should not be used\n+  // by any clients of this package.\n+  virtual const char* Name() const = 0;\n+\n+  // Advanced functions: these are used to reduce the space requirements\n+  // for internal data structures like index blocks.\n+\n+  // If *start < limit, changes *start to a short string in [start,limit).\n+  // Simple comparator implementations may return with *start unchanged,\n+  // i.e., an implementation of this method that does nothing is correct.\n+  virtual void FindShortestSeparator(\n+      std::string* start,\n+      const Slice& limit) const = 0;\n+\n+  // Changes *key to a short string >= *key.\n+  // Simple comparator implementations may return with *key unchanged,\n+  // i.e., an implementation of this method that does nothing is correct.\n+  virtual void FindShortSuccessor(std::string* key) const = 0;\n+};\n+\n+// Return a builtin comparator that uses lexicographic byte-wise\n+// ordering.  The result remains the property of this module and\n+// must not be deleted.\n+extern const Comparator* BytewiseComparator();\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_COMPARATOR_H_"
      },
      {
        "sha": "da8b11a8c05b054ef231ba34d88ef51ec1e55eb0",
        "filename": "include/leveldb/db.h",
        "status": "added",
        "additions": 161,
        "deletions": 0,
        "changes": 161,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/db.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/db.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/include/leveldb/db.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,161 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_DB_H_\n+#define STORAGE_LEVELDB_INCLUDE_DB_H_\n+\n+#include <stdint.h>\n+#include <stdio.h>\n+#include \"leveldb/iterator.h\"\n+#include \"leveldb/options.h\"\n+\n+namespace leveldb {\n+\n+// Update Makefile if you change these\n+static const int kMajorVersion = 1;\n+static const int kMinorVersion = 12;\n+\n+struct Options;\n+struct ReadOptions;\n+struct WriteOptions;\n+class WriteBatch;\n+\n+// Abstract handle to particular state of a DB.\n+// A Snapshot is an immutable object and can therefore be safely\n+// accessed from multiple threads without any external synchronization.\n+class Snapshot {\n+ protected:\n+  virtual ~Snapshot();\n+};\n+\n+// A range of keys\n+struct Range {\n+  Slice start;          // Included in the range\n+  Slice limit;          // Not included in the range\n+\n+  Range() { }\n+  Range(const Slice& s, const Slice& l) : start(s), limit(l) { }\n+};\n+\n+// A DB is a persistent ordered map from keys to values.\n+// A DB is safe for concurrent access from multiple threads without\n+// any external synchronization.\n+class DB {\n+ public:\n+  // Open the database with the specified \"name\".\n+  // Stores a pointer to a heap-allocated database in *dbptr and returns\n+  // OK on success.\n+  // Stores NULL in *dbptr and returns a non-OK status on error.\n+  // Caller should delete *dbptr when it is no longer needed.\n+  static Status Open(const Options& options,\n+                     const std::string& name,\n+                     DB** dbptr);\n+\n+  DB() { }\n+  virtual ~DB();\n+\n+  // Set the database entry for \"key\" to \"value\".  Returns OK on success,\n+  // and a non-OK status on error.\n+  // Note: consider setting options.sync = true.\n+  virtual Status Put(const WriteOptions& options,\n+                     const Slice& key,\n+                     const Slice& value) = 0;\n+\n+  // Remove the database entry (if any) for \"key\".  Returns OK on\n+  // success, and a non-OK status on error.  It is not an error if \"key\"\n+  // did not exist in the database.\n+  // Note: consider setting options.sync = true.\n+  virtual Status Delete(const WriteOptions& options, const Slice& key) = 0;\n+\n+  // Apply the specified updates to the database.\n+  // Returns OK on success, non-OK on failure.\n+  // Note: consider setting options.sync = true.\n+  virtual Status Write(const WriteOptions& options, WriteBatch* updates) = 0;\n+\n+  // If the database contains an entry for \"key\" store the\n+  // corresponding value in *value and return OK.\n+  //\n+  // If there is no entry for \"key\" leave *value unchanged and return\n+  // a status for which Status::IsNotFound() returns true.\n+  //\n+  // May return some other Status on an error.\n+  virtual Status Get(const ReadOptions& options,\n+                     const Slice& key, std::string* value) = 0;\n+\n+  // Return a heap-allocated iterator over the contents of the database.\n+  // The result of NewIterator() is initially invalid (caller must\n+  // call one of the Seek methods on the iterator before using it).\n+  //\n+  // Caller should delete the iterator when it is no longer needed.\n+  // The returned iterator should be deleted before this db is deleted.\n+  virtual Iterator* NewIterator(const ReadOptions& options) = 0;\n+\n+  // Return a handle to the current DB state.  Iterators created with\n+  // this handle will all observe a stable snapshot of the current DB\n+  // state.  The caller must call ReleaseSnapshot(result) when the\n+  // snapshot is no longer needed.\n+  virtual const Snapshot* GetSnapshot() = 0;\n+\n+  // Release a previously acquired snapshot.  The caller must not\n+  // use \"snapshot\" after this call.\n+  virtual void ReleaseSnapshot(const Snapshot* snapshot) = 0;\n+\n+  // DB implementations can export properties about their state\n+  // via this method.  If \"property\" is a valid property understood by this\n+  // DB implementation, fills \"*value\" with its current value and returns\n+  // true.  Otherwise returns false.\n+  //\n+  //\n+  // Valid property names include:\n+  //\n+  //  \"leveldb.num-files-at-level<N>\" - return the number of files at level <N>,\n+  //     where <N> is an ASCII representation of a level number (e.g. \"0\").\n+  //  \"leveldb.stats\" - returns a multi-line string that describes statistics\n+  //     about the internal operation of the DB.\n+  //  \"leveldb.sstables\" - returns a multi-line string that describes all\n+  //     of the sstables that make up the db contents.\n+  virtual bool GetProperty(const Slice& property, std::string* value) = 0;\n+\n+  // For each i in [0,n-1], store in \"sizes[i]\", the approximate\n+  // file system space used by keys in \"[range[i].start .. range[i].limit)\".\n+  //\n+  // Note that the returned sizes measure file system space usage, so\n+  // if the user data compresses by a factor of ten, the returned\n+  // sizes will be one-tenth the size of the corresponding user data size.\n+  //\n+  // The results may not include the sizes of recently written data.\n+  virtual void GetApproximateSizes(const Range* range, int n,\n+                                   uint64_t* sizes) = 0;\n+\n+  // Compact the underlying storage for the key range [*begin,*end].\n+  // In particular, deleted and overwritten versions are discarded,\n+  // and the data is rearranged to reduce the cost of operations\n+  // needed to access the data.  This operation should typically only\n+  // be invoked by users who understand the underlying implementation.\n+  //\n+  // begin==NULL is treated as a key before all keys in the database.\n+  // end==NULL is treated as a key after all keys in the database.\n+  // Therefore the following call will compact the entire database:\n+  //    db->CompactRange(NULL, NULL);\n+  virtual void CompactRange(const Slice* begin, const Slice* end) = 0;\n+\n+ private:\n+  // No copying allowed\n+  DB(const DB&);\n+  void operator=(const DB&);\n+};\n+\n+// Destroy the contents of the specified database.\n+// Be very careful using this method.\n+Status DestroyDB(const std::string& name, const Options& options);\n+\n+// If a DB cannot be opened, you may attempt to call this method to\n+// resurrect as much of the contents of the database as possible.\n+// Some data may be lost, so be careful when calling this function\n+// on a database that contains important information.\n+Status RepairDB(const std::string& dbname, const Options& options);\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_DB_H_"
      },
      {
        "sha": "fa32289f581fd4d222dc74ea177a78138b71fbc2",
        "filename": "include/leveldb/env.h",
        "status": "added",
        "additions": 333,
        "deletions": 0,
        "changes": 333,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/env.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/env.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/include/leveldb/env.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,333 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// An Env is an interface used by the leveldb implementation to access\n+// operating system functionality like the filesystem etc.  Callers\n+// may wish to provide a custom Env object when opening a database to\n+// get fine gain control; e.g., to rate limit file system operations.\n+//\n+// All Env implementations are safe for concurrent access from\n+// multiple threads without any external synchronization.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_ENV_H_\n+#define STORAGE_LEVELDB_INCLUDE_ENV_H_\n+\n+#include <cstdarg>\n+#include <string>\n+#include <vector>\n+#include <stdint.h>\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+class FileLock;\n+class Logger;\n+class RandomAccessFile;\n+class SequentialFile;\n+class Slice;\n+class WritableFile;\n+\n+class Env {\n+ public:\n+  Env() { }\n+  virtual ~Env();\n+\n+  // Return a default environment suitable for the current operating\n+  // system.  Sophisticated users may wish to provide their own Env\n+  // implementation instead of relying on this default environment.\n+  //\n+  // The result of Default() belongs to leveldb and must never be deleted.\n+  static Env* Default();\n+\n+  // Create a brand new sequentially-readable file with the specified name.\n+  // On success, stores a pointer to the new file in *result and returns OK.\n+  // On failure stores NULL in *result and returns non-OK.  If the file does\n+  // not exist, returns a non-OK status.\n+  //\n+  // The returned file will only be accessed by one thread at a time.\n+  virtual Status NewSequentialFile(const std::string& fname,\n+                                   SequentialFile** result) = 0;\n+\n+  // Create a brand new random access read-only file with the\n+  // specified name.  On success, stores a pointer to the new file in\n+  // *result and returns OK.  On failure stores NULL in *result and\n+  // returns non-OK.  If the file does not exist, returns a non-OK\n+  // status.\n+  //\n+  // The returned file may be concurrently accessed by multiple threads.\n+  virtual Status NewRandomAccessFile(const std::string& fname,\n+                                     RandomAccessFile** result) = 0;\n+\n+  // Create an object that writes to a new file with the specified\n+  // name.  Deletes any existing file with the same name and creates a\n+  // new file.  On success, stores a pointer to the new file in\n+  // *result and returns OK.  On failure stores NULL in *result and\n+  // returns non-OK.\n+  //\n+  // The returned file will only be accessed by one thread at a time.\n+  virtual Status NewWritableFile(const std::string& fname,\n+                                 WritableFile** result) = 0;\n+\n+  // Returns true iff the named file exists.\n+  virtual bool FileExists(const std::string& fname) = 0;\n+\n+  // Store in *result the names of the children of the specified directory.\n+  // The names are relative to \"dir\".\n+  // Original contents of *results are dropped.\n+  virtual Status GetChildren(const std::string& dir,\n+                             std::vector<std::string>* result) = 0;\n+\n+  // Delete the named file.\n+  virtual Status DeleteFile(const std::string& fname) = 0;\n+\n+  // Create the specified directory.\n+  virtual Status CreateDir(const std::string& dirname) = 0;\n+\n+  // Delete the specified directory.\n+  virtual Status DeleteDir(const std::string& dirname) = 0;\n+\n+  // Store the size of fname in *file_size.\n+  virtual Status GetFileSize(const std::string& fname, uint64_t* file_size) = 0;\n+\n+  // Rename file src to target.\n+  virtual Status RenameFile(const std::string& src,\n+                            const std::string& target) = 0;\n+\n+  // Lock the specified file.  Used to prevent concurrent access to\n+  // the same db by multiple processes.  On failure, stores NULL in\n+  // *lock and returns non-OK.\n+  //\n+  // On success, stores a pointer to the object that represents the\n+  // acquired lock in *lock and returns OK.  The caller should call\n+  // UnlockFile(*lock) to release the lock.  If the process exits,\n+  // the lock will be automatically released.\n+  //\n+  // If somebody else already holds the lock, finishes immediately\n+  // with a failure.  I.e., this call does not wait for existing locks\n+  // to go away.\n+  //\n+  // May create the named file if it does not already exist.\n+  virtual Status LockFile(const std::string& fname, FileLock** lock) = 0;\n+\n+  // Release the lock acquired by a previous successful call to LockFile.\n+  // REQUIRES: lock was returned by a successful LockFile() call\n+  // REQUIRES: lock has not already been unlocked.\n+  virtual Status UnlockFile(FileLock* lock) = 0;\n+\n+  // Arrange to run \"(*function)(arg)\" once in a background thread.\n+  //\n+  // \"function\" may run in an unspecified thread.  Multiple functions\n+  // added to the same Env may run concurrently in different threads.\n+  // I.e., the caller may not assume that background work items are\n+  // serialized.\n+  virtual void Schedule(\n+      void (*function)(void* arg),\n+      void* arg) = 0;\n+\n+  // Start a new thread, invoking \"function(arg)\" within the new thread.\n+  // When \"function(arg)\" returns, the thread will be destroyed.\n+  virtual void StartThread(void (*function)(void* arg), void* arg) = 0;\n+\n+  // *path is set to a temporary directory that can be used for testing. It may\n+  // or many not have just been created. The directory may or may not differ\n+  // between runs of the same process, but subsequent calls will return the\n+  // same directory.\n+  virtual Status GetTestDirectory(std::string* path) = 0;\n+\n+  // Create and return a log file for storing informational messages.\n+  virtual Status NewLogger(const std::string& fname, Logger** result) = 0;\n+\n+  // Returns the number of micro-seconds since some fixed point in time. Only\n+  // useful for computing deltas of time.\n+  virtual uint64_t NowMicros() = 0;\n+\n+  // Sleep/delay the thread for the perscribed number of micro-seconds.\n+  virtual void SleepForMicroseconds(int micros) = 0;\n+\n+ private:\n+  // No copying allowed\n+  Env(const Env&);\n+  void operator=(const Env&);\n+};\n+\n+// A file abstraction for reading sequentially through a file\n+class SequentialFile {\n+ public:\n+  SequentialFile() { }\n+  virtual ~SequentialFile();\n+\n+  // Read up to \"n\" bytes from the file.  \"scratch[0..n-1]\" may be\n+  // written by this routine.  Sets \"*result\" to the data that was\n+  // read (including if fewer than \"n\" bytes were successfully read).\n+  // May set \"*result\" to point at data in \"scratch[0..n-1]\", so\n+  // \"scratch[0..n-1]\" must be live when \"*result\" is used.\n+  // If an error was encountered, returns a non-OK status.\n+  //\n+  // REQUIRES: External synchronization\n+  virtual Status Read(size_t n, Slice* result, char* scratch) = 0;\n+\n+  // Skip \"n\" bytes from the file. This is guaranteed to be no\n+  // slower that reading the same data, but may be faster.\n+  //\n+  // If end of file is reached, skipping will stop at the end of the\n+  // file, and Skip will return OK.\n+  //\n+  // REQUIRES: External synchronization\n+  virtual Status Skip(uint64_t n) = 0;\n+\n+ private:\n+  // No copying allowed\n+  SequentialFile(const SequentialFile&);\n+  void operator=(const SequentialFile&);\n+};\n+\n+// A file abstraction for randomly reading the contents of a file.\n+class RandomAccessFile {\n+ public:\n+  RandomAccessFile() { }\n+  virtual ~RandomAccessFile();\n+\n+  // Read up to \"n\" bytes from the file starting at \"offset\".\n+  // \"scratch[0..n-1]\" may be written by this routine.  Sets \"*result\"\n+  // to the data that was read (including if fewer than \"n\" bytes were\n+  // successfully read).  May set \"*result\" to point at data in\n+  // \"scratch[0..n-1]\", so \"scratch[0..n-1]\" must be live when\n+  // \"*result\" is used.  If an error was encountered, returns a non-OK\n+  // status.\n+  //\n+  // Safe for concurrent use by multiple threads.\n+  virtual Status Read(uint64_t offset, size_t n, Slice* result,\n+                      char* scratch) const = 0;\n+\n+ private:\n+  // No copying allowed\n+  RandomAccessFile(const RandomAccessFile&);\n+  void operator=(const RandomAccessFile&);\n+};\n+\n+// A file abstraction for sequential writing.  The implementation\n+// must provide buffering since callers may append small fragments\n+// at a time to the file.\n+class WritableFile {\n+ public:\n+  WritableFile() { }\n+  virtual ~WritableFile();\n+\n+  virtual Status Append(const Slice& data) = 0;\n+  virtual Status Close() = 0;\n+  virtual Status Flush() = 0;\n+  virtual Status Sync() = 0;\n+\n+ private:\n+  // No copying allowed\n+  WritableFile(const WritableFile&);\n+  void operator=(const WritableFile&);\n+};\n+\n+// An interface for writing log messages.\n+class Logger {\n+ public:\n+  Logger() { }\n+  virtual ~Logger();\n+\n+  // Write an entry to the log file with the specified format.\n+  virtual void Logv(const char* format, va_list ap) = 0;\n+\n+ private:\n+  // No copying allowed\n+  Logger(const Logger&);\n+  void operator=(const Logger&);\n+};\n+\n+\n+// Identifies a locked file.\n+class FileLock {\n+ public:\n+  FileLock() { }\n+  virtual ~FileLock();\n+ private:\n+  // No copying allowed\n+  FileLock(const FileLock&);\n+  void operator=(const FileLock&);\n+};\n+\n+// Log the specified data to *info_log if info_log is non-NULL.\n+extern void Log(Logger* info_log, const char* format, ...)\n+#   if defined(__GNUC__) || defined(__clang__)\n+    __attribute__((__format__ (__printf__, 2, 3)))\n+#   endif\n+    ;\n+\n+// A utility routine: write \"data\" to the named file.\n+extern Status WriteStringToFile(Env* env, const Slice& data,\n+                                const std::string& fname);\n+\n+// A utility routine: read contents of named file into *data\n+extern Status ReadFileToString(Env* env, const std::string& fname,\n+                               std::string* data);\n+\n+// An implementation of Env that forwards all calls to another Env.\n+// May be useful to clients who wish to override just part of the\n+// functionality of another Env.\n+class EnvWrapper : public Env {\n+ public:\n+  // Initialize an EnvWrapper that delegates all calls to *t\n+  explicit EnvWrapper(Env* t) : target_(t) { }\n+  virtual ~EnvWrapper();\n+\n+  // Return the target to which this Env forwards all calls\n+  Env* target() const { return target_; }\n+\n+  // The following text is boilerplate that forwards all methods to target()\n+  Status NewSequentialFile(const std::string& f, SequentialFile** r) {\n+    return target_->NewSequentialFile(f, r);\n+  }\n+  Status NewRandomAccessFile(const std::string& f, RandomAccessFile** r) {\n+    return target_->NewRandomAccessFile(f, r);\n+  }\n+  Status NewWritableFile(const std::string& f, WritableFile** r) {\n+    return target_->NewWritableFile(f, r);\n+  }\n+  bool FileExists(const std::string& f) { return target_->FileExists(f); }\n+  Status GetChildren(const std::string& dir, std::vector<std::string>* r) {\n+    return target_->GetChildren(dir, r);\n+  }\n+  Status DeleteFile(const std::string& f) { return target_->DeleteFile(f); }\n+  Status CreateDir(const std::string& d) { return target_->CreateDir(d); }\n+  Status DeleteDir(const std::string& d) { return target_->DeleteDir(d); }\n+  Status GetFileSize(const std::string& f, uint64_t* s) {\n+    return target_->GetFileSize(f, s);\n+  }\n+  Status RenameFile(const std::string& s, const std::string& t) {\n+    return target_->RenameFile(s, t);\n+  }\n+  Status LockFile(const std::string& f, FileLock** l) {\n+    return target_->LockFile(f, l);\n+  }\n+  Status UnlockFile(FileLock* l) { return target_->UnlockFile(l); }\n+  void Schedule(void (*f)(void*), void* a) {\n+    return target_->Schedule(f, a);\n+  }\n+  void StartThread(void (*f)(void*), void* a) {\n+    return target_->StartThread(f, a);\n+  }\n+  virtual Status GetTestDirectory(std::string* path) {\n+    return target_->GetTestDirectory(path);\n+  }\n+  virtual Status NewLogger(const std::string& fname, Logger** result) {\n+    return target_->NewLogger(fname, result);\n+  }\n+  uint64_t NowMicros() {\n+    return target_->NowMicros();\n+  }\n+  void SleepForMicroseconds(int micros) {\n+    target_->SleepForMicroseconds(micros);\n+  }\n+ private:\n+  Env* target_;\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_ENV_H_"
      },
      {
        "sha": "1fba08001fc335a14bde90fa5a1c5d58025ae038",
        "filename": "include/leveldb/filter_policy.h",
        "status": "added",
        "additions": 70,
        "deletions": 0,
        "changes": 70,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/filter_policy.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/filter_policy.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/include/leveldb/filter_policy.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,70 @@\n+// Copyright (c) 2012 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// A database can be configured with a custom FilterPolicy object.\n+// This object is responsible for creating a small filter from a set\n+// of keys.  These filters are stored in leveldb and are consulted\n+// automatically by leveldb to decide whether or not to read some\n+// information from disk. In many cases, a filter can cut down the\n+// number of disk seeks form a handful to a single disk seek per\n+// DB::Get() call.\n+//\n+// Most people will want to use the builtin bloom filter support (see\n+// NewBloomFilterPolicy() below).\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_FILTER_POLICY_H_\n+#define STORAGE_LEVELDB_INCLUDE_FILTER_POLICY_H_\n+\n+#include <string>\n+\n+namespace leveldb {\n+\n+class Slice;\n+\n+class FilterPolicy {\n+ public:\n+  virtual ~FilterPolicy();\n+\n+  // Return the name of this policy.  Note that if the filter encoding\n+  // changes in an incompatible way, the name returned by this method\n+  // must be changed.  Otherwise, old incompatible filters may be\n+  // passed to methods of this type.\n+  virtual const char* Name() const = 0;\n+\n+  // keys[0,n-1] contains a list of keys (potentially with duplicates)\n+  // that are ordered according to the user supplied comparator.\n+  // Append a filter that summarizes keys[0,n-1] to *dst.\n+  //\n+  // Warning: do not change the initial contents of *dst.  Instead,\n+  // append the newly constructed filter to *dst.\n+  virtual void CreateFilter(const Slice* keys, int n, std::string* dst)\n+      const = 0;\n+\n+  // \"filter\" contains the data appended by a preceding call to\n+  // CreateFilter() on this class.  This method must return true if\n+  // the key was in the list of keys passed to CreateFilter().\n+  // This method may return true or false if the key was not on the\n+  // list, but it should aim to return false with a high probability.\n+  virtual bool KeyMayMatch(const Slice& key, const Slice& filter) const = 0;\n+};\n+\n+// Return a new filter policy that uses a bloom filter with approximately\n+// the specified number of bits per key.  A good value for bits_per_key\n+// is 10, which yields a filter with ~ 1% false positive rate.\n+//\n+// Callers must delete the result after any database that is using the\n+// result has been closed.\n+//\n+// Note: if you are using a custom comparator that ignores some parts\n+// of the keys being compared, you must not use NewBloomFilterPolicy()\n+// and must provide your own FilterPolicy that also ignores the\n+// corresponding parts of the keys.  For example, if the comparator\n+// ignores trailing spaces, it would be incorrect to use a\n+// FilterPolicy (like NewBloomFilterPolicy) that does not ignore\n+// trailing spaces in keys.\n+extern const FilterPolicy* NewBloomFilterPolicy(int bits_per_key);\n+\n+}\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_FILTER_POLICY_H_"
      },
      {
        "sha": "ad543eb46cde9af30f9250ee2eaa7f0979cc2994",
        "filename": "include/leveldb/iterator.h",
        "status": "added",
        "additions": 100,
        "deletions": 0,
        "changes": 100,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/iterator.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/iterator.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/include/leveldb/iterator.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,100 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// An iterator yields a sequence of key/value pairs from a source.\n+// The following class defines the interface.  Multiple implementations\n+// are provided by this library.  In particular, iterators are provided\n+// to access the contents of a Table or a DB.\n+//\n+// Multiple threads can invoke const methods on an Iterator without\n+// external synchronization, but if any of the threads may call a\n+// non-const method, all threads accessing the same Iterator must use\n+// external synchronization.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_ITERATOR_H_\n+#define STORAGE_LEVELDB_INCLUDE_ITERATOR_H_\n+\n+#include \"leveldb/slice.h\"\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+class Iterator {\n+ public:\n+  Iterator();\n+  virtual ~Iterator();\n+\n+  // An iterator is either positioned at a key/value pair, or\n+  // not valid.  This method returns true iff the iterator is valid.\n+  virtual bool Valid() const = 0;\n+\n+  // Position at the first key in the source.  The iterator is Valid()\n+  // after this call iff the source is not empty.\n+  virtual void SeekToFirst() = 0;\n+\n+  // Position at the last key in the source.  The iterator is\n+  // Valid() after this call iff the source is not empty.\n+  virtual void SeekToLast() = 0;\n+\n+  // Position at the first key in the source that at or past target\n+  // The iterator is Valid() after this call iff the source contains\n+  // an entry that comes at or past target.\n+  virtual void Seek(const Slice& target) = 0;\n+\n+  // Moves to the next entry in the source.  After this call, Valid() is\n+  // true iff the iterator was not positioned at the last entry in the source.\n+  // REQUIRES: Valid()\n+  virtual void Next() = 0;\n+\n+  // Moves to the previous entry in the source.  After this call, Valid() is\n+  // true iff the iterator was not positioned at the first entry in source.\n+  // REQUIRES: Valid()\n+  virtual void Prev() = 0;\n+\n+  // Return the key for the current entry.  The underlying storage for\n+  // the returned slice is valid only until the next modification of\n+  // the iterator.\n+  // REQUIRES: Valid()\n+  virtual Slice key() const = 0;\n+\n+  // Return the value for the current entry.  The underlying storage for\n+  // the returned slice is valid only until the next modification of\n+  // the iterator.\n+  // REQUIRES: !AtEnd() && !AtStart()\n+  virtual Slice value() const = 0;\n+\n+  // If an error has occurred, return it.  Else return an ok status.\n+  virtual Status status() const = 0;\n+\n+  // Clients are allowed to register function/arg1/arg2 triples that\n+  // will be invoked when this iterator is destroyed.\n+  //\n+  // Note that unlike all of the preceding methods, this method is\n+  // not abstract and therefore clients should not override it.\n+  typedef void (*CleanupFunction)(void* arg1, void* arg2);\n+  void RegisterCleanup(CleanupFunction function, void* arg1, void* arg2);\n+\n+ private:\n+  struct Cleanup {\n+    CleanupFunction function;\n+    void* arg1;\n+    void* arg2;\n+    Cleanup* next;\n+  };\n+  Cleanup cleanup_;\n+\n+  // No copying allowed\n+  Iterator(const Iterator&);\n+  void operator=(const Iterator&);\n+};\n+\n+// Return an empty iterator (yields nothing).\n+extern Iterator* NewEmptyIterator();\n+\n+// Return an empty iterator with the specified status.\n+extern Iterator* NewErrorIterator(const Status& status);\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_ITERATOR_H_"
      },
      {
        "sha": "fdda718d3090638c7378f4418e4d024dd2e68bda",
        "filename": "include/leveldb/options.h",
        "status": "added",
        "additions": 195,
        "deletions": 0,
        "changes": 195,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/options.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/options.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/include/leveldb/options.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,195 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_OPTIONS_H_\n+#define STORAGE_LEVELDB_INCLUDE_OPTIONS_H_\n+\n+#include <stddef.h>\n+\n+namespace leveldb {\n+\n+class Cache;\n+class Comparator;\n+class Env;\n+class FilterPolicy;\n+class Logger;\n+class Snapshot;\n+\n+// DB contents are stored in a set of blocks, each of which holds a\n+// sequence of key,value pairs.  Each block may be compressed before\n+// being stored in a file.  The following enum describes which\n+// compression method (if any) is used to compress a block.\n+enum CompressionType {\n+  // NOTE: do not change the values of existing entries, as these are\n+  // part of the persistent format on disk.\n+  kNoCompression     = 0x0,\n+  kSnappyCompression = 0x1\n+};\n+\n+// Options to control the behavior of a database (passed to DB::Open)\n+struct Options {\n+  // -------------------\n+  // Parameters that affect behavior\n+\n+  // Comparator used to define the order of keys in the table.\n+  // Default: a comparator that uses lexicographic byte-wise ordering\n+  //\n+  // REQUIRES: The client must ensure that the comparator supplied\n+  // here has the same name and orders keys *exactly* the same as the\n+  // comparator provided to previous open calls on the same DB.\n+  const Comparator* comparator;\n+\n+  // If true, the database will be created if it is missing.\n+  // Default: false\n+  bool create_if_missing;\n+\n+  // If true, an error is raised if the database already exists.\n+  // Default: false\n+  bool error_if_exists;\n+\n+  // If true, the implementation will do aggressive checking of the\n+  // data it is processing and will stop early if it detects any\n+  // errors.  This may have unforeseen ramifications: for example, a\n+  // corruption of one DB entry may cause a large number of entries to\n+  // become unreadable or for the entire DB to become unopenable.\n+  // Default: false\n+  bool paranoid_checks;\n+\n+  // Use the specified object to interact with the environment,\n+  // e.g. to read/write files, schedule background work, etc.\n+  // Default: Env::Default()\n+  Env* env;\n+\n+  // Any internal progress/error information generated by the db will\n+  // be written to info_log if it is non-NULL, or to a file stored\n+  // in the same directory as the DB contents if info_log is NULL.\n+  // Default: NULL\n+  Logger* info_log;\n+\n+  // -------------------\n+  // Parameters that affect performance\n+\n+  // Amount of data to build up in memory (backed by an unsorted log\n+  // on disk) before converting to a sorted on-disk file.\n+  //\n+  // Larger values increase performance, especially during bulk loads.\n+  // Up to two write buffers may be held in memory at the same time,\n+  // so you may wish to adjust this parameter to control memory usage.\n+  // Also, a larger write buffer will result in a longer recovery time\n+  // the next time the database is opened.\n+  //\n+  // Default: 4MB\n+  size_t write_buffer_size;\n+\n+  // Number of open files that can be used by the DB.  You may need to\n+  // increase this if your database has a large working set (budget\n+  // one open file per 2MB of working set).\n+  //\n+  // Default: 1000\n+  int max_open_files;\n+\n+  // Control over blocks (user data is stored in a set of blocks, and\n+  // a block is the unit of reading from disk).\n+\n+  // If non-NULL, use the specified cache for blocks.\n+  // If NULL, leveldb will automatically create and use an 8MB internal cache.\n+  // Default: NULL\n+  Cache* block_cache;\n+\n+  // Approximate size of user data packed per block.  Note that the\n+  // block size specified here corresponds to uncompressed data.  The\n+  // actual size of the unit read from disk may be smaller if\n+  // compression is enabled.  This parameter can be changed dynamically.\n+  //\n+  // Default: 4K\n+  size_t block_size;\n+\n+  // Number of keys between restart points for delta encoding of keys.\n+  // This parameter can be changed dynamically.  Most clients should\n+  // leave this parameter alone.\n+  //\n+  // Default: 16\n+  int block_restart_interval;\n+\n+  // Compress blocks using the specified compression algorithm.  This\n+  // parameter can be changed dynamically.\n+  //\n+  // Default: kSnappyCompression, which gives lightweight but fast\n+  // compression.\n+  //\n+  // Typical speeds of kSnappyCompression on an Intel(R) Core(TM)2 2.4GHz:\n+  //    ~200-500MB/s compression\n+  //    ~400-800MB/s decompression\n+  // Note that these speeds are significantly faster than most\n+  // persistent storage speeds, and therefore it is typically never\n+  // worth switching to kNoCompression.  Even if the input data is\n+  // incompressible, the kSnappyCompression implementation will\n+  // efficiently detect that and will switch to uncompressed mode.\n+  CompressionType compression;\n+\n+  // If non-NULL, use the specified filter policy to reduce disk reads.\n+  // Many applications will benefit from passing the result of\n+  // NewBloomFilterPolicy() here.\n+  //\n+  // Default: NULL\n+  const FilterPolicy* filter_policy;\n+\n+  // Create an Options object with default values for all fields.\n+  Options();\n+};\n+\n+// Options that control read operations\n+struct ReadOptions {\n+  // If true, all data read from underlying storage will be\n+  // verified against corresponding checksums.\n+  // Default: false\n+  bool verify_checksums;\n+\n+  // Should the data read for this iteration be cached in memory?\n+  // Callers may wish to set this field to false for bulk scans.\n+  // Default: true\n+  bool fill_cache;\n+\n+  // If \"snapshot\" is non-NULL, read as of the supplied snapshot\n+  // (which must belong to the DB that is being read and which must\n+  // not have been released).  If \"snapshot\" is NULL, use an impliicit\n+  // snapshot of the state at the beginning of this read operation.\n+  // Default: NULL\n+  const Snapshot* snapshot;\n+\n+  ReadOptions()\n+      : verify_checksums(false),\n+        fill_cache(true),\n+        snapshot(NULL) {\n+  }\n+};\n+\n+// Options that control write operations\n+struct WriteOptions {\n+  // If true, the write will be flushed from the operating system\n+  // buffer cache (by calling WritableFile::Sync()) before the write\n+  // is considered complete.  If this flag is true, writes will be\n+  // slower.\n+  //\n+  // If this flag is false, and the machine crashes, some recent\n+  // writes may be lost.  Note that if it is just the process that\n+  // crashes (i.e., the machine does not reboot), no writes will be\n+  // lost even if sync==false.\n+  //\n+  // In other words, a DB write with sync==false has similar\n+  // crash semantics as the \"write()\" system call.  A DB write\n+  // with sync==true has similar crash semantics to a \"write()\"\n+  // system call followed by \"fsync()\".\n+  //\n+  // Default: false\n+  bool sync;\n+\n+  WriteOptions()\n+      : sync(false) {\n+  }\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_OPTIONS_H_"
      },
      {
        "sha": "74ea8fa49af6782b54ba07528844e665e8ea8095",
        "filename": "include/leveldb/slice.h",
        "status": "added",
        "additions": 109,
        "deletions": 0,
        "changes": 109,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/slice.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/slice.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/include/leveldb/slice.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,109 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// Slice is a simple structure containing a pointer into some external\n+// storage and a size.  The user of a Slice must ensure that the slice\n+// is not used after the corresponding external storage has been\n+// deallocated.\n+//\n+// Multiple threads can invoke const methods on a Slice without\n+// external synchronization, but if any of the threads may call a\n+// non-const method, all threads accessing the same Slice must use\n+// external synchronization.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_SLICE_H_\n+#define STORAGE_LEVELDB_INCLUDE_SLICE_H_\n+\n+#include <assert.h>\n+#include <stddef.h>\n+#include <string.h>\n+#include <string>\n+\n+namespace leveldb {\n+\n+class Slice {\n+ public:\n+  // Create an empty slice.\n+  Slice() : data_(\"\"), size_(0) { }\n+\n+  // Create a slice that refers to d[0,n-1].\n+  Slice(const char* d, size_t n) : data_(d), size_(n) { }\n+\n+  // Create a slice that refers to the contents of \"s\"\n+  Slice(const std::string& s) : data_(s.data()), size_(s.size()) { }\n+\n+  // Create a slice that refers to s[0,strlen(s)-1]\n+  Slice(const char* s) : data_(s), size_(strlen(s)) { }\n+\n+  // Return a pointer to the beginning of the referenced data\n+  const char* data() const { return data_; }\n+\n+  // Return the length (in bytes) of the referenced data\n+  size_t size() const { return size_; }\n+\n+  // Return true iff the length of the referenced data is zero\n+  bool empty() const { return size_ == 0; }\n+\n+  // Return the ith byte in the referenced data.\n+  // REQUIRES: n < size()\n+  char operator[](size_t n) const {\n+    assert(n < size());\n+    return data_[n];\n+  }\n+\n+  // Change this slice to refer to an empty array\n+  void clear() { data_ = \"\"; size_ = 0; }\n+\n+  // Drop the first \"n\" bytes from this slice.\n+  void remove_prefix(size_t n) {\n+    assert(n <= size());\n+    data_ += n;\n+    size_ -= n;\n+  }\n+\n+  // Return a string that contains the copy of the referenced data.\n+  std::string ToString() const { return std::string(data_, size_); }\n+\n+  // Three-way comparison.  Returns value:\n+  //   <  0 iff \"*this\" <  \"b\",\n+  //   == 0 iff \"*this\" == \"b\",\n+  //   >  0 iff \"*this\" >  \"b\"\n+  int compare(const Slice& b) const;\n+\n+  // Return true iff \"x\" is a prefix of \"*this\"\n+  bool starts_with(const Slice& x) const {\n+    return ((size_ >= x.size_) &&\n+            (memcmp(data_, x.data_, x.size_) == 0));\n+  }\n+\n+ private:\n+  const char* data_;\n+  size_t size_;\n+\n+  // Intentionally copyable\n+};\n+\n+inline bool operator==(const Slice& x, const Slice& y) {\n+  return ((x.size() == y.size()) &&\n+          (memcmp(x.data(), y.data(), x.size()) == 0));\n+}\n+\n+inline bool operator!=(const Slice& x, const Slice& y) {\n+  return !(x == y);\n+}\n+\n+inline int Slice::compare(const Slice& b) const {\n+  const int min_len = (size_ < b.size_) ? size_ : b.size_;\n+  int r = memcmp(data_, b.data_, min_len);\n+  if (r == 0) {\n+    if (size_ < b.size_) r = -1;\n+    else if (size_ > b.size_) r = +1;\n+  }\n+  return r;\n+}\n+\n+}  // namespace leveldb\n+\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_SLICE_H_"
      },
      {
        "sha": "11dbd4b47ed3883b7dd5092c21685441f6000c26",
        "filename": "include/leveldb/status.h",
        "status": "added",
        "additions": 106,
        "deletions": 0,
        "changes": 106,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/status.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/status.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/include/leveldb/status.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,106 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// A Status encapsulates the result of an operation.  It may indicate success,\n+// or it may indicate an error with an associated error message.\n+//\n+// Multiple threads can invoke const methods on a Status without\n+// external synchronization, but if any of the threads may call a\n+// non-const method, all threads accessing the same Status must use\n+// external synchronization.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_STATUS_H_\n+#define STORAGE_LEVELDB_INCLUDE_STATUS_H_\n+\n+#include <string>\n+#include \"leveldb/slice.h\"\n+\n+namespace leveldb {\n+\n+class Status {\n+ public:\n+  // Create a success status.\n+  Status() : state_(NULL) { }\n+  ~Status() { delete[] state_; }\n+\n+  // Copy the specified status.\n+  Status(const Status& s);\n+  void operator=(const Status& s);\n+\n+  // Return a success status.\n+  static Status OK() { return Status(); }\n+\n+  // Return error status of an appropriate type.\n+  static Status NotFound(const Slice& msg, const Slice& msg2 = Slice()) {\n+    return Status(kNotFound, msg, msg2);\n+  }\n+  static Status Corruption(const Slice& msg, const Slice& msg2 = Slice()) {\n+    return Status(kCorruption, msg, msg2);\n+  }\n+  static Status NotSupported(const Slice& msg, const Slice& msg2 = Slice()) {\n+    return Status(kNotSupported, msg, msg2);\n+  }\n+  static Status InvalidArgument(const Slice& msg, const Slice& msg2 = Slice()) {\n+    return Status(kInvalidArgument, msg, msg2);\n+  }\n+  static Status IOError(const Slice& msg, const Slice& msg2 = Slice()) {\n+    return Status(kIOError, msg, msg2);\n+  }\n+\n+  // Returns true iff the status indicates success.\n+  bool ok() const { return (state_ == NULL); }\n+\n+  // Returns true iff the status indicates a NotFound error.\n+  bool IsNotFound() const { return code() == kNotFound; }\n+\n+  // Returns true iff the status indicates a Corruption error.\n+  bool IsCorruption() const { return code() == kCorruption; }\n+\n+  // Returns true iff the status indicates an IOError.\n+  bool IsIOError() const { return code() == kIOError; }\n+\n+  // Return a string representation of this status suitable for printing.\n+  // Returns the string \"OK\" for success.\n+  std::string ToString() const;\n+\n+ private:\n+  // OK status has a NULL state_.  Otherwise, state_ is a new[] array\n+  // of the following form:\n+  //    state_[0..3] == length of message\n+  //    state_[4]    == code\n+  //    state_[5..]  == message\n+  const char* state_;\n+\n+  enum Code {\n+    kOk = 0,\n+    kNotFound = 1,\n+    kCorruption = 2,\n+    kNotSupported = 3,\n+    kInvalidArgument = 4,\n+    kIOError = 5\n+  };\n+\n+  Code code() const {\n+    return (state_ == NULL) ? kOk : static_cast<Code>(state_[4]);\n+  }\n+\n+  Status(Code code, const Slice& msg, const Slice& msg2);\n+  static const char* CopyState(const char* s);\n+};\n+\n+inline Status::Status(const Status& s) {\n+  state_ = (s.state_ == NULL) ? NULL : CopyState(s.state_);\n+}\n+inline void Status::operator=(const Status& s) {\n+  // The following condition catches both aliasing (when this == &s),\n+  // and the common case where both s and *this are ok.\n+  if (state_ != s.state_) {\n+    delete[] state_;\n+    state_ = (s.state_ == NULL) ? NULL : CopyState(s.state_);\n+  }\n+}\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_STATUS_H_"
      },
      {
        "sha": "a9746c3f5ea90250d8bde12d9ec7e9091fd5bd51",
        "filename": "include/leveldb/table.h",
        "status": "added",
        "additions": 85,
        "deletions": 0,
        "changes": 85,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/table.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/table.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/include/leveldb/table.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,85 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_TABLE_H_\n+#define STORAGE_LEVELDB_INCLUDE_TABLE_H_\n+\n+#include <stdint.h>\n+#include \"leveldb/iterator.h\"\n+\n+namespace leveldb {\n+\n+class Block;\n+class BlockHandle;\n+class Footer;\n+struct Options;\n+class RandomAccessFile;\n+struct ReadOptions;\n+class TableCache;\n+\n+// A Table is a sorted map from strings to strings.  Tables are\n+// immutable and persistent.  A Table may be safely accessed from\n+// multiple threads without external synchronization.\n+class Table {\n+ public:\n+  // Attempt to open the table that is stored in bytes [0..file_size)\n+  // of \"file\", and read the metadata entries necessary to allow\n+  // retrieving data from the table.\n+  //\n+  // If successful, returns ok and sets \"*table\" to the newly opened\n+  // table.  The client should delete \"*table\" when no longer needed.\n+  // If there was an error while initializing the table, sets \"*table\"\n+  // to NULL and returns a non-ok status.  Does not take ownership of\n+  // \"*source\", but the client must ensure that \"source\" remains live\n+  // for the duration of the returned table's lifetime.\n+  //\n+  // *file must remain live while this Table is in use.\n+  static Status Open(const Options& options,\n+                     RandomAccessFile* file,\n+                     uint64_t file_size,\n+                     Table** table);\n+\n+  ~Table();\n+\n+  // Returns a new iterator over the table contents.\n+  // The result of NewIterator() is initially invalid (caller must\n+  // call one of the Seek methods on the iterator before using it).\n+  Iterator* NewIterator(const ReadOptions&) const;\n+\n+  // Given a key, return an approximate byte offset in the file where\n+  // the data for that key begins (or would begin if the key were\n+  // present in the file).  The returned value is in terms of file\n+  // bytes, and so includes effects like compression of the underlying data.\n+  // E.g., the approximate offset of the last key in the table will\n+  // be close to the file length.\n+  uint64_t ApproximateOffsetOf(const Slice& key) const;\n+\n+ private:\n+  struct Rep;\n+  Rep* rep_;\n+\n+  explicit Table(Rep* rep) { rep_ = rep; }\n+  static Iterator* BlockReader(void*, const ReadOptions&, const Slice&);\n+\n+  // Calls (*handle_result)(arg, ...) with the entry found after a call\n+  // to Seek(key).  May not make such a call if filter policy says\n+  // that key is not present.\n+  friend class TableCache;\n+  Status InternalGet(\n+      const ReadOptions&, const Slice& key,\n+      void* arg,\n+      void (*handle_result)(void* arg, const Slice& k, const Slice& v));\n+\n+\n+  void ReadMeta(const Footer& footer);\n+  void ReadFilter(const Slice& filter_handle_value);\n+\n+  // No copying allowed\n+  Table(const Table&);\n+  void operator=(const Table&);\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_TABLE_H_"
      },
      {
        "sha": "5fd1dc71f1cb7541ef62397b6795946ad8c20652",
        "filename": "include/leveldb/table_builder.h",
        "status": "added",
        "additions": 92,
        "deletions": 0,
        "changes": 92,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/table_builder.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/table_builder.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/include/leveldb/table_builder.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,92 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// TableBuilder provides the interface used to build a Table\n+// (an immutable and sorted map from keys to values).\n+//\n+// Multiple threads can invoke const methods on a TableBuilder without\n+// external synchronization, but if any of the threads may call a\n+// non-const method, all threads accessing the same TableBuilder must use\n+// external synchronization.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_TABLE_BUILDER_H_\n+#define STORAGE_LEVELDB_INCLUDE_TABLE_BUILDER_H_\n+\n+#include <stdint.h>\n+#include \"leveldb/options.h\"\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+class BlockBuilder;\n+class BlockHandle;\n+class WritableFile;\n+\n+class TableBuilder {\n+ public:\n+  // Create a builder that will store the contents of the table it is\n+  // building in *file.  Does not close the file.  It is up to the\n+  // caller to close the file after calling Finish().\n+  TableBuilder(const Options& options, WritableFile* file);\n+\n+  // REQUIRES: Either Finish() or Abandon() has been called.\n+  ~TableBuilder();\n+\n+  // Change the options used by this builder.  Note: only some of the\n+  // option fields can be changed after construction.  If a field is\n+  // not allowed to change dynamically and its value in the structure\n+  // passed to the constructor is different from its value in the\n+  // structure passed to this method, this method will return an error\n+  // without changing any fields.\n+  Status ChangeOptions(const Options& options);\n+\n+  // Add key,value to the table being constructed.\n+  // REQUIRES: key is after any previously added key according to comparator.\n+  // REQUIRES: Finish(), Abandon() have not been called\n+  void Add(const Slice& key, const Slice& value);\n+\n+  // Advanced operation: flush any buffered key/value pairs to file.\n+  // Can be used to ensure that two adjacent entries never live in\n+  // the same data block.  Most clients should not need to use this method.\n+  // REQUIRES: Finish(), Abandon() have not been called\n+  void Flush();\n+\n+  // Return non-ok iff some error has been detected.\n+  Status status() const;\n+\n+  // Finish building the table.  Stops using the file passed to the\n+  // constructor after this function returns.\n+  // REQUIRES: Finish(), Abandon() have not been called\n+  Status Finish();\n+\n+  // Indicate that the contents of this builder should be abandoned.  Stops\n+  // using the file passed to the constructor after this function returns.\n+  // If the caller is not going to call Finish(), it must call Abandon()\n+  // before destroying this builder.\n+  // REQUIRES: Finish(), Abandon() have not been called\n+  void Abandon();\n+\n+  // Number of calls to Add() so far.\n+  uint64_t NumEntries() const;\n+\n+  // Size of the file generated so far.  If invoked after a successful\n+  // Finish() call, returns the size of the final generated file.\n+  uint64_t FileSize() const;\n+\n+ private:\n+  bool ok() const { return status().ok(); }\n+  void WriteBlock(BlockBuilder* block, BlockHandle* handle);\n+  void WriteRawBlock(const Slice& data, CompressionType, BlockHandle* handle);\n+\n+  struct Rep;\n+  Rep* rep_;\n+\n+  // No copying allowed\n+  TableBuilder(const TableBuilder&);\n+  void operator=(const TableBuilder&);\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_TABLE_BUILDER_H_"
      },
      {
        "sha": "ee9aab68e0d83dc4d94835ee21cf926c1ff0c0db",
        "filename": "include/leveldb/write_batch.h",
        "status": "added",
        "additions": 64,
        "deletions": 0,
        "changes": 64,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/write_batch.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/include/leveldb/write_batch.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/include/leveldb/write_batch.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,64 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// WriteBatch holds a collection of updates to apply atomically to a DB.\n+//\n+// The updates are applied in the order in which they are added\n+// to the WriteBatch.  For example, the value of \"key\" will be \"v3\"\n+// after the following batch is written:\n+//\n+//    batch.Put(\"key\", \"v1\");\n+//    batch.Delete(\"key\");\n+//    batch.Put(\"key\", \"v2\");\n+//    batch.Put(\"key\", \"v3\");\n+//\n+// Multiple threads can invoke const methods on a WriteBatch without\n+// external synchronization, but if any of the threads may call a\n+// non-const method, all threads accessing the same WriteBatch must use\n+// external synchronization.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_WRITE_BATCH_H_\n+#define STORAGE_LEVELDB_INCLUDE_WRITE_BATCH_H_\n+\n+#include <string>\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+class Slice;\n+\n+class WriteBatch {\n+ public:\n+  WriteBatch();\n+  ~WriteBatch();\n+\n+  // Store the mapping \"key->value\" in the database.\n+  void Put(const Slice& key, const Slice& value);\n+\n+  // If the database contains a mapping for \"key\", erase it.  Else do nothing.\n+  void Delete(const Slice& key);\n+\n+  // Clear all updates buffered in this batch.\n+  void Clear();\n+\n+  // Support for iterating over the contents of a batch.\n+  class Handler {\n+   public:\n+    virtual ~Handler();\n+    virtual void Put(const Slice& key, const Slice& value) = 0;\n+    virtual void Delete(const Slice& key) = 0;\n+  };\n+  Status Iterate(Handler* handler) const;\n+\n+ private:\n+  friend class WriteBatchInternal;\n+\n+  std::string rep_;  // See comment in write_batch.cc for the format of rep_\n+\n+  // Intentionally copyable\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_WRITE_BATCH_H_"
      },
      {
        "sha": "1b1cf8bb28da5cf6a117ffdc4cb6527ef1585d6b",
        "filename": "issues/issue178_test.cc",
        "status": "added",
        "additions": 92,
        "deletions": 0,
        "changes": 92,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/issues/issue178_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/issues/issue178_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/issues/issue178_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,92 @@\n+// Copyright (c) 2013 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+// Test for issue 178: a manual compaction causes deleted data to reappear.\n+#include <iostream>\n+#include <sstream>\n+#include <cstdlib>\n+\n+#include \"leveldb/db.h\"\n+#include \"leveldb/write_batch.h\"\n+#include \"util/testharness.h\"\n+\n+namespace {\n+\n+const int kNumKeys = 1100000;\n+\n+std::string Key1(int i) {\n+  char buf[100];\n+  snprintf(buf, sizeof(buf), \"my_key_%d\", i);\n+  return buf;\n+}\n+\n+std::string Key2(int i) {\n+  return Key1(i) + \"_xxx\";\n+}\n+\n+class Issue178 { };\n+\n+TEST(Issue178, Test) {\n+  // Get rid of any state from an old run.\n+  std::string dbpath = leveldb::test::TmpDir() + \"/leveldb_cbug_test\";\n+  DestroyDB(dbpath, leveldb::Options());\n+\n+  // Open database.  Disable compression since it affects the creation\n+  // of layers and the code below is trying to test against a very\n+  // specific scenario.\n+  leveldb::DB* db;\n+  leveldb::Options db_options;\n+  db_options.create_if_missing = true;\n+  db_options.compression = leveldb::kNoCompression;\n+  ASSERT_OK(leveldb::DB::Open(db_options, dbpath, &db));\n+\n+  // create first key range\n+  leveldb::WriteBatch batch;\n+  for (size_t i = 0; i < kNumKeys; i++) {\n+    batch.Put(Key1(i), \"value for range 1 key\");\n+  }\n+  ASSERT_OK(db->Write(leveldb::WriteOptions(), &batch));\n+\n+  // create second key range\n+  batch.Clear();\n+  for (size_t i = 0; i < kNumKeys; i++) {\n+    batch.Put(Key2(i), \"value for range 2 key\");\n+  }\n+  ASSERT_OK(db->Write(leveldb::WriteOptions(), &batch));\n+\n+  // delete second key range\n+  batch.Clear();\n+  for (size_t i = 0; i < kNumKeys; i++) {\n+    batch.Delete(Key2(i));\n+  }\n+  ASSERT_OK(db->Write(leveldb::WriteOptions(), &batch));\n+\n+  // compact database\n+  std::string start_key = Key1(0);\n+  std::string end_key = Key1(kNumKeys - 1);\n+  leveldb::Slice least(start_key.data(), start_key.size());\n+  leveldb::Slice greatest(end_key.data(), end_key.size());\n+\n+  // commenting out the line below causes the example to work correctly\n+  db->CompactRange(&least, &greatest);\n+\n+  // count the keys\n+  leveldb::Iterator* iter = db->NewIterator(leveldb::ReadOptions());\n+  size_t num_keys = 0;\n+  for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+    num_keys++;\n+  }\n+  delete iter;\n+  ASSERT_EQ(kNumKeys, num_keys) << \"Bad number of keys\";\n+\n+  // close database\n+  delete db;\n+  DestroyDB(dbpath, leveldb::Options());\n+}\n+\n+}  // anonymous namespace\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "422563e25ce0230d92373637799da8d28ab60223",
        "filename": "port/README",
        "status": "added",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/README",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/README",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/port/README?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,10 @@\n+This directory contains interfaces and implementations that isolate the\n+rest of the package from platform details.\n+\n+Code in the rest of the package includes \"port.h\" from this directory.\n+\"port.h\" in turn includes a platform specific \"port_<platform>.h\" file\n+that provides the platform specific implementation.\n+\n+See port_posix.h for an example of what must be provided in a platform\n+specific header file.\n+"
      },
      {
        "sha": "e17bf435eab33e55a87046e705a1ecb04dad7e24",
        "filename": "port/atomic_pointer.h",
        "status": "added",
        "additions": 224,
        "deletions": 0,
        "changes": 224,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/atomic_pointer.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/atomic_pointer.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/port/atomic_pointer.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,224 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+// AtomicPointer provides storage for a lock-free pointer.\n+// Platform-dependent implementation of AtomicPointer:\n+// - If the platform provides a cheap barrier, we use it with raw pointers\n+// - If cstdatomic is present (on newer versions of gcc, it is), we use\n+//   a cstdatomic-based AtomicPointer.  However we prefer the memory\n+//   barrier based version, because at least on a gcc 4.4 32-bit build\n+//   on linux, we have encountered a buggy <cstdatomic>\n+//   implementation.  Also, some <cstdatomic> implementations are much\n+//   slower than a memory-barrier based implementation (~16ns for\n+//   <cstdatomic> based acquire-load vs. ~1ns for a barrier based\n+//   acquire-load).\n+// This code is based on atomicops-internals-* in Google's perftools:\n+// http://code.google.com/p/google-perftools/source/browse/#svn%2Ftrunk%2Fsrc%2Fbase\n+\n+#ifndef PORT_ATOMIC_POINTER_H_\n+#define PORT_ATOMIC_POINTER_H_\n+\n+#include <stdint.h>\n+#ifdef LEVELDB_CSTDATOMIC_PRESENT\n+#include <cstdatomic>\n+#endif\n+#ifdef OS_WIN\n+#include <windows.h>\n+#endif\n+#ifdef OS_MACOSX\n+#include <libkern/OSAtomic.h>\n+#endif\n+\n+#if defined(_M_X64) || defined(__x86_64__)\n+#define ARCH_CPU_X86_FAMILY 1\n+#elif defined(_M_IX86) || defined(__i386__) || defined(__i386)\n+#define ARCH_CPU_X86_FAMILY 1\n+#elif defined(__ARMEL__)\n+#define ARCH_CPU_ARM_FAMILY 1\n+#elif defined(__ppc__) || defined(__powerpc__) || defined(__powerpc64__)\n+#define ARCH_CPU_PPC_FAMILY 1\n+#endif\n+\n+namespace leveldb {\n+namespace port {\n+\n+// Define MemoryBarrier() if available\n+// Windows on x86\n+#if defined(OS_WIN) && defined(COMPILER_MSVC) && defined(ARCH_CPU_X86_FAMILY)\n+// windows.h already provides a MemoryBarrier(void) macro\n+// http://msdn.microsoft.com/en-us/library/ms684208(v=vs.85).aspx\n+#define LEVELDB_HAVE_MEMORY_BARRIER\n+\n+// Gcc on x86\n+#elif defined(ARCH_CPU_X86_FAMILY) && defined(__GNUC__)\n+inline void MemoryBarrier() {\n+  // See http://gcc.gnu.org/ml/gcc/2003-04/msg01180.html for a discussion on\n+  // this idiom. Also see http://en.wikipedia.org/wiki/Memory_ordering.\n+  __asm__ __volatile__(\"\" : : : \"memory\");\n+}\n+#define LEVELDB_HAVE_MEMORY_BARRIER\n+\n+// Sun Studio\n+#elif defined(ARCH_CPU_X86_FAMILY) && defined(__SUNPRO_CC)\n+inline void MemoryBarrier() {\n+  // See http://gcc.gnu.org/ml/gcc/2003-04/msg01180.html for a discussion on\n+  // this idiom. Also see http://en.wikipedia.org/wiki/Memory_ordering.\n+  asm volatile(\"\" : : : \"memory\");\n+}\n+#define LEVELDB_HAVE_MEMORY_BARRIER\n+\n+// Mac OS\n+#elif defined(OS_MACOSX)\n+inline void MemoryBarrier() {\n+  OSMemoryBarrier();\n+}\n+#define LEVELDB_HAVE_MEMORY_BARRIER\n+\n+// ARM Linux\n+#elif defined(ARCH_CPU_ARM_FAMILY) && defined(__linux__)\n+typedef void (*LinuxKernelMemoryBarrierFunc)(void);\n+// The Linux ARM kernel provides a highly optimized device-specific memory\n+// barrier function at a fixed memory address that is mapped in every\n+// user-level process.\n+//\n+// This beats using CPU-specific instructions which are, on single-core\n+// devices, un-necessary and very costly (e.g. ARMv7-A \"dmb\" takes more\n+// than 180ns on a Cortex-A8 like the one on a Nexus One). Benchmarking\n+// shows that the extra function call cost is completely negligible on\n+// multi-core devices.\n+//\n+inline void MemoryBarrier() {\n+  (*(LinuxKernelMemoryBarrierFunc)0xffff0fa0)();\n+}\n+#define LEVELDB_HAVE_MEMORY_BARRIER\n+\n+// PPC\n+#elif defined(ARCH_CPU_PPC_FAMILY) && defined(__GNUC__)\n+inline void MemoryBarrier() {\n+  // TODO for some powerpc expert: is there a cheaper suitable variant?\n+  // Perhaps by having separate barriers for acquire and release ops.\n+  asm volatile(\"sync\" : : : \"memory\");\n+}\n+#define LEVELDB_HAVE_MEMORY_BARRIER\n+\n+#endif\n+\n+// AtomicPointer built using platform-specific MemoryBarrier()\n+#if defined(LEVELDB_HAVE_MEMORY_BARRIER)\n+class AtomicPointer {\n+ private:\n+  void* rep_;\n+ public:\n+  AtomicPointer() { }\n+  explicit AtomicPointer(void* p) : rep_(p) {}\n+  inline void* NoBarrier_Load() const { return rep_; }\n+  inline void NoBarrier_Store(void* v) { rep_ = v; }\n+  inline void* Acquire_Load() const {\n+    void* result = rep_;\n+    MemoryBarrier();\n+    return result;\n+  }\n+  inline void Release_Store(void* v) {\n+    MemoryBarrier();\n+    rep_ = v;\n+  }\n+};\n+\n+// AtomicPointer based on <cstdatomic>\n+#elif defined(LEVELDB_CSTDATOMIC_PRESENT)\n+class AtomicPointer {\n+ private:\n+  std::atomic<void*> rep_;\n+ public:\n+  AtomicPointer() { }\n+  explicit AtomicPointer(void* v) : rep_(v) { }\n+  inline void* Acquire_Load() const {\n+    return rep_.load(std::memory_order_acquire);\n+  }\n+  inline void Release_Store(void* v) {\n+    rep_.store(v, std::memory_order_release);\n+  }\n+  inline void* NoBarrier_Load() const {\n+    return rep_.load(std::memory_order_relaxed);\n+  }\n+  inline void NoBarrier_Store(void* v) {\n+    rep_.store(v, std::memory_order_relaxed);\n+  }\n+};\n+\n+// Atomic pointer based on sparc memory barriers\n+#elif defined(__sparcv9) && defined(__GNUC__)\n+class AtomicPointer {\n+ private:\n+  void* rep_;\n+ public:\n+  AtomicPointer() { }\n+  explicit AtomicPointer(void* v) : rep_(v) { }\n+  inline void* Acquire_Load() const {\n+    void* val;\n+    __asm__ __volatile__ (\n+        \"ldx [%[rep_]], %[val] \\n\\t\"\n+         \"membar #LoadLoad|#LoadStore \\n\\t\"\n+        : [val] \"=r\" (val)\n+        : [rep_] \"r\" (&rep_)\n+        : \"memory\");\n+    return val;\n+  }\n+  inline void Release_Store(void* v) {\n+    __asm__ __volatile__ (\n+        \"membar #LoadStore|#StoreStore \\n\\t\"\n+        \"stx %[v], [%[rep_]] \\n\\t\"\n+        :\n+        : [rep_] \"r\" (&rep_), [v] \"r\" (v)\n+        : \"memory\");\n+  }\n+  inline void* NoBarrier_Load() const { return rep_; }\n+  inline void NoBarrier_Store(void* v) { rep_ = v; }\n+};\n+\n+// Atomic pointer based on ia64 acq/rel\n+#elif defined(__ia64) && defined(__GNUC__)\n+class AtomicPointer {\n+ private:\n+  void* rep_;\n+ public:\n+  AtomicPointer() { }\n+  explicit AtomicPointer(void* v) : rep_(v) { }\n+  inline void* Acquire_Load() const {\n+    void* val    ;\n+    __asm__ __volatile__ (\n+        \"ld8.acq %[val] = [%[rep_]] \\n\\t\"\n+        : [val] \"=r\" (val)\n+        : [rep_] \"r\" (&rep_)\n+        : \"memory\"\n+        );\n+    return val;\n+  }\n+  inline void Release_Store(void* v) {\n+    __asm__ __volatile__ (\n+        \"st8.rel [%[rep_]] = %[v]  \\n\\t\"\n+        :\n+        : [rep_] \"r\" (&rep_), [v] \"r\" (v)\n+        : \"memory\"\n+        );\n+  }\n+  inline void* NoBarrier_Load() const { return rep_; }\n+  inline void NoBarrier_Store(void* v) { rep_ = v; }\n+};\n+\n+// We have neither MemoryBarrier(), nor <cstdatomic>\n+#else\n+#error Please implement AtomicPointer for this platform.\n+\n+#endif\n+\n+#undef LEVELDB_HAVE_MEMORY_BARRIER\n+#undef ARCH_CPU_X86_FAMILY\n+#undef ARCH_CPU_ARM_FAMILY\n+#undef ARCH_CPU_PPC_FAMILY\n+\n+}  // namespace port\n+}  // namespace leveldb\n+\n+#endif  // PORT_ATOMIC_POINTER_H_"
      },
      {
        "sha": "4baafa8e22fd290cfd73ad4daf0b5245e0d109c1",
        "filename": "port/port.h",
        "status": "added",
        "additions": 21,
        "deletions": 0,
        "changes": 21,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/port.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/port.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/port/port.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,21 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_PORT_PORT_H_\n+#define STORAGE_LEVELDB_PORT_PORT_H_\n+\n+#include <string.h>\n+\n+// Include the appropriate platform specific file below.  If you are\n+// porting to a new platform, see \"port_example.h\" for documentation\n+// of what the new port_<platform>.h file must provide.\n+#if defined(LEVELDB_PLATFORM_POSIX)\n+#  include \"port/port_posix.h\"\n+#elif defined(LEVELDB_PLATFORM_CHROMIUM)\n+#  include \"port/port_chromium.h\"\n+#elif defined(LEVELDB_PLATFORM_WINDOWS)\n+#  include \"port/port_win.h\"\n+#endif\n+\n+#endif  // STORAGE_LEVELDB_PORT_PORT_H_"
      },
      {
        "sha": "ab9e489b32d8eb4ec8a43da07a20ad917fb35a1b",
        "filename": "port/port_example.h",
        "status": "added",
        "additions": 135,
        "deletions": 0,
        "changes": 135,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/port_example.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/port_example.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/port/port_example.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,135 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// This file contains the specification, but not the implementations,\n+// of the types/operations/etc. that should be defined by a platform\n+// specific port_<platform>.h file.  Use this file as a reference for\n+// how to port this package to a new platform.\n+\n+#ifndef STORAGE_LEVELDB_PORT_PORT_EXAMPLE_H_\n+#define STORAGE_LEVELDB_PORT_PORT_EXAMPLE_H_\n+\n+namespace leveldb {\n+namespace port {\n+\n+// TODO(jorlow): Many of these belong more in the environment class rather than\n+//               here. We should try moving them and see if it affects perf.\n+\n+// The following boolean constant must be true on a little-endian machine\n+// and false otherwise.\n+static const bool kLittleEndian = true /* or some other expression */;\n+\n+// ------------------ Threading -------------------\n+\n+// A Mutex represents an exclusive lock.\n+class Mutex {\n+ public:\n+  Mutex();\n+  ~Mutex();\n+\n+  // Lock the mutex.  Waits until other lockers have exited.\n+  // Will deadlock if the mutex is already locked by this thread.\n+  void Lock();\n+\n+  // Unlock the mutex.\n+  // REQUIRES: This mutex was locked by this thread.\n+  void Unlock();\n+\n+  // Optionally crash if this thread does not hold this mutex.\n+  // The implementation must be fast, especially if NDEBUG is\n+  // defined.  The implementation is allowed to skip all checks.\n+  void AssertHeld();\n+};\n+\n+class CondVar {\n+ public:\n+  explicit CondVar(Mutex* mu);\n+  ~CondVar();\n+\n+  // Atomically release *mu and block on this condition variable until\n+  // either a call to SignalAll(), or a call to Signal() that picks\n+  // this thread to wakeup.\n+  // REQUIRES: this thread holds *mu\n+  void Wait();\n+\n+  // If there are some threads waiting, wake up at least one of them.\n+  void Signal();\n+\n+  // Wake up all waiting threads.\n+  void SignallAll();\n+};\n+\n+// Thread-safe initialization.\n+// Used as follows:\n+//      static port::OnceType init_control = LEVELDB_ONCE_INIT;\n+//      static void Initializer() { ... do something ...; }\n+//      ...\n+//      port::InitOnce(&init_control, &Initializer);\n+typedef intptr_t OnceType;\n+#define LEVELDB_ONCE_INIT 0\n+extern void InitOnce(port::OnceType*, void (*initializer)());\n+\n+// A type that holds a pointer that can be read or written atomically\n+// (i.e., without word-tearing.)\n+class AtomicPointer {\n+ private:\n+  intptr_t rep_;\n+ public:\n+  // Initialize to arbitrary value\n+  AtomicPointer();\n+\n+  // Initialize to hold v\n+  explicit AtomicPointer(void* v) : rep_(v) { }\n+\n+  // Read and return the stored pointer with the guarantee that no\n+  // later memory access (read or write) by this thread can be\n+  // reordered ahead of this read.\n+  void* Acquire_Load() const;\n+\n+  // Set v as the stored pointer with the guarantee that no earlier\n+  // memory access (read or write) by this thread can be reordered\n+  // after this store.\n+  void Release_Store(void* v);\n+\n+  // Read the stored pointer with no ordering guarantees.\n+  void* NoBarrier_Load() const;\n+\n+  // Set va as the stored pointer with no ordering guarantees.\n+  void NoBarrier_Store(void* v);\n+};\n+\n+// ------------------ Compression -------------------\n+\n+// Store the snappy compression of \"input[0,input_length-1]\" in *output.\n+// Returns false if snappy is not supported by this port.\n+extern bool Snappy_Compress(const char* input, size_t input_length,\n+                            std::string* output);\n+\n+// If input[0,input_length-1] looks like a valid snappy compressed\n+// buffer, store the size of the uncompressed data in *result and\n+// return true.  Else return false.\n+extern bool Snappy_GetUncompressedLength(const char* input, size_t length,\n+                                         size_t* result);\n+\n+// Attempt to snappy uncompress input[0,input_length-1] into *output.\n+// Returns true if successful, false if the input is invalid lightweight\n+// compressed data.\n+//\n+// REQUIRES: at least the first \"n\" bytes of output[] must be writable\n+// where \"n\" is the result of a successful call to\n+// Snappy_GetUncompressedLength.\n+extern bool Snappy_Uncompress(const char* input_data, size_t input_length,\n+                              char* output);\n+\n+// ------------------ Miscellaneous -------------------\n+\n+// If heap profiling is not supported, returns false.\n+// Else repeatedly calls (*func)(arg, data, n) and then returns true.\n+// The concatenation of all \"data[0,n-1]\" fragments is the heap profile.\n+extern bool GetHeapProfile(void (*func)(void*, const char*, int), void* arg);\n+\n+}  // namespace port\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_PORT_PORT_EXAMPLE_H_"
      },
      {
        "sha": "5ba127a5b91bfa036189aa29ee2aeb9b02a034d8",
        "filename": "port/port_posix.cc",
        "status": "added",
        "additions": 54,
        "deletions": 0,
        "changes": 54,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/port_posix.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/port_posix.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/port/port_posix.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,54 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"port/port_posix.h\"\n+\n+#include <cstdlib>\n+#include <stdio.h>\n+#include <string.h>\n+#include \"util/logging.h\"\n+\n+namespace leveldb {\n+namespace port {\n+\n+static void PthreadCall(const char* label, int result) {\n+  if (result != 0) {\n+    fprintf(stderr, \"pthread %s: %s\\n\", label, strerror(result));\n+    abort();\n+  }\n+}\n+\n+Mutex::Mutex() { PthreadCall(\"init mutex\", pthread_mutex_init(&mu_, NULL)); }\n+\n+Mutex::~Mutex() { PthreadCall(\"destroy mutex\", pthread_mutex_destroy(&mu_)); }\n+\n+void Mutex::Lock() { PthreadCall(\"lock\", pthread_mutex_lock(&mu_)); }\n+\n+void Mutex::Unlock() { PthreadCall(\"unlock\", pthread_mutex_unlock(&mu_)); }\n+\n+CondVar::CondVar(Mutex* mu)\n+    : mu_(mu) {\n+    PthreadCall(\"init cv\", pthread_cond_init(&cv_, NULL));\n+}\n+\n+CondVar::~CondVar() { PthreadCall(\"destroy cv\", pthread_cond_destroy(&cv_)); }\n+\n+void CondVar::Wait() {\n+  PthreadCall(\"wait\", pthread_cond_wait(&cv_, &mu_->mu_));\n+}\n+\n+void CondVar::Signal() {\n+  PthreadCall(\"signal\", pthread_cond_signal(&cv_));\n+}\n+\n+void CondVar::SignalAll() {\n+  PthreadCall(\"broadcast\", pthread_cond_broadcast(&cv_));\n+}\n+\n+void InitOnce(OnceType* once, void (*initializer)()) {\n+  PthreadCall(\"once\", pthread_once(once, initializer));\n+}\n+\n+}  // namespace port\n+}  // namespace leveldb"
      },
      {
        "sha": "f2b89bffb99c232c03922806868b0ccf663bc111",
        "filename": "port/port_posix.h",
        "status": "added",
        "additions": 157,
        "deletions": 0,
        "changes": 157,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/port_posix.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/port_posix.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/port/port_posix.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,157 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// See port_example.h for documentation for the following types/functions.\n+\n+#ifndef STORAGE_LEVELDB_PORT_PORT_POSIX_H_\n+#define STORAGE_LEVELDB_PORT_PORT_POSIX_H_\n+\n+#undef PLATFORM_IS_LITTLE_ENDIAN\n+#if defined(OS_MACOSX)\n+  #include <machine/endian.h>\n+  #if defined(__DARWIN_LITTLE_ENDIAN) && defined(__DARWIN_BYTE_ORDER)\n+    #define PLATFORM_IS_LITTLE_ENDIAN \\\n+        (__DARWIN_BYTE_ORDER == __DARWIN_LITTLE_ENDIAN)\n+  #endif\n+#elif defined(OS_SOLARIS)\n+  #include <sys/isa_defs.h>\n+  #ifdef _LITTLE_ENDIAN\n+    #define PLATFORM_IS_LITTLE_ENDIAN true\n+  #else\n+    #define PLATFORM_IS_LITTLE_ENDIAN false\n+  #endif\n+#elif defined(OS_FREEBSD)\n+  #include <sys/types.h>\n+  #include <sys/endian.h>\n+  #define PLATFORM_IS_LITTLE_ENDIAN (_BYTE_ORDER == _LITTLE_ENDIAN)\n+#elif defined(OS_OPENBSD) || defined(OS_NETBSD) ||\\\n+      defined(OS_DRAGONFLYBSD)\n+  #include <sys/types.h>\n+  #include <sys/endian.h>\n+#elif defined(OS_HPUX)\n+  #define PLATFORM_IS_LITTLE_ENDIAN false\n+#elif defined(OS_ANDROID)\n+  // Due to a bug in the NDK x86 <sys/endian.h> definition,\n+  // _BYTE_ORDER must be used instead of __BYTE_ORDER on Android.\n+  // See http://code.google.com/p/android/issues/detail?id=39824\n+  #include <endian.h>\n+  #define PLATFORM_IS_LITTLE_ENDIAN  (_BYTE_ORDER == _LITTLE_ENDIAN)\n+#else\n+  #include <endian.h>\n+#endif\n+\n+#include <pthread.h>\n+#ifdef SNAPPY\n+#include <snappy.h>\n+#endif\n+#include <stdint.h>\n+#include <string>\n+#include \"port/atomic_pointer.h\"\n+\n+#ifndef PLATFORM_IS_LITTLE_ENDIAN\n+#define PLATFORM_IS_LITTLE_ENDIAN (__BYTE_ORDER == __LITTLE_ENDIAN)\n+#endif\n+\n+#if defined(OS_MACOSX) || defined(OS_SOLARIS) || defined(OS_FREEBSD) ||\\\n+    defined(OS_NETBSD) || defined(OS_OPENBSD) || defined(OS_DRAGONFLYBSD) ||\\\n+    defined(OS_ANDROID) || defined(OS_HPUX)\n+// Use fread/fwrite/fflush on platforms without _unlocked variants\n+#define fread_unlocked fread\n+#define fwrite_unlocked fwrite\n+#define fflush_unlocked fflush\n+#endif\n+\n+#if defined(OS_MACOSX) || defined(OS_FREEBSD) ||\\\n+    defined(OS_OPENBSD) || defined(OS_DRAGONFLYBSD)\n+// Use fsync() on platforms without fdatasync()\n+#define fdatasync fsync\n+#endif\n+\n+#if defined(OS_ANDROID) && __ANDROID_API__ < 9\n+// fdatasync() was only introduced in API level 9 on Android. Use fsync()\n+// when targetting older platforms.\n+#define fdatasync fsync\n+#endif\n+\n+namespace leveldb {\n+namespace port {\n+\n+static const bool kLittleEndian = PLATFORM_IS_LITTLE_ENDIAN;\n+#undef PLATFORM_IS_LITTLE_ENDIAN\n+\n+class CondVar;\n+\n+class Mutex {\n+ public:\n+  Mutex();\n+  ~Mutex();\n+\n+  void Lock();\n+  void Unlock();\n+  void AssertHeld() { }\n+\n+ private:\n+  friend class CondVar;\n+  pthread_mutex_t mu_;\n+\n+  // No copying\n+  Mutex(const Mutex&);\n+  void operator=(const Mutex&);\n+};\n+\n+class CondVar {\n+ public:\n+  explicit CondVar(Mutex* mu);\n+  ~CondVar();\n+  void Wait();\n+  void Signal();\n+  void SignalAll();\n+ private:\n+  pthread_cond_t cv_;\n+  Mutex* mu_;\n+};\n+\n+typedef pthread_once_t OnceType;\n+#define LEVELDB_ONCE_INIT PTHREAD_ONCE_INIT\n+extern void InitOnce(OnceType* once, void (*initializer)());\n+\n+inline bool Snappy_Compress(const char* input, size_t length,\n+                            ::std::string* output) {\n+#ifdef SNAPPY\n+  output->resize(snappy::MaxCompressedLength(length));\n+  size_t outlen;\n+  snappy::RawCompress(input, length, &(*output)[0], &outlen);\n+  output->resize(outlen);\n+  return true;\n+#endif\n+\n+  return false;\n+}\n+\n+inline bool Snappy_GetUncompressedLength(const char* input, size_t length,\n+                                         size_t* result) {\n+#ifdef SNAPPY\n+  return snappy::GetUncompressedLength(input, length, result);\n+#else\n+  return false;\n+#endif\n+}\n+\n+inline bool Snappy_Uncompress(const char* input, size_t length,\n+                              char* output) {\n+#ifdef SNAPPY\n+  return snappy::RawUncompress(input, length, output);\n+#else\n+  return false;\n+#endif\n+}\n+\n+inline bool GetHeapProfile(void (*func)(void*, const char*, int), void* arg) {\n+  return false;\n+}\n+\n+} // namespace port\n+} // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_PORT_PORT_POSIX_H_"
      },
      {
        "sha": "1b0f060a19caabfbb5dffe8ef87f1d90536d44f2",
        "filename": "port/port_win.cc",
        "status": "added",
        "additions": 147,
        "deletions": 0,
        "changes": 147,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/port_win.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/port_win.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/port/port_win.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,147 @@\n+// LevelDB Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// See port_example.h for documentation for the following types/functions.\n+\n+// Redistribution and use in source and binary forms, with or without\n+// modification, are permitted provided that the following conditions are met:\n+// \n+//  * Redistributions of source code must retain the above copyright\n+//    notice, this list of conditions and the following disclaimer.\n+//  * Redistributions in binary form must reproduce the above copyright\n+//    notice, this list of conditions and the following disclaimer in the\n+//    documentation and/or other materials provided with the distribution.\n+//  * Neither the name of the University of California, Berkeley nor the\n+//    names of its contributors may be used to endorse or promote products\n+//    derived from this software without specific prior written permission.\n+// \n+// THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND ANY\n+// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n+// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n+// DISCLAIMED. IN NO EVENT SHALL THE REGENTS AND CONTRIBUTORS BE LIABLE FOR ANY\n+// DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n+// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n+// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n+// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+//\n+\n+#include \"port/port_win.h\"\n+\n+#include <windows.h>\n+#include <cassert>\n+\n+namespace leveldb {\n+namespace port {\n+\n+Mutex::Mutex() :\n+    cs_(NULL) {\n+  assert(!cs_);\n+  cs_ = static_cast<void *>(new CRITICAL_SECTION());\n+  ::InitializeCriticalSection(static_cast<CRITICAL_SECTION *>(cs_));\n+  assert(cs_);\n+}\n+\n+Mutex::~Mutex() {\n+  assert(cs_);\n+  ::DeleteCriticalSection(static_cast<CRITICAL_SECTION *>(cs_));\n+  delete static_cast<CRITICAL_SECTION *>(cs_);\n+  cs_ = NULL;\n+  assert(!cs_);\n+}\n+\n+void Mutex::Lock() {\n+  assert(cs_);\n+  ::EnterCriticalSection(static_cast<CRITICAL_SECTION *>(cs_));\n+}\n+\n+void Mutex::Unlock() {\n+  assert(cs_);\n+  ::LeaveCriticalSection(static_cast<CRITICAL_SECTION *>(cs_));\n+}\n+\n+void Mutex::AssertHeld() {\n+  assert(cs_);\n+  assert(1);\n+}\n+\n+CondVar::CondVar(Mutex* mu) :\n+    waiting_(0), \n+    mu_(mu), \n+    sem1_(::CreateSemaphore(NULL, 0, 10000, NULL)), \n+    sem2_(::CreateSemaphore(NULL, 0, 10000, NULL)) {\n+  assert(mu_);\n+}\n+\n+CondVar::~CondVar() {\n+  ::CloseHandle(sem1_);\n+  ::CloseHandle(sem2_);\n+}\n+\n+void CondVar::Wait() {\n+  mu_->AssertHeld();\n+\n+  wait_mtx_.Lock();\n+  ++waiting_;\n+  wait_mtx_.Unlock();\n+\n+  mu_->Unlock();\n+\n+  // initiate handshake\n+  ::WaitForSingleObject(sem1_, INFINITE);\n+  ::ReleaseSemaphore(sem2_, 1, NULL);\n+  mu_->Lock();\n+}\n+\n+void CondVar::Signal() {\n+  wait_mtx_.Lock();\n+  if (waiting_ > 0) {\n+    --waiting_;\n+\n+    // finalize handshake\n+    ::ReleaseSemaphore(sem1_, 1, NULL);\n+    ::WaitForSingleObject(sem2_, INFINITE);\n+  }\n+  wait_mtx_.Unlock();\n+}\n+\n+void CondVar::SignalAll() {\n+  wait_mtx_.Lock();\n+  ::ReleaseSemaphore(sem1_, waiting_, NULL);\n+  while(waiting_ > 0) {\n+    --waiting_;\n+    ::WaitForSingleObject(sem2_, INFINITE);\n+  }\n+  wait_mtx_.Unlock();\n+}\n+\n+AtomicPointer::AtomicPointer(void* v) {\n+  Release_Store(v);\n+}\n+\n+void InitOnce(OnceType* once, void (*initializer)()) {\n+  once->InitOnce(initializer);\n+}\n+\n+void* AtomicPointer::Acquire_Load() const {\n+  void * p = NULL;\n+  InterlockedExchangePointer(&p, rep_);\n+  return p;\n+}\n+\n+void AtomicPointer::Release_Store(void* v) {\n+  InterlockedExchangePointer(&rep_, v);\n+}\n+\n+void* AtomicPointer::NoBarrier_Load() const {\n+  return rep_;\n+}\n+\n+void AtomicPointer::NoBarrier_Store(void* v) {\n+  rep_ = v;\n+}\n+\n+}\n+}"
      },
      {
        "sha": "45bf2f0ea749d60abb2de3d7b357e7a70eac94de",
        "filename": "port/port_win.h",
        "status": "added",
        "additions": 174,
        "deletions": 0,
        "changes": 174,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/port_win.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/port_win.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/port/port_win.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,174 @@\n+// LevelDB Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// See port_example.h for documentation for the following types/functions.\n+\n+// Redistribution and use in source and binary forms, with or without\n+// modification, are permitted provided that the following conditions are met:\n+// \n+//  * Redistributions of source code must retain the above copyright\n+//    notice, this list of conditions and the following disclaimer.\n+//  * Redistributions in binary form must reproduce the above copyright\n+//    notice, this list of conditions and the following disclaimer in the\n+//    documentation and/or other materials provided with the distribution.\n+//  * Neither the name of the University of California, Berkeley nor the\n+//    names of its contributors may be used to endorse or promote products\n+//    derived from this software without specific prior written permission.\n+// \n+// THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND ANY\n+// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n+// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n+// DISCLAIMED. IN NO EVENT SHALL THE REGENTS AND CONTRIBUTORS BE LIABLE FOR ANY\n+// DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n+// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n+// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n+// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+//\n+\n+#ifndef STORAGE_LEVELDB_PORT_PORT_WIN_H_\n+#define STORAGE_LEVELDB_PORT_PORT_WIN_H_\n+\n+#ifdef _MSC_VER\n+#define snprintf _snprintf\n+#define close _close\n+#define fread_unlocked _fread_nolock\n+#endif\n+\n+#include <string>\n+#include <stdint.h>\n+#ifdef SNAPPY\n+#include <snappy.h>\n+#endif\n+\n+namespace leveldb {\n+namespace port {\n+\n+// Windows is little endian (for now :p)\n+static const bool kLittleEndian = true;\n+\n+class CondVar;\n+\n+class Mutex {\n+ public:\n+  Mutex();\n+  ~Mutex();\n+\n+  void Lock();\n+  void Unlock();\n+  void AssertHeld();\n+\n+ private:\n+  friend class CondVar;\n+  // critical sections are more efficient than mutexes\n+  // but they are not recursive and can only be used to synchronize threads within the same process\n+  // we use opaque void * to avoid including windows.h in port_win.h\n+  void * cs_;\n+\n+  // No copying\n+  Mutex(const Mutex&);\n+  void operator=(const Mutex&);\n+};\n+\n+// the Win32 API offers a dependable condition variable mechanism, but only starting with\n+// Windows 2008 and Vista\n+// no matter what we will implement our own condition variable with a semaphore\n+// implementation as described in a paper written by Andrew D. Birrell in 2003\n+class CondVar {\n+ public:\n+  explicit CondVar(Mutex* mu);\n+  ~CondVar();\n+  void Wait();\n+  void Signal();\n+  void SignalAll();\n+ private:\n+  Mutex* mu_;\n+  \n+  Mutex wait_mtx_;\n+  long waiting_;\n+  \n+  void * sem1_;\n+  void * sem2_;\n+  \n+  \n+};\n+\n+class OnceType {\n+public:\n+//    OnceType() : init_(false) {}\n+    OnceType(const OnceType &once) : init_(once.init_) {}\n+    OnceType(bool f) : init_(f) {}\n+    void InitOnce(void (*initializer)()) {\n+        mutex_.Lock();\n+        if (!init_) {\n+            init_ = true;\n+            initializer();\n+        }\n+        mutex_.Unlock();\n+    }\n+\n+private:\n+    bool init_;\n+    Mutex mutex_;\n+};\n+\n+#define LEVELDB_ONCE_INIT false\n+extern void InitOnce(port::OnceType*, void (*initializer)());\n+\n+// Storage for a lock-free pointer\n+class AtomicPointer {\n+ private:\n+  void * rep_;\n+ public:\n+  AtomicPointer() : rep_(NULL) { }\n+  explicit AtomicPointer(void* v); \n+  void* Acquire_Load() const;\n+\n+  void Release_Store(void* v);\n+\n+  void* NoBarrier_Load() const;\n+\n+  void NoBarrier_Store(void* v);\n+};\n+\n+inline bool Snappy_Compress(const char* input, size_t length,\n+                            ::std::string* output) {\n+#ifdef SNAPPY\n+  output->resize(snappy::MaxCompressedLength(length));\n+  size_t outlen;\n+  snappy::RawCompress(input, length, &(*output)[0], &outlen);\n+  output->resize(outlen);\n+  return true;\n+#endif\n+\n+  return false;\n+}\n+\n+inline bool Snappy_GetUncompressedLength(const char* input, size_t length,\n+                                         size_t* result) {\n+#ifdef SNAPPY\n+  return snappy::GetUncompressedLength(input, length, result);\n+#else\n+  return false;\n+#endif\n+}\n+\n+inline bool Snappy_Uncompress(const char* input, size_t length,\n+                              char* output) {\n+#ifdef SNAPPY\n+  return snappy::RawUncompress(input, length, output);\n+#else\n+  return false;\n+#endif\n+}\n+\n+inline bool GetHeapProfile(void (*func)(void*, const char*, int), void* arg) {\n+  return false;\n+}\n+\n+}\n+}\n+\n+#endif  // STORAGE_LEVELDB_PORT_PORT_WIN_H_"
      },
      {
        "sha": "6f9b6a7924d68e16940254d6d21888eaae2a240d",
        "filename": "port/thread_annotations.h",
        "status": "added",
        "additions": 59,
        "deletions": 0,
        "changes": 59,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/thread_annotations.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/thread_annotations.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/port/thread_annotations.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,59 @@\n+// Copyright (c) 2012 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_PORT_THREAD_ANNOTATIONS_H\n+\n+// Some environments provide custom macros to aid in static thread-safety\n+// analysis.  Provide empty definitions of such macros unless they are already\n+// defined.\n+\n+#ifndef EXCLUSIVE_LOCKS_REQUIRED\n+#define EXCLUSIVE_LOCKS_REQUIRED(...)\n+#endif\n+\n+#ifndef SHARED_LOCKS_REQUIRED\n+#define SHARED_LOCKS_REQUIRED(...)\n+#endif\n+\n+#ifndef LOCKS_EXCLUDED\n+#define LOCKS_EXCLUDED(...)\n+#endif\n+\n+#ifndef LOCK_RETURNED\n+#define LOCK_RETURNED(x)\n+#endif\n+\n+#ifndef LOCKABLE\n+#define LOCKABLE\n+#endif\n+\n+#ifndef SCOPED_LOCKABLE\n+#define SCOPED_LOCKABLE\n+#endif\n+\n+#ifndef EXCLUSIVE_LOCK_FUNCTION\n+#define EXCLUSIVE_LOCK_FUNCTION(...)\n+#endif\n+\n+#ifndef SHARED_LOCK_FUNCTION\n+#define SHARED_LOCK_FUNCTION(...)\n+#endif\n+\n+#ifndef EXCLUSIVE_TRYLOCK_FUNCTION\n+#define EXCLUSIVE_TRYLOCK_FUNCTION(...)\n+#endif\n+\n+#ifndef SHARED_TRYLOCK_FUNCTION\n+#define SHARED_TRYLOCK_FUNCTION(...)\n+#endif\n+\n+#ifndef UNLOCK_FUNCTION\n+#define UNLOCK_FUNCTION(...)\n+#endif\n+\n+#ifndef NO_THREAD_SAFETY_ANALYSIS\n+#define NO_THREAD_SAFETY_ANALYSIS\n+#endif\n+\n+#endif  // STORAGE_LEVELDB_PORT_THREAD_ANNOTATIONS_H"
      },
      {
        "sha": "39edd0db13f436dc57dd28ed4013ab4d55a28a31",
        "filename": "port/win/stdint.h",
        "status": "added",
        "additions": 24,
        "deletions": 0,
        "changes": 24,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/win/stdint.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/port/win/stdint.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/port/win/stdint.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "patch": "@@ -0,0 +1,24 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+// MSVC didn't ship with this file until the 2010 version.\n+\n+#ifndef STORAGE_LEVELDB_PORT_WIN_STDINT_H_\n+#define STORAGE_LEVELDB_PORT_WIN_STDINT_H_\n+\n+#if !defined(_MSC_VER)\n+#error This file should only be included when compiling with MSVC.\n+#endif\n+\n+// Define C99 equivalent types.\n+typedef signed char           int8_t;\n+typedef signed short          int16_t;\n+typedef signed int            int32_t;\n+typedef signed long long      int64_t;\n+typedef unsigned char         uint8_t;\n+typedef unsigned short        uint16_t;\n+typedef unsigned int          uint32_t;\n+typedef unsigned long long    uint64_t;\n+\n+#endif  // STORAGE_LEVELDB_PORT_WIN_STDINT_H_"
      },
      {
        "sha": "79ea9d9ee5fe8af28eb07f48b9534c2979fe842a",
        "filename": "table/block.cc",
        "status": "added",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/block.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/block.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/block.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "2493eb9f9fd9da41aafcac80180c3f831928a32d",
        "filename": "table/block.h",
        "status": "added",
        "additions": 44,
        "deletions": 0,
        "changes": 44,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/block.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/block.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/block.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "db660cd07cf50ad1b54310c21b22108ab6994802",
        "filename": "table/block_builder.cc",
        "status": "added",
        "additions": 109,
        "deletions": 0,
        "changes": 109,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/block_builder.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/block_builder.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/block_builder.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "5b545bd1afb4f4e65c36d8430dde09e0f543259f",
        "filename": "table/block_builder.h",
        "status": "added",
        "additions": 57,
        "deletions": 0,
        "changes": 57,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/block_builder.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/block_builder.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/block_builder.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "203e15c8bcb13b8776842052a725ad2a3909fcf5",
        "filename": "table/filter_block.cc",
        "status": "added",
        "additions": 111,
        "deletions": 0,
        "changes": 111,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/filter_block.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/filter_block.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/filter_block.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "c67d010bd106756c456dea013c5babdf0a18494f",
        "filename": "table/filter_block.h",
        "status": "added",
        "additions": 68,
        "deletions": 0,
        "changes": 68,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/filter_block.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/filter_block.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/filter_block.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "3a2a07cf53ca606b2d0e9890e6b9cfa02a678398",
        "filename": "table/filter_block_test.cc",
        "status": "added",
        "additions": 128,
        "deletions": 0,
        "changes": 128,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/filter_block_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/filter_block_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/filter_block_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "cda1decdf35476ecd5c44d7f3a8e69111e620124",
        "filename": "table/format.cc",
        "status": "added",
        "additions": 145,
        "deletions": 0,
        "changes": 145,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/format.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/format.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/format.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "6c0b80c0179c7fffbf6ee2af802a10ec02d73998",
        "filename": "table/format.h",
        "status": "added",
        "additions": 108,
        "deletions": 0,
        "changes": 108,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/format.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/format.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/format.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "3d1c87fdece73d4c1ef16a0a762f70059b9443e6",
        "filename": "table/iterator.cc",
        "status": "added",
        "additions": 67,
        "deletions": 0,
        "changes": 67,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/iterator.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/iterator.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/iterator.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "9e16b3dbedebe65f36fbbfa9e180fd1fa80e84a3",
        "filename": "table/iterator_wrapper.h",
        "status": "added",
        "additions": 63,
        "deletions": 0,
        "changes": 63,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/iterator_wrapper.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/iterator_wrapper.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/iterator_wrapper.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "2dde4dc21fde9d86e98f5a3f3b493745d07a22f7",
        "filename": "table/merger.cc",
        "status": "added",
        "additions": 197,
        "deletions": 0,
        "changes": 197,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/merger.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/merger.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/merger.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "91ddd80faa35bfcf7edb81ee2f22ed3f29b58f98",
        "filename": "table/merger.h",
        "status": "added",
        "additions": 26,
        "deletions": 0,
        "changes": 26,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/merger.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/merger.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/merger.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "71c1756e5f440c38c3e712e0359886fe22d6d5a0",
        "filename": "table/table.cc",
        "status": "added",
        "additions": 275,
        "deletions": 0,
        "changes": 275,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/table.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/table.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/table.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "62002c84f2b18f479fdb1cd3ab142c179e1f3a6f",
        "filename": "table/table_builder.cc",
        "status": "added",
        "additions": 270,
        "deletions": 0,
        "changes": 270,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/table_builder.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/table_builder.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/table_builder.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "c723bf84cf5f55fae9b56ab324ebfdc83ba9be52",
        "filename": "table/table_test.cc",
        "status": "added",
        "additions": 868,
        "deletions": 0,
        "changes": 868,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/table_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/table_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/table_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "7822ebab9c32ce579c42f9621545d7283e8332b9",
        "filename": "table/two_level_iterator.cc",
        "status": "added",
        "additions": 182,
        "deletions": 0,
        "changes": 182,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/two_level_iterator.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/two_level_iterator.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/two_level_iterator.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "629ca34525414582e39df8ad7a48eff72e0e450f",
        "filename": "table/two_level_iterator.h",
        "status": "added",
        "additions": 34,
        "deletions": 0,
        "changes": 34,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/two_level_iterator.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/table/two_level_iterator.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/table/two_level_iterator.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "9551d6a3a27c8c2bd13cc7e48882aaecdb75ba20",
        "filename": "util/arena.cc",
        "status": "added",
        "additions": 68,
        "deletions": 0,
        "changes": 68,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/arena.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/arena.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/arena.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "8f7dde226c450745646f29986d0302630db6f1f5",
        "filename": "util/arena.h",
        "status": "added",
        "additions": 68,
        "deletions": 0,
        "changes": 68,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/arena.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/arena.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/arena.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "63d1778034550a7394029df41d6b24f9053f5663",
        "filename": "util/arena_test.cc",
        "status": "added",
        "additions": 68,
        "deletions": 0,
        "changes": 68,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/arena_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/arena_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/arena_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "d7941cd21fab64079cbef1f62060a48f0d86c74d",
        "filename": "util/bloom.cc",
        "status": "added",
        "additions": 95,
        "deletions": 0,
        "changes": 95,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/bloom.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/bloom.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/bloom.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "0bf8e8d6ebeae860d7d553538511ef2ac6e80f22",
        "filename": "util/bloom_test.cc",
        "status": "added",
        "additions": 160,
        "deletions": 0,
        "changes": 160,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/bloom_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/bloom_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/bloom_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "8b197bc02a98e639dc72a58bd7de59603ab77cbc",
        "filename": "util/cache.cc",
        "status": "added",
        "additions": 325,
        "deletions": 0,
        "changes": 325,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/cache.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/cache.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/cache.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "43716715a89f476700845c303e1651448ecef62c",
        "filename": "util/cache_test.cc",
        "status": "added",
        "additions": 186,
        "deletions": 0,
        "changes": 186,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/cache_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/cache_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/cache_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "21e3186d5dcff984a11563fd0d09c714426a29c3",
        "filename": "util/coding.cc",
        "status": "added",
        "additions": 194,
        "deletions": 0,
        "changes": 194,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/coding.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/coding.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/coding.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "3993c4a755dfa5a0f8b966f1a698a371fa08556d",
        "filename": "util/coding.h",
        "status": "added",
        "additions": 104,
        "deletions": 0,
        "changes": 104,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/coding.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/coding.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/coding.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "fb5726e33521d01270b481b93d9c5490c6f0b53f",
        "filename": "util/coding_test.cc",
        "status": "added",
        "additions": 196,
        "deletions": 0,
        "changes": 196,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/coding_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/coding_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/coding_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "4b7b5724ef3be5f6c7ed9f95dcb47a99f2ee2f9b",
        "filename": "util/comparator.cc",
        "status": "added",
        "additions": 81,
        "deletions": 0,
        "changes": 81,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/comparator.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/comparator.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/comparator.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "6db9e770774d7ebc0b0c3bc4a230b5b8254cd1f4",
        "filename": "util/crc32c.cc",
        "status": "added",
        "additions": 332,
        "deletions": 0,
        "changes": 332,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/crc32c.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/crc32c.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/crc32c.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "1d7e5c075d86d6cf2551cfcd0fd9ac5adce6fc38",
        "filename": "util/crc32c.h",
        "status": "added",
        "additions": 45,
        "deletions": 0,
        "changes": 45,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/crc32c.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/crc32c.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/crc32c.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "4b957ee120c8f805c0240d3d90eaf16a759437ba",
        "filename": "util/crc32c_test.cc",
        "status": "added",
        "additions": 72,
        "deletions": 0,
        "changes": 72,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/crc32c_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/crc32c_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/crc32c_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "c2600e964a260c87f22afc8c5a3e7788b4c7e350",
        "filename": "util/env.cc",
        "status": "added",
        "additions": 96,
        "deletions": 0,
        "changes": 96,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/env.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/env.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/env.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "6badfdc230f52f235ec7f7523e213dba9d628451",
        "filename": "util/env_posix.cc",
        "status": "added",
        "additions": 701,
        "deletions": 0,
        "changes": 701,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/env_posix.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/env_posix.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/env_posix.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "b72cb4438425bca83d9a6ca0d207dbc8590efb2e",
        "filename": "util/env_test.cc",
        "status": "added",
        "additions": 104,
        "deletions": 0,
        "changes": 104,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/env_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/env_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/env_test.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "ef2ecae8306623e613419782daed98044a117435",
        "filename": "util/env_win.cc",
        "status": "added",
        "additions": 1031,
        "deletions": 0,
        "changes": 1031,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/env_win.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/env_win.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/env_win.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "7b045c8c91d6f6d600308e50966ccf56e53638bf",
        "filename": "util/filter_policy.cc",
        "status": "added",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/filter_policy.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/filter_policy.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/filter_policy.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "07cf022060d41ea2139a438886d268a92cb586af",
        "filename": "util/hash.cc",
        "status": "added",
        "additions": 52,
        "deletions": 0,
        "changes": 52,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/hash.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/hash.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/hash.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "8889d56be80e2f6342a1a292c6a0075d2481de80",
        "filename": "util/hash.h",
        "status": "added",
        "additions": 19,
        "deletions": 0,
        "changes": 19,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/hash.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/hash.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/hash.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "bb95f583eac6a6f916a83e409045f0a226bd9ae7",
        "filename": "util/histogram.cc",
        "status": "added",
        "additions": 139,
        "deletions": 0,
        "changes": 139,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/histogram.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/histogram.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/histogram.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "1ef9f3c8abdfc50858be433110611086bb3c0da6",
        "filename": "util/histogram.h",
        "status": "added",
        "additions": 42,
        "deletions": 0,
        "changes": 42,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/histogram.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/histogram.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/histogram.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "22cf2785123c45ab77fa158256a45d8e700c1463",
        "filename": "util/logging.cc",
        "status": "added",
        "additions": 81,
        "deletions": 0,
        "changes": 81,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/logging.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/logging.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/logging.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "b0c5da813e8658c9712b5529f4b219cb1a945508",
        "filename": "util/logging.h",
        "status": "added",
        "additions": 47,
        "deletions": 0,
        "changes": 47,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/logging.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/logging.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/logging.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "1ff5a9efa15efa166b427ef9611ccc58c96a3984",
        "filename": "util/mutexlock.h",
        "status": "added",
        "additions": 41,
        "deletions": 0,
        "changes": 41,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/mutexlock.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/mutexlock.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/mutexlock.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "76af5b9302d437d9847b2a93c87697232d027cac",
        "filename": "util/options.cc",
        "status": "added",
        "additions": 29,
        "deletions": 0,
        "changes": 29,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/options.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/options.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/options.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "c063c2b7cb200dffa3c253fe76ee851910cfbe2f",
        "filename": "util/posix_logger.h",
        "status": "added",
        "additions": 98,
        "deletions": 0,
        "changes": 98,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/posix_logger.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/posix_logger.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/posix_logger.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "07538242ea559ad31396b994a5172f13ecb3d775",
        "filename": "util/random.h",
        "status": "added",
        "additions": 59,
        "deletions": 0,
        "changes": 59,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/random.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/random.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/random.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "a44f35b3149fa8fe88d9ca32dbf92fbb9f6d534c",
        "filename": "util/status.cc",
        "status": "added",
        "additions": 75,
        "deletions": 0,
        "changes": 75,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/status.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/status.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/status.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "eb1bdd554a3ea2f06cd45053ab3df8c71c9610a8",
        "filename": "util/testharness.cc",
        "status": "added",
        "additions": 77,
        "deletions": 0,
        "changes": 77,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/testharness.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/testharness.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/testharness.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "da4fe68bb4e76ee69af136d76f9417d349fa9605",
        "filename": "util/testharness.h",
        "status": "added",
        "additions": 138,
        "deletions": 0,
        "changes": 138,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/testharness.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/testharness.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/testharness.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "538d09516d217d614ac45cefe38bb1503d3b1d5c",
        "filename": "util/testutil.cc",
        "status": "added",
        "additions": 51,
        "deletions": 0,
        "changes": 51,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/testutil.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/testutil.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/testutil.cc?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      },
      {
        "sha": "824e655bd2c5d7955aa83887966ad0f54fb77192",
        "filename": "util/testutil.h",
        "status": "added",
        "additions": 53,
        "deletions": 0,
        "changes": 53,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/testutil.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332/util/testutil.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/util/testutil.h?ref=cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      }
    ]
  },
  {
    "sha": "e6552eed63f34fd68cf6c785dd23a241049fa73f",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzplNjU1MmVlZDYzZjM0ZmQ2OGNmNmM3ODVkZDIzYTI0MTA0OWZhNzNm",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "sipa@ulyssis.org",
        "date": "2013-08-20T12:17:45Z"
      },
      "committer": {
        "name": "Pieter Wuille",
        "email": "sipa@ulyssis.org",
        "date": "2013-08-20T12:17:45Z"
      },
      "message": "Merge commit 'cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332' as 'src/leveldb'",
      "tree": {
        "sha": "671d69e331a388feec6bce852c5967b08be57d41",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/671d69e331a388feec6bce852c5967b08be57d41"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/e6552eed63f34fd68cf6c785dd23a241049fa73f",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/e6552eed63f34fd68cf6c785dd23a241049fa73f",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/e6552eed63f34fd68cf6c785dd23a241049fa73f",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/e6552eed63f34fd68cf6c785dd23a241049fa73f/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "5c739c574fdc9197b317221305e14e7701a452b3",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5c739c574fdc9197b317221305e14e7701a452b3",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/5c739c574fdc9197b317221305e14e7701a452b3"
      },
      {
        "sha": "cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      }
    ],
    "stats": {
      "total": 27817,
      "additions": 27817,
      "deletions": 0
    },
    "files": [
      {
        "sha": "71d87a4eeb60b9599e6a7a23d61a659c7befa553",
        "filename": "src/leveldb/.gitignore",
        "status": "added",
        "additions": 13,
        "deletions": 0,
        "changes": 13,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/.gitignore",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/.gitignore",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/.gitignore?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,13 @@\n+build_config.mk\n+*.a\n+*.o\n+*.dylib*\n+*.so\n+*.so.*\n+*_test\n+db_bench\n+leveldbutil\n+Release\n+Debug\n+Benchmark\n+vs2010.*"
      },
      {
        "sha": "fc40194ab94f41405bd48d085b9f3fcbe1704234",
        "filename": "src/leveldb/AUTHORS",
        "status": "added",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/AUTHORS",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/AUTHORS",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/AUTHORS?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,11 @@\n+# Names should be added to this file like so:\n+# Name or Organization <email address>\n+\n+Google Inc.\n+\n+# Initial version authors:\n+Jeffrey Dean <jeff@google.com>\n+Sanjay Ghemawat <sanjay@google.com>\n+\n+# Partial list of contributors:\n+Kevin Regan <kevin.d.regan@gmail.com>"
      },
      {
        "sha": "8e80208cd72b3225c87d9111c4d7cab13af1c2ac",
        "filename": "src/leveldb/LICENSE",
        "status": "added",
        "additions": 27,
        "deletions": 0,
        "changes": 27,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/LICENSE",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/LICENSE",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/LICENSE?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,27 @@\n+Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+\n+Redistribution and use in source and binary forms, with or without\n+modification, are permitted provided that the following conditions are\n+met:\n+\n+   * Redistributions of source code must retain the above copyright\n+notice, this list of conditions and the following disclaimer.\n+   * Redistributions in binary form must reproduce the above\n+copyright notice, this list of conditions and the following disclaimer\n+in the documentation and/or other materials provided with the\n+distribution.\n+   * Neither the name of Google Inc. nor the names of its\n+contributors may be used to endorse or promote products derived from\n+this software without specific prior written permission.\n+\n+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n+\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n+OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n+LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n+DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n+THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
      },
      {
        "sha": "96af7765be41edc6b8e83c646efcdb3b9c99d025",
        "filename": "src/leveldb/Makefile",
        "status": "added",
        "additions": 206,
        "deletions": 0,
        "changes": 206,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/Makefile",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/Makefile",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/Makefile?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,206 @@\n+# Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+# Use of this source code is governed by a BSD-style license that can be\n+# found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#-----------------------------------------------\n+# Uncomment exactly one of the lines labelled (A), (B), and (C) below\n+# to switch between compilation modes.\n+\n+OPT ?= -O2 -DNDEBUG       # (A) Production use (optimized mode)\n+# OPT ?= -g2              # (B) Debug mode, w/ full line-level debugging symbols\n+# OPT ?= -O2 -g2 -DNDEBUG # (C) Profiling mode: opt, but w/debugging symbols\n+#-----------------------------------------------\n+\n+# detect what platform we're building on\n+$(shell CC=\"$(CC)\" CXX=\"$(CXX)\" TARGET_OS=\"$(TARGET_OS)\" \\\n+    ./build_detect_platform build_config.mk ./)\n+# this file is generated by the previous line to set build flags and sources\n+include build_config.mk\n+\n+CFLAGS += -I. -I./include $(PLATFORM_CCFLAGS) $(OPT)\n+CXXFLAGS += -I. -I./include $(PLATFORM_CXXFLAGS) $(OPT)\n+\n+LDFLAGS += $(PLATFORM_LDFLAGS)\n+LIBS += $(PLATFORM_LIBS)\n+\n+LIBOBJECTS = $(SOURCES:.cc=.o)\n+MEMENVOBJECTS = $(MEMENV_SOURCES:.cc=.o)\n+\n+TESTUTIL = ./util/testutil.o\n+TESTHARNESS = ./util/testharness.o $(TESTUTIL)\n+\n+TESTS = \\\n+\tarena_test \\\n+\tbloom_test \\\n+\tc_test \\\n+\tcache_test \\\n+\tcoding_test \\\n+\tcorruption_test \\\n+\tcrc32c_test \\\n+\tdb_test \\\n+\tdbformat_test \\\n+\tenv_test \\\n+\tfilename_test \\\n+\tfilter_block_test \\\n+\tissue178_test \\\n+\tlog_test \\\n+\tmemenv_test \\\n+\tskiplist_test \\\n+\ttable_test \\\n+\tversion_edit_test \\\n+\tversion_set_test \\\n+\twrite_batch_test\n+\n+PROGRAMS = db_bench leveldbutil $(TESTS)\n+BENCHMARKS = db_bench_sqlite3 db_bench_tree_db\n+\n+LIBRARY = libleveldb.a\n+MEMENVLIBRARY = libmemenv.a\n+\n+default: all\n+\n+# Should we build shared libraries?\n+ifneq ($(PLATFORM_SHARED_EXT),)\n+\n+ifneq ($(PLATFORM_SHARED_VERSIONED),true)\n+SHARED1 = libleveldb.$(PLATFORM_SHARED_EXT)\n+SHARED2 = $(SHARED1)\n+SHARED3 = $(SHARED1)\n+SHARED = $(SHARED1)\n+else\n+# Update db.h if you change these.\n+SHARED_MAJOR = 1\n+SHARED_MINOR = 12\n+SHARED1 = libleveldb.$(PLATFORM_SHARED_EXT)\n+SHARED2 = $(SHARED1).$(SHARED_MAJOR)\n+SHARED3 = $(SHARED1).$(SHARED_MAJOR).$(SHARED_MINOR)\n+SHARED = $(SHARED1) $(SHARED2) $(SHARED3)\n+$(SHARED1): $(SHARED3)\n+\tln -fs $(SHARED3) $(SHARED1)\n+$(SHARED2): $(SHARED3)\n+\tln -fs $(SHARED3) $(SHARED2)\n+endif\n+\n+$(SHARED3):\n+\t$(CXX) $(LDFLAGS) $(PLATFORM_SHARED_LDFLAGS)$(SHARED2) $(CXXFLAGS) $(PLATFORM_SHARED_CFLAGS) $(SOURCES) -o $(SHARED3) $(LIBS)\n+\n+endif  # PLATFORM_SHARED_EXT\n+\n+all: $(SHARED) $(LIBRARY)\n+\n+check: all $(PROGRAMS) $(TESTS)\n+\tfor t in $(TESTS); do echo \"***** Running $$t\"; ./$$t || exit 1; done\n+\n+clean:\n+\t-rm -f $(PROGRAMS) $(BENCHMARKS) $(LIBRARY) $(SHARED) $(MEMENVLIBRARY) */*.o */*/*.o ios-x86/*/*.o ios-arm/*/*.o build_config.mk\n+\t-rm -rf ios-x86/* ios-arm/*\n+\n+$(LIBRARY): $(LIBOBJECTS)\n+\trm -f $@\n+\t$(AR) -rs $@ $(LIBOBJECTS)\n+\n+db_bench: db/db_bench.o $(LIBOBJECTS) $(TESTUTIL)\n+\t$(CXX) $(LDFLAGS) db/db_bench.o $(LIBOBJECTS) $(TESTUTIL) -o $@ $(LIBS)\n+\n+db_bench_sqlite3: doc/bench/db_bench_sqlite3.o $(LIBOBJECTS) $(TESTUTIL)\n+\t$(CXX) $(LDFLAGS) doc/bench/db_bench_sqlite3.o $(LIBOBJECTS) $(TESTUTIL) -o $@ -lsqlite3 $(LIBS)\n+\n+db_bench_tree_db: doc/bench/db_bench_tree_db.o $(LIBOBJECTS) $(TESTUTIL)\n+\t$(CXX) $(LDFLAGS) doc/bench/db_bench_tree_db.o $(LIBOBJECTS) $(TESTUTIL) -o $@ -lkyotocabinet $(LIBS)\n+\n+leveldbutil: db/leveldb_main.o $(LIBOBJECTS)\n+\t$(CXX) $(LDFLAGS) db/leveldb_main.o $(LIBOBJECTS) -o $@ $(LIBS)\n+\n+arena_test: util/arena_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) util/arena_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+bloom_test: util/bloom_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) util/bloom_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+c_test: db/c_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/c_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+cache_test: util/cache_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) util/cache_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+coding_test: util/coding_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) util/coding_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+corruption_test: db/corruption_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/corruption_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+crc32c_test: util/crc32c_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) util/crc32c_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+db_test: db/db_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/db_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+dbformat_test: db/dbformat_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/dbformat_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+env_test: util/env_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) util/env_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+filename_test: db/filename_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/filename_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+filter_block_test: table/filter_block_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) table/filter_block_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+issue178_test: issues/issue178_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) issues/issue178_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+log_test: db/log_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/log_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+table_test: table/table_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) table/table_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+skiplist_test: db/skiplist_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/skiplist_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+version_edit_test: db/version_edit_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/version_edit_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+version_set_test: db/version_set_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/version_set_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+write_batch_test: db/write_batch_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/write_batch_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+$(MEMENVLIBRARY) : $(MEMENVOBJECTS)\n+\trm -f $@\n+\t$(AR) -rs $@ $(MEMENVOBJECTS)\n+\n+memenv_test : helpers/memenv/memenv_test.o $(MEMENVLIBRARY) $(LIBRARY) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) helpers/memenv/memenv_test.o $(MEMENVLIBRARY) $(LIBRARY) $(TESTHARNESS) -o $@ $(LIBS)\n+\n+ifeq ($(PLATFORM), IOS)\n+# For iOS, create universal object files to be used on both the simulator and\n+# a device.\n+PLATFORMSROOT=/Applications/Xcode.app/Contents/Developer/Platforms\n+SIMULATORROOT=$(PLATFORMSROOT)/iPhoneSimulator.platform/Developer\n+DEVICEROOT=$(PLATFORMSROOT)/iPhoneOS.platform/Developer\n+IOSVERSION=$(shell defaults read $(PLATFORMSROOT)/iPhoneOS.platform/version CFBundleShortVersionString)\n+\n+.cc.o:\n+\tmkdir -p ios-x86/$(dir $@)\n+\t$(CXX) $(CXXFLAGS) -isysroot $(SIMULATORROOT)/SDKs/iPhoneSimulator$(IOSVERSION).sdk -arch i686 -c $< -o ios-x86/$@\n+\tmkdir -p ios-arm/$(dir $@)\n+\t$(DEVICEROOT)/usr/bin/$(CXX) $(CXXFLAGS) -isysroot $(DEVICEROOT)/SDKs/iPhoneOS$(IOSVERSION).sdk -arch armv6 -arch armv7 -c $< -o ios-arm/$@\n+\tlipo ios-x86/$@ ios-arm/$@ -create -output $@\n+\n+.c.o:\n+\tmkdir -p ios-x86/$(dir $@)\n+\t$(CC) $(CFLAGS) -isysroot $(SIMULATORROOT)/SDKs/iPhoneSimulator$(IOSVERSION).sdk -arch i686 -c $< -o ios-x86/$@\n+\tmkdir -p ios-arm/$(dir $@)\n+\t$(DEVICEROOT)/usr/bin/$(CC) $(CFLAGS) -isysroot $(DEVICEROOT)/SDKs/iPhoneOS$(IOSVERSION).sdk -arch armv6 -arch armv7 -c $< -o ios-arm/$@\n+\tlipo ios-x86/$@ ios-arm/$@ -create -output $@\n+\n+else\n+.cc.o:\n+\t$(CXX) $(CXXFLAGS) -c $< -o $@\n+\n+.c.o:\n+\t$(CC) $(CFLAGS) -c $< -o $@\n+endif"
      },
      {
        "sha": "3fd99242d7bbcfeffecfd01c0870f89382766baf",
        "filename": "src/leveldb/NEWS",
        "status": "added",
        "additions": 17,
        "deletions": 0,
        "changes": 17,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/NEWS",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/NEWS",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/NEWS?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,17 @@\n+Release 1.2 2011-05-16\n+----------------------\n+\n+Fixes for larger databases (tested up to one billion 100-byte entries,\n+i.e., ~100GB).\n+\n+(1) Place hard limit on number of level-0 files.  This fixes errors\n+of the form \"too many open files\".\n+\n+(2) Fixed memtable management.  Before the fix, a heavy write burst\n+could cause unbounded memory usage.\n+\n+A fix for a logging bug where the reader would incorrectly complain\n+about corruption.\n+\n+Allow public access to WriteBatch contents so that users can easily\n+wrap a DB."
      },
      {
        "sha": "3618adeeedbea04a14e00d5a1ef33dd4f0a7be06",
        "filename": "src/leveldb/README",
        "status": "added",
        "additions": 51,
        "deletions": 0,
        "changes": 51,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/README",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/README",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/README?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,51 @@\n+leveldb: A key-value store\n+Authors: Sanjay Ghemawat (sanjay@google.com) and Jeff Dean (jeff@google.com)\n+\n+The code under this directory implements a system for maintaining a\n+persistent key/value store.\n+\n+See doc/index.html for more explanation.\n+See doc/impl.html for a brief overview of the implementation.\n+\n+The public interface is in include/*.h.  Callers should not include or\n+rely on the details of any other header files in this package.  Those\n+internal APIs may be changed without warning.\n+\n+Guide to header files:\n+\n+include/db.h\n+    Main interface to the DB: Start here\n+\n+include/options.h\n+    Control over the behavior of an entire database, and also\n+    control over the behavior of individual reads and writes.\n+\n+include/comparator.h\n+    Abstraction for user-specified comparison function.  If you want\n+    just bytewise comparison of keys, you can use the default comparator,\n+    but clients can write their own comparator implementations if they\n+    want custom ordering (e.g. to handle different character\n+    encodings, etc.)\n+\n+include/iterator.h\n+    Interface for iterating over data. You can get an iterator\n+    from a DB object.\n+\n+include/write_batch.h\n+    Interface for atomically applying multiple updates to a database.\n+\n+include/slice.h\n+    A simple module for maintaining a pointer and a length into some\n+    other byte array.\n+\n+include/status.h\n+    Status is returned from many of the public interfaces and is used\n+    to report success and various kinds of errors.\n+\n+include/env.h\n+    Abstraction of the OS environment.  A posix implementation of\n+    this interface is in util/env_posix.cc\n+\n+include/table.h\n+include/table_builder.h\n+    Lower-level modules that most clients probably won't use directly"
      },
      {
        "sha": "e603c07137f1ea198a3de9ca3bbb369e1c069f61",
        "filename": "src/leveldb/TODO",
        "status": "added",
        "additions": 14,
        "deletions": 0,
        "changes": 14,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/TODO",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/TODO",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/TODO?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,14 @@\n+ss\n+- Stats\n+\n+db\n+- Maybe implement DB::BulkDeleteForRange(start_key, end_key)\n+  that would blow away files whose ranges are entirely contained\n+  within [start_key..end_key]?  For Chrome, deletion of obsolete\n+  object stores, etc. can be done in the background anyway, so\n+  probably not that important.\n+- There have been requests for MultiGet.\n+\n+After a range is completely deleted, what gets rid of the\n+corresponding files if we do no future changes to that range.  Make\n+the conditions for triggering compactions fire in more situations?"
      },
      {
        "sha": "5b76c2448fe1e475b97d12235bdff53b857557cf",
        "filename": "src/leveldb/WINDOWS.md",
        "status": "added",
        "additions": 39,
        "deletions": 0,
        "changes": 39,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/WINDOWS.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/WINDOWS.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/WINDOWS.md?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,39 @@\n+# Building LevelDB On Windows\n+\n+## Prereqs \n+\n+Install the [Windows Software Development Kit version 7.1](http://www.microsoft.com/downloads/dlx/en-us/listdetailsview.aspx?FamilyID=6b6c21d2-2006-4afa-9702-529fa782d63b).\n+\n+Download and extract the [Snappy source distribution](http://snappy.googlecode.com/files/snappy-1.0.5.tar.gz)\n+\n+1. Open the \"Windows SDK 7.1 Command Prompt\" :\n+   Start Menu -> \"Microsoft Windows SDK v7.1\" > \"Windows SDK 7.1 Command Prompt\"\n+2. Change the directory to the leveldb project\n+\n+## Building the Static lib \n+\n+* 32 bit Version \n+\n+        setenv /x86\n+        msbuild.exe /p:Configuration=Release /p:Platform=Win32 /p:Snappy=..\\snappy-1.0.5\n+\n+* 64 bit Version \n+\n+        setenv /x64\n+        msbuild.exe /p:Configuration=Release /p:Platform=x64 /p:Snappy=..\\snappy-1.0.5\n+\n+\n+## Building and Running the Benchmark app\n+\n+* 32 bit Version \n+\n+\t    setenv /x86\n+\t    msbuild.exe /p:Configuration=Benchmark /p:Platform=Win32 /p:Snappy=..\\snappy-1.0.5\n+\t\tBenchmark\\leveldb.exe\n+\n+* 64 bit Version \n+\n+\t    setenv /x64\n+\t    msbuild.exe /p:Configuration=Benchmark /p:Platform=x64 /p:Snappy=..\\snappy-1.0.5\n+\t    x64\\Benchmark\\leveldb.exe\n+"
      },
      {
        "sha": "bdfd64172cd1b9ca15dd436d96b9dba8904c281b",
        "filename": "src/leveldb/build_detect_platform",
        "status": "added",
        "additions": 214,
        "deletions": 0,
        "changes": 214,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/build_detect_platform",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/build_detect_platform",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/build_detect_platform?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,214 @@\n+#!/bin/sh\n+#\n+# Detects OS we're compiling on and outputs a file specified by the first\n+# argument, which in turn gets read while processing Makefile.\n+#\n+# The output will set the following variables:\n+#   CC                          C Compiler path\n+#   CXX                         C++ Compiler path\n+#   PLATFORM_LDFLAGS            Linker flags\n+#   PLATFORM_LIBS               Libraries flags\n+#   PLATFORM_SHARED_EXT         Extension for shared libraries\n+#   PLATFORM_SHARED_LDFLAGS     Flags for building shared library\n+#                               This flag is embedded just before the name\n+#                               of the shared library without intervening spaces\n+#   PLATFORM_SHARED_CFLAGS      Flags for compiling objects for shared library\n+#   PLATFORM_CCFLAGS            C compiler flags\n+#   PLATFORM_CXXFLAGS           C++ compiler flags.  Will contain:\n+#   PLATFORM_SHARED_VERSIONED   Set to 'true' if platform supports versioned\n+#                               shared libraries, empty otherwise.\n+#\n+# The PLATFORM_CCFLAGS and PLATFORM_CXXFLAGS might include the following:\n+#\n+#       -DLEVELDB_CSTDATOMIC_PRESENT if <cstdatomic> is present\n+#       -DLEVELDB_PLATFORM_POSIX     for Posix-based platforms\n+#       -DSNAPPY                     if the Snappy library is present\n+#\n+\n+OUTPUT=$1\n+PREFIX=$2\n+if test -z \"$OUTPUT\" || test -z \"$PREFIX\"; then\n+  echo \"usage: $0 <output-filename> <directory_prefix>\" >&2\n+  exit 1\n+fi\n+\n+# Delete existing output, if it exists\n+rm -f $OUTPUT\n+touch $OUTPUT\n+\n+if test -z \"$CC\"; then\n+    CC=cc\n+fi\n+\n+if test -z \"$CXX\"; then\n+    CXX=g++\n+fi\n+\n+if test -z \"$TMPDIR\"; then\n+    TMPDIR=/tmp\n+fi\n+\n+# Detect OS\n+if test -z \"$TARGET_OS\"; then\n+    TARGET_OS=`uname -s`\n+fi\n+\n+COMMON_FLAGS=\n+CROSS_COMPILE=\n+PLATFORM_CCFLAGS=\n+PLATFORM_CXXFLAGS=\n+PLATFORM_LDFLAGS=\n+PLATFORM_LIBS=\n+PLATFORM_SHARED_EXT=\"so\"\n+PLATFORM_SHARED_LDFLAGS=\"-shared -Wl,-soname -Wl,\"\n+PLATFORM_SHARED_CFLAGS=\"-fPIC\"\n+PLATFORM_SHARED_VERSIONED=true\n+\n+MEMCMP_FLAG=\n+if [ \"$CXX\" = \"g++\" ]; then\n+    # Use libc's memcmp instead of GCC's memcmp.  This results in ~40%\n+    # performance improvement on readrandom under gcc 4.4.3 on Linux/x86.\n+    MEMCMP_FLAG=\"-fno-builtin-memcmp\"\n+fi\n+\n+case \"$TARGET_OS\" in\n+    Darwin)\n+        PLATFORM=OS_MACOSX\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -DOS_MACOSX\"\n+        PLATFORM_SHARED_EXT=dylib\n+        [ -z \"$INSTALL_PATH\" ] && INSTALL_PATH=`pwd`\n+        PLATFORM_SHARED_LDFLAGS=\"-dynamiclib -install_name $INSTALL_PATH/\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    Linux)\n+        PLATFORM=OS_LINUX\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -pthread -DOS_LINUX\"\n+        PLATFORM_LDFLAGS=\"-pthread\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    SunOS)\n+        PLATFORM=OS_SOLARIS\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_SOLARIS\"\n+        PLATFORM_LIBS=\"-lpthread -lrt\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    FreeBSD)\n+        PLATFORM=OS_FREEBSD\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_FREEBSD\"\n+        PLATFORM_LIBS=\"-lpthread\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    GNU/kFreeBSD)\n+        PLATFORM=OS_KFREEBSD\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_KFREEBSD\"\n+        PLATFORM_LIBS=\"-lpthread\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    NetBSD)\n+        PLATFORM=OS_NETBSD\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_NETBSD\"\n+        PLATFORM_LIBS=\"-lpthread -lgcc_s\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    OpenBSD)\n+        PLATFORM=OS_OPENBSD\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_OPENBSD\"\n+        PLATFORM_LDFLAGS=\"-pthread\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    DragonFly)\n+        PLATFORM=OS_DRAGONFLYBSD\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_DRAGONFLYBSD\"\n+        PLATFORM_LIBS=\"-lpthread\"\n+        PORT_FILE=port/port_posix.cc\n+        ;;\n+    OS_ANDROID_CROSSCOMPILE)\n+        PLATFORM=OS_ANDROID\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_ANDROID -DLEVELDB_PLATFORM_POSIX\"\n+        PLATFORM_LDFLAGS=\"\"  # All pthread features are in the Android C library\n+        PORT_FILE=port/port_posix.cc\n+        CROSS_COMPILE=true\n+        ;;\n+    HP-UX)\n+        PLATFORM=OS_HPUX\n+        COMMON_FLAGS=\"$MEMCMP_FLAG -D_REENTRANT -DOS_HPUX\"\n+        PLATFORM_LDFLAGS=\"-pthread\"\n+        PORT_FILE=port/port_posix.cc\n+        # man ld: +h internal_name\n+        PLATFORM_SHARED_LDFLAGS=\"-shared -Wl,+h -Wl,\"\n+        ;;\n+    OS_WINDOWS_CROSSCOMPILE | NATIVE_WINDOWS)\n+        PLATFORM=OS_WINDOWS\n+        COMMON_FLAGS=\"-fno-builtin-memcmp -D_REENTRANT -DOS_WINDOWS -DLEVELDB_PLATFORM_WINDOWS -DWINVER=0x0500 -D__USE_MINGW_ANSI_STDIO=1\"\n+        PLATFORM_SOURCES=\"util/env_win.cc\"\n+        PLATFORM_LIBS=\"-lshlwapi\"\n+        PORT_FILE=port/port_win.cc\n+        CROSS_COMPILE=true\n+        ;;\n+    *)\n+        echo \"Unknown platform!\" >&2\n+        exit 1\n+esac\n+\n+# We want to make a list of all cc files within util, db, table, and helpers\n+# except for the test and benchmark files. By default, find will output a list\n+# of all files matching either rule, so we need to append -print to make the\n+# prune take effect.\n+DIRS=\"$PREFIX/db $PREFIX/util $PREFIX/table\"\n+\n+set -f # temporarily disable globbing so that our patterns aren't expanded\n+PRUNE_TEST=\"-name *test*.cc -prune\"\n+PRUNE_BENCH=\"-name *_bench.cc -prune\"\n+PRUNE_TOOL=\"-name leveldb_main.cc -prune\"\n+PORTABLE_FILES=`find $DIRS $PRUNE_TEST -o $PRUNE_BENCH -o $PRUNE_TOOL -o -name '*.cc' -print | sort | sed \"s,^$PREFIX/,,\" | tr \"\\n\" \" \"`\n+\n+set +f # re-enable globbing\n+\n+# The sources consist of the portable files, plus the platform-specific port\n+# file.\n+echo \"SOURCES=$PORTABLE_FILES $PORT_FILE\" >> $OUTPUT\n+echo \"MEMENV_SOURCES=helpers/memenv/memenv.cc\" >> $OUTPUT\n+\n+if [ \"$CROSS_COMPILE\" = \"true\" ]; then\n+    # Cross-compiling; do not try any compilation tests.\n+    true\n+else\n+    CXXOUTPUT=\"${TMPDIR}/leveldb_build_detect_platform-cxx.$$\"\n+\n+    # If -std=c++0x works, use <cstdatomic>.  Otherwise use port_posix.h.\n+    $CXX $CXXFLAGS -std=c++0x -x c++ - -o $CXXOUTPUT 2>/dev/null  <<EOF\n+      #include <cstdatomic>\n+      int main() {}\n+EOF\n+    if [ \"$?\" = 0 ]; then\n+        COMMON_FLAGS=\"$COMMON_FLAGS -DLEVELDB_PLATFORM_POSIX -DLEVELDB_CSTDATOMIC_PRESENT\"\n+        PLATFORM_CXXFLAGS=\"-std=c++0x\"\n+    else\n+        COMMON_FLAGS=\"$COMMON_FLAGS -DLEVELDB_PLATFORM_POSIX\"\n+    fi\n+\n+    # Test whether tcmalloc is available\n+    $CXX $CXXFLAGS -x c++ - -o $CXXOUTPUT -ltcmalloc 2>/dev/null  <<EOF\n+      int main() {}\n+EOF\n+    if [ \"$?\" = 0 ]; then\n+        PLATFORM_LIBS=\"$PLATFORM_LIBS -ltcmalloc\"\n+    fi\n+\n+    rm -f $CXXOUTPUT 2>/dev/null\n+fi\n+\n+PLATFORM_CCFLAGS=\"$PLATFORM_CCFLAGS $COMMON_FLAGS\"\n+PLATFORM_CXXFLAGS=\"$PLATFORM_CXXFLAGS $COMMON_FLAGS\"\n+\n+echo \"CC=$CC\" >> $OUTPUT\n+echo \"CXX=$CXX\" >> $OUTPUT\n+echo \"PLATFORM=$PLATFORM\" >> $OUTPUT\n+echo \"PLATFORM_LDFLAGS=$PLATFORM_LDFLAGS\" >> $OUTPUT\n+echo \"PLATFORM_LIBS=$PLATFORM_LIBS\" >> $OUTPUT\n+echo \"PLATFORM_CCFLAGS=$PLATFORM_CCFLAGS\" >> $OUTPUT\n+echo \"PLATFORM_CXXFLAGS=$PLATFORM_CXXFLAGS\" >> $OUTPUT\n+echo \"PLATFORM_SHARED_CFLAGS=$PLATFORM_SHARED_CFLAGS\" >> $OUTPUT\n+echo \"PLATFORM_SHARED_EXT=$PLATFORM_SHARED_EXT\" >> $OUTPUT\n+echo \"PLATFORM_SHARED_LDFLAGS=$PLATFORM_SHARED_LDFLAGS\" >> $OUTPUT\n+echo \"PLATFORM_SHARED_VERSIONED=$PLATFORM_SHARED_VERSIONED\" >> $OUTPUT"
      },
      {
        "sha": "f4198821973c94f2e28148f79a10ca2c48d8d1e6",
        "filename": "src/leveldb/db/builder.cc",
        "status": "added",
        "additions": 88,
        "deletions": 0,
        "changes": 88,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/builder.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/builder.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/builder.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,88 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/builder.h\"\n+\n+#include \"db/filename.h\"\n+#include \"db/dbformat.h\"\n+#include \"db/table_cache.h\"\n+#include \"db/version_edit.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/iterator.h\"\n+\n+namespace leveldb {\n+\n+Status BuildTable(const std::string& dbname,\n+                  Env* env,\n+                  const Options& options,\n+                  TableCache* table_cache,\n+                  Iterator* iter,\n+                  FileMetaData* meta) {\n+  Status s;\n+  meta->file_size = 0;\n+  iter->SeekToFirst();\n+\n+  std::string fname = TableFileName(dbname, meta->number);\n+  if (iter->Valid()) {\n+    WritableFile* file;\n+    s = env->NewWritableFile(fname, &file);\n+    if (!s.ok()) {\n+      return s;\n+    }\n+\n+    TableBuilder* builder = new TableBuilder(options, file);\n+    meta->smallest.DecodeFrom(iter->key());\n+    for (; iter->Valid(); iter->Next()) {\n+      Slice key = iter->key();\n+      meta->largest.DecodeFrom(key);\n+      builder->Add(key, iter->value());\n+    }\n+\n+    // Finish and check for builder errors\n+    if (s.ok()) {\n+      s = builder->Finish();\n+      if (s.ok()) {\n+        meta->file_size = builder->FileSize();\n+        assert(meta->file_size > 0);\n+      }\n+    } else {\n+      builder->Abandon();\n+    }\n+    delete builder;\n+\n+    // Finish and check for file errors\n+    if (s.ok()) {\n+      s = file->Sync();\n+    }\n+    if (s.ok()) {\n+      s = file->Close();\n+    }\n+    delete file;\n+    file = NULL;\n+\n+    if (s.ok()) {\n+      // Verify that the table is usable\n+      Iterator* it = table_cache->NewIterator(ReadOptions(),\n+                                              meta->number,\n+                                              meta->file_size);\n+      s = it->status();\n+      delete it;\n+    }\n+  }\n+\n+  // Check for input iterator errors\n+  if (!iter->status().ok()) {\n+    s = iter->status();\n+  }\n+\n+  if (s.ok() && meta->file_size > 0) {\n+    // Keep it\n+  } else {\n+    env->DeleteFile(fname);\n+  }\n+  return s;\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "62431fcf44f4545490291d8ec1ab098c2fc2ba88",
        "filename": "src/leveldb/db/builder.h",
        "status": "added",
        "additions": 34,
        "deletions": 0,
        "changes": 34,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/builder.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/builder.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/builder.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,34 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_BUILDER_H_\n+#define STORAGE_LEVELDB_DB_BUILDER_H_\n+\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+struct Options;\n+struct FileMetaData;\n+\n+class Env;\n+class Iterator;\n+class TableCache;\n+class VersionEdit;\n+\n+// Build a Table file from the contents of *iter.  The generated file\n+// will be named according to meta->number.  On success, the rest of\n+// *meta will be filled with metadata about the generated table.\n+// If no data is present in *iter, meta->file_size will be set to\n+// zero, and no Table file will be produced.\n+extern Status BuildTable(const std::string& dbname,\n+                         Env* env,\n+                         const Options& options,\n+                         TableCache* table_cache,\n+                         Iterator* iter,\n+                         FileMetaData* meta);\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_BUILDER_H_"
      },
      {
        "sha": "08ff0ad90ac00d63f05d5d71fb89f9f701894058",
        "filename": "src/leveldb/db/c.cc",
        "status": "added",
        "additions": 595,
        "deletions": 0,
        "changes": 595,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/c.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/c.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/c.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,595 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"leveldb/c.h\"\n+\n+#include <stdlib.h>\n+#include <unistd.h>\n+#include \"leveldb/cache.h\"\n+#include \"leveldb/comparator.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/filter_policy.h\"\n+#include \"leveldb/iterator.h\"\n+#include \"leveldb/options.h\"\n+#include \"leveldb/status.h\"\n+#include \"leveldb/write_batch.h\"\n+\n+using leveldb::Cache;\n+using leveldb::Comparator;\n+using leveldb::CompressionType;\n+using leveldb::DB;\n+using leveldb::Env;\n+using leveldb::FileLock;\n+using leveldb::FilterPolicy;\n+using leveldb::Iterator;\n+using leveldb::kMajorVersion;\n+using leveldb::kMinorVersion;\n+using leveldb::Logger;\n+using leveldb::NewBloomFilterPolicy;\n+using leveldb::NewLRUCache;\n+using leveldb::Options;\n+using leveldb::RandomAccessFile;\n+using leveldb::Range;\n+using leveldb::ReadOptions;\n+using leveldb::SequentialFile;\n+using leveldb::Slice;\n+using leveldb::Snapshot;\n+using leveldb::Status;\n+using leveldb::WritableFile;\n+using leveldb::WriteBatch;\n+using leveldb::WriteOptions;\n+\n+extern \"C\" {\n+\n+struct leveldb_t              { DB*               rep; };\n+struct leveldb_iterator_t     { Iterator*         rep; };\n+struct leveldb_writebatch_t   { WriteBatch        rep; };\n+struct leveldb_snapshot_t     { const Snapshot*   rep; };\n+struct leveldb_readoptions_t  { ReadOptions       rep; };\n+struct leveldb_writeoptions_t { WriteOptions      rep; };\n+struct leveldb_options_t      { Options           rep; };\n+struct leveldb_cache_t        { Cache*            rep; };\n+struct leveldb_seqfile_t      { SequentialFile*   rep; };\n+struct leveldb_randomfile_t   { RandomAccessFile* rep; };\n+struct leveldb_writablefile_t { WritableFile*     rep; };\n+struct leveldb_logger_t       { Logger*           rep; };\n+struct leveldb_filelock_t     { FileLock*         rep; };\n+\n+struct leveldb_comparator_t : public Comparator {\n+  void* state_;\n+  void (*destructor_)(void*);\n+  int (*compare_)(\n+      void*,\n+      const char* a, size_t alen,\n+      const char* b, size_t blen);\n+  const char* (*name_)(void*);\n+\n+  virtual ~leveldb_comparator_t() {\n+    (*destructor_)(state_);\n+  }\n+\n+  virtual int Compare(const Slice& a, const Slice& b) const {\n+    return (*compare_)(state_, a.data(), a.size(), b.data(), b.size());\n+  }\n+\n+  virtual const char* Name() const {\n+    return (*name_)(state_);\n+  }\n+\n+  // No-ops since the C binding does not support key shortening methods.\n+  virtual void FindShortestSeparator(std::string*, const Slice&) const { }\n+  virtual void FindShortSuccessor(std::string* key) const { }\n+};\n+\n+struct leveldb_filterpolicy_t : public FilterPolicy {\n+  void* state_;\n+  void (*destructor_)(void*);\n+  const char* (*name_)(void*);\n+  char* (*create_)(\n+      void*,\n+      const char* const* key_array, const size_t* key_length_array,\n+      int num_keys,\n+      size_t* filter_length);\n+  unsigned char (*key_match_)(\n+      void*,\n+      const char* key, size_t length,\n+      const char* filter, size_t filter_length);\n+\n+  virtual ~leveldb_filterpolicy_t() {\n+    (*destructor_)(state_);\n+  }\n+\n+  virtual const char* Name() const {\n+    return (*name_)(state_);\n+  }\n+\n+  virtual void CreateFilter(const Slice* keys, int n, std::string* dst) const {\n+    std::vector<const char*> key_pointers(n);\n+    std::vector<size_t> key_sizes(n);\n+    for (int i = 0; i < n; i++) {\n+      key_pointers[i] = keys[i].data();\n+      key_sizes[i] = keys[i].size();\n+    }\n+    size_t len;\n+    char* filter = (*create_)(state_, &key_pointers[0], &key_sizes[0], n, &len);\n+    dst->append(filter, len);\n+    free(filter);\n+  }\n+\n+  virtual bool KeyMayMatch(const Slice& key, const Slice& filter) const {\n+    return (*key_match_)(state_, key.data(), key.size(),\n+                         filter.data(), filter.size());\n+  }\n+};\n+\n+struct leveldb_env_t {\n+  Env* rep;\n+  bool is_default;\n+};\n+\n+static bool SaveError(char** errptr, const Status& s) {\n+  assert(errptr != NULL);\n+  if (s.ok()) {\n+    return false;\n+  } else if (*errptr == NULL) {\n+    *errptr = strdup(s.ToString().c_str());\n+  } else {\n+    // TODO(sanjay): Merge with existing error?\n+    free(*errptr);\n+    *errptr = strdup(s.ToString().c_str());\n+  }\n+  return true;\n+}\n+\n+static char* CopyString(const std::string& str) {\n+  char* result = reinterpret_cast<char*>(malloc(sizeof(char) * str.size()));\n+  memcpy(result, str.data(), sizeof(char) * str.size());\n+  return result;\n+}\n+\n+leveldb_t* leveldb_open(\n+    const leveldb_options_t* options,\n+    const char* name,\n+    char** errptr) {\n+  DB* db;\n+  if (SaveError(errptr, DB::Open(options->rep, std::string(name), &db))) {\n+    return NULL;\n+  }\n+  leveldb_t* result = new leveldb_t;\n+  result->rep = db;\n+  return result;\n+}\n+\n+void leveldb_close(leveldb_t* db) {\n+  delete db->rep;\n+  delete db;\n+}\n+\n+void leveldb_put(\n+    leveldb_t* db,\n+    const leveldb_writeoptions_t* options,\n+    const char* key, size_t keylen,\n+    const char* val, size_t vallen,\n+    char** errptr) {\n+  SaveError(errptr,\n+            db->rep->Put(options->rep, Slice(key, keylen), Slice(val, vallen)));\n+}\n+\n+void leveldb_delete(\n+    leveldb_t* db,\n+    const leveldb_writeoptions_t* options,\n+    const char* key, size_t keylen,\n+    char** errptr) {\n+  SaveError(errptr, db->rep->Delete(options->rep, Slice(key, keylen)));\n+}\n+\n+\n+void leveldb_write(\n+    leveldb_t* db,\n+    const leveldb_writeoptions_t* options,\n+    leveldb_writebatch_t* batch,\n+    char** errptr) {\n+  SaveError(errptr, db->rep->Write(options->rep, &batch->rep));\n+}\n+\n+char* leveldb_get(\n+    leveldb_t* db,\n+    const leveldb_readoptions_t* options,\n+    const char* key, size_t keylen,\n+    size_t* vallen,\n+    char** errptr) {\n+  char* result = NULL;\n+  std::string tmp;\n+  Status s = db->rep->Get(options->rep, Slice(key, keylen), &tmp);\n+  if (s.ok()) {\n+    *vallen = tmp.size();\n+    result = CopyString(tmp);\n+  } else {\n+    *vallen = 0;\n+    if (!s.IsNotFound()) {\n+      SaveError(errptr, s);\n+    }\n+  }\n+  return result;\n+}\n+\n+leveldb_iterator_t* leveldb_create_iterator(\n+    leveldb_t* db,\n+    const leveldb_readoptions_t* options) {\n+  leveldb_iterator_t* result = new leveldb_iterator_t;\n+  result->rep = db->rep->NewIterator(options->rep);\n+  return result;\n+}\n+\n+const leveldb_snapshot_t* leveldb_create_snapshot(\n+    leveldb_t* db) {\n+  leveldb_snapshot_t* result = new leveldb_snapshot_t;\n+  result->rep = db->rep->GetSnapshot();\n+  return result;\n+}\n+\n+void leveldb_release_snapshot(\n+    leveldb_t* db,\n+    const leveldb_snapshot_t* snapshot) {\n+  db->rep->ReleaseSnapshot(snapshot->rep);\n+  delete snapshot;\n+}\n+\n+char* leveldb_property_value(\n+    leveldb_t* db,\n+    const char* propname) {\n+  std::string tmp;\n+  if (db->rep->GetProperty(Slice(propname), &tmp)) {\n+    // We use strdup() since we expect human readable output.\n+    return strdup(tmp.c_str());\n+  } else {\n+    return NULL;\n+  }\n+}\n+\n+void leveldb_approximate_sizes(\n+    leveldb_t* db,\n+    int num_ranges,\n+    const char* const* range_start_key, const size_t* range_start_key_len,\n+    const char* const* range_limit_key, const size_t* range_limit_key_len,\n+    uint64_t* sizes) {\n+  Range* ranges = new Range[num_ranges];\n+  for (int i = 0; i < num_ranges; i++) {\n+    ranges[i].start = Slice(range_start_key[i], range_start_key_len[i]);\n+    ranges[i].limit = Slice(range_limit_key[i], range_limit_key_len[i]);\n+  }\n+  db->rep->GetApproximateSizes(ranges, num_ranges, sizes);\n+  delete[] ranges;\n+}\n+\n+void leveldb_compact_range(\n+    leveldb_t* db,\n+    const char* start_key, size_t start_key_len,\n+    const char* limit_key, size_t limit_key_len) {\n+  Slice a, b;\n+  db->rep->CompactRange(\n+      // Pass NULL Slice if corresponding \"const char*\" is NULL\n+      (start_key ? (a = Slice(start_key, start_key_len), &a) : NULL),\n+      (limit_key ? (b = Slice(limit_key, limit_key_len), &b) : NULL));\n+}\n+\n+void leveldb_destroy_db(\n+    const leveldb_options_t* options,\n+    const char* name,\n+    char** errptr) {\n+  SaveError(errptr, DestroyDB(name, options->rep));\n+}\n+\n+void leveldb_repair_db(\n+    const leveldb_options_t* options,\n+    const char* name,\n+    char** errptr) {\n+  SaveError(errptr, RepairDB(name, options->rep));\n+}\n+\n+void leveldb_iter_destroy(leveldb_iterator_t* iter) {\n+  delete iter->rep;\n+  delete iter;\n+}\n+\n+unsigned char leveldb_iter_valid(const leveldb_iterator_t* iter) {\n+  return iter->rep->Valid();\n+}\n+\n+void leveldb_iter_seek_to_first(leveldb_iterator_t* iter) {\n+  iter->rep->SeekToFirst();\n+}\n+\n+void leveldb_iter_seek_to_last(leveldb_iterator_t* iter) {\n+  iter->rep->SeekToLast();\n+}\n+\n+void leveldb_iter_seek(leveldb_iterator_t* iter, const char* k, size_t klen) {\n+  iter->rep->Seek(Slice(k, klen));\n+}\n+\n+void leveldb_iter_next(leveldb_iterator_t* iter) {\n+  iter->rep->Next();\n+}\n+\n+void leveldb_iter_prev(leveldb_iterator_t* iter) {\n+  iter->rep->Prev();\n+}\n+\n+const char* leveldb_iter_key(const leveldb_iterator_t* iter, size_t* klen) {\n+  Slice s = iter->rep->key();\n+  *klen = s.size();\n+  return s.data();\n+}\n+\n+const char* leveldb_iter_value(const leveldb_iterator_t* iter, size_t* vlen) {\n+  Slice s = iter->rep->value();\n+  *vlen = s.size();\n+  return s.data();\n+}\n+\n+void leveldb_iter_get_error(const leveldb_iterator_t* iter, char** errptr) {\n+  SaveError(errptr, iter->rep->status());\n+}\n+\n+leveldb_writebatch_t* leveldb_writebatch_create() {\n+  return new leveldb_writebatch_t;\n+}\n+\n+void leveldb_writebatch_destroy(leveldb_writebatch_t* b) {\n+  delete b;\n+}\n+\n+void leveldb_writebatch_clear(leveldb_writebatch_t* b) {\n+  b->rep.Clear();\n+}\n+\n+void leveldb_writebatch_put(\n+    leveldb_writebatch_t* b,\n+    const char* key, size_t klen,\n+    const char* val, size_t vlen) {\n+  b->rep.Put(Slice(key, klen), Slice(val, vlen));\n+}\n+\n+void leveldb_writebatch_delete(\n+    leveldb_writebatch_t* b,\n+    const char* key, size_t klen) {\n+  b->rep.Delete(Slice(key, klen));\n+}\n+\n+void leveldb_writebatch_iterate(\n+    leveldb_writebatch_t* b,\n+    void* state,\n+    void (*put)(void*, const char* k, size_t klen, const char* v, size_t vlen),\n+    void (*deleted)(void*, const char* k, size_t klen)) {\n+  class H : public WriteBatch::Handler {\n+   public:\n+    void* state_;\n+    void (*put_)(void*, const char* k, size_t klen, const char* v, size_t vlen);\n+    void (*deleted_)(void*, const char* k, size_t klen);\n+    virtual void Put(const Slice& key, const Slice& value) {\n+      (*put_)(state_, key.data(), key.size(), value.data(), value.size());\n+    }\n+    virtual void Delete(const Slice& key) {\n+      (*deleted_)(state_, key.data(), key.size());\n+    }\n+  };\n+  H handler;\n+  handler.state_ = state;\n+  handler.put_ = put;\n+  handler.deleted_ = deleted;\n+  b->rep.Iterate(&handler);\n+}\n+\n+leveldb_options_t* leveldb_options_create() {\n+  return new leveldb_options_t;\n+}\n+\n+void leveldb_options_destroy(leveldb_options_t* options) {\n+  delete options;\n+}\n+\n+void leveldb_options_set_comparator(\n+    leveldb_options_t* opt,\n+    leveldb_comparator_t* cmp) {\n+  opt->rep.comparator = cmp;\n+}\n+\n+void leveldb_options_set_filter_policy(\n+    leveldb_options_t* opt,\n+    leveldb_filterpolicy_t* policy) {\n+  opt->rep.filter_policy = policy;\n+}\n+\n+void leveldb_options_set_create_if_missing(\n+    leveldb_options_t* opt, unsigned char v) {\n+  opt->rep.create_if_missing = v;\n+}\n+\n+void leveldb_options_set_error_if_exists(\n+    leveldb_options_t* opt, unsigned char v) {\n+  opt->rep.error_if_exists = v;\n+}\n+\n+void leveldb_options_set_paranoid_checks(\n+    leveldb_options_t* opt, unsigned char v) {\n+  opt->rep.paranoid_checks = v;\n+}\n+\n+void leveldb_options_set_env(leveldb_options_t* opt, leveldb_env_t* env) {\n+  opt->rep.env = (env ? env->rep : NULL);\n+}\n+\n+void leveldb_options_set_info_log(leveldb_options_t* opt, leveldb_logger_t* l) {\n+  opt->rep.info_log = (l ? l->rep : NULL);\n+}\n+\n+void leveldb_options_set_write_buffer_size(leveldb_options_t* opt, size_t s) {\n+  opt->rep.write_buffer_size = s;\n+}\n+\n+void leveldb_options_set_max_open_files(leveldb_options_t* opt, int n) {\n+  opt->rep.max_open_files = n;\n+}\n+\n+void leveldb_options_set_cache(leveldb_options_t* opt, leveldb_cache_t* c) {\n+  opt->rep.block_cache = c->rep;\n+}\n+\n+void leveldb_options_set_block_size(leveldb_options_t* opt, size_t s) {\n+  opt->rep.block_size = s;\n+}\n+\n+void leveldb_options_set_block_restart_interval(leveldb_options_t* opt, int n) {\n+  opt->rep.block_restart_interval = n;\n+}\n+\n+void leveldb_options_set_compression(leveldb_options_t* opt, int t) {\n+  opt->rep.compression = static_cast<CompressionType>(t);\n+}\n+\n+leveldb_comparator_t* leveldb_comparator_create(\n+    void* state,\n+    void (*destructor)(void*),\n+    int (*compare)(\n+        void*,\n+        const char* a, size_t alen,\n+        const char* b, size_t blen),\n+    const char* (*name)(void*)) {\n+  leveldb_comparator_t* result = new leveldb_comparator_t;\n+  result->state_ = state;\n+  result->destructor_ = destructor;\n+  result->compare_ = compare;\n+  result->name_ = name;\n+  return result;\n+}\n+\n+void leveldb_comparator_destroy(leveldb_comparator_t* cmp) {\n+  delete cmp;\n+}\n+\n+leveldb_filterpolicy_t* leveldb_filterpolicy_create(\n+    void* state,\n+    void (*destructor)(void*),\n+    char* (*create_filter)(\n+        void*,\n+        const char* const* key_array, const size_t* key_length_array,\n+        int num_keys,\n+        size_t* filter_length),\n+    unsigned char (*key_may_match)(\n+        void*,\n+        const char* key, size_t length,\n+        const char* filter, size_t filter_length),\n+    const char* (*name)(void*)) {\n+  leveldb_filterpolicy_t* result = new leveldb_filterpolicy_t;\n+  result->state_ = state;\n+  result->destructor_ = destructor;\n+  result->create_ = create_filter;\n+  result->key_match_ = key_may_match;\n+  result->name_ = name;\n+  return result;\n+}\n+\n+void leveldb_filterpolicy_destroy(leveldb_filterpolicy_t* filter) {\n+  delete filter;\n+}\n+\n+leveldb_filterpolicy_t* leveldb_filterpolicy_create_bloom(int bits_per_key) {\n+  // Make a leveldb_filterpolicy_t, but override all of its methods so\n+  // they delegate to a NewBloomFilterPolicy() instead of user\n+  // supplied C functions.\n+  struct Wrapper : public leveldb_filterpolicy_t {\n+    const FilterPolicy* rep_;\n+    ~Wrapper() { delete rep_; }\n+    const char* Name() const { return rep_->Name(); }\n+    void CreateFilter(const Slice* keys, int n, std::string* dst) const {\n+      return rep_->CreateFilter(keys, n, dst);\n+    }\n+    bool KeyMayMatch(const Slice& key, const Slice& filter) const {\n+      return rep_->KeyMayMatch(key, filter);\n+    }\n+    static void DoNothing(void*) { }\n+  };\n+  Wrapper* wrapper = new Wrapper;\n+  wrapper->rep_ = NewBloomFilterPolicy(bits_per_key);\n+  wrapper->state_ = NULL;\n+  wrapper->destructor_ = &Wrapper::DoNothing;\n+  return wrapper;\n+}\n+\n+leveldb_readoptions_t* leveldb_readoptions_create() {\n+  return new leveldb_readoptions_t;\n+}\n+\n+void leveldb_readoptions_destroy(leveldb_readoptions_t* opt) {\n+  delete opt;\n+}\n+\n+void leveldb_readoptions_set_verify_checksums(\n+    leveldb_readoptions_t* opt,\n+    unsigned char v) {\n+  opt->rep.verify_checksums = v;\n+}\n+\n+void leveldb_readoptions_set_fill_cache(\n+    leveldb_readoptions_t* opt, unsigned char v) {\n+  opt->rep.fill_cache = v;\n+}\n+\n+void leveldb_readoptions_set_snapshot(\n+    leveldb_readoptions_t* opt,\n+    const leveldb_snapshot_t* snap) {\n+  opt->rep.snapshot = (snap ? snap->rep : NULL);\n+}\n+\n+leveldb_writeoptions_t* leveldb_writeoptions_create() {\n+  return new leveldb_writeoptions_t;\n+}\n+\n+void leveldb_writeoptions_destroy(leveldb_writeoptions_t* opt) {\n+  delete opt;\n+}\n+\n+void leveldb_writeoptions_set_sync(\n+    leveldb_writeoptions_t* opt, unsigned char v) {\n+  opt->rep.sync = v;\n+}\n+\n+leveldb_cache_t* leveldb_cache_create_lru(size_t capacity) {\n+  leveldb_cache_t* c = new leveldb_cache_t;\n+  c->rep = NewLRUCache(capacity);\n+  return c;\n+}\n+\n+void leveldb_cache_destroy(leveldb_cache_t* cache) {\n+  delete cache->rep;\n+  delete cache;\n+}\n+\n+leveldb_env_t* leveldb_create_default_env() {\n+  leveldb_env_t* result = new leveldb_env_t;\n+  result->rep = Env::Default();\n+  result->is_default = true;\n+  return result;\n+}\n+\n+void leveldb_env_destroy(leveldb_env_t* env) {\n+  if (!env->is_default) delete env->rep;\n+  delete env;\n+}\n+\n+void leveldb_free(void* ptr) {\n+  free(ptr);\n+}\n+\n+int leveldb_major_version() {\n+  return kMajorVersion;\n+}\n+\n+int leveldb_minor_version() {\n+  return kMinorVersion;\n+}\n+\n+}  // end extern \"C\""
      },
      {
        "sha": "7cd5ee02076ab96303a2b1f25dbabbe721caaba7",
        "filename": "src/leveldb/db/c_test.c",
        "status": "added",
        "additions": 390,
        "deletions": 0,
        "changes": 390,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/c_test.c",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/c_test.c",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/c_test.c?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,390 @@\n+/* Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+   Use of this source code is governed by a BSD-style license that can be\n+   found in the LICENSE file. See the AUTHORS file for names of contributors. */\n+\n+#include \"leveldb/c.h\"\n+\n+#include <stddef.h>\n+#include <stdio.h>\n+#include <stdlib.h>\n+#include <string.h>\n+#include <sys/types.h>\n+#include <unistd.h>\n+\n+const char* phase = \"\";\n+static char dbname[200];\n+\n+static void StartPhase(const char* name) {\n+  fprintf(stderr, \"=== Test %s\\n\", name);\n+  phase = name;\n+}\n+\n+static const char* GetTempDir(void) {\n+    const char* ret = getenv(\"TEST_TMPDIR\");\n+    if (ret == NULL || ret[0] == '\\0')\n+        ret = \"/tmp\";\n+    return ret;\n+}\n+\n+#define CheckNoError(err)                                               \\\n+  if ((err) != NULL) {                                                  \\\n+    fprintf(stderr, \"%s:%d: %s: %s\\n\", __FILE__, __LINE__, phase, (err)); \\\n+    abort();                                                            \\\n+  }\n+\n+#define CheckCondition(cond)                                            \\\n+  if (!(cond)) {                                                        \\\n+    fprintf(stderr, \"%s:%d: %s: %s\\n\", __FILE__, __LINE__, phase, #cond); \\\n+    abort();                                                            \\\n+  }\n+\n+static void CheckEqual(const char* expected, const char* v, size_t n) {\n+  if (expected == NULL && v == NULL) {\n+    // ok\n+  } else if (expected != NULL && v != NULL && n == strlen(expected) &&\n+             memcmp(expected, v, n) == 0) {\n+    // ok\n+    return;\n+  } else {\n+    fprintf(stderr, \"%s: expected '%s', got '%s'\\n\",\n+            phase,\n+            (expected ? expected : \"(null)\"),\n+            (v ? v : \"(null\"));\n+    abort();\n+  }\n+}\n+\n+static void Free(char** ptr) {\n+  if (*ptr) {\n+    free(*ptr);\n+    *ptr = NULL;\n+  }\n+}\n+\n+static void CheckGet(\n+    leveldb_t* db,\n+    const leveldb_readoptions_t* options,\n+    const char* key,\n+    const char* expected) {\n+  char* err = NULL;\n+  size_t val_len;\n+  char* val;\n+  val = leveldb_get(db, options, key, strlen(key), &val_len, &err);\n+  CheckNoError(err);\n+  CheckEqual(expected, val, val_len);\n+  Free(&val);\n+}\n+\n+static void CheckIter(leveldb_iterator_t* iter,\n+                      const char* key, const char* val) {\n+  size_t len;\n+  const char* str;\n+  str = leveldb_iter_key(iter, &len);\n+  CheckEqual(key, str, len);\n+  str = leveldb_iter_value(iter, &len);\n+  CheckEqual(val, str, len);\n+}\n+\n+// Callback from leveldb_writebatch_iterate()\n+static void CheckPut(void* ptr,\n+                     const char* k, size_t klen,\n+                     const char* v, size_t vlen) {\n+  int* state = (int*) ptr;\n+  CheckCondition(*state < 2);\n+  switch (*state) {\n+    case 0:\n+      CheckEqual(\"bar\", k, klen);\n+      CheckEqual(\"b\", v, vlen);\n+      break;\n+    case 1:\n+      CheckEqual(\"box\", k, klen);\n+      CheckEqual(\"c\", v, vlen);\n+      break;\n+  }\n+  (*state)++;\n+}\n+\n+// Callback from leveldb_writebatch_iterate()\n+static void CheckDel(void* ptr, const char* k, size_t klen) {\n+  int* state = (int*) ptr;\n+  CheckCondition(*state == 2);\n+  CheckEqual(\"bar\", k, klen);\n+  (*state)++;\n+}\n+\n+static void CmpDestroy(void* arg) { }\n+\n+static int CmpCompare(void* arg, const char* a, size_t alen,\n+                      const char* b, size_t blen) {\n+  int n = (alen < blen) ? alen : blen;\n+  int r = memcmp(a, b, n);\n+  if (r == 0) {\n+    if (alen < blen) r = -1;\n+    else if (alen > blen) r = +1;\n+  }\n+  return r;\n+}\n+\n+static const char* CmpName(void* arg) {\n+  return \"foo\";\n+}\n+\n+// Custom filter policy\n+static unsigned char fake_filter_result = 1;\n+static void FilterDestroy(void* arg) { }\n+static const char* FilterName(void* arg) {\n+  return \"TestFilter\";\n+}\n+static char* FilterCreate(\n+    void* arg,\n+    const char* const* key_array, const size_t* key_length_array,\n+    int num_keys,\n+    size_t* filter_length) {\n+  *filter_length = 4;\n+  char* result = malloc(4);\n+  memcpy(result, \"fake\", 4);\n+  return result;\n+}\n+unsigned char FilterKeyMatch(\n+    void* arg,\n+    const char* key, size_t length,\n+    const char* filter, size_t filter_length) {\n+  CheckCondition(filter_length == 4);\n+  CheckCondition(memcmp(filter, \"fake\", 4) == 0);\n+  return fake_filter_result;\n+}\n+\n+int main(int argc, char** argv) {\n+  leveldb_t* db;\n+  leveldb_comparator_t* cmp;\n+  leveldb_cache_t* cache;\n+  leveldb_env_t* env;\n+  leveldb_options_t* options;\n+  leveldb_readoptions_t* roptions;\n+  leveldb_writeoptions_t* woptions;\n+  char* err = NULL;\n+  int run = -1;\n+\n+  CheckCondition(leveldb_major_version() >= 1);\n+  CheckCondition(leveldb_minor_version() >= 1);\n+\n+  snprintf(dbname, sizeof(dbname),\n+           \"%s/leveldb_c_test-%d\",\n+           GetTempDir(),\n+           ((int) geteuid()));\n+\n+  StartPhase(\"create_objects\");\n+  cmp = leveldb_comparator_create(NULL, CmpDestroy, CmpCompare, CmpName);\n+  env = leveldb_create_default_env();\n+  cache = leveldb_cache_create_lru(100000);\n+\n+  options = leveldb_options_create();\n+  leveldb_options_set_comparator(options, cmp);\n+  leveldb_options_set_error_if_exists(options, 1);\n+  leveldb_options_set_cache(options, cache);\n+  leveldb_options_set_env(options, env);\n+  leveldb_options_set_info_log(options, NULL);\n+  leveldb_options_set_write_buffer_size(options, 100000);\n+  leveldb_options_set_paranoid_checks(options, 1);\n+  leveldb_options_set_max_open_files(options, 10);\n+  leveldb_options_set_block_size(options, 1024);\n+  leveldb_options_set_block_restart_interval(options, 8);\n+  leveldb_options_set_compression(options, leveldb_no_compression);\n+\n+  roptions = leveldb_readoptions_create();\n+  leveldb_readoptions_set_verify_checksums(roptions, 1);\n+  leveldb_readoptions_set_fill_cache(roptions, 0);\n+\n+  woptions = leveldb_writeoptions_create();\n+  leveldb_writeoptions_set_sync(woptions, 1);\n+\n+  StartPhase(\"destroy\");\n+  leveldb_destroy_db(options, dbname, &err);\n+  Free(&err);\n+\n+  StartPhase(\"open_error\");\n+  db = leveldb_open(options, dbname, &err);\n+  CheckCondition(err != NULL);\n+  Free(&err);\n+\n+  StartPhase(\"leveldb_free\");\n+  db = leveldb_open(options, dbname, &err);\n+  CheckCondition(err != NULL);\n+  leveldb_free(err);\n+  err = NULL;\n+\n+  StartPhase(\"open\");\n+  leveldb_options_set_create_if_missing(options, 1);\n+  db = leveldb_open(options, dbname, &err);\n+  CheckNoError(err);\n+  CheckGet(db, roptions, \"foo\", NULL);\n+\n+  StartPhase(\"put\");\n+  leveldb_put(db, woptions, \"foo\", 3, \"hello\", 5, &err);\n+  CheckNoError(err);\n+  CheckGet(db, roptions, \"foo\", \"hello\");\n+\n+  StartPhase(\"compactall\");\n+  leveldb_compact_range(db, NULL, 0, NULL, 0);\n+  CheckGet(db, roptions, \"foo\", \"hello\");\n+\n+  StartPhase(\"compactrange\");\n+  leveldb_compact_range(db, \"a\", 1, \"z\", 1);\n+  CheckGet(db, roptions, \"foo\", \"hello\");\n+\n+  StartPhase(\"writebatch\");\n+  {\n+    leveldb_writebatch_t* wb = leveldb_writebatch_create();\n+    leveldb_writebatch_put(wb, \"foo\", 3, \"a\", 1);\n+    leveldb_writebatch_clear(wb);\n+    leveldb_writebatch_put(wb, \"bar\", 3, \"b\", 1);\n+    leveldb_writebatch_put(wb, \"box\", 3, \"c\", 1);\n+    leveldb_writebatch_delete(wb, \"bar\", 3);\n+    leveldb_write(db, woptions, wb, &err);\n+    CheckNoError(err);\n+    CheckGet(db, roptions, \"foo\", \"hello\");\n+    CheckGet(db, roptions, \"bar\", NULL);\n+    CheckGet(db, roptions, \"box\", \"c\");\n+    int pos = 0;\n+    leveldb_writebatch_iterate(wb, &pos, CheckPut, CheckDel);\n+    CheckCondition(pos == 3);\n+    leveldb_writebatch_destroy(wb);\n+  }\n+\n+  StartPhase(\"iter\");\n+  {\n+    leveldb_iterator_t* iter = leveldb_create_iterator(db, roptions);\n+    CheckCondition(!leveldb_iter_valid(iter));\n+    leveldb_iter_seek_to_first(iter);\n+    CheckCondition(leveldb_iter_valid(iter));\n+    CheckIter(iter, \"box\", \"c\");\n+    leveldb_iter_next(iter);\n+    CheckIter(iter, \"foo\", \"hello\");\n+    leveldb_iter_prev(iter);\n+    CheckIter(iter, \"box\", \"c\");\n+    leveldb_iter_prev(iter);\n+    CheckCondition(!leveldb_iter_valid(iter));\n+    leveldb_iter_seek_to_last(iter);\n+    CheckIter(iter, \"foo\", \"hello\");\n+    leveldb_iter_seek(iter, \"b\", 1);\n+    CheckIter(iter, \"box\", \"c\");\n+    leveldb_iter_get_error(iter, &err);\n+    CheckNoError(err);\n+    leveldb_iter_destroy(iter);\n+  }\n+\n+  StartPhase(\"approximate_sizes\");\n+  {\n+    int i;\n+    int n = 20000;\n+    char keybuf[100];\n+    char valbuf[100];\n+    uint64_t sizes[2];\n+    const char* start[2] = { \"a\", \"k00000000000000010000\" };\n+    size_t start_len[2] = { 1, 21 };\n+    const char* limit[2] = { \"k00000000000000010000\", \"z\" };\n+    size_t limit_len[2] = { 21, 1 };\n+    leveldb_writeoptions_set_sync(woptions, 0);\n+    for (i = 0; i < n; i++) {\n+      snprintf(keybuf, sizeof(keybuf), \"k%020d\", i);\n+      snprintf(valbuf, sizeof(valbuf), \"v%020d\", i);\n+      leveldb_put(db, woptions, keybuf, strlen(keybuf), valbuf, strlen(valbuf),\n+                  &err);\n+      CheckNoError(err);\n+    }\n+    leveldb_approximate_sizes(db, 2, start, start_len, limit, limit_len, sizes);\n+    CheckCondition(sizes[0] > 0);\n+    CheckCondition(sizes[1] > 0);\n+  }\n+\n+  StartPhase(\"property\");\n+  {\n+    char* prop = leveldb_property_value(db, \"nosuchprop\");\n+    CheckCondition(prop == NULL);\n+    prop = leveldb_property_value(db, \"leveldb.stats\");\n+    CheckCondition(prop != NULL);\n+    Free(&prop);\n+  }\n+\n+  StartPhase(\"snapshot\");\n+  {\n+    const leveldb_snapshot_t* snap;\n+    snap = leveldb_create_snapshot(db);\n+    leveldb_delete(db, woptions, \"foo\", 3, &err);\n+    CheckNoError(err);\n+    leveldb_readoptions_set_snapshot(roptions, snap);\n+    CheckGet(db, roptions, \"foo\", \"hello\");\n+    leveldb_readoptions_set_snapshot(roptions, NULL);\n+    CheckGet(db, roptions, \"foo\", NULL);\n+    leveldb_release_snapshot(db, snap);\n+  }\n+\n+  StartPhase(\"repair\");\n+  {\n+    leveldb_close(db);\n+    leveldb_options_set_create_if_missing(options, 0);\n+    leveldb_options_set_error_if_exists(options, 0);\n+    leveldb_repair_db(options, dbname, &err);\n+    CheckNoError(err);\n+    db = leveldb_open(options, dbname, &err);\n+    CheckNoError(err);\n+    CheckGet(db, roptions, \"foo\", NULL);\n+    CheckGet(db, roptions, \"bar\", NULL);\n+    CheckGet(db, roptions, \"box\", \"c\");\n+    leveldb_options_set_create_if_missing(options, 1);\n+    leveldb_options_set_error_if_exists(options, 1);\n+  }\n+\n+  StartPhase(\"filter\");\n+  for (run = 0; run < 2; run++) {\n+    // First run uses custom filter, second run uses bloom filter\n+    CheckNoError(err);\n+    leveldb_filterpolicy_t* policy;\n+    if (run == 0) {\n+      policy = leveldb_filterpolicy_create(\n+          NULL, FilterDestroy, FilterCreate, FilterKeyMatch, FilterName);\n+    } else {\n+      policy = leveldb_filterpolicy_create_bloom(10);\n+    }\n+\n+    // Create new database\n+    leveldb_close(db);\n+    leveldb_destroy_db(options, dbname, &err);\n+    leveldb_options_set_filter_policy(options, policy);\n+    db = leveldb_open(options, dbname, &err);\n+    CheckNoError(err);\n+    leveldb_put(db, woptions, \"foo\", 3, \"foovalue\", 8, &err);\n+    CheckNoError(err);\n+    leveldb_put(db, woptions, \"bar\", 3, \"barvalue\", 8, &err);\n+    CheckNoError(err);\n+    leveldb_compact_range(db, NULL, 0, NULL, 0);\n+\n+    fake_filter_result = 1;\n+    CheckGet(db, roptions, \"foo\", \"foovalue\");\n+    CheckGet(db, roptions, \"bar\", \"barvalue\");\n+    if (phase == 0) {\n+      // Must not find value when custom filter returns false\n+      fake_filter_result = 0;\n+      CheckGet(db, roptions, \"foo\", NULL);\n+      CheckGet(db, roptions, \"bar\", NULL);\n+      fake_filter_result = 1;\n+\n+      CheckGet(db, roptions, \"foo\", \"foovalue\");\n+      CheckGet(db, roptions, \"bar\", \"barvalue\");\n+    }\n+    leveldb_options_set_filter_policy(options, NULL);\n+    leveldb_filterpolicy_destroy(policy);\n+  }\n+\n+  StartPhase(\"cleanup\");\n+  leveldb_close(db);\n+  leveldb_options_destroy(options);\n+  leveldb_readoptions_destroy(roptions);\n+  leveldb_writeoptions_destroy(woptions);\n+  leveldb_cache_destroy(cache);\n+  leveldb_comparator_destroy(cmp);\n+  leveldb_env_destroy(env);\n+\n+  fprintf(stderr, \"PASS\\n\");\n+  return 0;\n+}"
      },
      {
        "sha": "31b2d5f4166bc200122c1773b1a3e7fdcff7c640",
        "filename": "src/leveldb/db/corruption_test.cc",
        "status": "added",
        "additions": 359,
        "deletions": 0,
        "changes": 359,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/corruption_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/corruption_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/corruption_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,359 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"leveldb/db.h\"\n+\n+#include <errno.h>\n+#include <fcntl.h>\n+#include <sys/stat.h>\n+#include <sys/types.h>\n+#include \"leveldb/cache.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/table.h\"\n+#include \"leveldb/write_batch.h\"\n+#include \"db/db_impl.h\"\n+#include \"db/filename.h\"\n+#include \"db/log_format.h\"\n+#include \"db/version_set.h\"\n+#include \"util/logging.h\"\n+#include \"util/testharness.h\"\n+#include \"util/testutil.h\"\n+\n+namespace leveldb {\n+\n+static const int kValueSize = 1000;\n+\n+class CorruptionTest {\n+ public:\n+  test::ErrorEnv env_;\n+  std::string dbname_;\n+  Cache* tiny_cache_;\n+  Options options_;\n+  DB* db_;\n+\n+  CorruptionTest() {\n+    tiny_cache_ = NewLRUCache(100);\n+    options_.env = &env_;\n+    dbname_ = test::TmpDir() + \"/db_test\";\n+    DestroyDB(dbname_, options_);\n+\n+    db_ = NULL;\n+    options_.create_if_missing = true;\n+    Reopen();\n+    options_.create_if_missing = false;\n+  }\n+\n+  ~CorruptionTest() {\n+     delete db_;\n+     DestroyDB(dbname_, Options());\n+     delete tiny_cache_;\n+  }\n+\n+  Status TryReopen(Options* options = NULL) {\n+    delete db_;\n+    db_ = NULL;\n+    Options opt = (options ? *options : options_);\n+    opt.env = &env_;\n+    opt.block_cache = tiny_cache_;\n+    return DB::Open(opt, dbname_, &db_);\n+  }\n+\n+  void Reopen(Options* options = NULL) {\n+    ASSERT_OK(TryReopen(options));\n+  }\n+\n+  void RepairDB() {\n+    delete db_;\n+    db_ = NULL;\n+    ASSERT_OK(::leveldb::RepairDB(dbname_, options_));\n+  }\n+\n+  void Build(int n) {\n+    std::string key_space, value_space;\n+    WriteBatch batch;\n+    for (int i = 0; i < n; i++) {\n+      //if ((i % 100) == 0) fprintf(stderr, \"@ %d of %d\\n\", i, n);\n+      Slice key = Key(i, &key_space);\n+      batch.Clear();\n+      batch.Put(key, Value(i, &value_space));\n+      ASSERT_OK(db_->Write(WriteOptions(), &batch));\n+    }\n+  }\n+\n+  void Check(int min_expected, int max_expected) {\n+    int next_expected = 0;\n+    int missed = 0;\n+    int bad_keys = 0;\n+    int bad_values = 0;\n+    int correct = 0;\n+    std::string value_space;\n+    Iterator* iter = db_->NewIterator(ReadOptions());\n+    for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+      uint64_t key;\n+      Slice in(iter->key());\n+      if (!ConsumeDecimalNumber(&in, &key) ||\n+          !in.empty() ||\n+          key < next_expected) {\n+        bad_keys++;\n+        continue;\n+      }\n+      missed += (key - next_expected);\n+      next_expected = key + 1;\n+      if (iter->value() != Value(key, &value_space)) {\n+        bad_values++;\n+      } else {\n+        correct++;\n+      }\n+    }\n+    delete iter;\n+\n+    fprintf(stderr,\n+            \"expected=%d..%d; got=%d; bad_keys=%d; bad_values=%d; missed=%d\\n\",\n+            min_expected, max_expected, correct, bad_keys, bad_values, missed);\n+    ASSERT_LE(min_expected, correct);\n+    ASSERT_GE(max_expected, correct);\n+  }\n+\n+  void Corrupt(FileType filetype, int offset, int bytes_to_corrupt) {\n+    // Pick file to corrupt\n+    std::vector<std::string> filenames;\n+    ASSERT_OK(env_.GetChildren(dbname_, &filenames));\n+    uint64_t number;\n+    FileType type;\n+    std::string fname;\n+    int picked_number = -1;\n+    for (int i = 0; i < filenames.size(); i++) {\n+      if (ParseFileName(filenames[i], &number, &type) &&\n+          type == filetype &&\n+          int(number) > picked_number) {  // Pick latest file\n+        fname = dbname_ + \"/\" + filenames[i];\n+        picked_number = number;\n+      }\n+    }\n+    ASSERT_TRUE(!fname.empty()) << filetype;\n+\n+    struct stat sbuf;\n+    if (stat(fname.c_str(), &sbuf) != 0) {\n+      const char* msg = strerror(errno);\n+      ASSERT_TRUE(false) << fname << \": \" << msg;\n+    }\n+\n+    if (offset < 0) {\n+      // Relative to end of file; make it absolute\n+      if (-offset > sbuf.st_size) {\n+        offset = 0;\n+      } else {\n+        offset = sbuf.st_size + offset;\n+      }\n+    }\n+    if (offset > sbuf.st_size) {\n+      offset = sbuf.st_size;\n+    }\n+    if (offset + bytes_to_corrupt > sbuf.st_size) {\n+      bytes_to_corrupt = sbuf.st_size - offset;\n+    }\n+\n+    // Do it\n+    std::string contents;\n+    Status s = ReadFileToString(Env::Default(), fname, &contents);\n+    ASSERT_TRUE(s.ok()) << s.ToString();\n+    for (int i = 0; i < bytes_to_corrupt; i++) {\n+      contents[i + offset] ^= 0x80;\n+    }\n+    s = WriteStringToFile(Env::Default(), contents, fname);\n+    ASSERT_TRUE(s.ok()) << s.ToString();\n+  }\n+\n+  int Property(const std::string& name) {\n+    std::string property;\n+    int result;\n+    if (db_->GetProperty(name, &property) &&\n+        sscanf(property.c_str(), \"%d\", &result) == 1) {\n+      return result;\n+    } else {\n+      return -1;\n+    }\n+  }\n+\n+  // Return the ith key\n+  Slice Key(int i, std::string* storage) {\n+    char buf[100];\n+    snprintf(buf, sizeof(buf), \"%016d\", i);\n+    storage->assign(buf, strlen(buf));\n+    return Slice(*storage);\n+  }\n+\n+  // Return the value to associate with the specified key\n+  Slice Value(int k, std::string* storage) {\n+    Random r(k);\n+    return test::RandomString(&r, kValueSize, storage);\n+  }\n+};\n+\n+TEST(CorruptionTest, Recovery) {\n+  Build(100);\n+  Check(100, 100);\n+  Corrupt(kLogFile, 19, 1);      // WriteBatch tag for first record\n+  Corrupt(kLogFile, log::kBlockSize + 1000, 1);  // Somewhere in second block\n+  Reopen();\n+\n+  // The 64 records in the first two log blocks are completely lost.\n+  Check(36, 36);\n+}\n+\n+TEST(CorruptionTest, RecoverWriteError) {\n+  env_.writable_file_error_ = true;\n+  Status s = TryReopen();\n+  ASSERT_TRUE(!s.ok());\n+}\n+\n+TEST(CorruptionTest, NewFileErrorDuringWrite) {\n+  // Do enough writing to force minor compaction\n+  env_.writable_file_error_ = true;\n+  const int num = 3 + (Options().write_buffer_size / kValueSize);\n+  std::string value_storage;\n+  Status s;\n+  for (int i = 0; s.ok() && i < num; i++) {\n+    WriteBatch batch;\n+    batch.Put(\"a\", Value(100, &value_storage));\n+    s = db_->Write(WriteOptions(), &batch);\n+  }\n+  ASSERT_TRUE(!s.ok());\n+  ASSERT_GE(env_.num_writable_file_errors_, 1);\n+  env_.writable_file_error_ = false;\n+  Reopen();\n+}\n+\n+TEST(CorruptionTest, TableFile) {\n+  Build(100);\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n+  dbi->TEST_CompactMemTable();\n+  dbi->TEST_CompactRange(0, NULL, NULL);\n+  dbi->TEST_CompactRange(1, NULL, NULL);\n+\n+  Corrupt(kTableFile, 100, 1);\n+  Check(99, 99);\n+}\n+\n+TEST(CorruptionTest, TableFileIndexData) {\n+  Build(10000);  // Enough to build multiple Tables\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n+  dbi->TEST_CompactMemTable();\n+\n+  Corrupt(kTableFile, -2000, 500);\n+  Reopen();\n+  Check(5000, 9999);\n+}\n+\n+TEST(CorruptionTest, MissingDescriptor) {\n+  Build(1000);\n+  RepairDB();\n+  Reopen();\n+  Check(1000, 1000);\n+}\n+\n+TEST(CorruptionTest, SequenceNumberRecovery) {\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v1\"));\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v2\"));\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v3\"));\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v4\"));\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v5\"));\n+  RepairDB();\n+  Reopen();\n+  std::string v;\n+  ASSERT_OK(db_->Get(ReadOptions(), \"foo\", &v));\n+  ASSERT_EQ(\"v5\", v);\n+  // Write something.  If sequence number was not recovered properly,\n+  // it will be hidden by an earlier write.\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v6\"));\n+  ASSERT_OK(db_->Get(ReadOptions(), \"foo\", &v));\n+  ASSERT_EQ(\"v6\", v);\n+  Reopen();\n+  ASSERT_OK(db_->Get(ReadOptions(), \"foo\", &v));\n+  ASSERT_EQ(\"v6\", v);\n+}\n+\n+TEST(CorruptionTest, CorruptedDescriptor) {\n+  ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"hello\"));\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n+  dbi->TEST_CompactMemTable();\n+  dbi->TEST_CompactRange(0, NULL, NULL);\n+\n+  Corrupt(kDescriptorFile, 0, 1000);\n+  Status s = TryReopen();\n+  ASSERT_TRUE(!s.ok());\n+\n+  RepairDB();\n+  Reopen();\n+  std::string v;\n+  ASSERT_OK(db_->Get(ReadOptions(), \"foo\", &v));\n+  ASSERT_EQ(\"hello\", v);\n+}\n+\n+TEST(CorruptionTest, CompactionInputError) {\n+  Build(10);\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n+  dbi->TEST_CompactMemTable();\n+  const int last = config::kMaxMemCompactLevel;\n+  ASSERT_EQ(1, Property(\"leveldb.num-files-at-level\" + NumberToString(last)));\n+\n+  Corrupt(kTableFile, 100, 1);\n+  Check(9, 9);\n+\n+  // Force compactions by writing lots of values\n+  Build(10000);\n+  Check(10000, 10000);\n+}\n+\n+TEST(CorruptionTest, CompactionInputErrorParanoid) {\n+  Options options;\n+  options.paranoid_checks = true;\n+  options.write_buffer_size = 1048576;\n+  Reopen(&options);\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n+\n+  // Fill levels >= 1 so memtable compaction outputs to level 1\n+  for (int level = 1; level < config::kNumLevels; level++) {\n+    dbi->Put(WriteOptions(), \"\", \"begin\");\n+    dbi->Put(WriteOptions(), \"~\", \"end\");\n+    dbi->TEST_CompactMemTable();\n+  }\n+\n+  Build(10);\n+  dbi->TEST_CompactMemTable();\n+  ASSERT_EQ(1, Property(\"leveldb.num-files-at-level0\"));\n+\n+  Corrupt(kTableFile, 100, 1);\n+  Check(9, 9);\n+\n+  // Write must eventually fail because of corrupted table\n+  Status s;\n+  std::string tmp1, tmp2;\n+  for (int i = 0; i < 10000 && s.ok(); i++) {\n+    s = db_->Put(WriteOptions(), Key(i, &tmp1), Value(i, &tmp2));\n+  }\n+  ASSERT_TRUE(!s.ok()) << \"write did not fail in corrupted paranoid db\";\n+}\n+\n+TEST(CorruptionTest, UnrelatedKeys) {\n+  Build(10);\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n+  dbi->TEST_CompactMemTable();\n+  Corrupt(kTableFile, 100, 1);\n+\n+  std::string tmp1, tmp2;\n+  ASSERT_OK(db_->Put(WriteOptions(), Key(1000, &tmp1), Value(1000, &tmp2)));\n+  std::string v;\n+  ASSERT_OK(db_->Get(ReadOptions(), Key(1000, &tmp1), &v));\n+  ASSERT_EQ(Value(1000, &tmp2).ToString(), v);\n+  dbi->TEST_CompactMemTable();\n+  ASSERT_OK(db_->Get(ReadOptions(), Key(1000, &tmp1), &v));\n+  ASSERT_EQ(Value(1000, &tmp2).ToString(), v);\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "7abdf87587dfc8d4fa0287d8ad45d82b2142d7fc",
        "filename": "src/leveldb/db/db_bench.cc",
        "status": "added",
        "additions": 979,
        "deletions": 0,
        "changes": 979,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/db_bench.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/db_bench.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_bench.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,979 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include <sys/types.h>\n+#include <stdio.h>\n+#include <stdlib.h>\n+#include \"db/db_impl.h\"\n+#include \"db/version_set.h\"\n+#include \"leveldb/cache.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/write_batch.h\"\n+#include \"port/port.h\"\n+#include \"util/crc32c.h\"\n+#include \"util/histogram.h\"\n+#include \"util/mutexlock.h\"\n+#include \"util/random.h\"\n+#include \"util/testutil.h\"\n+\n+// Comma-separated list of operations to run in the specified order\n+//   Actual benchmarks:\n+//      fillseq       -- write N values in sequential key order in async mode\n+//      fillrandom    -- write N values in random key order in async mode\n+//      overwrite     -- overwrite N values in random key order in async mode\n+//      fillsync      -- write N/100 values in random key order in sync mode\n+//      fill100K      -- write N/1000 100K values in random order in async mode\n+//      deleteseq     -- delete N keys in sequential order\n+//      deleterandom  -- delete N keys in random order\n+//      readseq       -- read N times sequentially\n+//      readreverse   -- read N times in reverse order\n+//      readrandom    -- read N times in random order\n+//      readmissing   -- read N missing keys in random order\n+//      readhot       -- read N times in random order from 1% section of DB\n+//      seekrandom    -- N random seeks\n+//      crc32c        -- repeated crc32c of 4K of data\n+//      acquireload   -- load N*1000 times\n+//   Meta operations:\n+//      compact     -- Compact the entire DB\n+//      stats       -- Print DB stats\n+//      sstables    -- Print sstable info\n+//      heapprofile -- Dump a heap profile (if supported by this port)\n+static const char* FLAGS_benchmarks =\n+    \"fillseq,\"\n+    \"fillsync,\"\n+    \"fillrandom,\"\n+    \"overwrite,\"\n+    \"readrandom,\"\n+    \"readrandom,\"  // Extra run to allow previous compactions to quiesce\n+    \"readseq,\"\n+    \"readreverse,\"\n+    \"compact,\"\n+    \"readrandom,\"\n+    \"readseq,\"\n+    \"readreverse,\"\n+    \"fill100K,\"\n+    \"crc32c,\"\n+    \"snappycomp,\"\n+    \"snappyuncomp,\"\n+    \"acquireload,\"\n+    ;\n+\n+// Number of key/values to place in database\n+static int FLAGS_num = 1000000;\n+\n+// Number of read operations to do.  If negative, do FLAGS_num reads.\n+static int FLAGS_reads = -1;\n+\n+// Number of concurrent threads to run.\n+static int FLAGS_threads = 1;\n+\n+// Size of each value\n+static int FLAGS_value_size = 100;\n+\n+// Arrange to generate values that shrink to this fraction of\n+// their original size after compression\n+static double FLAGS_compression_ratio = 0.5;\n+\n+// Print histogram of operation timings\n+static bool FLAGS_histogram = false;\n+\n+// Number of bytes to buffer in memtable before compacting\n+// (initialized to default value by \"main\")\n+static int FLAGS_write_buffer_size = 0;\n+\n+// Number of bytes to use as a cache of uncompressed data.\n+// Negative means use default settings.\n+static int FLAGS_cache_size = -1;\n+\n+// Maximum number of files to keep open at the same time (use default if == 0)\n+static int FLAGS_open_files = 0;\n+\n+// Bloom filter bits per key.\n+// Negative means use default settings.\n+static int FLAGS_bloom_bits = -1;\n+\n+// If true, do not destroy the existing database.  If you set this\n+// flag and also specify a benchmark that wants a fresh database, that\n+// benchmark will fail.\n+static bool FLAGS_use_existing_db = false;\n+\n+// Use the db with the following name.\n+static const char* FLAGS_db = NULL;\n+\n+namespace leveldb {\n+\n+namespace {\n+\n+// Helper for quickly generating random data.\n+class RandomGenerator {\n+ private:\n+  std::string data_;\n+  int pos_;\n+\n+ public:\n+  RandomGenerator() {\n+    // We use a limited amount of data over and over again and ensure\n+    // that it is larger than the compression window (32KB), and also\n+    // large enough to serve all typical value sizes we want to write.\n+    Random rnd(301);\n+    std::string piece;\n+    while (data_.size() < 1048576) {\n+      // Add a short fragment that is as compressible as specified\n+      // by FLAGS_compression_ratio.\n+      test::CompressibleString(&rnd, FLAGS_compression_ratio, 100, &piece);\n+      data_.append(piece);\n+    }\n+    pos_ = 0;\n+  }\n+\n+  Slice Generate(int len) {\n+    if (pos_ + len > data_.size()) {\n+      pos_ = 0;\n+      assert(len < data_.size());\n+    }\n+    pos_ += len;\n+    return Slice(data_.data() + pos_ - len, len);\n+  }\n+};\n+\n+static Slice TrimSpace(Slice s) {\n+  int start = 0;\n+  while (start < s.size() && isspace(s[start])) {\n+    start++;\n+  }\n+  int limit = s.size();\n+  while (limit > start && isspace(s[limit-1])) {\n+    limit--;\n+  }\n+  return Slice(s.data() + start, limit - start);\n+}\n+\n+static void AppendWithSpace(std::string* str, Slice msg) {\n+  if (msg.empty()) return;\n+  if (!str->empty()) {\n+    str->push_back(' ');\n+  }\n+  str->append(msg.data(), msg.size());\n+}\n+\n+class Stats {\n+ private:\n+  double start_;\n+  double finish_;\n+  double seconds_;\n+  int done_;\n+  int next_report_;\n+  int64_t bytes_;\n+  double last_op_finish_;\n+  Histogram hist_;\n+  std::string message_;\n+\n+ public:\n+  Stats() { Start(); }\n+\n+  void Start() {\n+    next_report_ = 100;\n+    last_op_finish_ = start_;\n+    hist_.Clear();\n+    done_ = 0;\n+    bytes_ = 0;\n+    seconds_ = 0;\n+    start_ = Env::Default()->NowMicros();\n+    finish_ = start_;\n+    message_.clear();\n+  }\n+\n+  void Merge(const Stats& other) {\n+    hist_.Merge(other.hist_);\n+    done_ += other.done_;\n+    bytes_ += other.bytes_;\n+    seconds_ += other.seconds_;\n+    if (other.start_ < start_) start_ = other.start_;\n+    if (other.finish_ > finish_) finish_ = other.finish_;\n+\n+    // Just keep the messages from one thread\n+    if (message_.empty()) message_ = other.message_;\n+  }\n+\n+  void Stop() {\n+    finish_ = Env::Default()->NowMicros();\n+    seconds_ = (finish_ - start_) * 1e-6;\n+  }\n+\n+  void AddMessage(Slice msg) {\n+    AppendWithSpace(&message_, msg);\n+  }\n+\n+  void FinishedSingleOp() {\n+    if (FLAGS_histogram) {\n+      double now = Env::Default()->NowMicros();\n+      double micros = now - last_op_finish_;\n+      hist_.Add(micros);\n+      if (micros > 20000) {\n+        fprintf(stderr, \"long op: %.1f micros%30s\\r\", micros, \"\");\n+        fflush(stderr);\n+      }\n+      last_op_finish_ = now;\n+    }\n+\n+    done_++;\n+    if (done_ >= next_report_) {\n+      if      (next_report_ < 1000)   next_report_ += 100;\n+      else if (next_report_ < 5000)   next_report_ += 500;\n+      else if (next_report_ < 10000)  next_report_ += 1000;\n+      else if (next_report_ < 50000)  next_report_ += 5000;\n+      else if (next_report_ < 100000) next_report_ += 10000;\n+      else if (next_report_ < 500000) next_report_ += 50000;\n+      else                            next_report_ += 100000;\n+      fprintf(stderr, \"... finished %d ops%30s\\r\", done_, \"\");\n+      fflush(stderr);\n+    }\n+  }\n+\n+  void AddBytes(int64_t n) {\n+    bytes_ += n;\n+  }\n+\n+  void Report(const Slice& name) {\n+    // Pretend at least one op was done in case we are running a benchmark\n+    // that does not call FinishedSingleOp().\n+    if (done_ < 1) done_ = 1;\n+\n+    std::string extra;\n+    if (bytes_ > 0) {\n+      // Rate is computed on actual elapsed time, not the sum of per-thread\n+      // elapsed times.\n+      double elapsed = (finish_ - start_) * 1e-6;\n+      char rate[100];\n+      snprintf(rate, sizeof(rate), \"%6.1f MB/s\",\n+               (bytes_ / 1048576.0) / elapsed);\n+      extra = rate;\n+    }\n+    AppendWithSpace(&extra, message_);\n+\n+    fprintf(stdout, \"%-12s : %11.3f micros/op;%s%s\\n\",\n+            name.ToString().c_str(),\n+            seconds_ * 1e6 / done_,\n+            (extra.empty() ? \"\" : \" \"),\n+            extra.c_str());\n+    if (FLAGS_histogram) {\n+      fprintf(stdout, \"Microseconds per op:\\n%s\\n\", hist_.ToString().c_str());\n+    }\n+    fflush(stdout);\n+  }\n+};\n+\n+// State shared by all concurrent executions of the same benchmark.\n+struct SharedState {\n+  port::Mutex mu;\n+  port::CondVar cv;\n+  int total;\n+\n+  // Each thread goes through the following states:\n+  //    (1) initializing\n+  //    (2) waiting for others to be initialized\n+  //    (3) running\n+  //    (4) done\n+\n+  int num_initialized;\n+  int num_done;\n+  bool start;\n+\n+  SharedState() : cv(&mu) { }\n+};\n+\n+// Per-thread state for concurrent executions of the same benchmark.\n+struct ThreadState {\n+  int tid;             // 0..n-1 when running in n threads\n+  Random rand;         // Has different seeds for different threads\n+  Stats stats;\n+  SharedState* shared;\n+\n+  ThreadState(int index)\n+      : tid(index),\n+        rand(1000 + index) {\n+  }\n+};\n+\n+}  // namespace\n+\n+class Benchmark {\n+ private:\n+  Cache* cache_;\n+  const FilterPolicy* filter_policy_;\n+  DB* db_;\n+  int num_;\n+  int value_size_;\n+  int entries_per_batch_;\n+  WriteOptions write_options_;\n+  int reads_;\n+  int heap_counter_;\n+\n+  void PrintHeader() {\n+    const int kKeySize = 16;\n+    PrintEnvironment();\n+    fprintf(stdout, \"Keys:       %d bytes each\\n\", kKeySize);\n+    fprintf(stdout, \"Values:     %d bytes each (%d bytes after compression)\\n\",\n+            FLAGS_value_size,\n+            static_cast<int>(FLAGS_value_size * FLAGS_compression_ratio + 0.5));\n+    fprintf(stdout, \"Entries:    %d\\n\", num_);\n+    fprintf(stdout, \"RawSize:    %.1f MB (estimated)\\n\",\n+            ((static_cast<int64_t>(kKeySize + FLAGS_value_size) * num_)\n+             / 1048576.0));\n+    fprintf(stdout, \"FileSize:   %.1f MB (estimated)\\n\",\n+            (((kKeySize + FLAGS_value_size * FLAGS_compression_ratio) * num_)\n+             / 1048576.0));\n+    PrintWarnings();\n+    fprintf(stdout, \"------------------------------------------------\\n\");\n+  }\n+\n+  void PrintWarnings() {\n+#if defined(__GNUC__) && !defined(__OPTIMIZE__)\n+    fprintf(stdout,\n+            \"WARNING: Optimization is disabled: benchmarks unnecessarily slow\\n\"\n+            );\n+#endif\n+#ifndef NDEBUG\n+    fprintf(stdout,\n+            \"WARNING: Assertions are enabled; benchmarks unnecessarily slow\\n\");\n+#endif\n+\n+    // See if snappy is working by attempting to compress a compressible string\n+    const char text[] = \"yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\";\n+    std::string compressed;\n+    if (!port::Snappy_Compress(text, sizeof(text), &compressed)) {\n+      fprintf(stdout, \"WARNING: Snappy compression is not enabled\\n\");\n+    } else if (compressed.size() >= sizeof(text)) {\n+      fprintf(stdout, \"WARNING: Snappy compression is not effective\\n\");\n+    }\n+  }\n+\n+  void PrintEnvironment() {\n+    fprintf(stderr, \"LevelDB:    version %d.%d\\n\",\n+            kMajorVersion, kMinorVersion);\n+\n+#if defined(__linux)\n+    time_t now = time(NULL);\n+    fprintf(stderr, \"Date:       %s\", ctime(&now));  // ctime() adds newline\n+\n+    FILE* cpuinfo = fopen(\"/proc/cpuinfo\", \"r\");\n+    if (cpuinfo != NULL) {\n+      char line[1000];\n+      int num_cpus = 0;\n+      std::string cpu_type;\n+      std::string cache_size;\n+      while (fgets(line, sizeof(line), cpuinfo) != NULL) {\n+        const char* sep = strchr(line, ':');\n+        if (sep == NULL) {\n+          continue;\n+        }\n+        Slice key = TrimSpace(Slice(line, sep - 1 - line));\n+        Slice val = TrimSpace(Slice(sep + 1));\n+        if (key == \"model name\") {\n+          ++num_cpus;\n+          cpu_type = val.ToString();\n+        } else if (key == \"cache size\") {\n+          cache_size = val.ToString();\n+        }\n+      }\n+      fclose(cpuinfo);\n+      fprintf(stderr, \"CPU:        %d * %s\\n\", num_cpus, cpu_type.c_str());\n+      fprintf(stderr, \"CPUCache:   %s\\n\", cache_size.c_str());\n+    }\n+#endif\n+  }\n+\n+ public:\n+  Benchmark()\n+  : cache_(FLAGS_cache_size >= 0 ? NewLRUCache(FLAGS_cache_size) : NULL),\n+    filter_policy_(FLAGS_bloom_bits >= 0\n+                   ? NewBloomFilterPolicy(FLAGS_bloom_bits)\n+                   : NULL),\n+    db_(NULL),\n+    num_(FLAGS_num),\n+    value_size_(FLAGS_value_size),\n+    entries_per_batch_(1),\n+    reads_(FLAGS_reads < 0 ? FLAGS_num : FLAGS_reads),\n+    heap_counter_(0) {\n+    std::vector<std::string> files;\n+    Env::Default()->GetChildren(FLAGS_db, &files);\n+    for (int i = 0; i < files.size(); i++) {\n+      if (Slice(files[i]).starts_with(\"heap-\")) {\n+        Env::Default()->DeleteFile(std::string(FLAGS_db) + \"/\" + files[i]);\n+      }\n+    }\n+    if (!FLAGS_use_existing_db) {\n+      DestroyDB(FLAGS_db, Options());\n+    }\n+  }\n+\n+  ~Benchmark() {\n+    delete db_;\n+    delete cache_;\n+    delete filter_policy_;\n+  }\n+\n+  void Run() {\n+    PrintHeader();\n+    Open();\n+\n+    const char* benchmarks = FLAGS_benchmarks;\n+    while (benchmarks != NULL) {\n+      const char* sep = strchr(benchmarks, ',');\n+      Slice name;\n+      if (sep == NULL) {\n+        name = benchmarks;\n+        benchmarks = NULL;\n+      } else {\n+        name = Slice(benchmarks, sep - benchmarks);\n+        benchmarks = sep + 1;\n+      }\n+\n+      // Reset parameters that may be overriddden bwlow\n+      num_ = FLAGS_num;\n+      reads_ = (FLAGS_reads < 0 ? FLAGS_num : FLAGS_reads);\n+      value_size_ = FLAGS_value_size;\n+      entries_per_batch_ = 1;\n+      write_options_ = WriteOptions();\n+\n+      void (Benchmark::*method)(ThreadState*) = NULL;\n+      bool fresh_db = false;\n+      int num_threads = FLAGS_threads;\n+\n+      if (name == Slice(\"fillseq\")) {\n+        fresh_db = true;\n+        method = &Benchmark::WriteSeq;\n+      } else if (name == Slice(\"fillbatch\")) {\n+        fresh_db = true;\n+        entries_per_batch_ = 1000;\n+        method = &Benchmark::WriteSeq;\n+      } else if (name == Slice(\"fillrandom\")) {\n+        fresh_db = true;\n+        method = &Benchmark::WriteRandom;\n+      } else if (name == Slice(\"overwrite\")) {\n+        fresh_db = false;\n+        method = &Benchmark::WriteRandom;\n+      } else if (name == Slice(\"fillsync\")) {\n+        fresh_db = true;\n+        num_ /= 1000;\n+        write_options_.sync = true;\n+        method = &Benchmark::WriteRandom;\n+      } else if (name == Slice(\"fill100K\")) {\n+        fresh_db = true;\n+        num_ /= 1000;\n+        value_size_ = 100 * 1000;\n+        method = &Benchmark::WriteRandom;\n+      } else if (name == Slice(\"readseq\")) {\n+        method = &Benchmark::ReadSequential;\n+      } else if (name == Slice(\"readreverse\")) {\n+        method = &Benchmark::ReadReverse;\n+      } else if (name == Slice(\"readrandom\")) {\n+        method = &Benchmark::ReadRandom;\n+      } else if (name == Slice(\"readmissing\")) {\n+        method = &Benchmark::ReadMissing;\n+      } else if (name == Slice(\"seekrandom\")) {\n+        method = &Benchmark::SeekRandom;\n+      } else if (name == Slice(\"readhot\")) {\n+        method = &Benchmark::ReadHot;\n+      } else if (name == Slice(\"readrandomsmall\")) {\n+        reads_ /= 1000;\n+        method = &Benchmark::ReadRandom;\n+      } else if (name == Slice(\"deleteseq\")) {\n+        method = &Benchmark::DeleteSeq;\n+      } else if (name == Slice(\"deleterandom\")) {\n+        method = &Benchmark::DeleteRandom;\n+      } else if (name == Slice(\"readwhilewriting\")) {\n+        num_threads++;  // Add extra thread for writing\n+        method = &Benchmark::ReadWhileWriting;\n+      } else if (name == Slice(\"compact\")) {\n+        method = &Benchmark::Compact;\n+      } else if (name == Slice(\"crc32c\")) {\n+        method = &Benchmark::Crc32c;\n+      } else if (name == Slice(\"acquireload\")) {\n+        method = &Benchmark::AcquireLoad;\n+      } else if (name == Slice(\"snappycomp\")) {\n+        method = &Benchmark::SnappyCompress;\n+      } else if (name == Slice(\"snappyuncomp\")) {\n+        method = &Benchmark::SnappyUncompress;\n+      } else if (name == Slice(\"heapprofile\")) {\n+        HeapProfile();\n+      } else if (name == Slice(\"stats\")) {\n+        PrintStats(\"leveldb.stats\");\n+      } else if (name == Slice(\"sstables\")) {\n+        PrintStats(\"leveldb.sstables\");\n+      } else {\n+        if (name != Slice()) {  // No error message for empty name\n+          fprintf(stderr, \"unknown benchmark '%s'\\n\", name.ToString().c_str());\n+        }\n+      }\n+\n+      if (fresh_db) {\n+        if (FLAGS_use_existing_db) {\n+          fprintf(stdout, \"%-12s : skipped (--use_existing_db is true)\\n\",\n+                  name.ToString().c_str());\n+          method = NULL;\n+        } else {\n+          delete db_;\n+          db_ = NULL;\n+          DestroyDB(FLAGS_db, Options());\n+          Open();\n+        }\n+      }\n+\n+      if (method != NULL) {\n+        RunBenchmark(num_threads, name, method);\n+      }\n+    }\n+  }\n+\n+ private:\n+  struct ThreadArg {\n+    Benchmark* bm;\n+    SharedState* shared;\n+    ThreadState* thread;\n+    void (Benchmark::*method)(ThreadState*);\n+  };\n+\n+  static void ThreadBody(void* v) {\n+    ThreadArg* arg = reinterpret_cast<ThreadArg*>(v);\n+    SharedState* shared = arg->shared;\n+    ThreadState* thread = arg->thread;\n+    {\n+      MutexLock l(&shared->mu);\n+      shared->num_initialized++;\n+      if (shared->num_initialized >= shared->total) {\n+        shared->cv.SignalAll();\n+      }\n+      while (!shared->start) {\n+        shared->cv.Wait();\n+      }\n+    }\n+\n+    thread->stats.Start();\n+    (arg->bm->*(arg->method))(thread);\n+    thread->stats.Stop();\n+\n+    {\n+      MutexLock l(&shared->mu);\n+      shared->num_done++;\n+      if (shared->num_done >= shared->total) {\n+        shared->cv.SignalAll();\n+      }\n+    }\n+  }\n+\n+  void RunBenchmark(int n, Slice name,\n+                    void (Benchmark::*method)(ThreadState*)) {\n+    SharedState shared;\n+    shared.total = n;\n+    shared.num_initialized = 0;\n+    shared.num_done = 0;\n+    shared.start = false;\n+\n+    ThreadArg* arg = new ThreadArg[n];\n+    for (int i = 0; i < n; i++) {\n+      arg[i].bm = this;\n+      arg[i].method = method;\n+      arg[i].shared = &shared;\n+      arg[i].thread = new ThreadState(i);\n+      arg[i].thread->shared = &shared;\n+      Env::Default()->StartThread(ThreadBody, &arg[i]);\n+    }\n+\n+    shared.mu.Lock();\n+    while (shared.num_initialized < n) {\n+      shared.cv.Wait();\n+    }\n+\n+    shared.start = true;\n+    shared.cv.SignalAll();\n+    while (shared.num_done < n) {\n+      shared.cv.Wait();\n+    }\n+    shared.mu.Unlock();\n+\n+    for (int i = 1; i < n; i++) {\n+      arg[0].thread->stats.Merge(arg[i].thread->stats);\n+    }\n+    arg[0].thread->stats.Report(name);\n+\n+    for (int i = 0; i < n; i++) {\n+      delete arg[i].thread;\n+    }\n+    delete[] arg;\n+  }\n+\n+  void Crc32c(ThreadState* thread) {\n+    // Checksum about 500MB of data total\n+    const int size = 4096;\n+    const char* label = \"(4K per op)\";\n+    std::string data(size, 'x');\n+    int64_t bytes = 0;\n+    uint32_t crc = 0;\n+    while (bytes < 500 * 1048576) {\n+      crc = crc32c::Value(data.data(), size);\n+      thread->stats.FinishedSingleOp();\n+      bytes += size;\n+    }\n+    // Print so result is not dead\n+    fprintf(stderr, \"... crc=0x%x\\r\", static_cast<unsigned int>(crc));\n+\n+    thread->stats.AddBytes(bytes);\n+    thread->stats.AddMessage(label);\n+  }\n+\n+  void AcquireLoad(ThreadState* thread) {\n+    int dummy;\n+    port::AtomicPointer ap(&dummy);\n+    int count = 0;\n+    void *ptr = NULL;\n+    thread->stats.AddMessage(\"(each op is 1000 loads)\");\n+    while (count < 100000) {\n+      for (int i = 0; i < 1000; i++) {\n+        ptr = ap.Acquire_Load();\n+      }\n+      count++;\n+      thread->stats.FinishedSingleOp();\n+    }\n+    if (ptr == NULL) exit(1); // Disable unused variable warning.\n+  }\n+\n+  void SnappyCompress(ThreadState* thread) {\n+    RandomGenerator gen;\n+    Slice input = gen.Generate(Options().block_size);\n+    int64_t bytes = 0;\n+    int64_t produced = 0;\n+    bool ok = true;\n+    std::string compressed;\n+    while (ok && bytes < 1024 * 1048576) {  // Compress 1G\n+      ok = port::Snappy_Compress(input.data(), input.size(), &compressed);\n+      produced += compressed.size();\n+      bytes += input.size();\n+      thread->stats.FinishedSingleOp();\n+    }\n+\n+    if (!ok) {\n+      thread->stats.AddMessage(\"(snappy failure)\");\n+    } else {\n+      char buf[100];\n+      snprintf(buf, sizeof(buf), \"(output: %.1f%%)\",\n+               (produced * 100.0) / bytes);\n+      thread->stats.AddMessage(buf);\n+      thread->stats.AddBytes(bytes);\n+    }\n+  }\n+\n+  void SnappyUncompress(ThreadState* thread) {\n+    RandomGenerator gen;\n+    Slice input = gen.Generate(Options().block_size);\n+    std::string compressed;\n+    bool ok = port::Snappy_Compress(input.data(), input.size(), &compressed);\n+    int64_t bytes = 0;\n+    char* uncompressed = new char[input.size()];\n+    while (ok && bytes < 1024 * 1048576) {  // Compress 1G\n+      ok =  port::Snappy_Uncompress(compressed.data(), compressed.size(),\n+                                    uncompressed);\n+      bytes += input.size();\n+      thread->stats.FinishedSingleOp();\n+    }\n+    delete[] uncompressed;\n+\n+    if (!ok) {\n+      thread->stats.AddMessage(\"(snappy failure)\");\n+    } else {\n+      thread->stats.AddBytes(bytes);\n+    }\n+  }\n+\n+  void Open() {\n+    assert(db_ == NULL);\n+    Options options;\n+    options.create_if_missing = !FLAGS_use_existing_db;\n+    options.block_cache = cache_;\n+    options.write_buffer_size = FLAGS_write_buffer_size;\n+    options.max_open_files = FLAGS_open_files;\n+    options.filter_policy = filter_policy_;\n+    Status s = DB::Open(options, FLAGS_db, &db_);\n+    if (!s.ok()) {\n+      fprintf(stderr, \"open error: %s\\n\", s.ToString().c_str());\n+      exit(1);\n+    }\n+  }\n+\n+  void WriteSeq(ThreadState* thread) {\n+    DoWrite(thread, true);\n+  }\n+\n+  void WriteRandom(ThreadState* thread) {\n+    DoWrite(thread, false);\n+  }\n+\n+  void DoWrite(ThreadState* thread, bool seq) {\n+    if (num_ != FLAGS_num) {\n+      char msg[100];\n+      snprintf(msg, sizeof(msg), \"(%d ops)\", num_);\n+      thread->stats.AddMessage(msg);\n+    }\n+\n+    RandomGenerator gen;\n+    WriteBatch batch;\n+    Status s;\n+    int64_t bytes = 0;\n+    for (int i = 0; i < num_; i += entries_per_batch_) {\n+      batch.Clear();\n+      for (int j = 0; j < entries_per_batch_; j++) {\n+        const int k = seq ? i+j : (thread->rand.Next() % FLAGS_num);\n+        char key[100];\n+        snprintf(key, sizeof(key), \"%016d\", k);\n+        batch.Put(key, gen.Generate(value_size_));\n+        bytes += value_size_ + strlen(key);\n+        thread->stats.FinishedSingleOp();\n+      }\n+      s = db_->Write(write_options_, &batch);\n+      if (!s.ok()) {\n+        fprintf(stderr, \"put error: %s\\n\", s.ToString().c_str());\n+        exit(1);\n+      }\n+    }\n+    thread->stats.AddBytes(bytes);\n+  }\n+\n+  void ReadSequential(ThreadState* thread) {\n+    Iterator* iter = db_->NewIterator(ReadOptions());\n+    int i = 0;\n+    int64_t bytes = 0;\n+    for (iter->SeekToFirst(); i < reads_ && iter->Valid(); iter->Next()) {\n+      bytes += iter->key().size() + iter->value().size();\n+      thread->stats.FinishedSingleOp();\n+      ++i;\n+    }\n+    delete iter;\n+    thread->stats.AddBytes(bytes);\n+  }\n+\n+  void ReadReverse(ThreadState* thread) {\n+    Iterator* iter = db_->NewIterator(ReadOptions());\n+    int i = 0;\n+    int64_t bytes = 0;\n+    for (iter->SeekToLast(); i < reads_ && iter->Valid(); iter->Prev()) {\n+      bytes += iter->key().size() + iter->value().size();\n+      thread->stats.FinishedSingleOp();\n+      ++i;\n+    }\n+    delete iter;\n+    thread->stats.AddBytes(bytes);\n+  }\n+\n+  void ReadRandom(ThreadState* thread) {\n+    ReadOptions options;\n+    std::string value;\n+    int found = 0;\n+    for (int i = 0; i < reads_; i++) {\n+      char key[100];\n+      const int k = thread->rand.Next() % FLAGS_num;\n+      snprintf(key, sizeof(key), \"%016d\", k);\n+      if (db_->Get(options, key, &value).ok()) {\n+        found++;\n+      }\n+      thread->stats.FinishedSingleOp();\n+    }\n+    char msg[100];\n+    snprintf(msg, sizeof(msg), \"(%d of %d found)\", found, num_);\n+    thread->stats.AddMessage(msg);\n+  }\n+\n+  void ReadMissing(ThreadState* thread) {\n+    ReadOptions options;\n+    std::string value;\n+    for (int i = 0; i < reads_; i++) {\n+      char key[100];\n+      const int k = thread->rand.Next() % FLAGS_num;\n+      snprintf(key, sizeof(key), \"%016d.\", k);\n+      db_->Get(options, key, &value);\n+      thread->stats.FinishedSingleOp();\n+    }\n+  }\n+\n+  void ReadHot(ThreadState* thread) {\n+    ReadOptions options;\n+    std::string value;\n+    const int range = (FLAGS_num + 99) / 100;\n+    for (int i = 0; i < reads_; i++) {\n+      char key[100];\n+      const int k = thread->rand.Next() % range;\n+      snprintf(key, sizeof(key), \"%016d\", k);\n+      db_->Get(options, key, &value);\n+      thread->stats.FinishedSingleOp();\n+    }\n+  }\n+\n+  void SeekRandom(ThreadState* thread) {\n+    ReadOptions options;\n+    std::string value;\n+    int found = 0;\n+    for (int i = 0; i < reads_; i++) {\n+      Iterator* iter = db_->NewIterator(options);\n+      char key[100];\n+      const int k = thread->rand.Next() % FLAGS_num;\n+      snprintf(key, sizeof(key), \"%016d\", k);\n+      iter->Seek(key);\n+      if (iter->Valid() && iter->key() == key) found++;\n+      delete iter;\n+      thread->stats.FinishedSingleOp();\n+    }\n+    char msg[100];\n+    snprintf(msg, sizeof(msg), \"(%d of %d found)\", found, num_);\n+    thread->stats.AddMessage(msg);\n+  }\n+\n+  void DoDelete(ThreadState* thread, bool seq) {\n+    RandomGenerator gen;\n+    WriteBatch batch;\n+    Status s;\n+    for (int i = 0; i < num_; i += entries_per_batch_) {\n+      batch.Clear();\n+      for (int j = 0; j < entries_per_batch_; j++) {\n+        const int k = seq ? i+j : (thread->rand.Next() % FLAGS_num);\n+        char key[100];\n+        snprintf(key, sizeof(key), \"%016d\", k);\n+        batch.Delete(key);\n+        thread->stats.FinishedSingleOp();\n+      }\n+      s = db_->Write(write_options_, &batch);\n+      if (!s.ok()) {\n+        fprintf(stderr, \"del error: %s\\n\", s.ToString().c_str());\n+        exit(1);\n+      }\n+    }\n+  }\n+\n+  void DeleteSeq(ThreadState* thread) {\n+    DoDelete(thread, true);\n+  }\n+\n+  void DeleteRandom(ThreadState* thread) {\n+    DoDelete(thread, false);\n+  }\n+\n+  void ReadWhileWriting(ThreadState* thread) {\n+    if (thread->tid > 0) {\n+      ReadRandom(thread);\n+    } else {\n+      // Special thread that keeps writing until other threads are done.\n+      RandomGenerator gen;\n+      while (true) {\n+        {\n+          MutexLock l(&thread->shared->mu);\n+          if (thread->shared->num_done + 1 >= thread->shared->num_initialized) {\n+            // Other threads have finished\n+            break;\n+          }\n+        }\n+\n+        const int k = thread->rand.Next() % FLAGS_num;\n+        char key[100];\n+        snprintf(key, sizeof(key), \"%016d\", k);\n+        Status s = db_->Put(write_options_, key, gen.Generate(value_size_));\n+        if (!s.ok()) {\n+          fprintf(stderr, \"put error: %s\\n\", s.ToString().c_str());\n+          exit(1);\n+        }\n+      }\n+\n+      // Do not count any of the preceding work/delay in stats.\n+      thread->stats.Start();\n+    }\n+  }\n+\n+  void Compact(ThreadState* thread) {\n+    db_->CompactRange(NULL, NULL);\n+  }\n+\n+  void PrintStats(const char* key) {\n+    std::string stats;\n+    if (!db_->GetProperty(key, &stats)) {\n+      stats = \"(failed)\";\n+    }\n+    fprintf(stdout, \"\\n%s\\n\", stats.c_str());\n+  }\n+\n+  static void WriteToFile(void* arg, const char* buf, int n) {\n+    reinterpret_cast<WritableFile*>(arg)->Append(Slice(buf, n));\n+  }\n+\n+  void HeapProfile() {\n+    char fname[100];\n+    snprintf(fname, sizeof(fname), \"%s/heap-%04d\", FLAGS_db, ++heap_counter_);\n+    WritableFile* file;\n+    Status s = Env::Default()->NewWritableFile(fname, &file);\n+    if (!s.ok()) {\n+      fprintf(stderr, \"%s\\n\", s.ToString().c_str());\n+      return;\n+    }\n+    bool ok = port::GetHeapProfile(WriteToFile, file);\n+    delete file;\n+    if (!ok) {\n+      fprintf(stderr, \"heap profiling not supported\\n\");\n+      Env::Default()->DeleteFile(fname);\n+    }\n+  }\n+};\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  FLAGS_write_buffer_size = leveldb::Options().write_buffer_size;\n+  FLAGS_open_files = leveldb::Options().max_open_files;\n+  std::string default_db_path;\n+\n+  for (int i = 1; i < argc; i++) {\n+    double d;\n+    int n;\n+    char junk;\n+    if (leveldb::Slice(argv[i]).starts_with(\"--benchmarks=\")) {\n+      FLAGS_benchmarks = argv[i] + strlen(\"--benchmarks=\");\n+    } else if (sscanf(argv[i], \"--compression_ratio=%lf%c\", &d, &junk) == 1) {\n+      FLAGS_compression_ratio = d;\n+    } else if (sscanf(argv[i], \"--histogram=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_histogram = n;\n+    } else if (sscanf(argv[i], \"--use_existing_db=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_use_existing_db = n;\n+    } else if (sscanf(argv[i], \"--num=%d%c\", &n, &junk) == 1) {\n+      FLAGS_num = n;\n+    } else if (sscanf(argv[i], \"--reads=%d%c\", &n, &junk) == 1) {\n+      FLAGS_reads = n;\n+    } else if (sscanf(argv[i], \"--threads=%d%c\", &n, &junk) == 1) {\n+      FLAGS_threads = n;\n+    } else if (sscanf(argv[i], \"--value_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_value_size = n;\n+    } else if (sscanf(argv[i], \"--write_buffer_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_write_buffer_size = n;\n+    } else if (sscanf(argv[i], \"--cache_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_cache_size = n;\n+    } else if (sscanf(argv[i], \"--bloom_bits=%d%c\", &n, &junk) == 1) {\n+      FLAGS_bloom_bits = n;\n+    } else if (sscanf(argv[i], \"--open_files=%d%c\", &n, &junk) == 1) {\n+      FLAGS_open_files = n;\n+    } else if (strncmp(argv[i], \"--db=\", 5) == 0) {\n+      FLAGS_db = argv[i] + 5;\n+    } else {\n+      fprintf(stderr, \"Invalid flag '%s'\\n\", argv[i]);\n+      exit(1);\n+    }\n+  }\n+\n+  // Choose a location for the test database if none given with --db=<path>\n+  if (FLAGS_db == NULL) {\n+      leveldb::Env::Default()->GetTestDirectory(&default_db_path);\n+      default_db_path += \"/dbbench\";\n+      FLAGS_db = default_db_path.c_str();\n+  }\n+\n+  leveldb::Benchmark benchmark;\n+  benchmark.Run();\n+  return 0;\n+}"
      },
      {
        "sha": "395d3172ad3a16b24552057599c776228bf84917",
        "filename": "src/leveldb/db/db_impl.cc",
        "status": "added",
        "additions": 1485,
        "deletions": 0,
        "changes": 1485,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/db_impl.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/db_impl.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_impl.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,1485 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/db_impl.h\"\n+\n+#include <algorithm>\n+#include <set>\n+#include <string>\n+#include <stdint.h>\n+#include <stdio.h>\n+#include <vector>\n+#include \"db/builder.h\"\n+#include \"db/db_iter.h\"\n+#include \"db/dbformat.h\"\n+#include \"db/filename.h\"\n+#include \"db/log_reader.h\"\n+#include \"db/log_writer.h\"\n+#include \"db/memtable.h\"\n+#include \"db/table_cache.h\"\n+#include \"db/version_set.h\"\n+#include \"db/write_batch_internal.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/status.h\"\n+#include \"leveldb/table.h\"\n+#include \"leveldb/table_builder.h\"\n+#include \"port/port.h\"\n+#include \"table/block.h\"\n+#include \"table/merger.h\"\n+#include \"table/two_level_iterator.h\"\n+#include \"util/coding.h\"\n+#include \"util/logging.h\"\n+#include \"util/mutexlock.h\"\n+\n+namespace leveldb {\n+\n+const int kNumNonTableCacheFiles = 10;\n+\n+// Information kept for every waiting writer\n+struct DBImpl::Writer {\n+  Status status;\n+  WriteBatch* batch;\n+  bool sync;\n+  bool done;\n+  port::CondVar cv;\n+\n+  explicit Writer(port::Mutex* mu) : cv(mu) { }\n+};\n+\n+struct DBImpl::CompactionState {\n+  Compaction* const compaction;\n+\n+  // Sequence numbers < smallest_snapshot are not significant since we\n+  // will never have to service a snapshot below smallest_snapshot.\n+  // Therefore if we have seen a sequence number S <= smallest_snapshot,\n+  // we can drop all entries for the same key with sequence numbers < S.\n+  SequenceNumber smallest_snapshot;\n+\n+  // Files produced by compaction\n+  struct Output {\n+    uint64_t number;\n+    uint64_t file_size;\n+    InternalKey smallest, largest;\n+  };\n+  std::vector<Output> outputs;\n+\n+  // State kept for output being generated\n+  WritableFile* outfile;\n+  TableBuilder* builder;\n+\n+  uint64_t total_bytes;\n+\n+  Output* current_output() { return &outputs[outputs.size()-1]; }\n+\n+  explicit CompactionState(Compaction* c)\n+      : compaction(c),\n+        outfile(NULL),\n+        builder(NULL),\n+        total_bytes(0) {\n+  }\n+};\n+\n+// Fix user-supplied options to be reasonable\n+template <class T,class V>\n+static void ClipToRange(T* ptr, V minvalue, V maxvalue) {\n+  if (static_cast<V>(*ptr) > maxvalue) *ptr = maxvalue;\n+  if (static_cast<V>(*ptr) < minvalue) *ptr = minvalue;\n+}\n+Options SanitizeOptions(const std::string& dbname,\n+                        const InternalKeyComparator* icmp,\n+                        const InternalFilterPolicy* ipolicy,\n+                        const Options& src) {\n+  Options result = src;\n+  result.comparator = icmp;\n+  result.filter_policy = (src.filter_policy != NULL) ? ipolicy : NULL;\n+  ClipToRange(&result.max_open_files,    64 + kNumNonTableCacheFiles, 50000);\n+  ClipToRange(&result.write_buffer_size, 64<<10,                      1<<30);\n+  ClipToRange(&result.block_size,        1<<10,                       4<<20);\n+  if (result.info_log == NULL) {\n+    // Open a log file in the same directory as the db\n+    src.env->CreateDir(dbname);  // In case it does not exist\n+    src.env->RenameFile(InfoLogFileName(dbname), OldInfoLogFileName(dbname));\n+    Status s = src.env->NewLogger(InfoLogFileName(dbname), &result.info_log);\n+    if (!s.ok()) {\n+      // No place suitable for logging\n+      result.info_log = NULL;\n+    }\n+  }\n+  if (result.block_cache == NULL) {\n+    result.block_cache = NewLRUCache(8 << 20);\n+  }\n+  return result;\n+}\n+\n+DBImpl::DBImpl(const Options& options, const std::string& dbname)\n+    : env_(options.env),\n+      internal_comparator_(options.comparator),\n+      internal_filter_policy_(options.filter_policy),\n+      options_(SanitizeOptions(\n+          dbname, &internal_comparator_, &internal_filter_policy_, options)),\n+      owns_info_log_(options_.info_log != options.info_log),\n+      owns_cache_(options_.block_cache != options.block_cache),\n+      dbname_(dbname),\n+      db_lock_(NULL),\n+      shutting_down_(NULL),\n+      bg_cv_(&mutex_),\n+      mem_(new MemTable(internal_comparator_)),\n+      imm_(NULL),\n+      logfile_(NULL),\n+      logfile_number_(0),\n+      log_(NULL),\n+      tmp_batch_(new WriteBatch),\n+      bg_compaction_scheduled_(false),\n+      manual_compaction_(NULL),\n+      consecutive_compaction_errors_(0) {\n+  mem_->Ref();\n+  has_imm_.Release_Store(NULL);\n+\n+  // Reserve ten files or so for other uses and give the rest to TableCache.\n+  const int table_cache_size = options.max_open_files - kNumNonTableCacheFiles;\n+  table_cache_ = new TableCache(dbname_, &options_, table_cache_size);\n+\n+  versions_ = new VersionSet(dbname_, &options_, table_cache_,\n+                             &internal_comparator_);\n+}\n+\n+DBImpl::~DBImpl() {\n+  // Wait for background work to finish\n+  mutex_.Lock();\n+  shutting_down_.Release_Store(this);  // Any non-NULL value is ok\n+  while (bg_compaction_scheduled_) {\n+    bg_cv_.Wait();\n+  }\n+  mutex_.Unlock();\n+\n+  if (db_lock_ != NULL) {\n+    env_->UnlockFile(db_lock_);\n+  }\n+\n+  delete versions_;\n+  if (mem_ != NULL) mem_->Unref();\n+  if (imm_ != NULL) imm_->Unref();\n+  delete tmp_batch_;\n+  delete log_;\n+  delete logfile_;\n+  delete table_cache_;\n+\n+  if (owns_info_log_) {\n+    delete options_.info_log;\n+  }\n+  if (owns_cache_) {\n+    delete options_.block_cache;\n+  }\n+}\n+\n+Status DBImpl::NewDB() {\n+  VersionEdit new_db;\n+  new_db.SetComparatorName(user_comparator()->Name());\n+  new_db.SetLogNumber(0);\n+  new_db.SetNextFile(2);\n+  new_db.SetLastSequence(0);\n+\n+  const std::string manifest = DescriptorFileName(dbname_, 1);\n+  WritableFile* file;\n+  Status s = env_->NewWritableFile(manifest, &file);\n+  if (!s.ok()) {\n+    return s;\n+  }\n+  {\n+    log::Writer log(file);\n+    std::string record;\n+    new_db.EncodeTo(&record);\n+    s = log.AddRecord(record);\n+    if (s.ok()) {\n+      s = file->Close();\n+    }\n+  }\n+  delete file;\n+  if (s.ok()) {\n+    // Make \"CURRENT\" file that points to the new manifest file.\n+    s = SetCurrentFile(env_, dbname_, 1);\n+  } else {\n+    env_->DeleteFile(manifest);\n+  }\n+  return s;\n+}\n+\n+void DBImpl::MaybeIgnoreError(Status* s) const {\n+  if (s->ok() || options_.paranoid_checks) {\n+    // No change needed\n+  } else {\n+    Log(options_.info_log, \"Ignoring error %s\", s->ToString().c_str());\n+    *s = Status::OK();\n+  }\n+}\n+\n+void DBImpl::DeleteObsoleteFiles() {\n+  // Make a set of all of the live files\n+  std::set<uint64_t> live = pending_outputs_;\n+  versions_->AddLiveFiles(&live);\n+\n+  std::vector<std::string> filenames;\n+  env_->GetChildren(dbname_, &filenames); // Ignoring errors on purpose\n+  uint64_t number;\n+  FileType type;\n+  for (size_t i = 0; i < filenames.size(); i++) {\n+    if (ParseFileName(filenames[i], &number, &type)) {\n+      bool keep = true;\n+      switch (type) {\n+        case kLogFile:\n+          keep = ((number >= versions_->LogNumber()) ||\n+                  (number == versions_->PrevLogNumber()));\n+          break;\n+        case kDescriptorFile:\n+          // Keep my manifest file, and any newer incarnations'\n+          // (in case there is a race that allows other incarnations)\n+          keep = (number >= versions_->ManifestFileNumber());\n+          break;\n+        case kTableFile:\n+          keep = (live.find(number) != live.end());\n+          break;\n+        case kTempFile:\n+          // Any temp files that are currently being written to must\n+          // be recorded in pending_outputs_, which is inserted into \"live\"\n+          keep = (live.find(number) != live.end());\n+          break;\n+        case kCurrentFile:\n+        case kDBLockFile:\n+        case kInfoLogFile:\n+          keep = true;\n+          break;\n+      }\n+\n+      if (!keep) {\n+        if (type == kTableFile) {\n+          table_cache_->Evict(number);\n+        }\n+        Log(options_.info_log, \"Delete type=%d #%lld\\n\",\n+            int(type),\n+            static_cast<unsigned long long>(number));\n+        env_->DeleteFile(dbname_ + \"/\" + filenames[i]);\n+      }\n+    }\n+  }\n+}\n+\n+Status DBImpl::Recover(VersionEdit* edit) {\n+  mutex_.AssertHeld();\n+\n+  // Ignore error from CreateDir since the creation of the DB is\n+  // committed only when the descriptor is created, and this directory\n+  // may already exist from a previous failed creation attempt.\n+  env_->CreateDir(dbname_);\n+  assert(db_lock_ == NULL);\n+  Status s = env_->LockFile(LockFileName(dbname_), &db_lock_);\n+  if (!s.ok()) {\n+    return s;\n+  }\n+\n+  if (!env_->FileExists(CurrentFileName(dbname_))) {\n+    if (options_.create_if_missing) {\n+      s = NewDB();\n+      if (!s.ok()) {\n+        return s;\n+      }\n+    } else {\n+      return Status::InvalidArgument(\n+          dbname_, \"does not exist (create_if_missing is false)\");\n+    }\n+  } else {\n+    if (options_.error_if_exists) {\n+      return Status::InvalidArgument(\n+          dbname_, \"exists (error_if_exists is true)\");\n+    }\n+  }\n+\n+  s = versions_->Recover();\n+  if (s.ok()) {\n+    SequenceNumber max_sequence(0);\n+\n+    // Recover from all newer log files than the ones named in the\n+    // descriptor (new log files may have been added by the previous\n+    // incarnation without registering them in the descriptor).\n+    //\n+    // Note that PrevLogNumber() is no longer used, but we pay\n+    // attention to it in case we are recovering a database\n+    // produced by an older version of leveldb.\n+    const uint64_t min_log = versions_->LogNumber();\n+    const uint64_t prev_log = versions_->PrevLogNumber();\n+    std::vector<std::string> filenames;\n+    s = env_->GetChildren(dbname_, &filenames);\n+    if (!s.ok()) {\n+      return s;\n+    }\n+    std::set<uint64_t> expected;\n+    versions_->AddLiveFiles(&expected);\n+    uint64_t number;\n+    FileType type;\n+    std::vector<uint64_t> logs;\n+    for (size_t i = 0; i < filenames.size(); i++) {\n+      if (ParseFileName(filenames[i], &number, &type)) {\n+        expected.erase(number);\n+        if (type == kLogFile && ((number >= min_log) || (number == prev_log)))\n+          logs.push_back(number);\n+      }\n+    }\n+    if (!expected.empty()) {\n+      char buf[50];\n+      snprintf(buf, sizeof(buf), \"%d missing files; e.g.\",\n+               static_cast<int>(expected.size()));\n+      return Status::Corruption(buf, TableFileName(dbname_, *(expected.begin())));\n+    }\n+\n+    // Recover in the order in which the logs were generated\n+    std::sort(logs.begin(), logs.end());\n+    for (size_t i = 0; i < logs.size(); i++) {\n+      s = RecoverLogFile(logs[i], edit, &max_sequence);\n+\n+      // The previous incarnation may not have written any MANIFEST\n+      // records after allocating this log number.  So we manually\n+      // update the file number allocation counter in VersionSet.\n+      versions_->MarkFileNumberUsed(logs[i]);\n+    }\n+\n+    if (s.ok()) {\n+      if (versions_->LastSequence() < max_sequence) {\n+        versions_->SetLastSequence(max_sequence);\n+      }\n+    }\n+  }\n+\n+  return s;\n+}\n+\n+Status DBImpl::RecoverLogFile(uint64_t log_number,\n+                              VersionEdit* edit,\n+                              SequenceNumber* max_sequence) {\n+  struct LogReporter : public log::Reader::Reporter {\n+    Env* env;\n+    Logger* info_log;\n+    const char* fname;\n+    Status* status;  // NULL if options_.paranoid_checks==false\n+    virtual void Corruption(size_t bytes, const Status& s) {\n+      Log(info_log, \"%s%s: dropping %d bytes; %s\",\n+          (this->status == NULL ? \"(ignoring error) \" : \"\"),\n+          fname, static_cast<int>(bytes), s.ToString().c_str());\n+      if (this->status != NULL && this->status->ok()) *this->status = s;\n+    }\n+  };\n+\n+  mutex_.AssertHeld();\n+\n+  // Open the log file\n+  std::string fname = LogFileName(dbname_, log_number);\n+  SequentialFile* file;\n+  Status status = env_->NewSequentialFile(fname, &file);\n+  if (!status.ok()) {\n+    MaybeIgnoreError(&status);\n+    return status;\n+  }\n+\n+  // Create the log reader.\n+  LogReporter reporter;\n+  reporter.env = env_;\n+  reporter.info_log = options_.info_log;\n+  reporter.fname = fname.c_str();\n+  reporter.status = (options_.paranoid_checks ? &status : NULL);\n+  // We intentially make log::Reader do checksumming even if\n+  // paranoid_checks==false so that corruptions cause entire commits\n+  // to be skipped instead of propagating bad information (like overly\n+  // large sequence numbers).\n+  log::Reader reader(file, &reporter, true/*checksum*/,\n+                     0/*initial_offset*/);\n+  Log(options_.info_log, \"Recovering log #%llu\",\n+      (unsigned long long) log_number);\n+\n+  // Read all the records and add to a memtable\n+  std::string scratch;\n+  Slice record;\n+  WriteBatch batch;\n+  MemTable* mem = NULL;\n+  while (reader.ReadRecord(&record, &scratch) &&\n+         status.ok()) {\n+    if (record.size() < 12) {\n+      reporter.Corruption(\n+          record.size(), Status::Corruption(\"log record too small\"));\n+      continue;\n+    }\n+    WriteBatchInternal::SetContents(&batch, record);\n+\n+    if (mem == NULL) {\n+      mem = new MemTable(internal_comparator_);\n+      mem->Ref();\n+    }\n+    status = WriteBatchInternal::InsertInto(&batch, mem);\n+    MaybeIgnoreError(&status);\n+    if (!status.ok()) {\n+      break;\n+    }\n+    const SequenceNumber last_seq =\n+        WriteBatchInternal::Sequence(&batch) +\n+        WriteBatchInternal::Count(&batch) - 1;\n+    if (last_seq > *max_sequence) {\n+      *max_sequence = last_seq;\n+    }\n+\n+    if (mem->ApproximateMemoryUsage() > options_.write_buffer_size) {\n+      status = WriteLevel0Table(mem, edit, NULL);\n+      if (!status.ok()) {\n+        // Reflect errors immediately so that conditions like full\n+        // file-systems cause the DB::Open() to fail.\n+        break;\n+      }\n+      mem->Unref();\n+      mem = NULL;\n+    }\n+  }\n+\n+  if (status.ok() && mem != NULL) {\n+    status = WriteLevel0Table(mem, edit, NULL);\n+    // Reflect errors immediately so that conditions like full\n+    // file-systems cause the DB::Open() to fail.\n+  }\n+\n+  if (mem != NULL) mem->Unref();\n+  delete file;\n+  return status;\n+}\n+\n+Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit,\n+                                Version* base) {\n+  mutex_.AssertHeld();\n+  const uint64_t start_micros = env_->NowMicros();\n+  FileMetaData meta;\n+  meta.number = versions_->NewFileNumber();\n+  pending_outputs_.insert(meta.number);\n+  Iterator* iter = mem->NewIterator();\n+  Log(options_.info_log, \"Level-0 table #%llu: started\",\n+      (unsigned long long) meta.number);\n+\n+  Status s;\n+  {\n+    mutex_.Unlock();\n+    s = BuildTable(dbname_, env_, options_, table_cache_, iter, &meta);\n+    mutex_.Lock();\n+  }\n+\n+  Log(options_.info_log, \"Level-0 table #%llu: %lld bytes %s\",\n+      (unsigned long long) meta.number,\n+      (unsigned long long) meta.file_size,\n+      s.ToString().c_str());\n+  delete iter;\n+  pending_outputs_.erase(meta.number);\n+\n+\n+  // Note that if file_size is zero, the file has been deleted and\n+  // should not be added to the manifest.\n+  int level = 0;\n+  if (s.ok() && meta.file_size > 0) {\n+    const Slice min_user_key = meta.smallest.user_key();\n+    const Slice max_user_key = meta.largest.user_key();\n+    if (base != NULL) {\n+      level = base->PickLevelForMemTableOutput(min_user_key, max_user_key);\n+    }\n+    edit->AddFile(level, meta.number, meta.file_size,\n+                  meta.smallest, meta.largest);\n+  }\n+\n+  CompactionStats stats;\n+  stats.micros = env_->NowMicros() - start_micros;\n+  stats.bytes_written = meta.file_size;\n+  stats_[level].Add(stats);\n+  return s;\n+}\n+\n+Status DBImpl::CompactMemTable() {\n+  mutex_.AssertHeld();\n+  assert(imm_ != NULL);\n+\n+  // Save the contents of the memtable as a new Table\n+  VersionEdit edit;\n+  Version* base = versions_->current();\n+  base->Ref();\n+  Status s = WriteLevel0Table(imm_, &edit, base);\n+  base->Unref();\n+\n+  if (s.ok() && shutting_down_.Acquire_Load()) {\n+    s = Status::IOError(\"Deleting DB during memtable compaction\");\n+  }\n+\n+  // Replace immutable memtable with the generated Table\n+  if (s.ok()) {\n+    edit.SetPrevLogNumber(0);\n+    edit.SetLogNumber(logfile_number_);  // Earlier logs no longer needed\n+    s = versions_->LogAndApply(&edit, &mutex_);\n+  }\n+\n+  if (s.ok()) {\n+    // Commit to the new state\n+    imm_->Unref();\n+    imm_ = NULL;\n+    has_imm_.Release_Store(NULL);\n+    DeleteObsoleteFiles();\n+  }\n+\n+  return s;\n+}\n+\n+void DBImpl::CompactRange(const Slice* begin, const Slice* end) {\n+  int max_level_with_files = 1;\n+  {\n+    MutexLock l(&mutex_);\n+    Version* base = versions_->current();\n+    for (int level = 1; level < config::kNumLevels; level++) {\n+      if (base->OverlapInLevel(level, begin, end)) {\n+        max_level_with_files = level;\n+      }\n+    }\n+  }\n+  TEST_CompactMemTable(); // TODO(sanjay): Skip if memtable does not overlap\n+  for (int level = 0; level < max_level_with_files; level++) {\n+    TEST_CompactRange(level, begin, end);\n+  }\n+}\n+\n+void DBImpl::TEST_CompactRange(int level, const Slice* begin,const Slice* end) {\n+  assert(level >= 0);\n+  assert(level + 1 < config::kNumLevels);\n+\n+  InternalKey begin_storage, end_storage;\n+\n+  ManualCompaction manual;\n+  manual.level = level;\n+  manual.done = false;\n+  if (begin == NULL) {\n+    manual.begin = NULL;\n+  } else {\n+    begin_storage = InternalKey(*begin, kMaxSequenceNumber, kValueTypeForSeek);\n+    manual.begin = &begin_storage;\n+  }\n+  if (end == NULL) {\n+    manual.end = NULL;\n+  } else {\n+    end_storage = InternalKey(*end, 0, static_cast<ValueType>(0));\n+    manual.end = &end_storage;\n+  }\n+\n+  MutexLock l(&mutex_);\n+  while (!manual.done) {\n+    while (manual_compaction_ != NULL) {\n+      bg_cv_.Wait();\n+    }\n+    manual_compaction_ = &manual;\n+    MaybeScheduleCompaction();\n+    while (manual_compaction_ == &manual) {\n+      bg_cv_.Wait();\n+    }\n+  }\n+}\n+\n+Status DBImpl::TEST_CompactMemTable() {\n+  // NULL batch means just wait for earlier writes to be done\n+  Status s = Write(WriteOptions(), NULL);\n+  if (s.ok()) {\n+    // Wait until the compaction completes\n+    MutexLock l(&mutex_);\n+    while (imm_ != NULL && bg_error_.ok()) {\n+      bg_cv_.Wait();\n+    }\n+    if (imm_ != NULL) {\n+      s = bg_error_;\n+    }\n+  }\n+  return s;\n+}\n+\n+void DBImpl::MaybeScheduleCompaction() {\n+  mutex_.AssertHeld();\n+  if (bg_compaction_scheduled_) {\n+    // Already scheduled\n+  } else if (shutting_down_.Acquire_Load()) {\n+    // DB is being deleted; no more background compactions\n+  } else if (imm_ == NULL &&\n+             manual_compaction_ == NULL &&\n+             !versions_->NeedsCompaction()) {\n+    // No work to be done\n+  } else {\n+    bg_compaction_scheduled_ = true;\n+    env_->Schedule(&DBImpl::BGWork, this);\n+  }\n+}\n+\n+void DBImpl::BGWork(void* db) {\n+  reinterpret_cast<DBImpl*>(db)->BackgroundCall();\n+}\n+\n+void DBImpl::BackgroundCall() {\n+  MutexLock l(&mutex_);\n+  assert(bg_compaction_scheduled_);\n+  if (!shutting_down_.Acquire_Load()) {\n+    Status s = BackgroundCompaction();\n+    if (s.ok()) {\n+      // Success\n+      consecutive_compaction_errors_ = 0;\n+    } else if (shutting_down_.Acquire_Load()) {\n+      // Error most likely due to shutdown; do not wait\n+    } else {\n+      // Wait a little bit before retrying background compaction in\n+      // case this is an environmental problem and we do not want to\n+      // chew up resources for failed compactions for the duration of\n+      // the problem.\n+      bg_cv_.SignalAll();  // In case a waiter can proceed despite the error\n+      Log(options_.info_log, \"Waiting after background compaction error: %s\",\n+          s.ToString().c_str());\n+      mutex_.Unlock();\n+      ++consecutive_compaction_errors_;\n+      int seconds_to_sleep = 1;\n+      for (int i = 0; i < 3 && i < consecutive_compaction_errors_ - 1; ++i) {\n+        seconds_to_sleep *= 2;\n+      }\n+      env_->SleepForMicroseconds(seconds_to_sleep * 1000000);\n+      mutex_.Lock();\n+    }\n+  }\n+\n+  bg_compaction_scheduled_ = false;\n+\n+  // Previous compaction may have produced too many files in a level,\n+  // so reschedule another compaction if needed.\n+  MaybeScheduleCompaction();\n+  bg_cv_.SignalAll();\n+}\n+\n+Status DBImpl::BackgroundCompaction() {\n+  mutex_.AssertHeld();\n+\n+  if (imm_ != NULL) {\n+    return CompactMemTable();\n+  }\n+\n+  Compaction* c;\n+  bool is_manual = (manual_compaction_ != NULL);\n+  InternalKey manual_end;\n+  if (is_manual) {\n+    ManualCompaction* m = manual_compaction_;\n+    c = versions_->CompactRange(m->level, m->begin, m->end);\n+    m->done = (c == NULL);\n+    if (c != NULL) {\n+      manual_end = c->input(0, c->num_input_files(0) - 1)->largest;\n+    }\n+    Log(options_.info_log,\n+        \"Manual compaction at level-%d from %s .. %s; will stop at %s\\n\",\n+        m->level,\n+        (m->begin ? m->begin->DebugString().c_str() : \"(begin)\"),\n+        (m->end ? m->end->DebugString().c_str() : \"(end)\"),\n+        (m->done ? \"(end)\" : manual_end.DebugString().c_str()));\n+  } else {\n+    c = versions_->PickCompaction();\n+  }\n+\n+  Status status;\n+  if (c == NULL) {\n+    // Nothing to do\n+  } else if (!is_manual && c->IsTrivialMove()) {\n+    // Move file to next level\n+    assert(c->num_input_files(0) == 1);\n+    FileMetaData* f = c->input(0, 0);\n+    c->edit()->DeleteFile(c->level(), f->number);\n+    c->edit()->AddFile(c->level() + 1, f->number, f->file_size,\n+                       f->smallest, f->largest);\n+    status = versions_->LogAndApply(c->edit(), &mutex_);\n+    VersionSet::LevelSummaryStorage tmp;\n+    Log(options_.info_log, \"Moved #%lld to level-%d %lld bytes %s: %s\\n\",\n+        static_cast<unsigned long long>(f->number),\n+        c->level() + 1,\n+        static_cast<unsigned long long>(f->file_size),\n+        status.ToString().c_str(),\n+        versions_->LevelSummary(&tmp));\n+  } else {\n+    CompactionState* compact = new CompactionState(c);\n+    status = DoCompactionWork(compact);\n+    CleanupCompaction(compact);\n+    c->ReleaseInputs();\n+    DeleteObsoleteFiles();\n+  }\n+  delete c;\n+\n+  if (status.ok()) {\n+    // Done\n+  } else if (shutting_down_.Acquire_Load()) {\n+    // Ignore compaction errors found during shutting down\n+  } else {\n+    Log(options_.info_log,\n+        \"Compaction error: %s\", status.ToString().c_str());\n+    if (options_.paranoid_checks && bg_error_.ok()) {\n+      bg_error_ = status;\n+    }\n+  }\n+\n+  if (is_manual) {\n+    ManualCompaction* m = manual_compaction_;\n+    if (!status.ok()) {\n+      m->done = true;\n+    }\n+    if (!m->done) {\n+      // We only compacted part of the requested range.  Update *m\n+      // to the range that is left to be compacted.\n+      m->tmp_storage = manual_end;\n+      m->begin = &m->tmp_storage;\n+    }\n+    manual_compaction_ = NULL;\n+  }\n+  return status;\n+}\n+\n+void DBImpl::CleanupCompaction(CompactionState* compact) {\n+  mutex_.AssertHeld();\n+  if (compact->builder != NULL) {\n+    // May happen if we get a shutdown call in the middle of compaction\n+    compact->builder->Abandon();\n+    delete compact->builder;\n+  } else {\n+    assert(compact->outfile == NULL);\n+  }\n+  delete compact->outfile;\n+  for (size_t i = 0; i < compact->outputs.size(); i++) {\n+    const CompactionState::Output& out = compact->outputs[i];\n+    pending_outputs_.erase(out.number);\n+  }\n+  delete compact;\n+}\n+\n+Status DBImpl::OpenCompactionOutputFile(CompactionState* compact) {\n+  assert(compact != NULL);\n+  assert(compact->builder == NULL);\n+  uint64_t file_number;\n+  {\n+    mutex_.Lock();\n+    file_number = versions_->NewFileNumber();\n+    pending_outputs_.insert(file_number);\n+    CompactionState::Output out;\n+    out.number = file_number;\n+    out.smallest.Clear();\n+    out.largest.Clear();\n+    compact->outputs.push_back(out);\n+    mutex_.Unlock();\n+  }\n+\n+  // Make the output file\n+  std::string fname = TableFileName(dbname_, file_number);\n+  Status s = env_->NewWritableFile(fname, &compact->outfile);\n+  if (s.ok()) {\n+    compact->builder = new TableBuilder(options_, compact->outfile);\n+  }\n+  return s;\n+}\n+\n+Status DBImpl::FinishCompactionOutputFile(CompactionState* compact,\n+                                          Iterator* input) {\n+  assert(compact != NULL);\n+  assert(compact->outfile != NULL);\n+  assert(compact->builder != NULL);\n+\n+  const uint64_t output_number = compact->current_output()->number;\n+  assert(output_number != 0);\n+\n+  // Check for iterator errors\n+  Status s = input->status();\n+  const uint64_t current_entries = compact->builder->NumEntries();\n+  if (s.ok()) {\n+    s = compact->builder->Finish();\n+  } else {\n+    compact->builder->Abandon();\n+  }\n+  const uint64_t current_bytes = compact->builder->FileSize();\n+  compact->current_output()->file_size = current_bytes;\n+  compact->total_bytes += current_bytes;\n+  delete compact->builder;\n+  compact->builder = NULL;\n+\n+  // Finish and check for file errors\n+  if (s.ok()) {\n+    s = compact->outfile->Sync();\n+  }\n+  if (s.ok()) {\n+    s = compact->outfile->Close();\n+  }\n+  delete compact->outfile;\n+  compact->outfile = NULL;\n+\n+  if (s.ok() && current_entries > 0) {\n+    // Verify that the table is usable\n+    Iterator* iter = table_cache_->NewIterator(ReadOptions(),\n+                                               output_number,\n+                                               current_bytes);\n+    s = iter->status();\n+    delete iter;\n+    if (s.ok()) {\n+      Log(options_.info_log,\n+          \"Generated table #%llu: %lld keys, %lld bytes\",\n+          (unsigned long long) output_number,\n+          (unsigned long long) current_entries,\n+          (unsigned long long) current_bytes);\n+    }\n+  }\n+  return s;\n+}\n+\n+\n+Status DBImpl::InstallCompactionResults(CompactionState* compact) {\n+  mutex_.AssertHeld();\n+  Log(options_.info_log,  \"Compacted %d@%d + %d@%d files => %lld bytes\",\n+      compact->compaction->num_input_files(0),\n+      compact->compaction->level(),\n+      compact->compaction->num_input_files(1),\n+      compact->compaction->level() + 1,\n+      static_cast<long long>(compact->total_bytes));\n+\n+  // Add compaction outputs\n+  compact->compaction->AddInputDeletions(compact->compaction->edit());\n+  const int level = compact->compaction->level();\n+  for (size_t i = 0; i < compact->outputs.size(); i++) {\n+    const CompactionState::Output& out = compact->outputs[i];\n+    compact->compaction->edit()->AddFile(\n+        level + 1,\n+        out.number, out.file_size, out.smallest, out.largest);\n+  }\n+  return versions_->LogAndApply(compact->compaction->edit(), &mutex_);\n+}\n+\n+Status DBImpl::DoCompactionWork(CompactionState* compact) {\n+  const uint64_t start_micros = env_->NowMicros();\n+  int64_t imm_micros = 0;  // Micros spent doing imm_ compactions\n+\n+  Log(options_.info_log,  \"Compacting %d@%d + %d@%d files\",\n+      compact->compaction->num_input_files(0),\n+      compact->compaction->level(),\n+      compact->compaction->num_input_files(1),\n+      compact->compaction->level() + 1);\n+\n+  assert(versions_->NumLevelFiles(compact->compaction->level()) > 0);\n+  assert(compact->builder == NULL);\n+  assert(compact->outfile == NULL);\n+  if (snapshots_.empty()) {\n+    compact->smallest_snapshot = versions_->LastSequence();\n+  } else {\n+    compact->smallest_snapshot = snapshots_.oldest()->number_;\n+  }\n+\n+  // Release mutex while we're actually doing the compaction work\n+  mutex_.Unlock();\n+\n+  Iterator* input = versions_->MakeInputIterator(compact->compaction);\n+  input->SeekToFirst();\n+  Status status;\n+  ParsedInternalKey ikey;\n+  std::string current_user_key;\n+  bool has_current_user_key = false;\n+  SequenceNumber last_sequence_for_key = kMaxSequenceNumber;\n+  for (; input->Valid() && !shutting_down_.Acquire_Load(); ) {\n+    // Prioritize immutable compaction work\n+    if (has_imm_.NoBarrier_Load() != NULL) {\n+      const uint64_t imm_start = env_->NowMicros();\n+      mutex_.Lock();\n+      if (imm_ != NULL) {\n+        CompactMemTable();\n+        bg_cv_.SignalAll();  // Wakeup MakeRoomForWrite() if necessary\n+      }\n+      mutex_.Unlock();\n+      imm_micros += (env_->NowMicros() - imm_start);\n+    }\n+\n+    Slice key = input->key();\n+    if (compact->compaction->ShouldStopBefore(key) &&\n+        compact->builder != NULL) {\n+      status = FinishCompactionOutputFile(compact, input);\n+      if (!status.ok()) {\n+        break;\n+      }\n+    }\n+\n+    // Handle key/value, add to state, etc.\n+    bool drop = false;\n+    if (!ParseInternalKey(key, &ikey)) {\n+      // Do not hide error keys\n+      current_user_key.clear();\n+      has_current_user_key = false;\n+      last_sequence_for_key = kMaxSequenceNumber;\n+    } else {\n+      if (!has_current_user_key ||\n+          user_comparator()->Compare(ikey.user_key,\n+                                     Slice(current_user_key)) != 0) {\n+        // First occurrence of this user key\n+        current_user_key.assign(ikey.user_key.data(), ikey.user_key.size());\n+        has_current_user_key = true;\n+        last_sequence_for_key = kMaxSequenceNumber;\n+      }\n+\n+      if (last_sequence_for_key <= compact->smallest_snapshot) {\n+        // Hidden by an newer entry for same user key\n+        drop = true;    // (A)\n+      } else if (ikey.type == kTypeDeletion &&\n+                 ikey.sequence <= compact->smallest_snapshot &&\n+                 compact->compaction->IsBaseLevelForKey(ikey.user_key)) {\n+        // For this user key:\n+        // (1) there is no data in higher levels\n+        // (2) data in lower levels will have larger sequence numbers\n+        // (3) data in layers that are being compacted here and have\n+        //     smaller sequence numbers will be dropped in the next\n+        //     few iterations of this loop (by rule (A) above).\n+        // Therefore this deletion marker is obsolete and can be dropped.\n+        drop = true;\n+      }\n+\n+      last_sequence_for_key = ikey.sequence;\n+    }\n+#if 0\n+    Log(options_.info_log,\n+        \"  Compact: %s, seq %d, type: %d %d, drop: %d, is_base: %d, \"\n+        \"%d smallest_snapshot: %d\",\n+        ikey.user_key.ToString().c_str(),\n+        (int)ikey.sequence, ikey.type, kTypeValue, drop,\n+        compact->compaction->IsBaseLevelForKey(ikey.user_key),\n+        (int)last_sequence_for_key, (int)compact->smallest_snapshot);\n+#endif\n+\n+    if (!drop) {\n+      // Open output file if necessary\n+      if (compact->builder == NULL) {\n+        status = OpenCompactionOutputFile(compact);\n+        if (!status.ok()) {\n+          break;\n+        }\n+      }\n+      if (compact->builder->NumEntries() == 0) {\n+        compact->current_output()->smallest.DecodeFrom(key);\n+      }\n+      compact->current_output()->largest.DecodeFrom(key);\n+      compact->builder->Add(key, input->value());\n+\n+      // Close output file if it is big enough\n+      if (compact->builder->FileSize() >=\n+          compact->compaction->MaxOutputFileSize()) {\n+        status = FinishCompactionOutputFile(compact, input);\n+        if (!status.ok()) {\n+          break;\n+        }\n+      }\n+    }\n+\n+    input->Next();\n+  }\n+\n+  if (status.ok() && shutting_down_.Acquire_Load()) {\n+    status = Status::IOError(\"Deleting DB during compaction\");\n+  }\n+  if (status.ok() && compact->builder != NULL) {\n+    status = FinishCompactionOutputFile(compact, input);\n+  }\n+  if (status.ok()) {\n+    status = input->status();\n+  }\n+  delete input;\n+  input = NULL;\n+\n+  CompactionStats stats;\n+  stats.micros = env_->NowMicros() - start_micros - imm_micros;\n+  for (int which = 0; which < 2; which++) {\n+    for (int i = 0; i < compact->compaction->num_input_files(which); i++) {\n+      stats.bytes_read += compact->compaction->input(which, i)->file_size;\n+    }\n+  }\n+  for (size_t i = 0; i < compact->outputs.size(); i++) {\n+    stats.bytes_written += compact->outputs[i].file_size;\n+  }\n+\n+  mutex_.Lock();\n+  stats_[compact->compaction->level() + 1].Add(stats);\n+\n+  if (status.ok()) {\n+    status = InstallCompactionResults(compact);\n+  }\n+  VersionSet::LevelSummaryStorage tmp;\n+  Log(options_.info_log,\n+      \"compacted to: %s\", versions_->LevelSummary(&tmp));\n+  return status;\n+}\n+\n+namespace {\n+struct IterState {\n+  port::Mutex* mu;\n+  Version* version;\n+  MemTable* mem;\n+  MemTable* imm;\n+};\n+\n+static void CleanupIteratorState(void* arg1, void* arg2) {\n+  IterState* state = reinterpret_cast<IterState*>(arg1);\n+  state->mu->Lock();\n+  state->mem->Unref();\n+  if (state->imm != NULL) state->imm->Unref();\n+  state->version->Unref();\n+  state->mu->Unlock();\n+  delete state;\n+}\n+}  // namespace\n+\n+Iterator* DBImpl::NewInternalIterator(const ReadOptions& options,\n+                                      SequenceNumber* latest_snapshot) {\n+  IterState* cleanup = new IterState;\n+  mutex_.Lock();\n+  *latest_snapshot = versions_->LastSequence();\n+\n+  // Collect together all needed child iterators\n+  std::vector<Iterator*> list;\n+  list.push_back(mem_->NewIterator());\n+  mem_->Ref();\n+  if (imm_ != NULL) {\n+    list.push_back(imm_->NewIterator());\n+    imm_->Ref();\n+  }\n+  versions_->current()->AddIterators(options, &list);\n+  Iterator* internal_iter =\n+      NewMergingIterator(&internal_comparator_, &list[0], list.size());\n+  versions_->current()->Ref();\n+\n+  cleanup->mu = &mutex_;\n+  cleanup->mem = mem_;\n+  cleanup->imm = imm_;\n+  cleanup->version = versions_->current();\n+  internal_iter->RegisterCleanup(CleanupIteratorState, cleanup, NULL);\n+\n+  mutex_.Unlock();\n+  return internal_iter;\n+}\n+\n+Iterator* DBImpl::TEST_NewInternalIterator() {\n+  SequenceNumber ignored;\n+  return NewInternalIterator(ReadOptions(), &ignored);\n+}\n+\n+int64_t DBImpl::TEST_MaxNextLevelOverlappingBytes() {\n+  MutexLock l(&mutex_);\n+  return versions_->MaxNextLevelOverlappingBytes();\n+}\n+\n+Status DBImpl::Get(const ReadOptions& options,\n+                   const Slice& key,\n+                   std::string* value) {\n+  Status s;\n+  MutexLock l(&mutex_);\n+  SequenceNumber snapshot;\n+  if (options.snapshot != NULL) {\n+    snapshot = reinterpret_cast<const SnapshotImpl*>(options.snapshot)->number_;\n+  } else {\n+    snapshot = versions_->LastSequence();\n+  }\n+\n+  MemTable* mem = mem_;\n+  MemTable* imm = imm_;\n+  Version* current = versions_->current();\n+  mem->Ref();\n+  if (imm != NULL) imm->Ref();\n+  current->Ref();\n+\n+  bool have_stat_update = false;\n+  Version::GetStats stats;\n+\n+  // Unlock while reading from files and memtables\n+  {\n+    mutex_.Unlock();\n+    // First look in the memtable, then in the immutable memtable (if any).\n+    LookupKey lkey(key, snapshot);\n+    if (mem->Get(lkey, value, &s)) {\n+      // Done\n+    } else if (imm != NULL && imm->Get(lkey, value, &s)) {\n+      // Done\n+    } else {\n+      s = current->Get(options, lkey, value, &stats);\n+      have_stat_update = true;\n+    }\n+    mutex_.Lock();\n+  }\n+\n+  if (have_stat_update && current->UpdateStats(stats)) {\n+    MaybeScheduleCompaction();\n+  }\n+  mem->Unref();\n+  if (imm != NULL) imm->Unref();\n+  current->Unref();\n+  return s;\n+}\n+\n+Iterator* DBImpl::NewIterator(const ReadOptions& options) {\n+  SequenceNumber latest_snapshot;\n+  Iterator* internal_iter = NewInternalIterator(options, &latest_snapshot);\n+  return NewDBIterator(\n+      &dbname_, env_, user_comparator(), internal_iter,\n+      (options.snapshot != NULL\n+       ? reinterpret_cast<const SnapshotImpl*>(options.snapshot)->number_\n+       : latest_snapshot));\n+}\n+\n+const Snapshot* DBImpl::GetSnapshot() {\n+  MutexLock l(&mutex_);\n+  return snapshots_.New(versions_->LastSequence());\n+}\n+\n+void DBImpl::ReleaseSnapshot(const Snapshot* s) {\n+  MutexLock l(&mutex_);\n+  snapshots_.Delete(reinterpret_cast<const SnapshotImpl*>(s));\n+}\n+\n+// Convenience methods\n+Status DBImpl::Put(const WriteOptions& o, const Slice& key, const Slice& val) {\n+  return DB::Put(o, key, val);\n+}\n+\n+Status DBImpl::Delete(const WriteOptions& options, const Slice& key) {\n+  return DB::Delete(options, key);\n+}\n+\n+Status DBImpl::Write(const WriteOptions& options, WriteBatch* my_batch) {\n+  Writer w(&mutex_);\n+  w.batch = my_batch;\n+  w.sync = options.sync;\n+  w.done = false;\n+\n+  MutexLock l(&mutex_);\n+  writers_.push_back(&w);\n+  while (!w.done && &w != writers_.front()) {\n+    w.cv.Wait();\n+  }\n+  if (w.done) {\n+    return w.status;\n+  }\n+\n+  // May temporarily unlock and wait.\n+  Status status = MakeRoomForWrite(my_batch == NULL);\n+  uint64_t last_sequence = versions_->LastSequence();\n+  Writer* last_writer = &w;\n+  if (status.ok() && my_batch != NULL) {  // NULL batch is for compactions\n+    WriteBatch* updates = BuildBatchGroup(&last_writer);\n+    WriteBatchInternal::SetSequence(updates, last_sequence + 1);\n+    last_sequence += WriteBatchInternal::Count(updates);\n+\n+    // Add to log and apply to memtable.  We can release the lock\n+    // during this phase since &w is currently responsible for logging\n+    // and protects against concurrent loggers and concurrent writes\n+    // into mem_.\n+    {\n+      mutex_.Unlock();\n+      status = log_->AddRecord(WriteBatchInternal::Contents(updates));\n+      if (status.ok() && options.sync) {\n+        status = logfile_->Sync();\n+      }\n+      if (status.ok()) {\n+        status = WriteBatchInternal::InsertInto(updates, mem_);\n+      }\n+      mutex_.Lock();\n+    }\n+    if (updates == tmp_batch_) tmp_batch_->Clear();\n+\n+    versions_->SetLastSequence(last_sequence);\n+  }\n+\n+  while (true) {\n+    Writer* ready = writers_.front();\n+    writers_.pop_front();\n+    if (ready != &w) {\n+      ready->status = status;\n+      ready->done = true;\n+      ready->cv.Signal();\n+    }\n+    if (ready == last_writer) break;\n+  }\n+\n+  // Notify new head of write queue\n+  if (!writers_.empty()) {\n+    writers_.front()->cv.Signal();\n+  }\n+\n+  return status;\n+}\n+\n+// REQUIRES: Writer list must be non-empty\n+// REQUIRES: First writer must have a non-NULL batch\n+WriteBatch* DBImpl::BuildBatchGroup(Writer** last_writer) {\n+  assert(!writers_.empty());\n+  Writer* first = writers_.front();\n+  WriteBatch* result = first->batch;\n+  assert(result != NULL);\n+\n+  size_t size = WriteBatchInternal::ByteSize(first->batch);\n+\n+  // Allow the group to grow up to a maximum size, but if the\n+  // original write is small, limit the growth so we do not slow\n+  // down the small write too much.\n+  size_t max_size = 1 << 20;\n+  if (size <= (128<<10)) {\n+    max_size = size + (128<<10);\n+  }\n+\n+  *last_writer = first;\n+  std::deque<Writer*>::iterator iter = writers_.begin();\n+  ++iter;  // Advance past \"first\"\n+  for (; iter != writers_.end(); ++iter) {\n+    Writer* w = *iter;\n+    if (w->sync && !first->sync) {\n+      // Do not include a sync write into a batch handled by a non-sync write.\n+      break;\n+    }\n+\n+    if (w->batch != NULL) {\n+      size += WriteBatchInternal::ByteSize(w->batch);\n+      if (size > max_size) {\n+        // Do not make batch too big\n+        break;\n+      }\n+\n+      // Append to *reuslt\n+      if (result == first->batch) {\n+        // Switch to temporary batch instead of disturbing caller's batch\n+        result = tmp_batch_;\n+        assert(WriteBatchInternal::Count(result) == 0);\n+        WriteBatchInternal::Append(result, first->batch);\n+      }\n+      WriteBatchInternal::Append(result, w->batch);\n+    }\n+    *last_writer = w;\n+  }\n+  return result;\n+}\n+\n+// REQUIRES: mutex_ is held\n+// REQUIRES: this thread is currently at the front of the writer queue\n+Status DBImpl::MakeRoomForWrite(bool force) {\n+  mutex_.AssertHeld();\n+  assert(!writers_.empty());\n+  bool allow_delay = !force;\n+  Status s;\n+  while (true) {\n+    if (!bg_error_.ok()) {\n+      // Yield previous error\n+      s = bg_error_;\n+      break;\n+    } else if (\n+        allow_delay &&\n+        versions_->NumLevelFiles(0) >= config::kL0_SlowdownWritesTrigger) {\n+      // We are getting close to hitting a hard limit on the number of\n+      // L0 files.  Rather than delaying a single write by several\n+      // seconds when we hit the hard limit, start delaying each\n+      // individual write by 1ms to reduce latency variance.  Also,\n+      // this delay hands over some CPU to the compaction thread in\n+      // case it is sharing the same core as the writer.\n+      mutex_.Unlock();\n+      env_->SleepForMicroseconds(1000);\n+      allow_delay = false;  // Do not delay a single write more than once\n+      mutex_.Lock();\n+    } else if (!force &&\n+               (mem_->ApproximateMemoryUsage() <= options_.write_buffer_size)) {\n+      // There is room in current memtable\n+      break;\n+    } else if (imm_ != NULL) {\n+      // We have filled up the current memtable, but the previous\n+      // one is still being compacted, so we wait.\n+      Log(options_.info_log, \"Current memtable full; waiting...\\n\");\n+      bg_cv_.Wait();\n+    } else if (versions_->NumLevelFiles(0) >= config::kL0_StopWritesTrigger) {\n+      // There are too many level-0 files.\n+      Log(options_.info_log, \"Too many L0 files; waiting...\\n\");\n+      bg_cv_.Wait();\n+    } else {\n+      // Attempt to switch to a new memtable and trigger compaction of old\n+      assert(versions_->PrevLogNumber() == 0);\n+      uint64_t new_log_number = versions_->NewFileNumber();\n+      WritableFile* lfile = NULL;\n+      s = env_->NewWritableFile(LogFileName(dbname_, new_log_number), &lfile);\n+      if (!s.ok()) {\n+        // Avoid chewing through file number space in a tight loop.\n+        versions_->ReuseFileNumber(new_log_number);\n+        break;\n+      }\n+      delete log_;\n+      delete logfile_;\n+      logfile_ = lfile;\n+      logfile_number_ = new_log_number;\n+      log_ = new log::Writer(lfile);\n+      imm_ = mem_;\n+      has_imm_.Release_Store(imm_);\n+      mem_ = new MemTable(internal_comparator_);\n+      mem_->Ref();\n+      force = false;   // Do not force another compaction if have room\n+      MaybeScheduleCompaction();\n+    }\n+  }\n+  return s;\n+}\n+\n+bool DBImpl::GetProperty(const Slice& property, std::string* value) {\n+  value->clear();\n+\n+  MutexLock l(&mutex_);\n+  Slice in = property;\n+  Slice prefix(\"leveldb.\");\n+  if (!in.starts_with(prefix)) return false;\n+  in.remove_prefix(prefix.size());\n+\n+  if (in.starts_with(\"num-files-at-level\")) {\n+    in.remove_prefix(strlen(\"num-files-at-level\"));\n+    uint64_t level;\n+    bool ok = ConsumeDecimalNumber(&in, &level) && in.empty();\n+    if (!ok || level >= config::kNumLevels) {\n+      return false;\n+    } else {\n+      char buf[100];\n+      snprintf(buf, sizeof(buf), \"%d\",\n+               versions_->NumLevelFiles(static_cast<int>(level)));\n+      *value = buf;\n+      return true;\n+    }\n+  } else if (in == \"stats\") {\n+    char buf[200];\n+    snprintf(buf, sizeof(buf),\n+             \"                               Compactions\\n\"\n+             \"Level  Files Size(MB) Time(sec) Read(MB) Write(MB)\\n\"\n+             \"--------------------------------------------------\\n\"\n+             );\n+    value->append(buf);\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      int files = versions_->NumLevelFiles(level);\n+      if (stats_[level].micros > 0 || files > 0) {\n+        snprintf(\n+            buf, sizeof(buf),\n+            \"%3d %8d %8.0f %9.0f %8.0f %9.0f\\n\",\n+            level,\n+            files,\n+            versions_->NumLevelBytes(level) / 1048576.0,\n+            stats_[level].micros / 1e6,\n+            stats_[level].bytes_read / 1048576.0,\n+            stats_[level].bytes_written / 1048576.0);\n+        value->append(buf);\n+      }\n+    }\n+    return true;\n+  } else if (in == \"sstables\") {\n+    *value = versions_->current()->DebugString();\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void DBImpl::GetApproximateSizes(\n+    const Range* range, int n,\n+    uint64_t* sizes) {\n+  // TODO(opt): better implementation\n+  Version* v;\n+  {\n+    MutexLock l(&mutex_);\n+    versions_->current()->Ref();\n+    v = versions_->current();\n+  }\n+\n+  for (int i = 0; i < n; i++) {\n+    // Convert user_key into a corresponding internal key.\n+    InternalKey k1(range[i].start, kMaxSequenceNumber, kValueTypeForSeek);\n+    InternalKey k2(range[i].limit, kMaxSequenceNumber, kValueTypeForSeek);\n+    uint64_t start = versions_->ApproximateOffsetOf(v, k1);\n+    uint64_t limit = versions_->ApproximateOffsetOf(v, k2);\n+    sizes[i] = (limit >= start ? limit - start : 0);\n+  }\n+\n+  {\n+    MutexLock l(&mutex_);\n+    v->Unref();\n+  }\n+}\n+\n+// Default implementations of convenience methods that subclasses of DB\n+// can call if they wish\n+Status DB::Put(const WriteOptions& opt, const Slice& key, const Slice& value) {\n+  WriteBatch batch;\n+  batch.Put(key, value);\n+  return Write(opt, &batch);\n+}\n+\n+Status DB::Delete(const WriteOptions& opt, const Slice& key) {\n+  WriteBatch batch;\n+  batch.Delete(key);\n+  return Write(opt, &batch);\n+}\n+\n+DB::~DB() { }\n+\n+Status DB::Open(const Options& options, const std::string& dbname,\n+                DB** dbptr) {\n+  *dbptr = NULL;\n+\n+  DBImpl* impl = new DBImpl(options, dbname);\n+  impl->mutex_.Lock();\n+  VersionEdit edit;\n+  Status s = impl->Recover(&edit); // Handles create_if_missing, error_if_exists\n+  if (s.ok()) {\n+    uint64_t new_log_number = impl->versions_->NewFileNumber();\n+    WritableFile* lfile;\n+    s = options.env->NewWritableFile(LogFileName(dbname, new_log_number),\n+                                     &lfile);\n+    if (s.ok()) {\n+      edit.SetLogNumber(new_log_number);\n+      impl->logfile_ = lfile;\n+      impl->logfile_number_ = new_log_number;\n+      impl->log_ = new log::Writer(lfile);\n+      s = impl->versions_->LogAndApply(&edit, &impl->mutex_);\n+    }\n+    if (s.ok()) {\n+      impl->DeleteObsoleteFiles();\n+      impl->MaybeScheduleCompaction();\n+    }\n+  }\n+  impl->mutex_.Unlock();\n+  if (s.ok()) {\n+    *dbptr = impl;\n+  } else {\n+    delete impl;\n+  }\n+  return s;\n+}\n+\n+Snapshot::~Snapshot() {\n+}\n+\n+Status DestroyDB(const std::string& dbname, const Options& options) {\n+  Env* env = options.env;\n+  std::vector<std::string> filenames;\n+  // Ignore error in case directory does not exist\n+  env->GetChildren(dbname, &filenames);\n+  if (filenames.empty()) {\n+    return Status::OK();\n+  }\n+\n+  FileLock* lock;\n+  const std::string lockname = LockFileName(dbname);\n+  Status result = env->LockFile(lockname, &lock);\n+  if (result.ok()) {\n+    uint64_t number;\n+    FileType type;\n+    for (size_t i = 0; i < filenames.size(); i++) {\n+      if (ParseFileName(filenames[i], &number, &type) &&\n+          type != kDBLockFile) {  // Lock file will be deleted at end\n+        Status del = env->DeleteFile(dbname + \"/\" + filenames[i]);\n+        if (result.ok() && !del.ok()) {\n+          result = del;\n+        }\n+      }\n+    }\n+    env->UnlockFile(lock);  // Ignore error since state is already gone\n+    env->DeleteFile(lockname);\n+    env->DeleteDir(dbname);  // Ignore error in case dir contains other files\n+  }\n+  return result;\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "3c8d711ae06de2b41dc6d9a3717808947663c09b",
        "filename": "src/leveldb/db/db_impl.h",
        "status": "added",
        "additions": 203,
        "deletions": 0,
        "changes": 203,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/db_impl.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/db_impl.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_impl.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,203 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_DB_IMPL_H_\n+#define STORAGE_LEVELDB_DB_DB_IMPL_H_\n+\n+#include <deque>\n+#include <set>\n+#include \"db/dbformat.h\"\n+#include \"db/log_writer.h\"\n+#include \"db/snapshot.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+#include \"port/port.h\"\n+#include \"port/thread_annotations.h\"\n+\n+namespace leveldb {\n+\n+class MemTable;\n+class TableCache;\n+class Version;\n+class VersionEdit;\n+class VersionSet;\n+\n+class DBImpl : public DB {\n+ public:\n+  DBImpl(const Options& options, const std::string& dbname);\n+  virtual ~DBImpl();\n+\n+  // Implementations of the DB interface\n+  virtual Status Put(const WriteOptions&, const Slice& key, const Slice& value);\n+  virtual Status Delete(const WriteOptions&, const Slice& key);\n+  virtual Status Write(const WriteOptions& options, WriteBatch* updates);\n+  virtual Status Get(const ReadOptions& options,\n+                     const Slice& key,\n+                     std::string* value);\n+  virtual Iterator* NewIterator(const ReadOptions&);\n+  virtual const Snapshot* GetSnapshot();\n+  virtual void ReleaseSnapshot(const Snapshot* snapshot);\n+  virtual bool GetProperty(const Slice& property, std::string* value);\n+  virtual void GetApproximateSizes(const Range* range, int n, uint64_t* sizes);\n+  virtual void CompactRange(const Slice* begin, const Slice* end);\n+\n+  // Extra methods (for testing) that are not in the public DB interface\n+\n+  // Compact any files in the named level that overlap [*begin,*end]\n+  void TEST_CompactRange(int level, const Slice* begin, const Slice* end);\n+\n+  // Force current memtable contents to be compacted.\n+  Status TEST_CompactMemTable();\n+\n+  // Return an internal iterator over the current state of the database.\n+  // The keys of this iterator are internal keys (see format.h).\n+  // The returned iterator should be deleted when no longer needed.\n+  Iterator* TEST_NewInternalIterator();\n+\n+  // Return the maximum overlapping data (in bytes) at next level for any\n+  // file at a level >= 1.\n+  int64_t TEST_MaxNextLevelOverlappingBytes();\n+\n+ private:\n+  friend class DB;\n+  struct CompactionState;\n+  struct Writer;\n+\n+  Iterator* NewInternalIterator(const ReadOptions&,\n+                                SequenceNumber* latest_snapshot);\n+\n+  Status NewDB();\n+\n+  // Recover the descriptor from persistent storage.  May do a significant\n+  // amount of work to recover recently logged updates.  Any changes to\n+  // be made to the descriptor are added to *edit.\n+  Status Recover(VersionEdit* edit) EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+\n+  void MaybeIgnoreError(Status* s) const;\n+\n+  // Delete any unneeded files and stale in-memory entries.\n+  void DeleteObsoleteFiles();\n+\n+  // Compact the in-memory write buffer to disk.  Switches to a new\n+  // log-file/memtable and writes a new descriptor iff successful.\n+  Status CompactMemTable()\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+\n+  Status RecoverLogFile(uint64_t log_number,\n+                        VersionEdit* edit,\n+                        SequenceNumber* max_sequence)\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+\n+  Status WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base)\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+\n+  Status MakeRoomForWrite(bool force /* compact even if there is room? */)\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+  WriteBatch* BuildBatchGroup(Writer** last_writer);\n+\n+  void MaybeScheduleCompaction() EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+  static void BGWork(void* db);\n+  void BackgroundCall();\n+  Status BackgroundCompaction() EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+  void CleanupCompaction(CompactionState* compact)\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+  Status DoCompactionWork(CompactionState* compact)\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+\n+  Status OpenCompactionOutputFile(CompactionState* compact);\n+  Status FinishCompactionOutputFile(CompactionState* compact, Iterator* input);\n+  Status InstallCompactionResults(CompactionState* compact)\n+      EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n+\n+  // Constant after construction\n+  Env* const env_;\n+  const InternalKeyComparator internal_comparator_;\n+  const InternalFilterPolicy internal_filter_policy_;\n+  const Options options_;  // options_.comparator == &internal_comparator_\n+  bool owns_info_log_;\n+  bool owns_cache_;\n+  const std::string dbname_;\n+\n+  // table_cache_ provides its own synchronization\n+  TableCache* table_cache_;\n+\n+  // Lock over the persistent DB state.  Non-NULL iff successfully acquired.\n+  FileLock* db_lock_;\n+\n+  // State below is protected by mutex_\n+  port::Mutex mutex_;\n+  port::AtomicPointer shutting_down_;\n+  port::CondVar bg_cv_;          // Signalled when background work finishes\n+  MemTable* mem_;\n+  MemTable* imm_;                // Memtable being compacted\n+  port::AtomicPointer has_imm_;  // So bg thread can detect non-NULL imm_\n+  WritableFile* logfile_;\n+  uint64_t logfile_number_;\n+  log::Writer* log_;\n+\n+  // Queue of writers.\n+  std::deque<Writer*> writers_;\n+  WriteBatch* tmp_batch_;\n+\n+  SnapshotList snapshots_;\n+\n+  // Set of table files to protect from deletion because they are\n+  // part of ongoing compactions.\n+  std::set<uint64_t> pending_outputs_;\n+\n+  // Has a background compaction been scheduled or is running?\n+  bool bg_compaction_scheduled_;\n+\n+  // Information for a manual compaction\n+  struct ManualCompaction {\n+    int level;\n+    bool done;\n+    const InternalKey* begin;   // NULL means beginning of key range\n+    const InternalKey* end;     // NULL means end of key range\n+    InternalKey tmp_storage;    // Used to keep track of compaction progress\n+  };\n+  ManualCompaction* manual_compaction_;\n+\n+  VersionSet* versions_;\n+\n+  // Have we encountered a background error in paranoid mode?\n+  Status bg_error_;\n+  int consecutive_compaction_errors_;\n+\n+  // Per level compaction stats.  stats_[level] stores the stats for\n+  // compactions that produced data for the specified \"level\".\n+  struct CompactionStats {\n+    int64_t micros;\n+    int64_t bytes_read;\n+    int64_t bytes_written;\n+\n+    CompactionStats() : micros(0), bytes_read(0), bytes_written(0) { }\n+\n+    void Add(const CompactionStats& c) {\n+      this->micros += c.micros;\n+      this->bytes_read += c.bytes_read;\n+      this->bytes_written += c.bytes_written;\n+    }\n+  };\n+  CompactionStats stats_[config::kNumLevels];\n+\n+  // No copying allowed\n+  DBImpl(const DBImpl&);\n+  void operator=(const DBImpl&);\n+\n+  const Comparator* user_comparator() const {\n+    return internal_comparator_.user_comparator();\n+  }\n+};\n+\n+// Sanitize db options.  The caller should delete result.info_log if\n+// it is not equal to src.info_log.\n+extern Options SanitizeOptions(const std::string& db,\n+                               const InternalKeyComparator* icmp,\n+                               const InternalFilterPolicy* ipolicy,\n+                               const Options& src);\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_DB_IMPL_H_"
      },
      {
        "sha": "87dca2ded46453dec5e04cef652d2545086af485",
        "filename": "src/leveldb/db/db_iter.cc",
        "status": "added",
        "additions": 299,
        "deletions": 0,
        "changes": 299,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/db_iter.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/db_iter.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_iter.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,299 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/db_iter.h\"\n+\n+#include \"db/filename.h\"\n+#include \"db/dbformat.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/iterator.h\"\n+#include \"port/port.h\"\n+#include \"util/logging.h\"\n+#include \"util/mutexlock.h\"\n+\n+namespace leveldb {\n+\n+#if 0\n+static void DumpInternalIter(Iterator* iter) {\n+  for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+    ParsedInternalKey k;\n+    if (!ParseInternalKey(iter->key(), &k)) {\n+      fprintf(stderr, \"Corrupt '%s'\\n\", EscapeString(iter->key()).c_str());\n+    } else {\n+      fprintf(stderr, \"@ '%s'\\n\", k.DebugString().c_str());\n+    }\n+  }\n+}\n+#endif\n+\n+namespace {\n+\n+// Memtables and sstables that make the DB representation contain\n+// (userkey,seq,type) => uservalue entries.  DBIter\n+// combines multiple entries for the same userkey found in the DB\n+// representation into a single entry while accounting for sequence\n+// numbers, deletion markers, overwrites, etc.\n+class DBIter: public Iterator {\n+ public:\n+  // Which direction is the iterator currently moving?\n+  // (1) When moving forward, the internal iterator is positioned at\n+  //     the exact entry that yields this->key(), this->value()\n+  // (2) When moving backwards, the internal iterator is positioned\n+  //     just before all entries whose user key == this->key().\n+  enum Direction {\n+    kForward,\n+    kReverse\n+  };\n+\n+  DBIter(const std::string* dbname, Env* env,\n+         const Comparator* cmp, Iterator* iter, SequenceNumber s)\n+      : dbname_(dbname),\n+        env_(env),\n+        user_comparator_(cmp),\n+        iter_(iter),\n+        sequence_(s),\n+        direction_(kForward),\n+        valid_(false) {\n+  }\n+  virtual ~DBIter() {\n+    delete iter_;\n+  }\n+  virtual bool Valid() const { return valid_; }\n+  virtual Slice key() const {\n+    assert(valid_);\n+    return (direction_ == kForward) ? ExtractUserKey(iter_->key()) : saved_key_;\n+  }\n+  virtual Slice value() const {\n+    assert(valid_);\n+    return (direction_ == kForward) ? iter_->value() : saved_value_;\n+  }\n+  virtual Status status() const {\n+    if (status_.ok()) {\n+      return iter_->status();\n+    } else {\n+      return status_;\n+    }\n+  }\n+\n+  virtual void Next();\n+  virtual void Prev();\n+  virtual void Seek(const Slice& target);\n+  virtual void SeekToFirst();\n+  virtual void SeekToLast();\n+\n+ private:\n+  void FindNextUserEntry(bool skipping, std::string* skip);\n+  void FindPrevUserEntry();\n+  bool ParseKey(ParsedInternalKey* key);\n+\n+  inline void SaveKey(const Slice& k, std::string* dst) {\n+    dst->assign(k.data(), k.size());\n+  }\n+\n+  inline void ClearSavedValue() {\n+    if (saved_value_.capacity() > 1048576) {\n+      std::string empty;\n+      swap(empty, saved_value_);\n+    } else {\n+      saved_value_.clear();\n+    }\n+  }\n+\n+  const std::string* const dbname_;\n+  Env* const env_;\n+  const Comparator* const user_comparator_;\n+  Iterator* const iter_;\n+  SequenceNumber const sequence_;\n+\n+  Status status_;\n+  std::string saved_key_;     // == current key when direction_==kReverse\n+  std::string saved_value_;   // == current raw value when direction_==kReverse\n+  Direction direction_;\n+  bool valid_;\n+\n+  // No copying allowed\n+  DBIter(const DBIter&);\n+  void operator=(const DBIter&);\n+};\n+\n+inline bool DBIter::ParseKey(ParsedInternalKey* ikey) {\n+  if (!ParseInternalKey(iter_->key(), ikey)) {\n+    status_ = Status::Corruption(\"corrupted internal key in DBIter\");\n+    return false;\n+  } else {\n+    return true;\n+  }\n+}\n+\n+void DBIter::Next() {\n+  assert(valid_);\n+\n+  if (direction_ == kReverse) {  // Switch directions?\n+    direction_ = kForward;\n+    // iter_ is pointing just before the entries for this->key(),\n+    // so advance into the range of entries for this->key() and then\n+    // use the normal skipping code below.\n+    if (!iter_->Valid()) {\n+      iter_->SeekToFirst();\n+    } else {\n+      iter_->Next();\n+    }\n+    if (!iter_->Valid()) {\n+      valid_ = false;\n+      saved_key_.clear();\n+      return;\n+    }\n+  }\n+\n+  // Temporarily use saved_key_ as storage for key to skip.\n+  std::string* skip = &saved_key_;\n+  SaveKey(ExtractUserKey(iter_->key()), skip);\n+  FindNextUserEntry(true, skip);\n+}\n+\n+void DBIter::FindNextUserEntry(bool skipping, std::string* skip) {\n+  // Loop until we hit an acceptable entry to yield\n+  assert(iter_->Valid());\n+  assert(direction_ == kForward);\n+  do {\n+    ParsedInternalKey ikey;\n+    if (ParseKey(&ikey) && ikey.sequence <= sequence_) {\n+      switch (ikey.type) {\n+        case kTypeDeletion:\n+          // Arrange to skip all upcoming entries for this key since\n+          // they are hidden by this deletion.\n+          SaveKey(ikey.user_key, skip);\n+          skipping = true;\n+          break;\n+        case kTypeValue:\n+          if (skipping &&\n+              user_comparator_->Compare(ikey.user_key, *skip) <= 0) {\n+            // Entry hidden\n+          } else {\n+            valid_ = true;\n+            saved_key_.clear();\n+            return;\n+          }\n+          break;\n+      }\n+    }\n+    iter_->Next();\n+  } while (iter_->Valid());\n+  saved_key_.clear();\n+  valid_ = false;\n+}\n+\n+void DBIter::Prev() {\n+  assert(valid_);\n+\n+  if (direction_ == kForward) {  // Switch directions?\n+    // iter_ is pointing at the current entry.  Scan backwards until\n+    // the key changes so we can use the normal reverse scanning code.\n+    assert(iter_->Valid());  // Otherwise valid_ would have been false\n+    SaveKey(ExtractUserKey(iter_->key()), &saved_key_);\n+    while (true) {\n+      iter_->Prev();\n+      if (!iter_->Valid()) {\n+        valid_ = false;\n+        saved_key_.clear();\n+        ClearSavedValue();\n+        return;\n+      }\n+      if (user_comparator_->Compare(ExtractUserKey(iter_->key()),\n+                                    saved_key_) < 0) {\n+        break;\n+      }\n+    }\n+    direction_ = kReverse;\n+  }\n+\n+  FindPrevUserEntry();\n+}\n+\n+void DBIter::FindPrevUserEntry() {\n+  assert(direction_ == kReverse);\n+\n+  ValueType value_type = kTypeDeletion;\n+  if (iter_->Valid()) {\n+    do {\n+      ParsedInternalKey ikey;\n+      if (ParseKey(&ikey) && ikey.sequence <= sequence_) {\n+        if ((value_type != kTypeDeletion) &&\n+            user_comparator_->Compare(ikey.user_key, saved_key_) < 0) {\n+          // We encountered a non-deleted value in entries for previous keys,\n+          break;\n+        }\n+        value_type = ikey.type;\n+        if (value_type == kTypeDeletion) {\n+          saved_key_.clear();\n+          ClearSavedValue();\n+        } else {\n+          Slice raw_value = iter_->value();\n+          if (saved_value_.capacity() > raw_value.size() + 1048576) {\n+            std::string empty;\n+            swap(empty, saved_value_);\n+          }\n+          SaveKey(ExtractUserKey(iter_->key()), &saved_key_);\n+          saved_value_.assign(raw_value.data(), raw_value.size());\n+        }\n+      }\n+      iter_->Prev();\n+    } while (iter_->Valid());\n+  }\n+\n+  if (value_type == kTypeDeletion) {\n+    // End\n+    valid_ = false;\n+    saved_key_.clear();\n+    ClearSavedValue();\n+    direction_ = kForward;\n+  } else {\n+    valid_ = true;\n+  }\n+}\n+\n+void DBIter::Seek(const Slice& target) {\n+  direction_ = kForward;\n+  ClearSavedValue();\n+  saved_key_.clear();\n+  AppendInternalKey(\n+      &saved_key_, ParsedInternalKey(target, sequence_, kValueTypeForSeek));\n+  iter_->Seek(saved_key_);\n+  if (iter_->Valid()) {\n+    FindNextUserEntry(false, &saved_key_ /* temporary storage */);\n+  } else {\n+    valid_ = false;\n+  }\n+}\n+\n+void DBIter::SeekToFirst() {\n+  direction_ = kForward;\n+  ClearSavedValue();\n+  iter_->SeekToFirst();\n+  if (iter_->Valid()) {\n+    FindNextUserEntry(false, &saved_key_ /* temporary storage */);\n+  } else {\n+    valid_ = false;\n+  }\n+}\n+\n+void DBIter::SeekToLast() {\n+  direction_ = kReverse;\n+  ClearSavedValue();\n+  iter_->SeekToLast();\n+  FindPrevUserEntry();\n+}\n+\n+}  // anonymous namespace\n+\n+Iterator* NewDBIterator(\n+    const std::string* dbname,\n+    Env* env,\n+    const Comparator* user_key_comparator,\n+    Iterator* internal_iter,\n+    const SequenceNumber& sequence) {\n+  return new DBIter(dbname, env, user_key_comparator, internal_iter, sequence);\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "d9e1b174ab8726eeca58197bb2c36f73099d13e5",
        "filename": "src/leveldb/db/db_iter.h",
        "status": "added",
        "additions": 26,
        "deletions": 0,
        "changes": 26,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/db_iter.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/db_iter.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_iter.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,26 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_DB_ITER_H_\n+#define STORAGE_LEVELDB_DB_DB_ITER_H_\n+\n+#include <stdint.h>\n+#include \"leveldb/db.h\"\n+#include \"db/dbformat.h\"\n+\n+namespace leveldb {\n+\n+// Return a new iterator that converts internal keys (yielded by\n+// \"*internal_iter\") that were live at the specified \"sequence\" number\n+// into appropriate user keys.\n+extern Iterator* NewDBIterator(\n+    const std::string* dbname,\n+    Env* env,\n+    const Comparator* user_key_comparator,\n+    Iterator* internal_iter,\n+    const SequenceNumber& sequence);\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_DB_ITER_H_"
      },
      {
        "sha": "49aae04dbd3bac4b8d057fd490751f71a2d5b294",
        "filename": "src/leveldb/db/db_test.cc",
        "status": "added",
        "additions": 2092,
        "deletions": 0,
        "changes": 2092,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/db_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/db_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,2092 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"leveldb/db.h\"\n+#include \"leveldb/filter_policy.h\"\n+#include \"db/db_impl.h\"\n+#include \"db/filename.h\"\n+#include \"db/version_set.h\"\n+#include \"db/write_batch_internal.h\"\n+#include \"leveldb/cache.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/table.h\"\n+#include \"util/hash.h\"\n+#include \"util/logging.h\"\n+#include \"util/mutexlock.h\"\n+#include \"util/testharness.h\"\n+#include \"util/testutil.h\"\n+\n+namespace leveldb {\n+\n+static std::string RandomString(Random* rnd, int len) {\n+  std::string r;\n+  test::RandomString(rnd, len, &r);\n+  return r;\n+}\n+\n+namespace {\n+class AtomicCounter {\n+ private:\n+  port::Mutex mu_;\n+  int count_;\n+ public:\n+  AtomicCounter() : count_(0) { }\n+  void Increment() {\n+    IncrementBy(1);\n+  }\n+  void IncrementBy(int count) {\n+    MutexLock l(&mu_);\n+    count_ += count;\n+  }\n+  int Read() {\n+    MutexLock l(&mu_);\n+    return count_;\n+  }\n+  void Reset() {\n+    MutexLock l(&mu_);\n+    count_ = 0;\n+  }\n+};\n+\n+void DelayMilliseconds(int millis) {\n+  Env::Default()->SleepForMicroseconds(millis * 1000);\n+}\n+}\n+\n+// Special Env used to delay background operations\n+class SpecialEnv : public EnvWrapper {\n+ public:\n+  // sstable Sync() calls are blocked while this pointer is non-NULL.\n+  port::AtomicPointer delay_sstable_sync_;\n+\n+  // Simulate no-space errors while this pointer is non-NULL.\n+  port::AtomicPointer no_space_;\n+\n+  // Simulate non-writable file system while this pointer is non-NULL\n+  port::AtomicPointer non_writable_;\n+\n+  // Force sync of manifest files to fail while this pointer is non-NULL\n+  port::AtomicPointer manifest_sync_error_;\n+\n+  // Force write to manifest files to fail while this pointer is non-NULL\n+  port::AtomicPointer manifest_write_error_;\n+\n+  bool count_random_reads_;\n+  AtomicCounter random_read_counter_;\n+\n+  AtomicCounter sleep_counter_;\n+  AtomicCounter sleep_time_counter_;\n+\n+  explicit SpecialEnv(Env* base) : EnvWrapper(base) {\n+    delay_sstable_sync_.Release_Store(NULL);\n+    no_space_.Release_Store(NULL);\n+    non_writable_.Release_Store(NULL);\n+    count_random_reads_ = false;\n+    manifest_sync_error_.Release_Store(NULL);\n+    manifest_write_error_.Release_Store(NULL);\n+  }\n+\n+  Status NewWritableFile(const std::string& f, WritableFile** r) {\n+    class SSTableFile : public WritableFile {\n+     private:\n+      SpecialEnv* env_;\n+      WritableFile* base_;\n+\n+     public:\n+      SSTableFile(SpecialEnv* env, WritableFile* base)\n+          : env_(env),\n+            base_(base) {\n+      }\n+      ~SSTableFile() { delete base_; }\n+      Status Append(const Slice& data) {\n+        if (env_->no_space_.Acquire_Load() != NULL) {\n+          // Drop writes on the floor\n+          return Status::OK();\n+        } else {\n+          return base_->Append(data);\n+        }\n+      }\n+      Status Close() { return base_->Close(); }\n+      Status Flush() { return base_->Flush(); }\n+      Status Sync() {\n+        while (env_->delay_sstable_sync_.Acquire_Load() != NULL) {\n+          DelayMilliseconds(100);\n+        }\n+        return base_->Sync();\n+      }\n+    };\n+    class ManifestFile : public WritableFile {\n+     private:\n+      SpecialEnv* env_;\n+      WritableFile* base_;\n+     public:\n+      ManifestFile(SpecialEnv* env, WritableFile* b) : env_(env), base_(b) { }\n+      ~ManifestFile() { delete base_; }\n+      Status Append(const Slice& data) {\n+        if (env_->manifest_write_error_.Acquire_Load() != NULL) {\n+          return Status::IOError(\"simulated writer error\");\n+        } else {\n+          return base_->Append(data);\n+        }\n+      }\n+      Status Close() { return base_->Close(); }\n+      Status Flush() { return base_->Flush(); }\n+      Status Sync() {\n+        if (env_->manifest_sync_error_.Acquire_Load() != NULL) {\n+          return Status::IOError(\"simulated sync error\");\n+        } else {\n+          return base_->Sync();\n+        }\n+      }\n+    };\n+\n+    if (non_writable_.Acquire_Load() != NULL) {\n+      return Status::IOError(\"simulated write error\");\n+    }\n+\n+    Status s = target()->NewWritableFile(f, r);\n+    if (s.ok()) {\n+      if (strstr(f.c_str(), \".sst\") != NULL) {\n+        *r = new SSTableFile(this, *r);\n+      } else if (strstr(f.c_str(), \"MANIFEST\") != NULL) {\n+        *r = new ManifestFile(this, *r);\n+      }\n+    }\n+    return s;\n+  }\n+\n+  Status NewRandomAccessFile(const std::string& f, RandomAccessFile** r) {\n+    class CountingFile : public RandomAccessFile {\n+     private:\n+      RandomAccessFile* target_;\n+      AtomicCounter* counter_;\n+     public:\n+      CountingFile(RandomAccessFile* target, AtomicCounter* counter)\n+          : target_(target), counter_(counter) {\n+      }\n+      virtual ~CountingFile() { delete target_; }\n+      virtual Status Read(uint64_t offset, size_t n, Slice* result,\n+                          char* scratch) const {\n+        counter_->Increment();\n+        return target_->Read(offset, n, result, scratch);\n+      }\n+    };\n+\n+    Status s = target()->NewRandomAccessFile(f, r);\n+    if (s.ok() && count_random_reads_) {\n+      *r = new CountingFile(*r, &random_read_counter_);\n+    }\n+    return s;\n+  }\n+\n+  virtual void SleepForMicroseconds(int micros) {\n+    sleep_counter_.Increment();\n+    sleep_time_counter_.IncrementBy(micros);\n+  }\n+\n+};\n+\n+class DBTest {\n+ private:\n+  const FilterPolicy* filter_policy_;\n+\n+  // Sequence of option configurations to try\n+  enum OptionConfig {\n+    kDefault,\n+    kFilter,\n+    kUncompressed,\n+    kEnd\n+  };\n+  int option_config_;\n+\n+ public:\n+  std::string dbname_;\n+  SpecialEnv* env_;\n+  DB* db_;\n+\n+  Options last_options_;\n+\n+  DBTest() : option_config_(kDefault),\n+             env_(new SpecialEnv(Env::Default())) {\n+    filter_policy_ = NewBloomFilterPolicy(10);\n+    dbname_ = test::TmpDir() + \"/db_test\";\n+    DestroyDB(dbname_, Options());\n+    db_ = NULL;\n+    Reopen();\n+  }\n+\n+  ~DBTest() {\n+    delete db_;\n+    DestroyDB(dbname_, Options());\n+    delete env_;\n+    delete filter_policy_;\n+  }\n+\n+  // Switch to a fresh database with the next option configuration to\n+  // test.  Return false if there are no more configurations to test.\n+  bool ChangeOptions() {\n+    option_config_++;\n+    if (option_config_ >= kEnd) {\n+      return false;\n+    } else {\n+      DestroyAndReopen();\n+      return true;\n+    }\n+  }\n+\n+  // Return the current option configuration.\n+  Options CurrentOptions() {\n+    Options options;\n+    switch (option_config_) {\n+      case kFilter:\n+        options.filter_policy = filter_policy_;\n+        break;\n+      case kUncompressed:\n+        options.compression = kNoCompression;\n+        break;\n+      default:\n+        break;\n+    }\n+    return options;\n+  }\n+\n+  DBImpl* dbfull() {\n+    return reinterpret_cast<DBImpl*>(db_);\n+  }\n+\n+  void Reopen(Options* options = NULL) {\n+    ASSERT_OK(TryReopen(options));\n+  }\n+\n+  void Close() {\n+    delete db_;\n+    db_ = NULL;\n+  }\n+\n+  void DestroyAndReopen(Options* options = NULL) {\n+    delete db_;\n+    db_ = NULL;\n+    DestroyDB(dbname_, Options());\n+    ASSERT_OK(TryReopen(options));\n+  }\n+\n+  Status TryReopen(Options* options) {\n+    delete db_;\n+    db_ = NULL;\n+    Options opts;\n+    if (options != NULL) {\n+      opts = *options;\n+    } else {\n+      opts = CurrentOptions();\n+      opts.create_if_missing = true;\n+    }\n+    last_options_ = opts;\n+\n+    return DB::Open(opts, dbname_, &db_);\n+  }\n+\n+  Status Put(const std::string& k, const std::string& v) {\n+    return db_->Put(WriteOptions(), k, v);\n+  }\n+\n+  Status Delete(const std::string& k) {\n+    return db_->Delete(WriteOptions(), k);\n+  }\n+\n+  std::string Get(const std::string& k, const Snapshot* snapshot = NULL) {\n+    ReadOptions options;\n+    options.snapshot = snapshot;\n+    std::string result;\n+    Status s = db_->Get(options, k, &result);\n+    if (s.IsNotFound()) {\n+      result = \"NOT_FOUND\";\n+    } else if (!s.ok()) {\n+      result = s.ToString();\n+    }\n+    return result;\n+  }\n+\n+  // Return a string that contains all key,value pairs in order,\n+  // formatted like \"(k1->v1)(k2->v2)\".\n+  std::string Contents() {\n+    std::vector<std::string> forward;\n+    std::string result;\n+    Iterator* iter = db_->NewIterator(ReadOptions());\n+    for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+      std::string s = IterStatus(iter);\n+      result.push_back('(');\n+      result.append(s);\n+      result.push_back(')');\n+      forward.push_back(s);\n+    }\n+\n+    // Check reverse iteration results are the reverse of forward results\n+    int matched = 0;\n+    for (iter->SeekToLast(); iter->Valid(); iter->Prev()) {\n+      ASSERT_LT(matched, forward.size());\n+      ASSERT_EQ(IterStatus(iter), forward[forward.size() - matched - 1]);\n+      matched++;\n+    }\n+    ASSERT_EQ(matched, forward.size());\n+\n+    delete iter;\n+    return result;\n+  }\n+\n+  std::string AllEntriesFor(const Slice& user_key) {\n+    Iterator* iter = dbfull()->TEST_NewInternalIterator();\n+    InternalKey target(user_key, kMaxSequenceNumber, kTypeValue);\n+    iter->Seek(target.Encode());\n+    std::string result;\n+    if (!iter->status().ok()) {\n+      result = iter->status().ToString();\n+    } else {\n+      result = \"[ \";\n+      bool first = true;\n+      while (iter->Valid()) {\n+        ParsedInternalKey ikey;\n+        if (!ParseInternalKey(iter->key(), &ikey)) {\n+          result += \"CORRUPTED\";\n+        } else {\n+          if (last_options_.comparator->Compare(ikey.user_key, user_key) != 0) {\n+            break;\n+          }\n+          if (!first) {\n+            result += \", \";\n+          }\n+          first = false;\n+          switch (ikey.type) {\n+            case kTypeValue:\n+              result += iter->value().ToString();\n+              break;\n+            case kTypeDeletion:\n+              result += \"DEL\";\n+              break;\n+          }\n+        }\n+        iter->Next();\n+      }\n+      if (!first) {\n+        result += \" \";\n+      }\n+      result += \"]\";\n+    }\n+    delete iter;\n+    return result;\n+  }\n+\n+  int NumTableFilesAtLevel(int level) {\n+    std::string property;\n+    ASSERT_TRUE(\n+        db_->GetProperty(\"leveldb.num-files-at-level\" + NumberToString(level),\n+                         &property));\n+    return atoi(property.c_str());\n+  }\n+\n+  int TotalTableFiles() {\n+    int result = 0;\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      result += NumTableFilesAtLevel(level);\n+    }\n+    return result;\n+  }\n+\n+  // Return spread of files per level\n+  std::string FilesPerLevel() {\n+    std::string result;\n+    int last_non_zero_offset = 0;\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      int f = NumTableFilesAtLevel(level);\n+      char buf[100];\n+      snprintf(buf, sizeof(buf), \"%s%d\", (level ? \",\" : \"\"), f);\n+      result += buf;\n+      if (f > 0) {\n+        last_non_zero_offset = result.size();\n+      }\n+    }\n+    result.resize(last_non_zero_offset);\n+    return result;\n+  }\n+\n+  int CountFiles() {\n+    std::vector<std::string> files;\n+    env_->GetChildren(dbname_, &files);\n+    return static_cast<int>(files.size());\n+  }\n+\n+  uint64_t Size(const Slice& start, const Slice& limit) {\n+    Range r(start, limit);\n+    uint64_t size;\n+    db_->GetApproximateSizes(&r, 1, &size);\n+    return size;\n+  }\n+\n+  void Compact(const Slice& start, const Slice& limit) {\n+    db_->CompactRange(&start, &limit);\n+  }\n+\n+  // Do n memtable compactions, each of which produces an sstable\n+  // covering the range [small,large].\n+  void MakeTables(int n, const std::string& small, const std::string& large) {\n+    for (int i = 0; i < n; i++) {\n+      Put(small, \"begin\");\n+      Put(large, \"end\");\n+      dbfull()->TEST_CompactMemTable();\n+    }\n+  }\n+\n+  // Prevent pushing of new sstables into deeper levels by adding\n+  // tables that cover a specified range to all levels.\n+  void FillLevels(const std::string& smallest, const std::string& largest) {\n+    MakeTables(config::kNumLevels, smallest, largest);\n+  }\n+\n+  void DumpFileCounts(const char* label) {\n+    fprintf(stderr, \"---\\n%s:\\n\", label);\n+    fprintf(stderr, \"maxoverlap: %lld\\n\",\n+            static_cast<long long>(\n+                dbfull()->TEST_MaxNextLevelOverlappingBytes()));\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      int num = NumTableFilesAtLevel(level);\n+      if (num > 0) {\n+        fprintf(stderr, \"  level %3d : %d files\\n\", level, num);\n+      }\n+    }\n+  }\n+\n+  std::string DumpSSTableList() {\n+    std::string property;\n+    db_->GetProperty(\"leveldb.sstables\", &property);\n+    return property;\n+  }\n+\n+  std::string IterStatus(Iterator* iter) {\n+    std::string result;\n+    if (iter->Valid()) {\n+      result = iter->key().ToString() + \"->\" + iter->value().ToString();\n+    } else {\n+      result = \"(invalid)\";\n+    }\n+    return result;\n+  }\n+\n+  bool DeleteAnSSTFile() {\n+    std::vector<std::string> filenames;\n+    ASSERT_OK(env_->GetChildren(dbname_, &filenames));\n+    uint64_t number;\n+    FileType type;\n+    for (size_t i = 0; i < filenames.size(); i++) {\n+      if (ParseFileName(filenames[i], &number, &type) && type == kTableFile) {\n+        ASSERT_OK(env_->DeleteFile(TableFileName(dbname_, number)));\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+};\n+\n+TEST(DBTest, Empty) {\n+  do {\n+    ASSERT_TRUE(db_ != NULL);\n+    ASSERT_EQ(\"NOT_FOUND\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, ReadWrite) {\n+  do {\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+    ASSERT_OK(Put(\"bar\", \"v2\"));\n+    ASSERT_OK(Put(\"foo\", \"v3\"));\n+    ASSERT_EQ(\"v3\", Get(\"foo\"));\n+    ASSERT_EQ(\"v2\", Get(\"bar\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, PutDeleteGet) {\n+  do {\n+    ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v1\"));\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+    ASSERT_OK(db_->Put(WriteOptions(), \"foo\", \"v2\"));\n+    ASSERT_EQ(\"v2\", Get(\"foo\"));\n+    ASSERT_OK(db_->Delete(WriteOptions(), \"foo\"));\n+    ASSERT_EQ(\"NOT_FOUND\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetFromImmutableLayer) {\n+  do {\n+    Options options = CurrentOptions();\n+    options.env = env_;\n+    options.write_buffer_size = 100000;  // Small write buffer\n+    Reopen(&options);\n+\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+\n+    env_->delay_sstable_sync_.Release_Store(env_);   // Block sync calls\n+    Put(\"k1\", std::string(100000, 'x'));             // Fill memtable\n+    Put(\"k2\", std::string(100000, 'y'));             // Trigger compaction\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+    env_->delay_sstable_sync_.Release_Store(NULL);   // Release sync calls\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetFromVersions) {\n+  do {\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetSnapshot) {\n+  do {\n+    // Try with both a short key and a long key\n+    for (int i = 0; i < 2; i++) {\n+      std::string key = (i == 0) ? std::string(\"foo\") : std::string(200, 'x');\n+      ASSERT_OK(Put(key, \"v1\"));\n+      const Snapshot* s1 = db_->GetSnapshot();\n+      ASSERT_OK(Put(key, \"v2\"));\n+      ASSERT_EQ(\"v2\", Get(key));\n+      ASSERT_EQ(\"v1\", Get(key, s1));\n+      dbfull()->TEST_CompactMemTable();\n+      ASSERT_EQ(\"v2\", Get(key));\n+      ASSERT_EQ(\"v1\", Get(key, s1));\n+      db_->ReleaseSnapshot(s1);\n+    }\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetLevel0Ordering) {\n+  do {\n+    // Check that we process level-0 files in correct order.  The code\n+    // below generates two level-0 files where the earlier one comes\n+    // before the later one in the level-0 file list since the earlier\n+    // one has a smaller \"smallest\" key.\n+    ASSERT_OK(Put(\"bar\", \"b\"));\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_OK(Put(\"foo\", \"v2\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"v2\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetOrderedByLevels) {\n+  do {\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    Compact(\"a\", \"z\");\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+    ASSERT_OK(Put(\"foo\", \"v2\"));\n+    ASSERT_EQ(\"v2\", Get(\"foo\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"v2\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetPicksCorrectFile) {\n+  do {\n+    // Arrange to have multiple files in a non-level-0 level.\n+    ASSERT_OK(Put(\"a\", \"va\"));\n+    Compact(\"a\", \"b\");\n+    ASSERT_OK(Put(\"x\", \"vx\"));\n+    Compact(\"x\", \"y\");\n+    ASSERT_OK(Put(\"f\", \"vf\"));\n+    Compact(\"f\", \"g\");\n+    ASSERT_EQ(\"va\", Get(\"a\"));\n+    ASSERT_EQ(\"vf\", Get(\"f\"));\n+    ASSERT_EQ(\"vx\", Get(\"x\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, GetEncountersEmptyLevel) {\n+  do {\n+    // Arrange for the following to happen:\n+    //   * sstable A in level 0\n+    //   * nothing in level 1\n+    //   * sstable B in level 2\n+    // Then do enough Get() calls to arrange for an automatic compaction\n+    // of sstable A.  A bug would cause the compaction to be marked as\n+    // occuring at level 1 (instead of the correct level 0).\n+\n+    // Step 1: First place sstables in levels 0 and 2\n+    int compaction_count = 0;\n+    while (NumTableFilesAtLevel(0) == 0 ||\n+           NumTableFilesAtLevel(2) == 0) {\n+      ASSERT_LE(compaction_count, 100) << \"could not fill levels 0 and 2\";\n+      compaction_count++;\n+      Put(\"a\", \"begin\");\n+      Put(\"z\", \"end\");\n+      dbfull()->TEST_CompactMemTable();\n+    }\n+\n+    // Step 2: clear level 1 if necessary.\n+    dbfull()->TEST_CompactRange(1, NULL, NULL);\n+    ASSERT_EQ(NumTableFilesAtLevel(0), 1);\n+    ASSERT_EQ(NumTableFilesAtLevel(1), 0);\n+    ASSERT_EQ(NumTableFilesAtLevel(2), 1);\n+\n+    // Step 3: read a bunch of times\n+    for (int i = 0; i < 1000; i++) {\n+      ASSERT_EQ(\"NOT_FOUND\", Get(\"missing\"));\n+    }\n+\n+    // Step 4: Wait for compaction to finish\n+    DelayMilliseconds(1000);\n+\n+    ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, IterEmpty) {\n+  Iterator* iter = db_->NewIterator(ReadOptions());\n+\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->Seek(\"foo\");\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  delete iter;\n+}\n+\n+TEST(DBTest, IterSingle) {\n+  ASSERT_OK(Put(\"a\", \"va\"));\n+  Iterator* iter = db_->NewIterator(ReadOptions());\n+\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->Seek(\"\");\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->Seek(\"a\");\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->Seek(\"b\");\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  delete iter;\n+}\n+\n+TEST(DBTest, IterMulti) {\n+  ASSERT_OK(Put(\"a\", \"va\"));\n+  ASSERT_OK(Put(\"b\", \"vb\"));\n+  ASSERT_OK(Put(\"c\", \"vc\"));\n+  Iterator* iter = db_->NewIterator(ReadOptions());\n+\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->Seek(\"\");\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Seek(\"a\");\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Seek(\"ax\");\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+  iter->Seek(\"b\");\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+  iter->Seek(\"z\");\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  // Switch from reverse to forward\n+  iter->SeekToLast();\n+  iter->Prev();\n+  iter->Prev();\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+\n+  // Switch from forward to reverse\n+  iter->SeekToFirst();\n+  iter->Next();\n+  iter->Next();\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+\n+  // Make sure iter stays at snapshot\n+  ASSERT_OK(Put(\"a\",  \"va2\"));\n+  ASSERT_OK(Put(\"a2\", \"va3\"));\n+  ASSERT_OK(Put(\"b\",  \"vb2\"));\n+  ASSERT_OK(Put(\"c\",  \"vc2\"));\n+  ASSERT_OK(Delete(\"b\"));\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"b->vb\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  delete iter;\n+}\n+\n+TEST(DBTest, IterSmallAndLargeMix) {\n+  ASSERT_OK(Put(\"a\", \"va\"));\n+  ASSERT_OK(Put(\"b\", std::string(100000, 'b')));\n+  ASSERT_OK(Put(\"c\", \"vc\"));\n+  ASSERT_OK(Put(\"d\", std::string(100000, 'd')));\n+  ASSERT_OK(Put(\"e\", std::string(100000, 'e')));\n+\n+  Iterator* iter = db_->NewIterator(ReadOptions());\n+\n+  iter->SeekToFirst();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"b->\" + std::string(100000, 'b'));\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"d->\" + std::string(100000, 'd'));\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"e->\" + std::string(100000, 'e'));\n+  iter->Next();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  iter->SeekToLast();\n+  ASSERT_EQ(IterStatus(iter), \"e->\" + std::string(100000, 'e'));\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"d->\" + std::string(100000, 'd'));\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"b->\" + std::string(100000, 'b'));\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"a->va\");\n+  iter->Prev();\n+  ASSERT_EQ(IterStatus(iter), \"(invalid)\");\n+\n+  delete iter;\n+}\n+\n+TEST(DBTest, IterMultiWithDelete) {\n+  do {\n+    ASSERT_OK(Put(\"a\", \"va\"));\n+    ASSERT_OK(Put(\"b\", \"vb\"));\n+    ASSERT_OK(Put(\"c\", \"vc\"));\n+    ASSERT_OK(Delete(\"b\"));\n+    ASSERT_EQ(\"NOT_FOUND\", Get(\"b\"));\n+\n+    Iterator* iter = db_->NewIterator(ReadOptions());\n+    iter->Seek(\"c\");\n+    ASSERT_EQ(IterStatus(iter), \"c->vc\");\n+    iter->Prev();\n+    ASSERT_EQ(IterStatus(iter), \"a->va\");\n+    delete iter;\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, Recover) {\n+  do {\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    ASSERT_OK(Put(\"baz\", \"v5\"));\n+\n+    Reopen();\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+    ASSERT_EQ(\"v5\", Get(\"baz\"));\n+    ASSERT_OK(Put(\"bar\", \"v2\"));\n+    ASSERT_OK(Put(\"foo\", \"v3\"));\n+\n+    Reopen();\n+    ASSERT_EQ(\"v3\", Get(\"foo\"));\n+    ASSERT_OK(Put(\"foo\", \"v4\"));\n+    ASSERT_EQ(\"v4\", Get(\"foo\"));\n+    ASSERT_EQ(\"v2\", Get(\"bar\"));\n+    ASSERT_EQ(\"v5\", Get(\"baz\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, RecoveryWithEmptyLog) {\n+  do {\n+    ASSERT_OK(Put(\"foo\", \"v1\"));\n+    ASSERT_OK(Put(\"foo\", \"v2\"));\n+    Reopen();\n+    Reopen();\n+    ASSERT_OK(Put(\"foo\", \"v3\"));\n+    Reopen();\n+    ASSERT_EQ(\"v3\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+// Check that writes done during a memtable compaction are recovered\n+// if the database is shutdown during the memtable compaction.\n+TEST(DBTest, RecoverDuringMemtableCompaction) {\n+  do {\n+    Options options = CurrentOptions();\n+    options.env = env_;\n+    options.write_buffer_size = 1000000;\n+    Reopen(&options);\n+\n+    // Trigger a long memtable compaction and reopen the database during it\n+    ASSERT_OK(Put(\"foo\", \"v1\"));                         // Goes to 1st log file\n+    ASSERT_OK(Put(\"big1\", std::string(10000000, 'x')));  // Fills memtable\n+    ASSERT_OK(Put(\"big2\", std::string(1000, 'y')));      // Triggers compaction\n+    ASSERT_OK(Put(\"bar\", \"v2\"));                         // Goes to new log file\n+\n+    Reopen(&options);\n+    ASSERT_EQ(\"v1\", Get(\"foo\"));\n+    ASSERT_EQ(\"v2\", Get(\"bar\"));\n+    ASSERT_EQ(std::string(10000000, 'x'), Get(\"big1\"));\n+    ASSERT_EQ(std::string(1000, 'y'), Get(\"big2\"));\n+  } while (ChangeOptions());\n+}\n+\n+static std::string Key(int i) {\n+  char buf[100];\n+  snprintf(buf, sizeof(buf), \"key%06d\", i);\n+  return std::string(buf);\n+}\n+\n+TEST(DBTest, MinorCompactionsHappen) {\n+  Options options = CurrentOptions();\n+  options.write_buffer_size = 10000;\n+  Reopen(&options);\n+\n+  const int N = 500;\n+\n+  int starting_num_tables = TotalTableFiles();\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_OK(Put(Key(i), Key(i) + std::string(1000, 'v')));\n+  }\n+  int ending_num_tables = TotalTableFiles();\n+  ASSERT_GT(ending_num_tables, starting_num_tables);\n+\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_EQ(Key(i) + std::string(1000, 'v'), Get(Key(i)));\n+  }\n+\n+  Reopen();\n+\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_EQ(Key(i) + std::string(1000, 'v'), Get(Key(i)));\n+  }\n+}\n+\n+TEST(DBTest, RecoverWithLargeLog) {\n+  {\n+    Options options = CurrentOptions();\n+    Reopen(&options);\n+    ASSERT_OK(Put(\"big1\", std::string(200000, '1')));\n+    ASSERT_OK(Put(\"big2\", std::string(200000, '2')));\n+    ASSERT_OK(Put(\"small3\", std::string(10, '3')));\n+    ASSERT_OK(Put(\"small4\", std::string(10, '4')));\n+    ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+  }\n+\n+  // Make sure that if we re-open with a small write buffer size that\n+  // we flush table files in the middle of a large log file.\n+  Options options = CurrentOptions();\n+  options.write_buffer_size = 100000;\n+  Reopen(&options);\n+  ASSERT_EQ(NumTableFilesAtLevel(0), 3);\n+  ASSERT_EQ(std::string(200000, '1'), Get(\"big1\"));\n+  ASSERT_EQ(std::string(200000, '2'), Get(\"big2\"));\n+  ASSERT_EQ(std::string(10, '3'), Get(\"small3\"));\n+  ASSERT_EQ(std::string(10, '4'), Get(\"small4\"));\n+  ASSERT_GT(NumTableFilesAtLevel(0), 1);\n+}\n+\n+TEST(DBTest, CompactionsGenerateMultipleFiles) {\n+  Options options = CurrentOptions();\n+  options.write_buffer_size = 100000000;        // Large write buffer\n+  Reopen(&options);\n+\n+  Random rnd(301);\n+\n+  // Write 8MB (80 values, each 100K)\n+  ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+  std::vector<std::string> values;\n+  for (int i = 0; i < 80; i++) {\n+    values.push_back(RandomString(&rnd, 100000));\n+    ASSERT_OK(Put(Key(i), values[i]));\n+  }\n+\n+  // Reopening moves updates to level-0\n+  Reopen(&options);\n+  dbfull()->TEST_CompactRange(0, NULL, NULL);\n+\n+  ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+  ASSERT_GT(NumTableFilesAtLevel(1), 1);\n+  for (int i = 0; i < 80; i++) {\n+    ASSERT_EQ(Get(Key(i)), values[i]);\n+  }\n+}\n+\n+TEST(DBTest, RepeatedWritesToSameKey) {\n+  Options options = CurrentOptions();\n+  options.env = env_;\n+  options.write_buffer_size = 100000;  // Small write buffer\n+  Reopen(&options);\n+\n+  // We must have at most one file per level except for level-0,\n+  // which may have up to kL0_StopWritesTrigger files.\n+  const int kMaxFiles = config::kNumLevels + config::kL0_StopWritesTrigger;\n+\n+  Random rnd(301);\n+  std::string value = RandomString(&rnd, 2 * options.write_buffer_size);\n+  for (int i = 0; i < 5 * kMaxFiles; i++) {\n+    Put(\"key\", value);\n+    ASSERT_LE(TotalTableFiles(), kMaxFiles);\n+    fprintf(stderr, \"after %d: %d files\\n\", int(i+1), TotalTableFiles());\n+  }\n+}\n+\n+TEST(DBTest, SparseMerge) {\n+  Options options = CurrentOptions();\n+  options.compression = kNoCompression;\n+  Reopen(&options);\n+\n+  FillLevels(\"A\", \"Z\");\n+\n+  // Suppose there is:\n+  //    small amount of data with prefix A\n+  //    large amount of data with prefix B\n+  //    small amount of data with prefix C\n+  // and that recent updates have made small changes to all three prefixes.\n+  // Check that we do not do a compaction that merges all of B in one shot.\n+  const std::string value(1000, 'x');\n+  Put(\"A\", \"va\");\n+  // Write approximately 100MB of \"B\" values\n+  for (int i = 0; i < 100000; i++) {\n+    char key[100];\n+    snprintf(key, sizeof(key), \"B%010d\", i);\n+    Put(key, value);\n+  }\n+  Put(\"C\", \"vc\");\n+  dbfull()->TEST_CompactMemTable();\n+  dbfull()->TEST_CompactRange(0, NULL, NULL);\n+\n+  // Make sparse update\n+  Put(\"A\",    \"va2\");\n+  Put(\"B100\", \"bvalue2\");\n+  Put(\"C\",    \"vc2\");\n+  dbfull()->TEST_CompactMemTable();\n+\n+  // Compactions should not cause us to create a situation where\n+  // a file overlaps too much data at the next level.\n+  ASSERT_LE(dbfull()->TEST_MaxNextLevelOverlappingBytes(), 20*1048576);\n+  dbfull()->TEST_CompactRange(0, NULL, NULL);\n+  ASSERT_LE(dbfull()->TEST_MaxNextLevelOverlappingBytes(), 20*1048576);\n+  dbfull()->TEST_CompactRange(1, NULL, NULL);\n+  ASSERT_LE(dbfull()->TEST_MaxNextLevelOverlappingBytes(), 20*1048576);\n+}\n+\n+static bool Between(uint64_t val, uint64_t low, uint64_t high) {\n+  bool result = (val >= low) && (val <= high);\n+  if (!result) {\n+    fprintf(stderr, \"Value %llu is not in range [%llu, %llu]\\n\",\n+            (unsigned long long)(val),\n+            (unsigned long long)(low),\n+            (unsigned long long)(high));\n+  }\n+  return result;\n+}\n+\n+TEST(DBTest, ApproximateSizes) {\n+  do {\n+    Options options = CurrentOptions();\n+    options.write_buffer_size = 100000000;        // Large write buffer\n+    options.compression = kNoCompression;\n+    DestroyAndReopen();\n+\n+    ASSERT_TRUE(Between(Size(\"\", \"xyz\"), 0, 0));\n+    Reopen(&options);\n+    ASSERT_TRUE(Between(Size(\"\", \"xyz\"), 0, 0));\n+\n+    // Write 8MB (80 values, each 100K)\n+    ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+    const int N = 80;\n+    static const int S1 = 100000;\n+    static const int S2 = 105000;  // Allow some expansion from metadata\n+    Random rnd(301);\n+    for (int i = 0; i < N; i++) {\n+      ASSERT_OK(Put(Key(i), RandomString(&rnd, S1)));\n+    }\n+\n+    // 0 because GetApproximateSizes() does not account for memtable space\n+    ASSERT_TRUE(Between(Size(\"\", Key(50)), 0, 0));\n+\n+    // Check sizes across recovery by reopening a few times\n+    for (int run = 0; run < 3; run++) {\n+      Reopen(&options);\n+\n+      for (int compact_start = 0; compact_start < N; compact_start += 10) {\n+        for (int i = 0; i < N; i += 10) {\n+          ASSERT_TRUE(Between(Size(\"\", Key(i)), S1*i, S2*i));\n+          ASSERT_TRUE(Between(Size(\"\", Key(i)+\".suffix\"), S1*(i+1), S2*(i+1)));\n+          ASSERT_TRUE(Between(Size(Key(i), Key(i+10)), S1*10, S2*10));\n+        }\n+        ASSERT_TRUE(Between(Size(\"\", Key(50)), S1*50, S2*50));\n+        ASSERT_TRUE(Between(Size(\"\", Key(50)+\".suffix\"), S1*50, S2*50));\n+\n+        std::string cstart_str = Key(compact_start);\n+        std::string cend_str = Key(compact_start + 9);\n+        Slice cstart = cstart_str;\n+        Slice cend = cend_str;\n+        dbfull()->TEST_CompactRange(0, &cstart, &cend);\n+      }\n+\n+      ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+      ASSERT_GT(NumTableFilesAtLevel(1), 0);\n+    }\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, ApproximateSizes_MixOfSmallAndLarge) {\n+  do {\n+    Options options = CurrentOptions();\n+    options.compression = kNoCompression;\n+    Reopen();\n+\n+    Random rnd(301);\n+    std::string big1 = RandomString(&rnd, 100000);\n+    ASSERT_OK(Put(Key(0), RandomString(&rnd, 10000)));\n+    ASSERT_OK(Put(Key(1), RandomString(&rnd, 10000)));\n+    ASSERT_OK(Put(Key(2), big1));\n+    ASSERT_OK(Put(Key(3), RandomString(&rnd, 10000)));\n+    ASSERT_OK(Put(Key(4), big1));\n+    ASSERT_OK(Put(Key(5), RandomString(&rnd, 10000)));\n+    ASSERT_OK(Put(Key(6), RandomString(&rnd, 300000)));\n+    ASSERT_OK(Put(Key(7), RandomString(&rnd, 10000)));\n+\n+    // Check sizes across recovery by reopening a few times\n+    for (int run = 0; run < 3; run++) {\n+      Reopen(&options);\n+\n+      ASSERT_TRUE(Between(Size(\"\", Key(0)), 0, 0));\n+      ASSERT_TRUE(Between(Size(\"\", Key(1)), 10000, 11000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(2)), 20000, 21000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(3)), 120000, 121000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(4)), 130000, 131000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(5)), 230000, 231000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(6)), 240000, 241000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(7)), 540000, 541000));\n+      ASSERT_TRUE(Between(Size(\"\", Key(8)), 550000, 560000));\n+\n+      ASSERT_TRUE(Between(Size(Key(3), Key(5)), 110000, 111000));\n+\n+      dbfull()->TEST_CompactRange(0, NULL, NULL);\n+    }\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, IteratorPinsRef) {\n+  Put(\"foo\", \"hello\");\n+\n+  // Get iterator that will yield the current contents of the DB.\n+  Iterator* iter = db_->NewIterator(ReadOptions());\n+\n+  // Write to force compactions\n+  Put(\"foo\", \"newvalue1\");\n+  for (int i = 0; i < 100; i++) {\n+    ASSERT_OK(Put(Key(i), Key(i) + std::string(100000, 'v'))); // 100K values\n+  }\n+  Put(\"foo\", \"newvalue2\");\n+\n+  iter->SeekToFirst();\n+  ASSERT_TRUE(iter->Valid());\n+  ASSERT_EQ(\"foo\", iter->key().ToString());\n+  ASSERT_EQ(\"hello\", iter->value().ToString());\n+  iter->Next();\n+  ASSERT_TRUE(!iter->Valid());\n+  delete iter;\n+}\n+\n+TEST(DBTest, Snapshot) {\n+  do {\n+    Put(\"foo\", \"v1\");\n+    const Snapshot* s1 = db_->GetSnapshot();\n+    Put(\"foo\", \"v2\");\n+    const Snapshot* s2 = db_->GetSnapshot();\n+    Put(\"foo\", \"v3\");\n+    const Snapshot* s3 = db_->GetSnapshot();\n+\n+    Put(\"foo\", \"v4\");\n+    ASSERT_EQ(\"v1\", Get(\"foo\", s1));\n+    ASSERT_EQ(\"v2\", Get(\"foo\", s2));\n+    ASSERT_EQ(\"v3\", Get(\"foo\", s3));\n+    ASSERT_EQ(\"v4\", Get(\"foo\"));\n+\n+    db_->ReleaseSnapshot(s3);\n+    ASSERT_EQ(\"v1\", Get(\"foo\", s1));\n+    ASSERT_EQ(\"v2\", Get(\"foo\", s2));\n+    ASSERT_EQ(\"v4\", Get(\"foo\"));\n+\n+    db_->ReleaseSnapshot(s1);\n+    ASSERT_EQ(\"v2\", Get(\"foo\", s2));\n+    ASSERT_EQ(\"v4\", Get(\"foo\"));\n+\n+    db_->ReleaseSnapshot(s2);\n+    ASSERT_EQ(\"v4\", Get(\"foo\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, HiddenValuesAreRemoved) {\n+  do {\n+    Random rnd(301);\n+    FillLevels(\"a\", \"z\");\n+\n+    std::string big = RandomString(&rnd, 50000);\n+    Put(\"foo\", big);\n+    Put(\"pastfoo\", \"v\");\n+    const Snapshot* snapshot = db_->GetSnapshot();\n+    Put(\"foo\", \"tiny\");\n+    Put(\"pastfoo2\", \"v2\");        // Advance sequence number one more\n+\n+    ASSERT_OK(dbfull()->TEST_CompactMemTable());\n+    ASSERT_GT(NumTableFilesAtLevel(0), 0);\n+\n+    ASSERT_EQ(big, Get(\"foo\", snapshot));\n+    ASSERT_TRUE(Between(Size(\"\", \"pastfoo\"), 50000, 60000));\n+    db_->ReleaseSnapshot(snapshot);\n+    ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ tiny, \" + big + \" ]\");\n+    Slice x(\"x\");\n+    dbfull()->TEST_CompactRange(0, NULL, &x);\n+    ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ tiny ]\");\n+    ASSERT_EQ(NumTableFilesAtLevel(0), 0);\n+    ASSERT_GE(NumTableFilesAtLevel(1), 1);\n+    dbfull()->TEST_CompactRange(1, NULL, &x);\n+    ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ tiny ]\");\n+\n+    ASSERT_TRUE(Between(Size(\"\", \"pastfoo\"), 0, 1000));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, DeletionMarkers1) {\n+  Put(\"foo\", \"v1\");\n+  ASSERT_OK(dbfull()->TEST_CompactMemTable());\n+  const int last = config::kMaxMemCompactLevel;\n+  ASSERT_EQ(NumTableFilesAtLevel(last), 1);   // foo => v1 is now in last level\n+\n+  // Place a table at level last-1 to prevent merging with preceding mutation\n+  Put(\"a\", \"begin\");\n+  Put(\"z\", \"end\");\n+  dbfull()->TEST_CompactMemTable();\n+  ASSERT_EQ(NumTableFilesAtLevel(last), 1);\n+  ASSERT_EQ(NumTableFilesAtLevel(last-1), 1);\n+\n+  Delete(\"foo\");\n+  Put(\"foo\", \"v2\");\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ v2, DEL, v1 ]\");\n+  ASSERT_OK(dbfull()->TEST_CompactMemTable());  // Moves to level last-2\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ v2, DEL, v1 ]\");\n+  Slice z(\"z\");\n+  dbfull()->TEST_CompactRange(last-2, NULL, &z);\n+  // DEL eliminated, but v1 remains because we aren't compacting that level\n+  // (DEL can be eliminated because v2 hides v1).\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ v2, v1 ]\");\n+  dbfull()->TEST_CompactRange(last-1, NULL, NULL);\n+  // Merging last-1 w/ last, so we are the base level for \"foo\", so\n+  // DEL is removed.  (as is v1).\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ v2 ]\");\n+}\n+\n+TEST(DBTest, DeletionMarkers2) {\n+  Put(\"foo\", \"v1\");\n+  ASSERT_OK(dbfull()->TEST_CompactMemTable());\n+  const int last = config::kMaxMemCompactLevel;\n+  ASSERT_EQ(NumTableFilesAtLevel(last), 1);   // foo => v1 is now in last level\n+\n+  // Place a table at level last-1 to prevent merging with preceding mutation\n+  Put(\"a\", \"begin\");\n+  Put(\"z\", \"end\");\n+  dbfull()->TEST_CompactMemTable();\n+  ASSERT_EQ(NumTableFilesAtLevel(last), 1);\n+  ASSERT_EQ(NumTableFilesAtLevel(last-1), 1);\n+\n+  Delete(\"foo\");\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ DEL, v1 ]\");\n+  ASSERT_OK(dbfull()->TEST_CompactMemTable());  // Moves to level last-2\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ DEL, v1 ]\");\n+  dbfull()->TEST_CompactRange(last-2, NULL, NULL);\n+  // DEL kept: \"last\" file overlaps\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ DEL, v1 ]\");\n+  dbfull()->TEST_CompactRange(last-1, NULL, NULL);\n+  // Merging last-1 w/ last, so we are the base level for \"foo\", so\n+  // DEL is removed.  (as is v1).\n+  ASSERT_EQ(AllEntriesFor(\"foo\"), \"[ ]\");\n+}\n+\n+TEST(DBTest, OverlapInLevel0) {\n+  do {\n+    ASSERT_EQ(config::kMaxMemCompactLevel, 2) << \"Fix test to match config\";\n+\n+    // Fill levels 1 and 2 to disable the pushing of new memtables to levels > 0.\n+    ASSERT_OK(Put(\"100\", \"v100\"));\n+    ASSERT_OK(Put(\"999\", \"v999\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_OK(Delete(\"100\"));\n+    ASSERT_OK(Delete(\"999\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"0,1,1\", FilesPerLevel());\n+\n+    // Make files spanning the following ranges in level-0:\n+    //  files[0]  200 .. 900\n+    //  files[1]  300 .. 500\n+    // Note that files are sorted by smallest key.\n+    ASSERT_OK(Put(\"300\", \"v300\"));\n+    ASSERT_OK(Put(\"500\", \"v500\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_OK(Put(\"200\", \"v200\"));\n+    ASSERT_OK(Put(\"600\", \"v600\"));\n+    ASSERT_OK(Put(\"900\", \"v900\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"2,1,1\", FilesPerLevel());\n+\n+    // Compact away the placeholder files we created initially\n+    dbfull()->TEST_CompactRange(1, NULL, NULL);\n+    dbfull()->TEST_CompactRange(2, NULL, NULL);\n+    ASSERT_EQ(\"2\", FilesPerLevel());\n+\n+    // Do a memtable compaction.  Before bug-fix, the compaction would\n+    // not detect the overlap with level-0 files and would incorrectly place\n+    // the deletion in a deeper level.\n+    ASSERT_OK(Delete(\"600\"));\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"3\", FilesPerLevel());\n+    ASSERT_EQ(\"NOT_FOUND\", Get(\"600\"));\n+  } while (ChangeOptions());\n+}\n+\n+TEST(DBTest, L0_CompactionBug_Issue44_a) {\n+  Reopen();\n+  ASSERT_OK(Put(\"b\", \"v\"));\n+  Reopen();\n+  ASSERT_OK(Delete(\"b\"));\n+  ASSERT_OK(Delete(\"a\"));\n+  Reopen();\n+  ASSERT_OK(Delete(\"a\"));\n+  Reopen();\n+  ASSERT_OK(Put(\"a\", \"v\"));\n+  Reopen();\n+  Reopen();\n+  ASSERT_EQ(\"(a->v)\", Contents());\n+  DelayMilliseconds(1000);  // Wait for compaction to finish\n+  ASSERT_EQ(\"(a->v)\", Contents());\n+}\n+\n+TEST(DBTest, L0_CompactionBug_Issue44_b) {\n+  Reopen();\n+  Put(\"\",\"\");\n+  Reopen();\n+  Delete(\"e\");\n+  Put(\"\",\"\");\n+  Reopen();\n+  Put(\"c\", \"cv\");\n+  Reopen();\n+  Put(\"\",\"\");\n+  Reopen();\n+  Put(\"\",\"\");\n+  DelayMilliseconds(1000);  // Wait for compaction to finish\n+  Reopen();\n+  Put(\"d\",\"dv\");\n+  Reopen();\n+  Put(\"\",\"\");\n+  Reopen();\n+  Delete(\"d\");\n+  Delete(\"b\");\n+  Reopen();\n+  ASSERT_EQ(\"(->)(c->cv)\", Contents());\n+  DelayMilliseconds(1000);  // Wait for compaction to finish\n+  ASSERT_EQ(\"(->)(c->cv)\", Contents());\n+}\n+\n+TEST(DBTest, ComparatorCheck) {\n+  class NewComparator : public Comparator {\n+   public:\n+    virtual const char* Name() const { return \"leveldb.NewComparator\"; }\n+    virtual int Compare(const Slice& a, const Slice& b) const {\n+      return BytewiseComparator()->Compare(a, b);\n+    }\n+    virtual void FindShortestSeparator(std::string* s, const Slice& l) const {\n+      BytewiseComparator()->FindShortestSeparator(s, l);\n+    }\n+    virtual void FindShortSuccessor(std::string* key) const {\n+      BytewiseComparator()->FindShortSuccessor(key);\n+    }\n+  };\n+  NewComparator cmp;\n+  Options new_options = CurrentOptions();\n+  new_options.comparator = &cmp;\n+  Status s = TryReopen(&new_options);\n+  ASSERT_TRUE(!s.ok());\n+  ASSERT_TRUE(s.ToString().find(\"comparator\") != std::string::npos)\n+      << s.ToString();\n+}\n+\n+TEST(DBTest, CustomComparator) {\n+  class NumberComparator : public Comparator {\n+   public:\n+    virtual const char* Name() const { return \"test.NumberComparator\"; }\n+    virtual int Compare(const Slice& a, const Slice& b) const {\n+      return ToNumber(a) - ToNumber(b);\n+    }\n+    virtual void FindShortestSeparator(std::string* s, const Slice& l) const {\n+      ToNumber(*s);     // Check format\n+      ToNumber(l);      // Check format\n+    }\n+    virtual void FindShortSuccessor(std::string* key) const {\n+      ToNumber(*key);   // Check format\n+    }\n+   private:\n+    static int ToNumber(const Slice& x) {\n+      // Check that there are no extra characters.\n+      ASSERT_TRUE(x.size() >= 2 && x[0] == '[' && x[x.size()-1] == ']')\n+          << EscapeString(x);\n+      int val;\n+      char ignored;\n+      ASSERT_TRUE(sscanf(x.ToString().c_str(), \"[%i]%c\", &val, &ignored) == 1)\n+          << EscapeString(x);\n+      return val;\n+    }\n+  };\n+  NumberComparator cmp;\n+  Options new_options = CurrentOptions();\n+  new_options.create_if_missing = true;\n+  new_options.comparator = &cmp;\n+  new_options.filter_policy = NULL;     // Cannot use bloom filters\n+  new_options.write_buffer_size = 1000;  // Compact more often\n+  DestroyAndReopen(&new_options);\n+  ASSERT_OK(Put(\"[10]\", \"ten\"));\n+  ASSERT_OK(Put(\"[0x14]\", \"twenty\"));\n+  for (int i = 0; i < 2; i++) {\n+    ASSERT_EQ(\"ten\", Get(\"[10]\"));\n+    ASSERT_EQ(\"ten\", Get(\"[0xa]\"));\n+    ASSERT_EQ(\"twenty\", Get(\"[20]\"));\n+    ASSERT_EQ(\"twenty\", Get(\"[0x14]\"));\n+    ASSERT_EQ(\"NOT_FOUND\", Get(\"[15]\"));\n+    ASSERT_EQ(\"NOT_FOUND\", Get(\"[0xf]\"));\n+    Compact(\"[0]\", \"[9999]\");\n+  }\n+\n+  for (int run = 0; run < 2; run++) {\n+    for (int i = 0; i < 1000; i++) {\n+      char buf[100];\n+      snprintf(buf, sizeof(buf), \"[%d]\", i*10);\n+      ASSERT_OK(Put(buf, buf));\n+    }\n+    Compact(\"[0]\", \"[1000000]\");\n+  }\n+}\n+\n+TEST(DBTest, ManualCompaction) {\n+  ASSERT_EQ(config::kMaxMemCompactLevel, 2)\n+      << \"Need to update this test to match kMaxMemCompactLevel\";\n+\n+  MakeTables(3, \"p\", \"q\");\n+  ASSERT_EQ(\"1,1,1\", FilesPerLevel());\n+\n+  // Compaction range falls before files\n+  Compact(\"\", \"c\");\n+  ASSERT_EQ(\"1,1,1\", FilesPerLevel());\n+\n+  // Compaction range falls after files\n+  Compact(\"r\", \"z\");\n+  ASSERT_EQ(\"1,1,1\", FilesPerLevel());\n+\n+  // Compaction range overlaps files\n+  Compact(\"p1\", \"p9\");\n+  ASSERT_EQ(\"0,0,1\", FilesPerLevel());\n+\n+  // Populate a different range\n+  MakeTables(3, \"c\", \"e\");\n+  ASSERT_EQ(\"1,1,2\", FilesPerLevel());\n+\n+  // Compact just the new range\n+  Compact(\"b\", \"f\");\n+  ASSERT_EQ(\"0,0,2\", FilesPerLevel());\n+\n+  // Compact all\n+  MakeTables(1, \"a\", \"z\");\n+  ASSERT_EQ(\"0,1,2\", FilesPerLevel());\n+  db_->CompactRange(NULL, NULL);\n+  ASSERT_EQ(\"0,0,1\", FilesPerLevel());\n+}\n+\n+TEST(DBTest, DBOpen_Options) {\n+  std::string dbname = test::TmpDir() + \"/db_options_test\";\n+  DestroyDB(dbname, Options());\n+\n+  // Does not exist, and create_if_missing == false: error\n+  DB* db = NULL;\n+  Options opts;\n+  opts.create_if_missing = false;\n+  Status s = DB::Open(opts, dbname, &db);\n+  ASSERT_TRUE(strstr(s.ToString().c_str(), \"does not exist\") != NULL);\n+  ASSERT_TRUE(db == NULL);\n+\n+  // Does not exist, and create_if_missing == true: OK\n+  opts.create_if_missing = true;\n+  s = DB::Open(opts, dbname, &db);\n+  ASSERT_OK(s);\n+  ASSERT_TRUE(db != NULL);\n+\n+  delete db;\n+  db = NULL;\n+\n+  // Does exist, and error_if_exists == true: error\n+  opts.create_if_missing = false;\n+  opts.error_if_exists = true;\n+  s = DB::Open(opts, dbname, &db);\n+  ASSERT_TRUE(strstr(s.ToString().c_str(), \"exists\") != NULL);\n+  ASSERT_TRUE(db == NULL);\n+\n+  // Does exist, and error_if_exists == false: OK\n+  opts.create_if_missing = true;\n+  opts.error_if_exists = false;\n+  s = DB::Open(opts, dbname, &db);\n+  ASSERT_OK(s);\n+  ASSERT_TRUE(db != NULL);\n+\n+  delete db;\n+  db = NULL;\n+}\n+\n+TEST(DBTest, Locking) {\n+  DB* db2 = NULL;\n+  Status s = DB::Open(CurrentOptions(), dbname_, &db2);\n+  ASSERT_TRUE(!s.ok()) << \"Locking did not prevent re-opening db\";\n+}\n+\n+// Check that number of files does not grow when we are out of space\n+TEST(DBTest, NoSpace) {\n+  Options options = CurrentOptions();\n+  options.env = env_;\n+  Reopen(&options);\n+\n+  ASSERT_OK(Put(\"foo\", \"v1\"));\n+  ASSERT_EQ(\"v1\", Get(\"foo\"));\n+  Compact(\"a\", \"z\");\n+  const int num_files = CountFiles();\n+  env_->no_space_.Release_Store(env_);   // Force out-of-space errors\n+  env_->sleep_counter_.Reset();\n+  for (int i = 0; i < 5; i++) {\n+    for (int level = 0; level < config::kNumLevels-1; level++) {\n+      dbfull()->TEST_CompactRange(level, NULL, NULL);\n+    }\n+  }\n+  env_->no_space_.Release_Store(NULL);\n+  ASSERT_LT(CountFiles(), num_files + 3);\n+\n+  // Check that compaction attempts slept after errors\n+  ASSERT_GE(env_->sleep_counter_.Read(), 5);\n+}\n+\n+TEST(DBTest, ExponentialBackoff) {\n+  Options options = CurrentOptions();\n+  options.env = env_;\n+  Reopen(&options);\n+\n+  ASSERT_OK(Put(\"foo\", \"v1\"));\n+  ASSERT_EQ(\"v1\", Get(\"foo\"));\n+  Compact(\"a\", \"z\");\n+  env_->non_writable_.Release_Store(env_);  // Force errors for new files\n+  env_->sleep_counter_.Reset();\n+  env_->sleep_time_counter_.Reset();\n+  for (int i = 0; i < 5; i++) {\n+    dbfull()->TEST_CompactRange(2, NULL, NULL);\n+  }\n+  env_->non_writable_.Release_Store(NULL);\n+\n+  // Wait for compaction to finish\n+  DelayMilliseconds(1000);\n+\n+  ASSERT_GE(env_->sleep_counter_.Read(), 5);\n+  ASSERT_LT(env_->sleep_counter_.Read(), 10);\n+  ASSERT_GE(env_->sleep_time_counter_.Read(), 10e6);\n+}\n+\n+TEST(DBTest, NonWritableFileSystem) {\n+  Options options = CurrentOptions();\n+  options.write_buffer_size = 1000;\n+  options.env = env_;\n+  Reopen(&options);\n+  ASSERT_OK(Put(\"foo\", \"v1\"));\n+  env_->non_writable_.Release_Store(env_);  // Force errors for new files\n+  std::string big(100000, 'x');\n+  int errors = 0;\n+  for (int i = 0; i < 20; i++) {\n+    fprintf(stderr, \"iter %d; errors %d\\n\", i, errors);\n+    if (!Put(\"foo\", big).ok()) {\n+      errors++;\n+      DelayMilliseconds(100);\n+    }\n+  }\n+  ASSERT_GT(errors, 0);\n+  env_->non_writable_.Release_Store(NULL);\n+}\n+\n+TEST(DBTest, ManifestWriteError) {\n+  // Test for the following problem:\n+  // (a) Compaction produces file F\n+  // (b) Log record containing F is written to MANIFEST file, but Sync() fails\n+  // (c) GC deletes F\n+  // (d) After reopening DB, reads fail since deleted F is named in log record\n+\n+  // We iterate twice.  In the second iteration, everything is the\n+  // same except the log record never makes it to the MANIFEST file.\n+  for (int iter = 0; iter < 2; iter++) {\n+    port::AtomicPointer* error_type = (iter == 0)\n+        ? &env_->manifest_sync_error_\n+        : &env_->manifest_write_error_;\n+\n+    // Insert foo=>bar mapping\n+    Options options = CurrentOptions();\n+    options.env = env_;\n+    options.create_if_missing = true;\n+    options.error_if_exists = false;\n+    DestroyAndReopen(&options);\n+    ASSERT_OK(Put(\"foo\", \"bar\"));\n+    ASSERT_EQ(\"bar\", Get(\"foo\"));\n+\n+    // Memtable compaction (will succeed)\n+    dbfull()->TEST_CompactMemTable();\n+    ASSERT_EQ(\"bar\", Get(\"foo\"));\n+    const int last = config::kMaxMemCompactLevel;\n+    ASSERT_EQ(NumTableFilesAtLevel(last), 1);   // foo=>bar is now in last level\n+\n+    // Merging compaction (will fail)\n+    error_type->Release_Store(env_);\n+    dbfull()->TEST_CompactRange(last, NULL, NULL);  // Should fail\n+    ASSERT_EQ(\"bar\", Get(\"foo\"));\n+\n+    // Recovery: should not lose data\n+    error_type->Release_Store(NULL);\n+    Reopen(&options);\n+    ASSERT_EQ(\"bar\", Get(\"foo\"));\n+  }\n+}\n+\n+TEST(DBTest, MissingSSTFile) {\n+  ASSERT_OK(Put(\"foo\", \"bar\"));\n+  ASSERT_EQ(\"bar\", Get(\"foo\"));\n+\n+  // Dump the memtable to disk.\n+  dbfull()->TEST_CompactMemTable();\n+  ASSERT_EQ(\"bar\", Get(\"foo\"));\n+\n+  Close();\n+  ASSERT_TRUE(DeleteAnSSTFile());\n+  Options options = CurrentOptions();\n+  options.paranoid_checks = true;\n+  Status s = TryReopen(&options);\n+  ASSERT_TRUE(!s.ok());\n+  ASSERT_TRUE(s.ToString().find(\"issing\") != std::string::npos)\n+      << s.ToString();\n+}\n+\n+TEST(DBTest, FilesDeletedAfterCompaction) {\n+  ASSERT_OK(Put(\"foo\", \"v2\"));\n+  Compact(\"a\", \"z\");\n+  const int num_files = CountFiles();\n+  for (int i = 0; i < 10; i++) {\n+    ASSERT_OK(Put(\"foo\", \"v2\"));\n+    Compact(\"a\", \"z\");\n+  }\n+  ASSERT_EQ(CountFiles(), num_files);\n+}\n+\n+TEST(DBTest, BloomFilter) {\n+  env_->count_random_reads_ = true;\n+  Options options = CurrentOptions();\n+  options.env = env_;\n+  options.block_cache = NewLRUCache(0);  // Prevent cache hits\n+  options.filter_policy = NewBloomFilterPolicy(10);\n+  Reopen(&options);\n+\n+  // Populate multiple layers\n+  const int N = 10000;\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_OK(Put(Key(i), Key(i)));\n+  }\n+  Compact(\"a\", \"z\");\n+  for (int i = 0; i < N; i += 100) {\n+    ASSERT_OK(Put(Key(i), Key(i)));\n+  }\n+  dbfull()->TEST_CompactMemTable();\n+\n+  // Prevent auto compactions triggered by seeks\n+  env_->delay_sstable_sync_.Release_Store(env_);\n+\n+  // Lookup present keys.  Should rarely read from small sstable.\n+  env_->random_read_counter_.Reset();\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_EQ(Key(i), Get(Key(i)));\n+  }\n+  int reads = env_->random_read_counter_.Read();\n+  fprintf(stderr, \"%d present => %d reads\\n\", N, reads);\n+  ASSERT_GE(reads, N);\n+  ASSERT_LE(reads, N + 2*N/100);\n+\n+  // Lookup present keys.  Should rarely read from either sstable.\n+  env_->random_read_counter_.Reset();\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_EQ(\"NOT_FOUND\", Get(Key(i) + \".missing\"));\n+  }\n+  reads = env_->random_read_counter_.Read();\n+  fprintf(stderr, \"%d missing => %d reads\\n\", N, reads);\n+  ASSERT_LE(reads, 3*N/100);\n+\n+  env_->delay_sstable_sync_.Release_Store(NULL);\n+  Close();\n+  delete options.block_cache;\n+  delete options.filter_policy;\n+}\n+\n+// Multi-threaded test:\n+namespace {\n+\n+static const int kNumThreads = 4;\n+static const int kTestSeconds = 10;\n+static const int kNumKeys = 1000;\n+\n+struct MTState {\n+  DBTest* test;\n+  port::AtomicPointer stop;\n+  port::AtomicPointer counter[kNumThreads];\n+  port::AtomicPointer thread_done[kNumThreads];\n+};\n+\n+struct MTThread {\n+  MTState* state;\n+  int id;\n+};\n+\n+static void MTThreadBody(void* arg) {\n+  MTThread* t = reinterpret_cast<MTThread*>(arg);\n+  int id = t->id;\n+  DB* db = t->state->test->db_;\n+  uintptr_t counter = 0;\n+  fprintf(stderr, \"... starting thread %d\\n\", id);\n+  Random rnd(1000 + id);\n+  std::string value;\n+  char valbuf[1500];\n+  while (t->state->stop.Acquire_Load() == NULL) {\n+    t->state->counter[id].Release_Store(reinterpret_cast<void*>(counter));\n+\n+    int key = rnd.Uniform(kNumKeys);\n+    char keybuf[20];\n+    snprintf(keybuf, sizeof(keybuf), \"%016d\", key);\n+\n+    if (rnd.OneIn(2)) {\n+      // Write values of the form <key, my id, counter>.\n+      // We add some padding for force compactions.\n+      snprintf(valbuf, sizeof(valbuf), \"%d.%d.%-1000d\",\n+               key, id, static_cast<int>(counter));\n+      ASSERT_OK(db->Put(WriteOptions(), Slice(keybuf), Slice(valbuf)));\n+    } else {\n+      // Read a value and verify that it matches the pattern written above.\n+      Status s = db->Get(ReadOptions(), Slice(keybuf), &value);\n+      if (s.IsNotFound()) {\n+        // Key has not yet been written\n+      } else {\n+        // Check that the writer thread counter is >= the counter in the value\n+        ASSERT_OK(s);\n+        int k, w, c;\n+        ASSERT_EQ(3, sscanf(value.c_str(), \"%d.%d.%d\", &k, &w, &c)) << value;\n+        ASSERT_EQ(k, key);\n+        ASSERT_GE(w, 0);\n+        ASSERT_LT(w, kNumThreads);\n+        ASSERT_LE(c, reinterpret_cast<uintptr_t>(\n+            t->state->counter[w].Acquire_Load()));\n+      }\n+    }\n+    counter++;\n+  }\n+  t->state->thread_done[id].Release_Store(t);\n+  fprintf(stderr, \"... stopping thread %d after %d ops\\n\", id, int(counter));\n+}\n+\n+}  // namespace\n+\n+TEST(DBTest, MultiThreaded) {\n+  do {\n+    // Initialize state\n+    MTState mt;\n+    mt.test = this;\n+    mt.stop.Release_Store(0);\n+    for (int id = 0; id < kNumThreads; id++) {\n+      mt.counter[id].Release_Store(0);\n+      mt.thread_done[id].Release_Store(0);\n+    }\n+\n+    // Start threads\n+    MTThread thread[kNumThreads];\n+    for (int id = 0; id < kNumThreads; id++) {\n+      thread[id].state = &mt;\n+      thread[id].id = id;\n+      env_->StartThread(MTThreadBody, &thread[id]);\n+    }\n+\n+    // Let them run for a while\n+    DelayMilliseconds(kTestSeconds * 1000);\n+\n+    // Stop the threads and wait for them to finish\n+    mt.stop.Release_Store(&mt);\n+    for (int id = 0; id < kNumThreads; id++) {\n+      while (mt.thread_done[id].Acquire_Load() == NULL) {\n+        DelayMilliseconds(100);\n+      }\n+    }\n+  } while (ChangeOptions());\n+}\n+\n+namespace {\n+typedef std::map<std::string, std::string> KVMap;\n+}\n+\n+class ModelDB: public DB {\n+ public:\n+  class ModelSnapshot : public Snapshot {\n+   public:\n+    KVMap map_;\n+  };\n+\n+  explicit ModelDB(const Options& options): options_(options) { }\n+  ~ModelDB() { }\n+  virtual Status Put(const WriteOptions& o, const Slice& k, const Slice& v) {\n+    return DB::Put(o, k, v);\n+  }\n+  virtual Status Delete(const WriteOptions& o, const Slice& key) {\n+    return DB::Delete(o, key);\n+  }\n+  virtual Status Get(const ReadOptions& options,\n+                     const Slice& key, std::string* value) {\n+    assert(false);      // Not implemented\n+    return Status::NotFound(key);\n+  }\n+  virtual Iterator* NewIterator(const ReadOptions& options) {\n+    if (options.snapshot == NULL) {\n+      KVMap* saved = new KVMap;\n+      *saved = map_;\n+      return new ModelIter(saved, true);\n+    } else {\n+      const KVMap* snapshot_state =\n+          &(reinterpret_cast<const ModelSnapshot*>(options.snapshot)->map_);\n+      return new ModelIter(snapshot_state, false);\n+    }\n+  }\n+  virtual const Snapshot* GetSnapshot() {\n+    ModelSnapshot* snapshot = new ModelSnapshot;\n+    snapshot->map_ = map_;\n+    return snapshot;\n+  }\n+\n+  virtual void ReleaseSnapshot(const Snapshot* snapshot) {\n+    delete reinterpret_cast<const ModelSnapshot*>(snapshot);\n+  }\n+  virtual Status Write(const WriteOptions& options, WriteBatch* batch) {\n+    class Handler : public WriteBatch::Handler {\n+     public:\n+      KVMap* map_;\n+      virtual void Put(const Slice& key, const Slice& value) {\n+        (*map_)[key.ToString()] = value.ToString();\n+      }\n+      virtual void Delete(const Slice& key) {\n+        map_->erase(key.ToString());\n+      }\n+    };\n+    Handler handler;\n+    handler.map_ = &map_;\n+    return batch->Iterate(&handler);\n+  }\n+\n+  virtual bool GetProperty(const Slice& property, std::string* value) {\n+    return false;\n+  }\n+  virtual void GetApproximateSizes(const Range* r, int n, uint64_t* sizes) {\n+    for (int i = 0; i < n; i++) {\n+      sizes[i] = 0;\n+    }\n+  }\n+  virtual void CompactRange(const Slice* start, const Slice* end) {\n+  }\n+\n+ private:\n+  class ModelIter: public Iterator {\n+   public:\n+    ModelIter(const KVMap* map, bool owned)\n+        : map_(map), owned_(owned), iter_(map_->end()) {\n+    }\n+    ~ModelIter() {\n+      if (owned_) delete map_;\n+    }\n+    virtual bool Valid() const { return iter_ != map_->end(); }\n+    virtual void SeekToFirst() { iter_ = map_->begin(); }\n+    virtual void SeekToLast() {\n+      if (map_->empty()) {\n+        iter_ = map_->end();\n+      } else {\n+        iter_ = map_->find(map_->rbegin()->first);\n+      }\n+    }\n+    virtual void Seek(const Slice& k) {\n+      iter_ = map_->lower_bound(k.ToString());\n+    }\n+    virtual void Next() { ++iter_; }\n+    virtual void Prev() { --iter_; }\n+    virtual Slice key() const { return iter_->first; }\n+    virtual Slice value() const { return iter_->second; }\n+    virtual Status status() const { return Status::OK(); }\n+   private:\n+    const KVMap* const map_;\n+    const bool owned_;  // Do we own map_\n+    KVMap::const_iterator iter_;\n+  };\n+  const Options options_;\n+  KVMap map_;\n+};\n+\n+static std::string RandomKey(Random* rnd) {\n+  int len = (rnd->OneIn(3)\n+             ? 1                // Short sometimes to encourage collisions\n+             : (rnd->OneIn(100) ? rnd->Skewed(10) : rnd->Uniform(10)));\n+  return test::RandomKey(rnd, len);\n+}\n+\n+static bool CompareIterators(int step,\n+                             DB* model,\n+                             DB* db,\n+                             const Snapshot* model_snap,\n+                             const Snapshot* db_snap) {\n+  ReadOptions options;\n+  options.snapshot = model_snap;\n+  Iterator* miter = model->NewIterator(options);\n+  options.snapshot = db_snap;\n+  Iterator* dbiter = db->NewIterator(options);\n+  bool ok = true;\n+  int count = 0;\n+  for (miter->SeekToFirst(), dbiter->SeekToFirst();\n+       ok && miter->Valid() && dbiter->Valid();\n+       miter->Next(), dbiter->Next()) {\n+    count++;\n+    if (miter->key().compare(dbiter->key()) != 0) {\n+      fprintf(stderr, \"step %d: Key mismatch: '%s' vs. '%s'\\n\",\n+              step,\n+              EscapeString(miter->key()).c_str(),\n+              EscapeString(dbiter->key()).c_str());\n+      ok = false;\n+      break;\n+    }\n+\n+    if (miter->value().compare(dbiter->value()) != 0) {\n+      fprintf(stderr, \"step %d: Value mismatch for key '%s': '%s' vs. '%s'\\n\",\n+              step,\n+              EscapeString(miter->key()).c_str(),\n+              EscapeString(miter->value()).c_str(),\n+              EscapeString(miter->value()).c_str());\n+      ok = false;\n+    }\n+  }\n+\n+  if (ok) {\n+    if (miter->Valid() != dbiter->Valid()) {\n+      fprintf(stderr, \"step %d: Mismatch at end of iterators: %d vs. %d\\n\",\n+              step, miter->Valid(), dbiter->Valid());\n+      ok = false;\n+    }\n+  }\n+  fprintf(stderr, \"%d entries compared: ok=%d\\n\", count, ok);\n+  delete miter;\n+  delete dbiter;\n+  return ok;\n+}\n+\n+TEST(DBTest, Randomized) {\n+  Random rnd(test::RandomSeed());\n+  do {\n+    ModelDB model(CurrentOptions());\n+    const int N = 10000;\n+    const Snapshot* model_snap = NULL;\n+    const Snapshot* db_snap = NULL;\n+    std::string k, v;\n+    for (int step = 0; step < N; step++) {\n+      if (step % 100 == 0) {\n+        fprintf(stderr, \"Step %d of %d\\n\", step, N);\n+      }\n+      // TODO(sanjay): Test Get() works\n+      int p = rnd.Uniform(100);\n+      if (p < 45) {                               // Put\n+        k = RandomKey(&rnd);\n+        v = RandomString(&rnd,\n+                         rnd.OneIn(20)\n+                         ? 100 + rnd.Uniform(100)\n+                         : rnd.Uniform(8));\n+        ASSERT_OK(model.Put(WriteOptions(), k, v));\n+        ASSERT_OK(db_->Put(WriteOptions(), k, v));\n+\n+      } else if (p < 90) {                        // Delete\n+        k = RandomKey(&rnd);\n+        ASSERT_OK(model.Delete(WriteOptions(), k));\n+        ASSERT_OK(db_->Delete(WriteOptions(), k));\n+\n+\n+      } else {                                    // Multi-element batch\n+        WriteBatch b;\n+        const int num = rnd.Uniform(8);\n+        for (int i = 0; i < num; i++) {\n+          if (i == 0 || !rnd.OneIn(10)) {\n+            k = RandomKey(&rnd);\n+          } else {\n+            // Periodically re-use the same key from the previous iter, so\n+            // we have multiple entries in the write batch for the same key\n+          }\n+          if (rnd.OneIn(2)) {\n+            v = RandomString(&rnd, rnd.Uniform(10));\n+            b.Put(k, v);\n+          } else {\n+            b.Delete(k);\n+          }\n+        }\n+        ASSERT_OK(model.Write(WriteOptions(), &b));\n+        ASSERT_OK(db_->Write(WriteOptions(), &b));\n+      }\n+\n+      if ((step % 100) == 0) {\n+        ASSERT_TRUE(CompareIterators(step, &model, db_, NULL, NULL));\n+        ASSERT_TRUE(CompareIterators(step, &model, db_, model_snap, db_snap));\n+        // Save a snapshot from each DB this time that we'll use next\n+        // time we compare things, to make sure the current state is\n+        // preserved with the snapshot\n+        if (model_snap != NULL) model.ReleaseSnapshot(model_snap);\n+        if (db_snap != NULL) db_->ReleaseSnapshot(db_snap);\n+\n+        Reopen();\n+        ASSERT_TRUE(CompareIterators(step, &model, db_, NULL, NULL));\n+\n+        model_snap = model.GetSnapshot();\n+        db_snap = db_->GetSnapshot();\n+      }\n+    }\n+    if (model_snap != NULL) model.ReleaseSnapshot(model_snap);\n+    if (db_snap != NULL) db_->ReleaseSnapshot(db_snap);\n+  } while (ChangeOptions());\n+}\n+\n+std::string MakeKey(unsigned int num) {\n+  char buf[30];\n+  snprintf(buf, sizeof(buf), \"%016u\", num);\n+  return std::string(buf);\n+}\n+\n+void BM_LogAndApply(int iters, int num_base_files) {\n+  std::string dbname = test::TmpDir() + \"/leveldb_test_benchmark\";\n+  DestroyDB(dbname, Options());\n+\n+  DB* db = NULL;\n+  Options opts;\n+  opts.create_if_missing = true;\n+  Status s = DB::Open(opts, dbname, &db);\n+  ASSERT_OK(s);\n+  ASSERT_TRUE(db != NULL);\n+\n+  delete db;\n+  db = NULL;\n+\n+  Env* env = Env::Default();\n+\n+  port::Mutex mu;\n+  MutexLock l(&mu);\n+\n+  InternalKeyComparator cmp(BytewiseComparator());\n+  Options options;\n+  VersionSet vset(dbname, &options, NULL, &cmp);\n+  ASSERT_OK(vset.Recover());\n+  VersionEdit vbase;\n+  uint64_t fnum = 1;\n+  for (int i = 0; i < num_base_files; i++) {\n+    InternalKey start(MakeKey(2*fnum), 1, kTypeValue);\n+    InternalKey limit(MakeKey(2*fnum+1), 1, kTypeDeletion);\n+    vbase.AddFile(2, fnum++, 1 /* file size */, start, limit);\n+  }\n+  ASSERT_OK(vset.LogAndApply(&vbase, &mu));\n+\n+  uint64_t start_micros = env->NowMicros();\n+\n+  for (int i = 0; i < iters; i++) {\n+    VersionEdit vedit;\n+    vedit.DeleteFile(2, fnum);\n+    InternalKey start(MakeKey(2*fnum), 1, kTypeValue);\n+    InternalKey limit(MakeKey(2*fnum+1), 1, kTypeDeletion);\n+    vedit.AddFile(2, fnum++, 1 /* file size */, start, limit);\n+    vset.LogAndApply(&vedit, &mu);\n+  }\n+  uint64_t stop_micros = env->NowMicros();\n+  unsigned int us = stop_micros - start_micros;\n+  char buf[16];\n+  snprintf(buf, sizeof(buf), \"%d\", num_base_files);\n+  fprintf(stderr,\n+          \"BM_LogAndApply/%-6s   %8d iters : %9u us (%7.0f us / iter)\\n\",\n+          buf, iters, us, ((float)us) / iters);\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  if (argc > 1 && std::string(argv[1]) == \"--benchmark\") {\n+    leveldb::BM_LogAndApply(1000, 1);\n+    leveldb::BM_LogAndApply(1000, 100);\n+    leveldb::BM_LogAndApply(1000, 10000);\n+    leveldb::BM_LogAndApply(100, 100000);\n+    return 0;\n+  }\n+\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "20a7ca44626050c01b7bde718581c663d38f42a9",
        "filename": "src/leveldb/db/dbformat.cc",
        "status": "added",
        "additions": 140,
        "deletions": 0,
        "changes": 140,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/dbformat.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/dbformat.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/dbformat.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,140 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include <stdio.h>\n+#include \"db/dbformat.h\"\n+#include \"port/port.h\"\n+#include \"util/coding.h\"\n+\n+namespace leveldb {\n+\n+static uint64_t PackSequenceAndType(uint64_t seq, ValueType t) {\n+  assert(seq <= kMaxSequenceNumber);\n+  assert(t <= kValueTypeForSeek);\n+  return (seq << 8) | t;\n+}\n+\n+void AppendInternalKey(std::string* result, const ParsedInternalKey& key) {\n+  result->append(key.user_key.data(), key.user_key.size());\n+  PutFixed64(result, PackSequenceAndType(key.sequence, key.type));\n+}\n+\n+std::string ParsedInternalKey::DebugString() const {\n+  char buf[50];\n+  snprintf(buf, sizeof(buf), \"' @ %llu : %d\",\n+           (unsigned long long) sequence,\n+           int(type));\n+  std::string result = \"'\";\n+  result += EscapeString(user_key.ToString());\n+  result += buf;\n+  return result;\n+}\n+\n+std::string InternalKey::DebugString() const {\n+  std::string result;\n+  ParsedInternalKey parsed;\n+  if (ParseInternalKey(rep_, &parsed)) {\n+    result = parsed.DebugString();\n+  } else {\n+    result = \"(bad)\";\n+    result.append(EscapeString(rep_));\n+  }\n+  return result;\n+}\n+\n+const char* InternalKeyComparator::Name() const {\n+  return \"leveldb.InternalKeyComparator\";\n+}\n+\n+int InternalKeyComparator::Compare(const Slice& akey, const Slice& bkey) const {\n+  // Order by:\n+  //    increasing user key (according to user-supplied comparator)\n+  //    decreasing sequence number\n+  //    decreasing type (though sequence# should be enough to disambiguate)\n+  int r = user_comparator_->Compare(ExtractUserKey(akey), ExtractUserKey(bkey));\n+  if (r == 0) {\n+    const uint64_t anum = DecodeFixed64(akey.data() + akey.size() - 8);\n+    const uint64_t bnum = DecodeFixed64(bkey.data() + bkey.size() - 8);\n+    if (anum > bnum) {\n+      r = -1;\n+    } else if (anum < bnum) {\n+      r = +1;\n+    }\n+  }\n+  return r;\n+}\n+\n+void InternalKeyComparator::FindShortestSeparator(\n+      std::string* start,\n+      const Slice& limit) const {\n+  // Attempt to shorten the user portion of the key\n+  Slice user_start = ExtractUserKey(*start);\n+  Slice user_limit = ExtractUserKey(limit);\n+  std::string tmp(user_start.data(), user_start.size());\n+  user_comparator_->FindShortestSeparator(&tmp, user_limit);\n+  if (tmp.size() < user_start.size() &&\n+      user_comparator_->Compare(user_start, tmp) < 0) {\n+    // User key has become shorter physically, but larger logically.\n+    // Tack on the earliest possible number to the shortened user key.\n+    PutFixed64(&tmp, PackSequenceAndType(kMaxSequenceNumber,kValueTypeForSeek));\n+    assert(this->Compare(*start, tmp) < 0);\n+    assert(this->Compare(tmp, limit) < 0);\n+    start->swap(tmp);\n+  }\n+}\n+\n+void InternalKeyComparator::FindShortSuccessor(std::string* key) const {\n+  Slice user_key = ExtractUserKey(*key);\n+  std::string tmp(user_key.data(), user_key.size());\n+  user_comparator_->FindShortSuccessor(&tmp);\n+  if (tmp.size() < user_key.size() &&\n+      user_comparator_->Compare(user_key, tmp) < 0) {\n+    // User key has become shorter physically, but larger logically.\n+    // Tack on the earliest possible number to the shortened user key.\n+    PutFixed64(&tmp, PackSequenceAndType(kMaxSequenceNumber,kValueTypeForSeek));\n+    assert(this->Compare(*key, tmp) < 0);\n+    key->swap(tmp);\n+  }\n+}\n+\n+const char* InternalFilterPolicy::Name() const {\n+  return user_policy_->Name();\n+}\n+\n+void InternalFilterPolicy::CreateFilter(const Slice* keys, int n,\n+                                        std::string* dst) const {\n+  // We rely on the fact that the code in table.cc does not mind us\n+  // adjusting keys[].\n+  Slice* mkey = const_cast<Slice*>(keys);\n+  for (int i = 0; i < n; i++) {\n+    mkey[i] = ExtractUserKey(keys[i]);\n+    // TODO(sanjay): Suppress dups?\n+  }\n+  user_policy_->CreateFilter(keys, n, dst);\n+}\n+\n+bool InternalFilterPolicy::KeyMayMatch(const Slice& key, const Slice& f) const {\n+  return user_policy_->KeyMayMatch(ExtractUserKey(key), f);\n+}\n+\n+LookupKey::LookupKey(const Slice& user_key, SequenceNumber s) {\n+  size_t usize = user_key.size();\n+  size_t needed = usize + 13;  // A conservative estimate\n+  char* dst;\n+  if (needed <= sizeof(space_)) {\n+    dst = space_;\n+  } else {\n+    dst = new char[needed];\n+  }\n+  start_ = dst;\n+  dst = EncodeVarint32(dst, usize + 8);\n+  kstart_ = dst;\n+  memcpy(dst, user_key.data(), usize);\n+  dst += usize;\n+  EncodeFixed64(dst, PackSequenceAndType(s, kValueTypeForSeek));\n+  dst += 8;\n+  end_ = dst;\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "f7f64dafb6d5b3d676ecde9d0fc86ab2942b2494",
        "filename": "src/leveldb/db/dbformat.h",
        "status": "added",
        "additions": 227,
        "deletions": 0,
        "changes": 227,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/dbformat.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/dbformat.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/dbformat.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,227 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_FORMAT_H_\n+#define STORAGE_LEVELDB_DB_FORMAT_H_\n+\n+#include <stdio.h>\n+#include \"leveldb/comparator.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/filter_policy.h\"\n+#include \"leveldb/slice.h\"\n+#include \"leveldb/table_builder.h\"\n+#include \"util/coding.h\"\n+#include \"util/logging.h\"\n+\n+namespace leveldb {\n+\n+// Grouping of constants.  We may want to make some of these\n+// parameters set via options.\n+namespace config {\n+static const int kNumLevels = 7;\n+\n+// Level-0 compaction is started when we hit this many files.\n+static const int kL0_CompactionTrigger = 4;\n+\n+// Soft limit on number of level-0 files.  We slow down writes at this point.\n+static const int kL0_SlowdownWritesTrigger = 8;\n+\n+// Maximum number of level-0 files.  We stop writes at this point.\n+static const int kL0_StopWritesTrigger = 12;\n+\n+// Maximum level to which a new compacted memtable is pushed if it\n+// does not create overlap.  We try to push to level 2 to avoid the\n+// relatively expensive level 0=>1 compactions and to avoid some\n+// expensive manifest file operations.  We do not push all the way to\n+// the largest level since that can generate a lot of wasted disk\n+// space if the same key space is being repeatedly overwritten.\n+static const int kMaxMemCompactLevel = 2;\n+\n+}  // namespace config\n+\n+class InternalKey;\n+\n+// Value types encoded as the last component of internal keys.\n+// DO NOT CHANGE THESE ENUM VALUES: they are embedded in the on-disk\n+// data structures.\n+enum ValueType {\n+  kTypeDeletion = 0x0,\n+  kTypeValue = 0x1\n+};\n+// kValueTypeForSeek defines the ValueType that should be passed when\n+// constructing a ParsedInternalKey object for seeking to a particular\n+// sequence number (since we sort sequence numbers in decreasing order\n+// and the value type is embedded as the low 8 bits in the sequence\n+// number in internal keys, we need to use the highest-numbered\n+// ValueType, not the lowest).\n+static const ValueType kValueTypeForSeek = kTypeValue;\n+\n+typedef uint64_t SequenceNumber;\n+\n+// We leave eight bits empty at the bottom so a type and sequence#\n+// can be packed together into 64-bits.\n+static const SequenceNumber kMaxSequenceNumber =\n+    ((0x1ull << 56) - 1);\n+\n+struct ParsedInternalKey {\n+  Slice user_key;\n+  SequenceNumber sequence;\n+  ValueType type;\n+\n+  ParsedInternalKey() { }  // Intentionally left uninitialized (for speed)\n+  ParsedInternalKey(const Slice& u, const SequenceNumber& seq, ValueType t)\n+      : user_key(u), sequence(seq), type(t) { }\n+  std::string DebugString() const;\n+};\n+\n+// Return the length of the encoding of \"key\".\n+inline size_t InternalKeyEncodingLength(const ParsedInternalKey& key) {\n+  return key.user_key.size() + 8;\n+}\n+\n+// Append the serialization of \"key\" to *result.\n+extern void AppendInternalKey(std::string* result,\n+                              const ParsedInternalKey& key);\n+\n+// Attempt to parse an internal key from \"internal_key\".  On success,\n+// stores the parsed data in \"*result\", and returns true.\n+//\n+// On error, returns false, leaves \"*result\" in an undefined state.\n+extern bool ParseInternalKey(const Slice& internal_key,\n+                             ParsedInternalKey* result);\n+\n+// Returns the user key portion of an internal key.\n+inline Slice ExtractUserKey(const Slice& internal_key) {\n+  assert(internal_key.size() >= 8);\n+  return Slice(internal_key.data(), internal_key.size() - 8);\n+}\n+\n+inline ValueType ExtractValueType(const Slice& internal_key) {\n+  assert(internal_key.size() >= 8);\n+  const size_t n = internal_key.size();\n+  uint64_t num = DecodeFixed64(internal_key.data() + n - 8);\n+  unsigned char c = num & 0xff;\n+  return static_cast<ValueType>(c);\n+}\n+\n+// A comparator for internal keys that uses a specified comparator for\n+// the user key portion and breaks ties by decreasing sequence number.\n+class InternalKeyComparator : public Comparator {\n+ private:\n+  const Comparator* user_comparator_;\n+ public:\n+  explicit InternalKeyComparator(const Comparator* c) : user_comparator_(c) { }\n+  virtual const char* Name() const;\n+  virtual int Compare(const Slice& a, const Slice& b) const;\n+  virtual void FindShortestSeparator(\n+      std::string* start,\n+      const Slice& limit) const;\n+  virtual void FindShortSuccessor(std::string* key) const;\n+\n+  const Comparator* user_comparator() const { return user_comparator_; }\n+\n+  int Compare(const InternalKey& a, const InternalKey& b) const;\n+};\n+\n+// Filter policy wrapper that converts from internal keys to user keys\n+class InternalFilterPolicy : public FilterPolicy {\n+ private:\n+  const FilterPolicy* const user_policy_;\n+ public:\n+  explicit InternalFilterPolicy(const FilterPolicy* p) : user_policy_(p) { }\n+  virtual const char* Name() const;\n+  virtual void CreateFilter(const Slice* keys, int n, std::string* dst) const;\n+  virtual bool KeyMayMatch(const Slice& key, const Slice& filter) const;\n+};\n+\n+// Modules in this directory should keep internal keys wrapped inside\n+// the following class instead of plain strings so that we do not\n+// incorrectly use string comparisons instead of an InternalKeyComparator.\n+class InternalKey {\n+ private:\n+  std::string rep_;\n+ public:\n+  InternalKey() { }   // Leave rep_ as empty to indicate it is invalid\n+  InternalKey(const Slice& user_key, SequenceNumber s, ValueType t) {\n+    AppendInternalKey(&rep_, ParsedInternalKey(user_key, s, t));\n+  }\n+\n+  void DecodeFrom(const Slice& s) { rep_.assign(s.data(), s.size()); }\n+  Slice Encode() const {\n+    assert(!rep_.empty());\n+    return rep_;\n+  }\n+\n+  Slice user_key() const { return ExtractUserKey(rep_); }\n+\n+  void SetFrom(const ParsedInternalKey& p) {\n+    rep_.clear();\n+    AppendInternalKey(&rep_, p);\n+  }\n+\n+  void Clear() { rep_.clear(); }\n+\n+  std::string DebugString() const;\n+};\n+\n+inline int InternalKeyComparator::Compare(\n+    const InternalKey& a, const InternalKey& b) const {\n+  return Compare(a.Encode(), b.Encode());\n+}\n+\n+inline bool ParseInternalKey(const Slice& internal_key,\n+                             ParsedInternalKey* result) {\n+  const size_t n = internal_key.size();\n+  if (n < 8) return false;\n+  uint64_t num = DecodeFixed64(internal_key.data() + n - 8);\n+  unsigned char c = num & 0xff;\n+  result->sequence = num >> 8;\n+  result->type = static_cast<ValueType>(c);\n+  result->user_key = Slice(internal_key.data(), n - 8);\n+  return (c <= static_cast<unsigned char>(kTypeValue));\n+}\n+\n+// A helper class useful for DBImpl::Get()\n+class LookupKey {\n+ public:\n+  // Initialize *this for looking up user_key at a snapshot with\n+  // the specified sequence number.\n+  LookupKey(const Slice& user_key, SequenceNumber sequence);\n+\n+  ~LookupKey();\n+\n+  // Return a key suitable for lookup in a MemTable.\n+  Slice memtable_key() const { return Slice(start_, end_ - start_); }\n+\n+  // Return an internal key (suitable for passing to an internal iterator)\n+  Slice internal_key() const { return Slice(kstart_, end_ - kstart_); }\n+\n+  // Return the user key\n+  Slice user_key() const { return Slice(kstart_, end_ - kstart_ - 8); }\n+\n+ private:\n+  // We construct a char array of the form:\n+  //    klength  varint32               <-- start_\n+  //    userkey  char[klength]          <-- kstart_\n+  //    tag      uint64\n+  //                                    <-- end_\n+  // The array is a suitable MemTable key.\n+  // The suffix starting with \"userkey\" can be used as an InternalKey.\n+  const char* start_;\n+  const char* kstart_;\n+  const char* end_;\n+  char space_[200];      // Avoid allocation for short keys\n+\n+  // No copying allowed\n+  LookupKey(const LookupKey&);\n+  void operator=(const LookupKey&);\n+};\n+\n+inline LookupKey::~LookupKey() {\n+  if (start_ != space_) delete[] start_;\n+}\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_FORMAT_H_"
      },
      {
        "sha": "5d82f5d313fad88ea4e1d079427bba13df667cfe",
        "filename": "src/leveldb/db/dbformat_test.cc",
        "status": "added",
        "additions": 112,
        "deletions": 0,
        "changes": 112,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/dbformat_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/dbformat_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/dbformat_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,112 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/dbformat.h\"\n+#include \"util/logging.h\"\n+#include \"util/testharness.h\"\n+\n+namespace leveldb {\n+\n+static std::string IKey(const std::string& user_key,\n+                        uint64_t seq,\n+                        ValueType vt) {\n+  std::string encoded;\n+  AppendInternalKey(&encoded, ParsedInternalKey(user_key, seq, vt));\n+  return encoded;\n+}\n+\n+static std::string Shorten(const std::string& s, const std::string& l) {\n+  std::string result = s;\n+  InternalKeyComparator(BytewiseComparator()).FindShortestSeparator(&result, l);\n+  return result;\n+}\n+\n+static std::string ShortSuccessor(const std::string& s) {\n+  std::string result = s;\n+  InternalKeyComparator(BytewiseComparator()).FindShortSuccessor(&result);\n+  return result;\n+}\n+\n+static void TestKey(const std::string& key,\n+                    uint64_t seq,\n+                    ValueType vt) {\n+  std::string encoded = IKey(key, seq, vt);\n+\n+  Slice in(encoded);\n+  ParsedInternalKey decoded(\"\", 0, kTypeValue);\n+\n+  ASSERT_TRUE(ParseInternalKey(in, &decoded));\n+  ASSERT_EQ(key, decoded.user_key.ToString());\n+  ASSERT_EQ(seq, decoded.sequence);\n+  ASSERT_EQ(vt, decoded.type);\n+\n+  ASSERT_TRUE(!ParseInternalKey(Slice(\"bar\"), &decoded));\n+}\n+\n+class FormatTest { };\n+\n+TEST(FormatTest, InternalKey_EncodeDecode) {\n+  const char* keys[] = { \"\", \"k\", \"hello\", \"longggggggggggggggggggggg\" };\n+  const uint64_t seq[] = {\n+    1, 2, 3,\n+    (1ull << 8) - 1, 1ull << 8, (1ull << 8) + 1,\n+    (1ull << 16) - 1, 1ull << 16, (1ull << 16) + 1,\n+    (1ull << 32) - 1, 1ull << 32, (1ull << 32) + 1\n+  };\n+  for (int k = 0; k < sizeof(keys) / sizeof(keys[0]); k++) {\n+    for (int s = 0; s < sizeof(seq) / sizeof(seq[0]); s++) {\n+      TestKey(keys[k], seq[s], kTypeValue);\n+      TestKey(\"hello\", 1, kTypeDeletion);\n+    }\n+  }\n+}\n+\n+TEST(FormatTest, InternalKeyShortSeparator) {\n+  // When user keys are same\n+  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"foo\", 99, kTypeValue)));\n+  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"foo\", 101, kTypeValue)));\n+  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"foo\", 100, kTypeValue)));\n+  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"foo\", 100, kTypeDeletion)));\n+\n+  // When user keys are misordered\n+  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"bar\", 99, kTypeValue)));\n+\n+  // When user keys are different, but correctly ordered\n+  ASSERT_EQ(IKey(\"g\", kMaxSequenceNumber, kValueTypeForSeek),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"hello\", 200, kTypeValue)));\n+\n+  // When start user key is prefix of limit user key\n+  ASSERT_EQ(IKey(\"foo\", 100, kTypeValue),\n+            Shorten(IKey(\"foo\", 100, kTypeValue),\n+                    IKey(\"foobar\", 200, kTypeValue)));\n+\n+  // When limit user key is prefix of start user key\n+  ASSERT_EQ(IKey(\"foobar\", 100, kTypeValue),\n+            Shorten(IKey(\"foobar\", 100, kTypeValue),\n+                    IKey(\"foo\", 200, kTypeValue)));\n+}\n+\n+TEST(FormatTest, InternalKeyShortestSuccessor) {\n+  ASSERT_EQ(IKey(\"g\", kMaxSequenceNumber, kValueTypeForSeek),\n+            ShortSuccessor(IKey(\"foo\", 100, kTypeValue)));\n+  ASSERT_EQ(IKey(\"\\xff\\xff\", 100, kTypeValue),\n+            ShortSuccessor(IKey(\"\\xff\\xff\", 100, kTypeValue)));\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "3c4d49f64eb6dfbf9d3740cbfda3f6ad218a4f52",
        "filename": "src/leveldb/db/filename.cc",
        "status": "added",
        "additions": 139,
        "deletions": 0,
        "changes": 139,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/filename.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/filename.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/filename.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,139 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include <ctype.h>\n+#include <stdio.h>\n+#include \"db/filename.h\"\n+#include \"db/dbformat.h\"\n+#include \"leveldb/env.h\"\n+#include \"util/logging.h\"\n+\n+namespace leveldb {\n+\n+// A utility routine: write \"data\" to the named file and Sync() it.\n+extern Status WriteStringToFileSync(Env* env, const Slice& data,\n+                                    const std::string& fname);\n+\n+static std::string MakeFileName(const std::string& name, uint64_t number,\n+                                const char* suffix) {\n+  char buf[100];\n+  snprintf(buf, sizeof(buf), \"/%06llu.%s\",\n+           static_cast<unsigned long long>(number),\n+           suffix);\n+  return name + buf;\n+}\n+\n+std::string LogFileName(const std::string& name, uint64_t number) {\n+  assert(number > 0);\n+  return MakeFileName(name, number, \"log\");\n+}\n+\n+std::string TableFileName(const std::string& name, uint64_t number) {\n+  assert(number > 0);\n+  return MakeFileName(name, number, \"sst\");\n+}\n+\n+std::string DescriptorFileName(const std::string& dbname, uint64_t number) {\n+  assert(number > 0);\n+  char buf[100];\n+  snprintf(buf, sizeof(buf), \"/MANIFEST-%06llu\",\n+           static_cast<unsigned long long>(number));\n+  return dbname + buf;\n+}\n+\n+std::string CurrentFileName(const std::string& dbname) {\n+  return dbname + \"/CURRENT\";\n+}\n+\n+std::string LockFileName(const std::string& dbname) {\n+  return dbname + \"/LOCK\";\n+}\n+\n+std::string TempFileName(const std::string& dbname, uint64_t number) {\n+  assert(number > 0);\n+  return MakeFileName(dbname, number, \"dbtmp\");\n+}\n+\n+std::string InfoLogFileName(const std::string& dbname) {\n+  return dbname + \"/LOG\";\n+}\n+\n+// Return the name of the old info log file for \"dbname\".\n+std::string OldInfoLogFileName(const std::string& dbname) {\n+  return dbname + \"/LOG.old\";\n+}\n+\n+\n+// Owned filenames have the form:\n+//    dbname/CURRENT\n+//    dbname/LOCK\n+//    dbname/LOG\n+//    dbname/LOG.old\n+//    dbname/MANIFEST-[0-9]+\n+//    dbname/[0-9]+.(log|sst)\n+bool ParseFileName(const std::string& fname,\n+                   uint64_t* number,\n+                   FileType* type) {\n+  Slice rest(fname);\n+  if (rest == \"CURRENT\") {\n+    *number = 0;\n+    *type = kCurrentFile;\n+  } else if (rest == \"LOCK\") {\n+    *number = 0;\n+    *type = kDBLockFile;\n+  } else if (rest == \"LOG\" || rest == \"LOG.old\") {\n+    *number = 0;\n+    *type = kInfoLogFile;\n+  } else if (rest.starts_with(\"MANIFEST-\")) {\n+    rest.remove_prefix(strlen(\"MANIFEST-\"));\n+    uint64_t num;\n+    if (!ConsumeDecimalNumber(&rest, &num)) {\n+      return false;\n+    }\n+    if (!rest.empty()) {\n+      return false;\n+    }\n+    *type = kDescriptorFile;\n+    *number = num;\n+  } else {\n+    // Avoid strtoull() to keep filename format independent of the\n+    // current locale\n+    uint64_t num;\n+    if (!ConsumeDecimalNumber(&rest, &num)) {\n+      return false;\n+    }\n+    Slice suffix = rest;\n+    if (suffix == Slice(\".log\")) {\n+      *type = kLogFile;\n+    } else if (suffix == Slice(\".sst\")) {\n+      *type = kTableFile;\n+    } else if (suffix == Slice(\".dbtmp\")) {\n+      *type = kTempFile;\n+    } else {\n+      return false;\n+    }\n+    *number = num;\n+  }\n+  return true;\n+}\n+\n+Status SetCurrentFile(Env* env, const std::string& dbname,\n+                      uint64_t descriptor_number) {\n+  // Remove leading \"dbname/\" and add newline to manifest file name\n+  std::string manifest = DescriptorFileName(dbname, descriptor_number);\n+  Slice contents = manifest;\n+  assert(contents.starts_with(dbname + \"/\"));\n+  contents.remove_prefix(dbname.size() + 1);\n+  std::string tmp = TempFileName(dbname, descriptor_number);\n+  Status s = WriteStringToFileSync(env, contents.ToString() + \"\\n\", tmp);\n+  if (s.ok()) {\n+    s = env->RenameFile(tmp, CurrentFileName(dbname));\n+  }\n+  if (!s.ok()) {\n+    env->DeleteFile(tmp);\n+  }\n+  return s;\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "d5d09b11468105324761761665bb8db72abf9540",
        "filename": "src/leveldb/db/filename.h",
        "status": "added",
        "additions": 80,
        "deletions": 0,
        "changes": 80,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/filename.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/filename.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/filename.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,80 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// File names used by DB code\n+\n+#ifndef STORAGE_LEVELDB_DB_FILENAME_H_\n+#define STORAGE_LEVELDB_DB_FILENAME_H_\n+\n+#include <stdint.h>\n+#include <string>\n+#include \"leveldb/slice.h\"\n+#include \"leveldb/status.h\"\n+#include \"port/port.h\"\n+\n+namespace leveldb {\n+\n+class Env;\n+\n+enum FileType {\n+  kLogFile,\n+  kDBLockFile,\n+  kTableFile,\n+  kDescriptorFile,\n+  kCurrentFile,\n+  kTempFile,\n+  kInfoLogFile  // Either the current one, or an old one\n+};\n+\n+// Return the name of the log file with the specified number\n+// in the db named by \"dbname\".  The result will be prefixed with\n+// \"dbname\".\n+extern std::string LogFileName(const std::string& dbname, uint64_t number);\n+\n+// Return the name of the sstable with the specified number\n+// in the db named by \"dbname\".  The result will be prefixed with\n+// \"dbname\".\n+extern std::string TableFileName(const std::string& dbname, uint64_t number);\n+\n+// Return the name of the descriptor file for the db named by\n+// \"dbname\" and the specified incarnation number.  The result will be\n+// prefixed with \"dbname\".\n+extern std::string DescriptorFileName(const std::string& dbname,\n+                                      uint64_t number);\n+\n+// Return the name of the current file.  This file contains the name\n+// of the current manifest file.  The result will be prefixed with\n+// \"dbname\".\n+extern std::string CurrentFileName(const std::string& dbname);\n+\n+// Return the name of the lock file for the db named by\n+// \"dbname\".  The result will be prefixed with \"dbname\".\n+extern std::string LockFileName(const std::string& dbname);\n+\n+// Return the name of a temporary file owned by the db named \"dbname\".\n+// The result will be prefixed with \"dbname\".\n+extern std::string TempFileName(const std::string& dbname, uint64_t number);\n+\n+// Return the name of the info log file for \"dbname\".\n+extern std::string InfoLogFileName(const std::string& dbname);\n+\n+// Return the name of the old info log file for \"dbname\".\n+extern std::string OldInfoLogFileName(const std::string& dbname);\n+\n+// If filename is a leveldb file, store the type of the file in *type.\n+// The number encoded in the filename is stored in *number.  If the\n+// filename was successfully parsed, returns true.  Else return false.\n+extern bool ParseFileName(const std::string& filename,\n+                          uint64_t* number,\n+                          FileType* type);\n+\n+// Make the CURRENT file point to the descriptor file with the\n+// specified number.\n+extern Status SetCurrentFile(Env* env, const std::string& dbname,\n+                             uint64_t descriptor_number);\n+\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_FILENAME_H_"
      },
      {
        "sha": "5a26da4728f6a0934f12d37d84e845ef6a00e8af",
        "filename": "src/leveldb/db/filename_test.cc",
        "status": "added",
        "additions": 122,
        "deletions": 0,
        "changes": 122,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/filename_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/filename_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/filename_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,122 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/filename.h\"\n+\n+#include \"db/dbformat.h\"\n+#include \"port/port.h\"\n+#include \"util/logging.h\"\n+#include \"util/testharness.h\"\n+\n+namespace leveldb {\n+\n+class FileNameTest { };\n+\n+TEST(FileNameTest, Parse) {\n+  Slice db;\n+  FileType type;\n+  uint64_t number;\n+\n+  // Successful parses\n+  static struct {\n+    const char* fname;\n+    uint64_t number;\n+    FileType type;\n+  } cases[] = {\n+    { \"100.log\",            100,   kLogFile },\n+    { \"0.log\",              0,     kLogFile },\n+    { \"0.sst\",              0,     kTableFile },\n+    { \"CURRENT\",            0,     kCurrentFile },\n+    { \"LOCK\",               0,     kDBLockFile },\n+    { \"MANIFEST-2\",         2,     kDescriptorFile },\n+    { \"MANIFEST-7\",         7,     kDescriptorFile },\n+    { \"LOG\",                0,     kInfoLogFile },\n+    { \"LOG.old\",            0,     kInfoLogFile },\n+    { \"18446744073709551615.log\", 18446744073709551615ull, kLogFile },\n+  };\n+  for (int i = 0; i < sizeof(cases) / sizeof(cases[0]); i++) {\n+    std::string f = cases[i].fname;\n+    ASSERT_TRUE(ParseFileName(f, &number, &type)) << f;\n+    ASSERT_EQ(cases[i].type, type) << f;\n+    ASSERT_EQ(cases[i].number, number) << f;\n+  }\n+\n+  // Errors\n+  static const char* errors[] = {\n+    \"\",\n+    \"foo\",\n+    \"foo-dx-100.log\",\n+    \".log\",\n+    \"\",\n+    \"manifest\",\n+    \"CURREN\",\n+    \"CURRENTX\",\n+    \"MANIFES\",\n+    \"MANIFEST\",\n+    \"MANIFEST-\",\n+    \"XMANIFEST-3\",\n+    \"MANIFEST-3x\",\n+    \"LOC\",\n+    \"LOCKx\",\n+    \"LO\",\n+    \"LOGx\",\n+    \"18446744073709551616.log\",\n+    \"184467440737095516150.log\",\n+    \"100\",\n+    \"100.\",\n+    \"100.lop\"\n+  };\n+  for (int i = 0; i < sizeof(errors) / sizeof(errors[0]); i++) {\n+    std::string f = errors[i];\n+    ASSERT_TRUE(!ParseFileName(f, &number, &type)) << f;\n+  }\n+}\n+\n+TEST(FileNameTest, Construction) {\n+  uint64_t number;\n+  FileType type;\n+  std::string fname;\n+\n+  fname = CurrentFileName(\"foo\");\n+  ASSERT_EQ(\"foo/\", std::string(fname.data(), 4));\n+  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n+  ASSERT_EQ(0, number);\n+  ASSERT_EQ(kCurrentFile, type);\n+\n+  fname = LockFileName(\"foo\");\n+  ASSERT_EQ(\"foo/\", std::string(fname.data(), 4));\n+  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n+  ASSERT_EQ(0, number);\n+  ASSERT_EQ(kDBLockFile, type);\n+\n+  fname = LogFileName(\"foo\", 192);\n+  ASSERT_EQ(\"foo/\", std::string(fname.data(), 4));\n+  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n+  ASSERT_EQ(192, number);\n+  ASSERT_EQ(kLogFile, type);\n+\n+  fname = TableFileName(\"bar\", 200);\n+  ASSERT_EQ(\"bar/\", std::string(fname.data(), 4));\n+  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n+  ASSERT_EQ(200, number);\n+  ASSERT_EQ(kTableFile, type);\n+\n+  fname = DescriptorFileName(\"bar\", 100);\n+  ASSERT_EQ(\"bar/\", std::string(fname.data(), 4));\n+  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n+  ASSERT_EQ(100, number);\n+  ASSERT_EQ(kDescriptorFile, type);\n+\n+  fname = TempFileName(\"tmp\", 999);\n+  ASSERT_EQ(\"tmp/\", std::string(fname.data(), 4));\n+  ASSERT_TRUE(ParseFileName(fname.c_str() + 4, &number, &type));\n+  ASSERT_EQ(999, number);\n+  ASSERT_EQ(kTempFile, type);\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "995d76107a16efd9677ec6d2c125c04502fcec3d",
        "filename": "src/leveldb/db/leveldb_main.cc",
        "status": "added",
        "additions": 238,
        "deletions": 0,
        "changes": 238,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/leveldb_main.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/leveldb_main.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/leveldb_main.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,238 @@\n+// Copyright (c) 2012 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include <stdio.h>\n+#include \"db/dbformat.h\"\n+#include \"db/filename.h\"\n+#include \"db/log_reader.h\"\n+#include \"db/version_edit.h\"\n+#include \"db/write_batch_internal.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/iterator.h\"\n+#include \"leveldb/options.h\"\n+#include \"leveldb/status.h\"\n+#include \"leveldb/table.h\"\n+#include \"leveldb/write_batch.h\"\n+#include \"util/logging.h\"\n+\n+namespace leveldb {\n+\n+namespace {\n+\n+bool GuessType(const std::string& fname, FileType* type) {\n+  size_t pos = fname.rfind('/');\n+  std::string basename;\n+  if (pos == std::string::npos) {\n+    basename = fname;\n+  } else {\n+    basename = std::string(fname.data() + pos + 1, fname.size() - pos - 1);\n+  }\n+  uint64_t ignored;\n+  return ParseFileName(basename, &ignored, type);\n+}\n+\n+// Notified when log reader encounters corruption.\n+class CorruptionReporter : public log::Reader::Reporter {\n+ public:\n+  virtual void Corruption(size_t bytes, const Status& status) {\n+    printf(\"corruption: %d bytes; %s\\n\",\n+            static_cast<int>(bytes),\n+            status.ToString().c_str());\n+  }\n+};\n+\n+// Print contents of a log file. (*func)() is called on every record.\n+bool PrintLogContents(Env* env, const std::string& fname,\n+                      void (*func)(Slice)) {\n+  SequentialFile* file;\n+  Status s = env->NewSequentialFile(fname, &file);\n+  if (!s.ok()) {\n+    fprintf(stderr, \"%s\\n\", s.ToString().c_str());\n+    return false;\n+  }\n+  CorruptionReporter reporter;\n+  log::Reader reader(file, &reporter, true, 0);\n+  Slice record;\n+  std::string scratch;\n+  while (reader.ReadRecord(&record, &scratch)) {\n+    printf(\"--- offset %llu; \",\n+           static_cast<unsigned long long>(reader.LastRecordOffset()));\n+    (*func)(record);\n+  }\n+  delete file;\n+  return true;\n+}\n+\n+// Called on every item found in a WriteBatch.\n+class WriteBatchItemPrinter : public WriteBatch::Handler {\n+ public:\n+  uint64_t offset_;\n+  uint64_t sequence_;\n+\n+  virtual void Put(const Slice& key, const Slice& value) {\n+    printf(\"  put '%s' '%s'\\n\",\n+           EscapeString(key).c_str(),\n+           EscapeString(value).c_str());\n+  }\n+  virtual void Delete(const Slice& key) {\n+    printf(\"  del '%s'\\n\",\n+           EscapeString(key).c_str());\n+  }\n+};\n+\n+\n+// Called on every log record (each one of which is a WriteBatch)\n+// found in a kLogFile.\n+static void WriteBatchPrinter(Slice record) {\n+  if (record.size() < 12) {\n+    printf(\"log record length %d is too small\\n\",\n+           static_cast<int>(record.size()));\n+    return;\n+  }\n+  WriteBatch batch;\n+  WriteBatchInternal::SetContents(&batch, record);\n+  printf(\"sequence %llu\\n\",\n+         static_cast<unsigned long long>(WriteBatchInternal::Sequence(&batch)));\n+  WriteBatchItemPrinter batch_item_printer;\n+  Status s = batch.Iterate(&batch_item_printer);\n+  if (!s.ok()) {\n+    printf(\"  error: %s\\n\", s.ToString().c_str());\n+  }\n+}\n+\n+bool DumpLog(Env* env, const std::string& fname) {\n+  return PrintLogContents(env, fname, WriteBatchPrinter);\n+}\n+\n+// Called on every log record (each one of which is a WriteBatch)\n+// found in a kDescriptorFile.\n+static void VersionEditPrinter(Slice record) {\n+  VersionEdit edit;\n+  Status s = edit.DecodeFrom(record);\n+  if (!s.ok()) {\n+    printf(\"%s\\n\", s.ToString().c_str());\n+    return;\n+  }\n+  printf(\"%s\", edit.DebugString().c_str());\n+}\n+\n+bool DumpDescriptor(Env* env, const std::string& fname) {\n+  return PrintLogContents(env, fname, VersionEditPrinter);\n+}\n+\n+bool DumpTable(Env* env, const std::string& fname) {\n+  uint64_t file_size;\n+  RandomAccessFile* file = NULL;\n+  Table* table = NULL;\n+  Status s = env->GetFileSize(fname, &file_size);\n+  if (s.ok()) {\n+    s = env->NewRandomAccessFile(fname, &file);\n+  }\n+  if (s.ok()) {\n+    // We use the default comparator, which may or may not match the\n+    // comparator used in this database. However this should not cause\n+    // problems since we only use Table operations that do not require\n+    // any comparisons.  In particular, we do not call Seek or Prev.\n+    s = Table::Open(Options(), file, file_size, &table);\n+  }\n+  if (!s.ok()) {\n+    fprintf(stderr, \"%s\\n\", s.ToString().c_str());\n+    delete table;\n+    delete file;\n+    return false;\n+  }\n+\n+  ReadOptions ro;\n+  ro.fill_cache = false;\n+  Iterator* iter = table->NewIterator(ro);\n+  for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+    ParsedInternalKey key;\n+    if (!ParseInternalKey(iter->key(), &key)) {\n+      printf(\"badkey '%s' => '%s'\\n\",\n+             EscapeString(iter->key()).c_str(),\n+             EscapeString(iter->value()).c_str());\n+    } else {\n+      char kbuf[20];\n+      const char* type;\n+      if (key.type == kTypeDeletion) {\n+        type = \"del\";\n+      } else if (key.type == kTypeValue) {\n+        type = \"val\";\n+      } else {\n+        snprintf(kbuf, sizeof(kbuf), \"%d\", static_cast<int>(key.type));\n+        type = kbuf;\n+      }\n+      printf(\"'%s' @ %8llu : %s => '%s'\\n\",\n+             EscapeString(key.user_key).c_str(),\n+             static_cast<unsigned long long>(key.sequence),\n+             type,\n+             EscapeString(iter->value()).c_str());\n+    }\n+  }\n+  s = iter->status();\n+  if (!s.ok()) {\n+    printf(\"iterator error: %s\\n\", s.ToString().c_str());\n+  }\n+\n+  delete iter;\n+  delete table;\n+  delete file;\n+  return true;\n+}\n+\n+bool DumpFile(Env* env, const std::string& fname) {\n+  FileType ftype;\n+  if (!GuessType(fname, &ftype)) {\n+    fprintf(stderr, \"%s: unknown file type\\n\", fname.c_str());\n+    return false;\n+  }\n+  switch (ftype) {\n+    case kLogFile:         return DumpLog(env, fname);\n+    case kDescriptorFile:  return DumpDescriptor(env, fname);\n+    case kTableFile:       return DumpTable(env, fname);\n+\n+    default: {\n+      fprintf(stderr, \"%s: not a dump-able file type\\n\", fname.c_str());\n+      break;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool HandleDumpCommand(Env* env, char** files, int num) {\n+  bool ok = true;\n+  for (int i = 0; i < num; i++) {\n+    ok &= DumpFile(env, files[i]);\n+  }\n+  return ok;\n+}\n+\n+}\n+}  // namespace leveldb\n+\n+static void Usage() {\n+  fprintf(\n+      stderr,\n+      \"Usage: leveldbutil command...\\n\"\n+      \"   dump files...         -- dump contents of specified files\\n\"\n+      );\n+}\n+\n+int main(int argc, char** argv) {\n+  leveldb::Env* env = leveldb::Env::Default();\n+  bool ok = true;\n+  if (argc < 2) {\n+    Usage();\n+    ok = false;\n+  } else {\n+    std::string command = argv[1];\n+    if (command == \"dump\") {\n+      ok = leveldb::HandleDumpCommand(env, argv+2, argc-2);\n+    } else {\n+      Usage();\n+      ok = false;\n+    }\n+  }\n+  return (ok ? 0 : 1);\n+}"
      },
      {
        "sha": "2690cb9789ee63e85260500a6736ec302d22f9b7",
        "filename": "src/leveldb/db/log_format.h",
        "status": "added",
        "additions": 35,
        "deletions": 0,
        "changes": 35,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/log_format.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/log_format.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/log_format.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,35 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// Log format information shared by reader and writer.\n+// See ../doc/log_format.txt for more detail.\n+\n+#ifndef STORAGE_LEVELDB_DB_LOG_FORMAT_H_\n+#define STORAGE_LEVELDB_DB_LOG_FORMAT_H_\n+\n+namespace leveldb {\n+namespace log {\n+\n+enum RecordType {\n+  // Zero is reserved for preallocated files\n+  kZeroType = 0,\n+\n+  kFullType = 1,\n+\n+  // For fragments\n+  kFirstType = 2,\n+  kMiddleType = 3,\n+  kLastType = 4\n+};\n+static const int kMaxRecordType = kLastType;\n+\n+static const int kBlockSize = 32768;\n+\n+// Header is checksum (4 bytes), type (1 byte), length (2 bytes).\n+static const int kHeaderSize = 4 + 1 + 2;\n+\n+}  // namespace log\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_LOG_FORMAT_H_"
      },
      {
        "sha": "b35f115aadac28055b16fce6f133bd5148ecec16",
        "filename": "src/leveldb/db/log_reader.cc",
        "status": "added",
        "additions": 259,
        "deletions": 0,
        "changes": 259,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/log_reader.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/log_reader.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/log_reader.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,259 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/log_reader.h\"\n+\n+#include <stdio.h>\n+#include \"leveldb/env.h\"\n+#include \"util/coding.h\"\n+#include \"util/crc32c.h\"\n+\n+namespace leveldb {\n+namespace log {\n+\n+Reader::Reporter::~Reporter() {\n+}\n+\n+Reader::Reader(SequentialFile* file, Reporter* reporter, bool checksum,\n+               uint64_t initial_offset)\n+    : file_(file),\n+      reporter_(reporter),\n+      checksum_(checksum),\n+      backing_store_(new char[kBlockSize]),\n+      buffer_(),\n+      eof_(false),\n+      last_record_offset_(0),\n+      end_of_buffer_offset_(0),\n+      initial_offset_(initial_offset) {\n+}\n+\n+Reader::~Reader() {\n+  delete[] backing_store_;\n+}\n+\n+bool Reader::SkipToInitialBlock() {\n+  size_t offset_in_block = initial_offset_ % kBlockSize;\n+  uint64_t block_start_location = initial_offset_ - offset_in_block;\n+\n+  // Don't search a block if we'd be in the trailer\n+  if (offset_in_block > kBlockSize - 6) {\n+    offset_in_block = 0;\n+    block_start_location += kBlockSize;\n+  }\n+\n+  end_of_buffer_offset_ = block_start_location;\n+\n+  // Skip to start of first block that can contain the initial record\n+  if (block_start_location > 0) {\n+    Status skip_status = file_->Skip(block_start_location);\n+    if (!skip_status.ok()) {\n+      ReportDrop(block_start_location, skip_status);\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+bool Reader::ReadRecord(Slice* record, std::string* scratch) {\n+  if (last_record_offset_ < initial_offset_) {\n+    if (!SkipToInitialBlock()) {\n+      return false;\n+    }\n+  }\n+\n+  scratch->clear();\n+  record->clear();\n+  bool in_fragmented_record = false;\n+  // Record offset of the logical record that we're reading\n+  // 0 is a dummy value to make compilers happy\n+  uint64_t prospective_record_offset = 0;\n+\n+  Slice fragment;\n+  while (true) {\n+    uint64_t physical_record_offset = end_of_buffer_offset_ - buffer_.size();\n+    const unsigned int record_type = ReadPhysicalRecord(&fragment);\n+    switch (record_type) {\n+      case kFullType:\n+        if (in_fragmented_record) {\n+          // Handle bug in earlier versions of log::Writer where\n+          // it could emit an empty kFirstType record at the tail end\n+          // of a block followed by a kFullType or kFirstType record\n+          // at the beginning of the next block.\n+          if (scratch->empty()) {\n+            in_fragmented_record = false;\n+          } else {\n+            ReportCorruption(scratch->size(), \"partial record without end(1)\");\n+          }\n+        }\n+        prospective_record_offset = physical_record_offset;\n+        scratch->clear();\n+        *record = fragment;\n+        last_record_offset_ = prospective_record_offset;\n+        return true;\n+\n+      case kFirstType:\n+        if (in_fragmented_record) {\n+          // Handle bug in earlier versions of log::Writer where\n+          // it could emit an empty kFirstType record at the tail end\n+          // of a block followed by a kFullType or kFirstType record\n+          // at the beginning of the next block.\n+          if (scratch->empty()) {\n+            in_fragmented_record = false;\n+          } else {\n+            ReportCorruption(scratch->size(), \"partial record without end(2)\");\n+          }\n+        }\n+        prospective_record_offset = physical_record_offset;\n+        scratch->assign(fragment.data(), fragment.size());\n+        in_fragmented_record = true;\n+        break;\n+\n+      case kMiddleType:\n+        if (!in_fragmented_record) {\n+          ReportCorruption(fragment.size(),\n+                           \"missing start of fragmented record(1)\");\n+        } else {\n+          scratch->append(fragment.data(), fragment.size());\n+        }\n+        break;\n+\n+      case kLastType:\n+        if (!in_fragmented_record) {\n+          ReportCorruption(fragment.size(),\n+                           \"missing start of fragmented record(2)\");\n+        } else {\n+          scratch->append(fragment.data(), fragment.size());\n+          *record = Slice(*scratch);\n+          last_record_offset_ = prospective_record_offset;\n+          return true;\n+        }\n+        break;\n+\n+      case kEof:\n+        if (in_fragmented_record) {\n+          ReportCorruption(scratch->size(), \"partial record without end(3)\");\n+          scratch->clear();\n+        }\n+        return false;\n+\n+      case kBadRecord:\n+        if (in_fragmented_record) {\n+          ReportCorruption(scratch->size(), \"error in middle of record\");\n+          in_fragmented_record = false;\n+          scratch->clear();\n+        }\n+        break;\n+\n+      default: {\n+        char buf[40];\n+        snprintf(buf, sizeof(buf), \"unknown record type %u\", record_type);\n+        ReportCorruption(\n+            (fragment.size() + (in_fragmented_record ? scratch->size() : 0)),\n+            buf);\n+        in_fragmented_record = false;\n+        scratch->clear();\n+        break;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+uint64_t Reader::LastRecordOffset() {\n+  return last_record_offset_;\n+}\n+\n+void Reader::ReportCorruption(size_t bytes, const char* reason) {\n+  ReportDrop(bytes, Status::Corruption(reason));\n+}\n+\n+void Reader::ReportDrop(size_t bytes, const Status& reason) {\n+  if (reporter_ != NULL &&\n+      end_of_buffer_offset_ - buffer_.size() - bytes >= initial_offset_) {\n+    reporter_->Corruption(bytes, reason);\n+  }\n+}\n+\n+unsigned int Reader::ReadPhysicalRecord(Slice* result) {\n+  while (true) {\n+    if (buffer_.size() < kHeaderSize) {\n+      if (!eof_) {\n+        // Last read was a full read, so this is a trailer to skip\n+        buffer_.clear();\n+        Status status = file_->Read(kBlockSize, &buffer_, backing_store_);\n+        end_of_buffer_offset_ += buffer_.size();\n+        if (!status.ok()) {\n+          buffer_.clear();\n+          ReportDrop(kBlockSize, status);\n+          eof_ = true;\n+          return kEof;\n+        } else if (buffer_.size() < kBlockSize) {\n+          eof_ = true;\n+        }\n+        continue;\n+      } else if (buffer_.size() == 0) {\n+        // End of file\n+        return kEof;\n+      } else {\n+        size_t drop_size = buffer_.size();\n+        buffer_.clear();\n+        ReportCorruption(drop_size, \"truncated record at end of file\");\n+        return kEof;\n+      }\n+    }\n+\n+    // Parse the header\n+    const char* header = buffer_.data();\n+    const uint32_t a = static_cast<uint32_t>(header[4]) & 0xff;\n+    const uint32_t b = static_cast<uint32_t>(header[5]) & 0xff;\n+    const unsigned int type = header[6];\n+    const uint32_t length = a | (b << 8);\n+    if (kHeaderSize + length > buffer_.size()) {\n+      size_t drop_size = buffer_.size();\n+      buffer_.clear();\n+      ReportCorruption(drop_size, \"bad record length\");\n+      return kBadRecord;\n+    }\n+\n+    if (type == kZeroType && length == 0) {\n+      // Skip zero length record without reporting any drops since\n+      // such records are produced by the mmap based writing code in\n+      // env_posix.cc that preallocates file regions.\n+      buffer_.clear();\n+      return kBadRecord;\n+    }\n+\n+    // Check crc\n+    if (checksum_) {\n+      uint32_t expected_crc = crc32c::Unmask(DecodeFixed32(header));\n+      uint32_t actual_crc = crc32c::Value(header + 6, 1 + length);\n+      if (actual_crc != expected_crc) {\n+        // Drop the rest of the buffer since \"length\" itself may have\n+        // been corrupted and if we trust it, we could find some\n+        // fragment of a real log record that just happens to look\n+        // like a valid log record.\n+        size_t drop_size = buffer_.size();\n+        buffer_.clear();\n+        ReportCorruption(drop_size, \"checksum mismatch\");\n+        return kBadRecord;\n+      }\n+    }\n+\n+    buffer_.remove_prefix(kHeaderSize + length);\n+\n+    // Skip physical record that started before initial_offset_\n+    if (end_of_buffer_offset_ - buffer_.size() - kHeaderSize - length <\n+        initial_offset_) {\n+      result->clear();\n+      return kBadRecord;\n+    }\n+\n+    *result = Slice(header + kHeaderSize, length);\n+    return type;\n+  }\n+}\n+\n+}  // namespace log\n+}  // namespace leveldb"
      },
      {
        "sha": "82d4bee68d0eea3a7fdd270e91b49545a536f45e",
        "filename": "src/leveldb/db/log_reader.h",
        "status": "added",
        "additions": 108,
        "deletions": 0,
        "changes": 108,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/log_reader.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/log_reader.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/log_reader.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,108 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_LOG_READER_H_\n+#define STORAGE_LEVELDB_DB_LOG_READER_H_\n+\n+#include <stdint.h>\n+\n+#include \"db/log_format.h\"\n+#include \"leveldb/slice.h\"\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+class SequentialFile;\n+\n+namespace log {\n+\n+class Reader {\n+ public:\n+  // Interface for reporting errors.\n+  class Reporter {\n+   public:\n+    virtual ~Reporter();\n+\n+    // Some corruption was detected.  \"size\" is the approximate number\n+    // of bytes dropped due to the corruption.\n+    virtual void Corruption(size_t bytes, const Status& status) = 0;\n+  };\n+\n+  // Create a reader that will return log records from \"*file\".\n+  // \"*file\" must remain live while this Reader is in use.\n+  //\n+  // If \"reporter\" is non-NULL, it is notified whenever some data is\n+  // dropped due to a detected corruption.  \"*reporter\" must remain\n+  // live while this Reader is in use.\n+  //\n+  // If \"checksum\" is true, verify checksums if available.\n+  //\n+  // The Reader will start reading at the first record located at physical\n+  // position >= initial_offset within the file.\n+  Reader(SequentialFile* file, Reporter* reporter, bool checksum,\n+         uint64_t initial_offset);\n+\n+  ~Reader();\n+\n+  // Read the next record into *record.  Returns true if read\n+  // successfully, false if we hit end of the input.  May use\n+  // \"*scratch\" as temporary storage.  The contents filled in *record\n+  // will only be valid until the next mutating operation on this\n+  // reader or the next mutation to *scratch.\n+  bool ReadRecord(Slice* record, std::string* scratch);\n+\n+  // Returns the physical offset of the last record returned by ReadRecord.\n+  //\n+  // Undefined before the first call to ReadRecord.\n+  uint64_t LastRecordOffset();\n+\n+ private:\n+  SequentialFile* const file_;\n+  Reporter* const reporter_;\n+  bool const checksum_;\n+  char* const backing_store_;\n+  Slice buffer_;\n+  bool eof_;   // Last Read() indicated EOF by returning < kBlockSize\n+\n+  // Offset of the last record returned by ReadRecord.\n+  uint64_t last_record_offset_;\n+  // Offset of the first location past the end of buffer_.\n+  uint64_t end_of_buffer_offset_;\n+\n+  // Offset at which to start looking for the first record to return\n+  uint64_t const initial_offset_;\n+\n+  // Extend record types with the following special values\n+  enum {\n+    kEof = kMaxRecordType + 1,\n+    // Returned whenever we find an invalid physical record.\n+    // Currently there are three situations in which this happens:\n+    // * The record has an invalid CRC (ReadPhysicalRecord reports a drop)\n+    // * The record is a 0-length record (No drop is reported)\n+    // * The record is below constructor's initial_offset (No drop is reported)\n+    kBadRecord = kMaxRecordType + 2\n+  };\n+\n+  // Skips all blocks that are completely before \"initial_offset_\".\n+  //\n+  // Returns true on success. Handles reporting.\n+  bool SkipToInitialBlock();\n+\n+  // Return type, or one of the preceding special values\n+  unsigned int ReadPhysicalRecord(Slice* result);\n+\n+  // Reports dropped bytes to the reporter.\n+  // buffer_ must be updated to remove the dropped bytes prior to invocation.\n+  void ReportCorruption(size_t bytes, const char* reason);\n+  void ReportDrop(size_t bytes, const Status& reason);\n+\n+  // No copying allowed\n+  Reader(const Reader&);\n+  void operator=(const Reader&);\n+};\n+\n+}  // namespace log\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_LOG_READER_H_"
      },
      {
        "sha": "4c5cf875733c16175743224e642f0507a0da663f",
        "filename": "src/leveldb/db/log_test.cc",
        "status": "added",
        "additions": 500,
        "deletions": 0,
        "changes": 500,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/log_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/log_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/log_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,500 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/log_reader.h\"\n+#include \"db/log_writer.h\"\n+#include \"leveldb/env.h\"\n+#include \"util/coding.h\"\n+#include \"util/crc32c.h\"\n+#include \"util/random.h\"\n+#include \"util/testharness.h\"\n+\n+namespace leveldb {\n+namespace log {\n+\n+// Construct a string of the specified length made out of the supplied\n+// partial string.\n+static std::string BigString(const std::string& partial_string, size_t n) {\n+  std::string result;\n+  while (result.size() < n) {\n+    result.append(partial_string);\n+  }\n+  result.resize(n);\n+  return result;\n+}\n+\n+// Construct a string from a number\n+static std::string NumberString(int n) {\n+  char buf[50];\n+  snprintf(buf, sizeof(buf), \"%d.\", n);\n+  return std::string(buf);\n+}\n+\n+// Return a skewed potentially long string\n+static std::string RandomSkewedString(int i, Random* rnd) {\n+  return BigString(NumberString(i), rnd->Skewed(17));\n+}\n+\n+class LogTest {\n+ private:\n+  class StringDest : public WritableFile {\n+   public:\n+    std::string contents_;\n+\n+    virtual Status Close() { return Status::OK(); }\n+    virtual Status Flush() { return Status::OK(); }\n+    virtual Status Sync() { return Status::OK(); }\n+    virtual Status Append(const Slice& slice) {\n+      contents_.append(slice.data(), slice.size());\n+      return Status::OK();\n+    }\n+  };\n+\n+  class StringSource : public SequentialFile {\n+   public:\n+    Slice contents_;\n+    bool force_error_;\n+    bool returned_partial_;\n+    StringSource() : force_error_(false), returned_partial_(false) { }\n+\n+    virtual Status Read(size_t n, Slice* result, char* scratch) {\n+      ASSERT_TRUE(!returned_partial_) << \"must not Read() after eof/error\";\n+\n+      if (force_error_) {\n+        force_error_ = false;\n+        returned_partial_ = true;\n+        return Status::Corruption(\"read error\");\n+      }\n+\n+      if (contents_.size() < n) {\n+        n = contents_.size();\n+        returned_partial_ = true;\n+      }\n+      *result = Slice(contents_.data(), n);\n+      contents_.remove_prefix(n);\n+      return Status::OK();\n+    }\n+\n+    virtual Status Skip(uint64_t n) {\n+      if (n > contents_.size()) {\n+        contents_.clear();\n+        return Status::NotFound(\"in-memory file skipepd past end\");\n+      }\n+\n+      contents_.remove_prefix(n);\n+\n+      return Status::OK();\n+    }\n+  };\n+\n+  class ReportCollector : public Reader::Reporter {\n+   public:\n+    size_t dropped_bytes_;\n+    std::string message_;\n+\n+    ReportCollector() : dropped_bytes_(0) { }\n+    virtual void Corruption(size_t bytes, const Status& status) {\n+      dropped_bytes_ += bytes;\n+      message_.append(status.ToString());\n+    }\n+  };\n+\n+  StringDest dest_;\n+  StringSource source_;\n+  ReportCollector report_;\n+  bool reading_;\n+  Writer writer_;\n+  Reader reader_;\n+\n+  // Record metadata for testing initial offset functionality\n+  static size_t initial_offset_record_sizes_[];\n+  static uint64_t initial_offset_last_record_offsets_[];\n+\n+ public:\n+  LogTest() : reading_(false),\n+              writer_(&dest_),\n+              reader_(&source_, &report_, true/*checksum*/,\n+                      0/*initial_offset*/) {\n+  }\n+\n+  void Write(const std::string& msg) {\n+    ASSERT_TRUE(!reading_) << \"Write() after starting to read\";\n+    writer_.AddRecord(Slice(msg));\n+  }\n+\n+  size_t WrittenBytes() const {\n+    return dest_.contents_.size();\n+  }\n+\n+  std::string Read() {\n+    if (!reading_) {\n+      reading_ = true;\n+      source_.contents_ = Slice(dest_.contents_);\n+    }\n+    std::string scratch;\n+    Slice record;\n+    if (reader_.ReadRecord(&record, &scratch)) {\n+      return record.ToString();\n+    } else {\n+      return \"EOF\";\n+    }\n+  }\n+\n+  void IncrementByte(int offset, int delta) {\n+    dest_.contents_[offset] += delta;\n+  }\n+\n+  void SetByte(int offset, char new_byte) {\n+    dest_.contents_[offset] = new_byte;\n+  }\n+\n+  void ShrinkSize(int bytes) {\n+    dest_.contents_.resize(dest_.contents_.size() - bytes);\n+  }\n+\n+  void FixChecksum(int header_offset, int len) {\n+    // Compute crc of type/len/data\n+    uint32_t crc = crc32c::Value(&dest_.contents_[header_offset+6], 1 + len);\n+    crc = crc32c::Mask(crc);\n+    EncodeFixed32(&dest_.contents_[header_offset], crc);\n+  }\n+\n+  void ForceError() {\n+    source_.force_error_ = true;\n+  }\n+\n+  size_t DroppedBytes() const {\n+    return report_.dropped_bytes_;\n+  }\n+\n+  std::string ReportMessage() const {\n+    return report_.message_;\n+  }\n+\n+  // Returns OK iff recorded error message contains \"msg\"\n+  std::string MatchError(const std::string& msg) const {\n+    if (report_.message_.find(msg) == std::string::npos) {\n+      return report_.message_;\n+    } else {\n+      return \"OK\";\n+    }\n+  }\n+\n+  void WriteInitialOffsetLog() {\n+    for (int i = 0; i < 4; i++) {\n+      std::string record(initial_offset_record_sizes_[i],\n+                         static_cast<char>('a' + i));\n+      Write(record);\n+    }\n+  }\n+\n+  void CheckOffsetPastEndReturnsNoRecords(uint64_t offset_past_end) {\n+    WriteInitialOffsetLog();\n+    reading_ = true;\n+    source_.contents_ = Slice(dest_.contents_);\n+    Reader* offset_reader = new Reader(&source_, &report_, true/*checksum*/,\n+                                       WrittenBytes() + offset_past_end);\n+    Slice record;\n+    std::string scratch;\n+    ASSERT_TRUE(!offset_reader->ReadRecord(&record, &scratch));\n+    delete offset_reader;\n+  }\n+\n+  void CheckInitialOffsetRecord(uint64_t initial_offset,\n+                                int expected_record_offset) {\n+    WriteInitialOffsetLog();\n+    reading_ = true;\n+    source_.contents_ = Slice(dest_.contents_);\n+    Reader* offset_reader = new Reader(&source_, &report_, true/*checksum*/,\n+                                       initial_offset);\n+    Slice record;\n+    std::string scratch;\n+    ASSERT_TRUE(offset_reader->ReadRecord(&record, &scratch));\n+    ASSERT_EQ(initial_offset_record_sizes_[expected_record_offset],\n+              record.size());\n+    ASSERT_EQ(initial_offset_last_record_offsets_[expected_record_offset],\n+              offset_reader->LastRecordOffset());\n+    ASSERT_EQ((char)('a' + expected_record_offset), record.data()[0]);\n+    delete offset_reader;\n+  }\n+\n+};\n+\n+size_t LogTest::initial_offset_record_sizes_[] =\n+    {10000,  // Two sizable records in first block\n+     10000,\n+     2 * log::kBlockSize - 1000,  // Span three blocks\n+     1};\n+\n+uint64_t LogTest::initial_offset_last_record_offsets_[] =\n+    {0,\n+     kHeaderSize + 10000,\n+     2 * (kHeaderSize + 10000),\n+     2 * (kHeaderSize + 10000) +\n+         (2 * log::kBlockSize - 1000) + 3 * kHeaderSize};\n+\n+\n+TEST(LogTest, Empty) {\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+TEST(LogTest, ReadWrite) {\n+  Write(\"foo\");\n+  Write(\"bar\");\n+  Write(\"\");\n+  Write(\"xxxx\");\n+  ASSERT_EQ(\"foo\", Read());\n+  ASSERT_EQ(\"bar\", Read());\n+  ASSERT_EQ(\"\", Read());\n+  ASSERT_EQ(\"xxxx\", Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(\"EOF\", Read());  // Make sure reads at eof work\n+}\n+\n+TEST(LogTest, ManyBlocks) {\n+  for (int i = 0; i < 100000; i++) {\n+    Write(NumberString(i));\n+  }\n+  for (int i = 0; i < 100000; i++) {\n+    ASSERT_EQ(NumberString(i), Read());\n+  }\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+TEST(LogTest, Fragmentation) {\n+  Write(\"small\");\n+  Write(BigString(\"medium\", 50000));\n+  Write(BigString(\"large\", 100000));\n+  ASSERT_EQ(\"small\", Read());\n+  ASSERT_EQ(BigString(\"medium\", 50000), Read());\n+  ASSERT_EQ(BigString(\"large\", 100000), Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+TEST(LogTest, MarginalTrailer) {\n+  // Make a trailer that is exactly the same length as an empty record.\n+  const int n = kBlockSize - 2*kHeaderSize;\n+  Write(BigString(\"foo\", n));\n+  ASSERT_EQ(kBlockSize - kHeaderSize, WrittenBytes());\n+  Write(\"\");\n+  Write(\"bar\");\n+  ASSERT_EQ(BigString(\"foo\", n), Read());\n+  ASSERT_EQ(\"\", Read());\n+  ASSERT_EQ(\"bar\", Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+TEST(LogTest, MarginalTrailer2) {\n+  // Make a trailer that is exactly the same length as an empty record.\n+  const int n = kBlockSize - 2*kHeaderSize;\n+  Write(BigString(\"foo\", n));\n+  ASSERT_EQ(kBlockSize - kHeaderSize, WrittenBytes());\n+  Write(\"bar\");\n+  ASSERT_EQ(BigString(\"foo\", n), Read());\n+  ASSERT_EQ(\"bar\", Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(0, DroppedBytes());\n+  ASSERT_EQ(\"\", ReportMessage());\n+}\n+\n+TEST(LogTest, ShortTrailer) {\n+  const int n = kBlockSize - 2*kHeaderSize + 4;\n+  Write(BigString(\"foo\", n));\n+  ASSERT_EQ(kBlockSize - kHeaderSize + 4, WrittenBytes());\n+  Write(\"\");\n+  Write(\"bar\");\n+  ASSERT_EQ(BigString(\"foo\", n), Read());\n+  ASSERT_EQ(\"\", Read());\n+  ASSERT_EQ(\"bar\", Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+TEST(LogTest, AlignedEof) {\n+  const int n = kBlockSize - 2*kHeaderSize + 4;\n+  Write(BigString(\"foo\", n));\n+  ASSERT_EQ(kBlockSize - kHeaderSize + 4, WrittenBytes());\n+  ASSERT_EQ(BigString(\"foo\", n), Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+TEST(LogTest, RandomRead) {\n+  const int N = 500;\n+  Random write_rnd(301);\n+  for (int i = 0; i < N; i++) {\n+    Write(RandomSkewedString(i, &write_rnd));\n+  }\n+  Random read_rnd(301);\n+  for (int i = 0; i < N; i++) {\n+    ASSERT_EQ(RandomSkewedString(i, &read_rnd), Read());\n+  }\n+  ASSERT_EQ(\"EOF\", Read());\n+}\n+\n+// Tests of all the error paths in log_reader.cc follow:\n+\n+TEST(LogTest, ReadError) {\n+  Write(\"foo\");\n+  ForceError();\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(kBlockSize, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"read error\"));\n+}\n+\n+TEST(LogTest, BadRecordType) {\n+  Write(\"foo\");\n+  // Type is stored in header[6]\n+  IncrementByte(6, 100);\n+  FixChecksum(0, 3);\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(3, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"unknown record type\"));\n+}\n+\n+TEST(LogTest, TruncatedTrailingRecord) {\n+  Write(\"foo\");\n+  ShrinkSize(4);   // Drop all payload as well as a header byte\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(kHeaderSize - 1, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"truncated record at end of file\"));\n+}\n+\n+TEST(LogTest, BadLength) {\n+  Write(\"foo\");\n+  ShrinkSize(1);\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(kHeaderSize + 2, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"bad record length\"));\n+}\n+\n+TEST(LogTest, ChecksumMismatch) {\n+  Write(\"foo\");\n+  IncrementByte(0, 10);\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(10, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"checksum mismatch\"));\n+}\n+\n+TEST(LogTest, UnexpectedMiddleType) {\n+  Write(\"foo\");\n+  SetByte(6, kMiddleType);\n+  FixChecksum(0, 3);\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(3, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"missing start\"));\n+}\n+\n+TEST(LogTest, UnexpectedLastType) {\n+  Write(\"foo\");\n+  SetByte(6, kLastType);\n+  FixChecksum(0, 3);\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(3, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"missing start\"));\n+}\n+\n+TEST(LogTest, UnexpectedFullType) {\n+  Write(\"foo\");\n+  Write(\"bar\");\n+  SetByte(6, kFirstType);\n+  FixChecksum(0, 3);\n+  ASSERT_EQ(\"bar\", Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(3, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"partial record without end\"));\n+}\n+\n+TEST(LogTest, UnexpectedFirstType) {\n+  Write(\"foo\");\n+  Write(BigString(\"bar\", 100000));\n+  SetByte(6, kFirstType);\n+  FixChecksum(0, 3);\n+  ASSERT_EQ(BigString(\"bar\", 100000), Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+  ASSERT_EQ(3, DroppedBytes());\n+  ASSERT_EQ(\"OK\", MatchError(\"partial record without end\"));\n+}\n+\n+TEST(LogTest, ErrorJoinsRecords) {\n+  // Consider two fragmented records:\n+  //    first(R1) last(R1) first(R2) last(R2)\n+  // where the middle two fragments disappear.  We do not want\n+  // first(R1),last(R2) to get joined and returned as a valid record.\n+\n+  // Write records that span two blocks\n+  Write(BigString(\"foo\", kBlockSize));\n+  Write(BigString(\"bar\", kBlockSize));\n+  Write(\"correct\");\n+\n+  // Wipe the middle block\n+  for (int offset = kBlockSize; offset < 2*kBlockSize; offset++) {\n+    SetByte(offset, 'x');\n+  }\n+\n+  ASSERT_EQ(\"correct\", Read());\n+  ASSERT_EQ(\"EOF\", Read());\n+  const int dropped = DroppedBytes();\n+  ASSERT_LE(dropped, 2*kBlockSize + 100);\n+  ASSERT_GE(dropped, 2*kBlockSize);\n+}\n+\n+TEST(LogTest, ReadStart) {\n+  CheckInitialOffsetRecord(0, 0);\n+}\n+\n+TEST(LogTest, ReadSecondOneOff) {\n+  CheckInitialOffsetRecord(1, 1);\n+}\n+\n+TEST(LogTest, ReadSecondTenThousand) {\n+  CheckInitialOffsetRecord(10000, 1);\n+}\n+\n+TEST(LogTest, ReadSecondStart) {\n+  CheckInitialOffsetRecord(10007, 1);\n+}\n+\n+TEST(LogTest, ReadThirdOneOff) {\n+  CheckInitialOffsetRecord(10008, 2);\n+}\n+\n+TEST(LogTest, ReadThirdStart) {\n+  CheckInitialOffsetRecord(20014, 2);\n+}\n+\n+TEST(LogTest, ReadFourthOneOff) {\n+  CheckInitialOffsetRecord(20015, 3);\n+}\n+\n+TEST(LogTest, ReadFourthFirstBlockTrailer) {\n+  CheckInitialOffsetRecord(log::kBlockSize - 4, 3);\n+}\n+\n+TEST(LogTest, ReadFourthMiddleBlock) {\n+  CheckInitialOffsetRecord(log::kBlockSize + 1, 3);\n+}\n+\n+TEST(LogTest, ReadFourthLastBlock) {\n+  CheckInitialOffsetRecord(2 * log::kBlockSize + 1, 3);\n+}\n+\n+TEST(LogTest, ReadFourthStart) {\n+  CheckInitialOffsetRecord(\n+      2 * (kHeaderSize + 1000) + (2 * log::kBlockSize - 1000) + 3 * kHeaderSize,\n+      3);\n+}\n+\n+TEST(LogTest, ReadEnd) {\n+  CheckOffsetPastEndReturnsNoRecords(0);\n+}\n+\n+TEST(LogTest, ReadPastEnd) {\n+  CheckOffsetPastEndReturnsNoRecords(5);\n+}\n+\n+}  // namespace log\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "2da99ac08866397270663a1203075944162ba290",
        "filename": "src/leveldb/db/log_writer.cc",
        "status": "added",
        "additions": 103,
        "deletions": 0,
        "changes": 103,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/log_writer.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/log_writer.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/log_writer.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,103 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/log_writer.h\"\n+\n+#include <stdint.h>\n+#include \"leveldb/env.h\"\n+#include \"util/coding.h\"\n+#include \"util/crc32c.h\"\n+\n+namespace leveldb {\n+namespace log {\n+\n+Writer::Writer(WritableFile* dest)\n+    : dest_(dest),\n+      block_offset_(0) {\n+  for (int i = 0; i <= kMaxRecordType; i++) {\n+    char t = static_cast<char>(i);\n+    type_crc_[i] = crc32c::Value(&t, 1);\n+  }\n+}\n+\n+Writer::~Writer() {\n+}\n+\n+Status Writer::AddRecord(const Slice& slice) {\n+  const char* ptr = slice.data();\n+  size_t left = slice.size();\n+\n+  // Fragment the record if necessary and emit it.  Note that if slice\n+  // is empty, we still want to iterate once to emit a single\n+  // zero-length record\n+  Status s;\n+  bool begin = true;\n+  do {\n+    const int leftover = kBlockSize - block_offset_;\n+    assert(leftover >= 0);\n+    if (leftover < kHeaderSize) {\n+      // Switch to a new block\n+      if (leftover > 0) {\n+        // Fill the trailer (literal below relies on kHeaderSize being 7)\n+        assert(kHeaderSize == 7);\n+        dest_->Append(Slice(\"\\x00\\x00\\x00\\x00\\x00\\x00\", leftover));\n+      }\n+      block_offset_ = 0;\n+    }\n+\n+    // Invariant: we never leave < kHeaderSize bytes in a block.\n+    assert(kBlockSize - block_offset_ - kHeaderSize >= 0);\n+\n+    const size_t avail = kBlockSize - block_offset_ - kHeaderSize;\n+    const size_t fragment_length = (left < avail) ? left : avail;\n+\n+    RecordType type;\n+    const bool end = (left == fragment_length);\n+    if (begin && end) {\n+      type = kFullType;\n+    } else if (begin) {\n+      type = kFirstType;\n+    } else if (end) {\n+      type = kLastType;\n+    } else {\n+      type = kMiddleType;\n+    }\n+\n+    s = EmitPhysicalRecord(type, ptr, fragment_length);\n+    ptr += fragment_length;\n+    left -= fragment_length;\n+    begin = false;\n+  } while (s.ok() && left > 0);\n+  return s;\n+}\n+\n+Status Writer::EmitPhysicalRecord(RecordType t, const char* ptr, size_t n) {\n+  assert(n <= 0xffff);  // Must fit in two bytes\n+  assert(block_offset_ + kHeaderSize + n <= kBlockSize);\n+\n+  // Format the header\n+  char buf[kHeaderSize];\n+  buf[4] = static_cast<char>(n & 0xff);\n+  buf[5] = static_cast<char>(n >> 8);\n+  buf[6] = static_cast<char>(t);\n+\n+  // Compute the crc of the record type and the payload.\n+  uint32_t crc = crc32c::Extend(type_crc_[t], ptr, n);\n+  crc = crc32c::Mask(crc);                 // Adjust for storage\n+  EncodeFixed32(buf, crc);\n+\n+  // Write the header and the payload\n+  Status s = dest_->Append(Slice(buf, kHeaderSize));\n+  if (s.ok()) {\n+    s = dest_->Append(Slice(ptr, n));\n+    if (s.ok()) {\n+      s = dest_->Flush();\n+    }\n+  }\n+  block_offset_ += kHeaderSize + n;\n+  return s;\n+}\n+\n+}  // namespace log\n+}  // namespace leveldb"
      },
      {
        "sha": "a3a954d96732542fac9aef1345ebd952075f737c",
        "filename": "src/leveldb/db/log_writer.h",
        "status": "added",
        "additions": 48,
        "deletions": 0,
        "changes": 48,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/log_writer.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/log_writer.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/log_writer.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,48 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_LOG_WRITER_H_\n+#define STORAGE_LEVELDB_DB_LOG_WRITER_H_\n+\n+#include <stdint.h>\n+#include \"db/log_format.h\"\n+#include \"leveldb/slice.h\"\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+class WritableFile;\n+\n+namespace log {\n+\n+class Writer {\n+ public:\n+  // Create a writer that will append data to \"*dest\".\n+  // \"*dest\" must be initially empty.\n+  // \"*dest\" must remain live while this Writer is in use.\n+  explicit Writer(WritableFile* dest);\n+  ~Writer();\n+\n+  Status AddRecord(const Slice& slice);\n+\n+ private:\n+  WritableFile* dest_;\n+  int block_offset_;       // Current offset in block\n+\n+  // crc32c values for all supported record types.  These are\n+  // pre-computed to reduce the overhead of computing the crc of the\n+  // record type stored in the header.\n+  uint32_t type_crc_[kMaxRecordType + 1];\n+\n+  Status EmitPhysicalRecord(RecordType type, const char* ptr, size_t length);\n+\n+  // No copying allowed\n+  Writer(const Writer&);\n+  void operator=(const Writer&);\n+};\n+\n+}  // namespace log\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_LOG_WRITER_H_"
      },
      {
        "sha": "bfec0a7e7a1dc210b44dd527b9547e33e829d9bb",
        "filename": "src/leveldb/db/memtable.cc",
        "status": "added",
        "additions": 145,
        "deletions": 0,
        "changes": 145,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/memtable.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/memtable.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/memtable.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,145 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/memtable.h\"\n+#include \"db/dbformat.h\"\n+#include \"leveldb/comparator.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/iterator.h\"\n+#include \"util/coding.h\"\n+\n+namespace leveldb {\n+\n+static Slice GetLengthPrefixedSlice(const char* data) {\n+  uint32_t len;\n+  const char* p = data;\n+  p = GetVarint32Ptr(p, p + 5, &len);  // +5: we assume \"p\" is not corrupted\n+  return Slice(p, len);\n+}\n+\n+MemTable::MemTable(const InternalKeyComparator& cmp)\n+    : comparator_(cmp),\n+      refs_(0),\n+      table_(comparator_, &arena_) {\n+}\n+\n+MemTable::~MemTable() {\n+  assert(refs_ == 0);\n+}\n+\n+size_t MemTable::ApproximateMemoryUsage() { return arena_.MemoryUsage(); }\n+\n+int MemTable::KeyComparator::operator()(const char* aptr, const char* bptr)\n+    const {\n+  // Internal keys are encoded as length-prefixed strings.\n+  Slice a = GetLengthPrefixedSlice(aptr);\n+  Slice b = GetLengthPrefixedSlice(bptr);\n+  return comparator.Compare(a, b);\n+}\n+\n+// Encode a suitable internal key target for \"target\" and return it.\n+// Uses *scratch as scratch space, and the returned pointer will point\n+// into this scratch space.\n+static const char* EncodeKey(std::string* scratch, const Slice& target) {\n+  scratch->clear();\n+  PutVarint32(scratch, target.size());\n+  scratch->append(target.data(), target.size());\n+  return scratch->data();\n+}\n+\n+class MemTableIterator: public Iterator {\n+ public:\n+  explicit MemTableIterator(MemTable::Table* table) : iter_(table) { }\n+\n+  virtual bool Valid() const { return iter_.Valid(); }\n+  virtual void Seek(const Slice& k) { iter_.Seek(EncodeKey(&tmp_, k)); }\n+  virtual void SeekToFirst() { iter_.SeekToFirst(); }\n+  virtual void SeekToLast() { iter_.SeekToLast(); }\n+  virtual void Next() { iter_.Next(); }\n+  virtual void Prev() { iter_.Prev(); }\n+  virtual Slice key() const { return GetLengthPrefixedSlice(iter_.key()); }\n+  virtual Slice value() const {\n+    Slice key_slice = GetLengthPrefixedSlice(iter_.key());\n+    return GetLengthPrefixedSlice(key_slice.data() + key_slice.size());\n+  }\n+\n+  virtual Status status() const { return Status::OK(); }\n+\n+ private:\n+  MemTable::Table::Iterator iter_;\n+  std::string tmp_;       // For passing to EncodeKey\n+\n+  // No copying allowed\n+  MemTableIterator(const MemTableIterator&);\n+  void operator=(const MemTableIterator&);\n+};\n+\n+Iterator* MemTable::NewIterator() {\n+  return new MemTableIterator(&table_);\n+}\n+\n+void MemTable::Add(SequenceNumber s, ValueType type,\n+                   const Slice& key,\n+                   const Slice& value) {\n+  // Format of an entry is concatenation of:\n+  //  key_size     : varint32 of internal_key.size()\n+  //  key bytes    : char[internal_key.size()]\n+  //  value_size   : varint32 of value.size()\n+  //  value bytes  : char[value.size()]\n+  size_t key_size = key.size();\n+  size_t val_size = value.size();\n+  size_t internal_key_size = key_size + 8;\n+  const size_t encoded_len =\n+      VarintLength(internal_key_size) + internal_key_size +\n+      VarintLength(val_size) + val_size;\n+  char* buf = arena_.Allocate(encoded_len);\n+  char* p = EncodeVarint32(buf, internal_key_size);\n+  memcpy(p, key.data(), key_size);\n+  p += key_size;\n+  EncodeFixed64(p, (s << 8) | type);\n+  p += 8;\n+  p = EncodeVarint32(p, val_size);\n+  memcpy(p, value.data(), val_size);\n+  assert((p + val_size) - buf == encoded_len);\n+  table_.Insert(buf);\n+}\n+\n+bool MemTable::Get(const LookupKey& key, std::string* value, Status* s) {\n+  Slice memkey = key.memtable_key();\n+  Table::Iterator iter(&table_);\n+  iter.Seek(memkey.data());\n+  if (iter.Valid()) {\n+    // entry format is:\n+    //    klength  varint32\n+    //    userkey  char[klength]\n+    //    tag      uint64\n+    //    vlength  varint32\n+    //    value    char[vlength]\n+    // Check that it belongs to same user key.  We do not check the\n+    // sequence number since the Seek() call above should have skipped\n+    // all entries with overly large sequence numbers.\n+    const char* entry = iter.key();\n+    uint32_t key_length;\n+    const char* key_ptr = GetVarint32Ptr(entry, entry+5, &key_length);\n+    if (comparator_.comparator.user_comparator()->Compare(\n+            Slice(key_ptr, key_length - 8),\n+            key.user_key()) == 0) {\n+      // Correct user key\n+      const uint64_t tag = DecodeFixed64(key_ptr + key_length - 8);\n+      switch (static_cast<ValueType>(tag & 0xff)) {\n+        case kTypeValue: {\n+          Slice v = GetLengthPrefixedSlice(key_ptr + key_length);\n+          value->assign(v.data(), v.size());\n+          return true;\n+        }\n+        case kTypeDeletion:\n+          *s = Status::NotFound(Slice());\n+          return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "92e90bb099f3356520c42d6bb887b2a35c6bbe39",
        "filename": "src/leveldb/db/memtable.h",
        "status": "added",
        "additions": 91,
        "deletions": 0,
        "changes": 91,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/memtable.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/memtable.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/memtable.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,91 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_MEMTABLE_H_\n+#define STORAGE_LEVELDB_DB_MEMTABLE_H_\n+\n+#include <string>\n+#include \"leveldb/db.h\"\n+#include \"db/dbformat.h\"\n+#include \"db/skiplist.h\"\n+#include \"util/arena.h\"\n+\n+namespace leveldb {\n+\n+class InternalKeyComparator;\n+class Mutex;\n+class MemTableIterator;\n+\n+class MemTable {\n+ public:\n+  // MemTables are reference counted.  The initial reference count\n+  // is zero and the caller must call Ref() at least once.\n+  explicit MemTable(const InternalKeyComparator& comparator);\n+\n+  // Increase reference count.\n+  void Ref() { ++refs_; }\n+\n+  // Drop reference count.  Delete if no more references exist.\n+  void Unref() {\n+    --refs_;\n+    assert(refs_ >= 0);\n+    if (refs_ <= 0) {\n+      delete this;\n+    }\n+  }\n+\n+  // Returns an estimate of the number of bytes of data in use by this\n+  // data structure.\n+  //\n+  // REQUIRES: external synchronization to prevent simultaneous\n+  // operations on the same MemTable.\n+  size_t ApproximateMemoryUsage();\n+\n+  // Return an iterator that yields the contents of the memtable.\n+  //\n+  // The caller must ensure that the underlying MemTable remains live\n+  // while the returned iterator is live.  The keys returned by this\n+  // iterator are internal keys encoded by AppendInternalKey in the\n+  // db/format.{h,cc} module.\n+  Iterator* NewIterator();\n+\n+  // Add an entry into memtable that maps key to value at the\n+  // specified sequence number and with the specified type.\n+  // Typically value will be empty if type==kTypeDeletion.\n+  void Add(SequenceNumber seq, ValueType type,\n+           const Slice& key,\n+           const Slice& value);\n+\n+  // If memtable contains a value for key, store it in *value and return true.\n+  // If memtable contains a deletion for key, store a NotFound() error\n+  // in *status and return true.\n+  // Else, return false.\n+  bool Get(const LookupKey& key, std::string* value, Status* s);\n+\n+ private:\n+  ~MemTable();  // Private since only Unref() should be used to delete it\n+\n+  struct KeyComparator {\n+    const InternalKeyComparator comparator;\n+    explicit KeyComparator(const InternalKeyComparator& c) : comparator(c) { }\n+    int operator()(const char* a, const char* b) const;\n+  };\n+  friend class MemTableIterator;\n+  friend class MemTableBackwardIterator;\n+\n+  typedef SkipList<const char*, KeyComparator> Table;\n+\n+  KeyComparator comparator_;\n+  int refs_;\n+  Arena arena_;\n+  Table table_;\n+\n+  // No copying allowed\n+  MemTable(const MemTable&);\n+  void operator=(const MemTable&);\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_MEMTABLE_H_"
      },
      {
        "sha": "022d52f3debe0c0e89a6825ca3778c5894c3783c",
        "filename": "src/leveldb/db/repair.cc",
        "status": "added",
        "additions": 389,
        "deletions": 0,
        "changes": 389,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/repair.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/repair.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/repair.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,389 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// We recover the contents of the descriptor from the other files we find.\n+// (1) Any log files are first converted to tables\n+// (2) We scan every table to compute\n+//     (a) smallest/largest for the table\n+//     (b) largest sequence number in the table\n+// (3) We generate descriptor contents:\n+//      - log number is set to zero\n+//      - next-file-number is set to 1 + largest file number we found\n+//      - last-sequence-number is set to largest sequence# found across\n+//        all tables (see 2c)\n+//      - compaction pointers are cleared\n+//      - every table file is added at level 0\n+//\n+// Possible optimization 1:\n+//   (a) Compute total size and use to pick appropriate max-level M\n+//   (b) Sort tables by largest sequence# in the table\n+//   (c) For each table: if it overlaps earlier table, place in level-0,\n+//       else place in level-M.\n+// Possible optimization 2:\n+//   Store per-table metadata (smallest, largest, largest-seq#, ...)\n+//   in the table's meta section to speed up ScanTable.\n+\n+#include \"db/builder.h\"\n+#include \"db/db_impl.h\"\n+#include \"db/dbformat.h\"\n+#include \"db/filename.h\"\n+#include \"db/log_reader.h\"\n+#include \"db/log_writer.h\"\n+#include \"db/memtable.h\"\n+#include \"db/table_cache.h\"\n+#include \"db/version_edit.h\"\n+#include \"db/write_batch_internal.h\"\n+#include \"leveldb/comparator.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+\n+namespace leveldb {\n+\n+namespace {\n+\n+class Repairer {\n+ public:\n+  Repairer(const std::string& dbname, const Options& options)\n+      : dbname_(dbname),\n+        env_(options.env),\n+        icmp_(options.comparator),\n+        ipolicy_(options.filter_policy),\n+        options_(SanitizeOptions(dbname, &icmp_, &ipolicy_, options)),\n+        owns_info_log_(options_.info_log != options.info_log),\n+        owns_cache_(options_.block_cache != options.block_cache),\n+        next_file_number_(1) {\n+    // TableCache can be small since we expect each table to be opened once.\n+    table_cache_ = new TableCache(dbname_, &options_, 10);\n+  }\n+\n+  ~Repairer() {\n+    delete table_cache_;\n+    if (owns_info_log_) {\n+      delete options_.info_log;\n+    }\n+    if (owns_cache_) {\n+      delete options_.block_cache;\n+    }\n+  }\n+\n+  Status Run() {\n+    Status status = FindFiles();\n+    if (status.ok()) {\n+      ConvertLogFilesToTables();\n+      ExtractMetaData();\n+      status = WriteDescriptor();\n+    }\n+    if (status.ok()) {\n+      unsigned long long bytes = 0;\n+      for (size_t i = 0; i < tables_.size(); i++) {\n+        bytes += tables_[i].meta.file_size;\n+      }\n+      Log(options_.info_log,\n+          \"**** Repaired leveldb %s; \"\n+          \"recovered %d files; %llu bytes. \"\n+          \"Some data may have been lost. \"\n+          \"****\",\n+          dbname_.c_str(),\n+          static_cast<int>(tables_.size()),\n+          bytes);\n+    }\n+    return status;\n+  }\n+\n+ private:\n+  struct TableInfo {\n+    FileMetaData meta;\n+    SequenceNumber max_sequence;\n+  };\n+\n+  std::string const dbname_;\n+  Env* const env_;\n+  InternalKeyComparator const icmp_;\n+  InternalFilterPolicy const ipolicy_;\n+  Options const options_;\n+  bool owns_info_log_;\n+  bool owns_cache_;\n+  TableCache* table_cache_;\n+  VersionEdit edit_;\n+\n+  std::vector<std::string> manifests_;\n+  std::vector<uint64_t> table_numbers_;\n+  std::vector<uint64_t> logs_;\n+  std::vector<TableInfo> tables_;\n+  uint64_t next_file_number_;\n+\n+  Status FindFiles() {\n+    std::vector<std::string> filenames;\n+    Status status = env_->GetChildren(dbname_, &filenames);\n+    if (!status.ok()) {\n+      return status;\n+    }\n+    if (filenames.empty()) {\n+      return Status::IOError(dbname_, \"repair found no files\");\n+    }\n+\n+    uint64_t number;\n+    FileType type;\n+    for (size_t i = 0; i < filenames.size(); i++) {\n+      if (ParseFileName(filenames[i], &number, &type)) {\n+        if (type == kDescriptorFile) {\n+          manifests_.push_back(filenames[i]);\n+        } else {\n+          if (number + 1 > next_file_number_) {\n+            next_file_number_ = number + 1;\n+          }\n+          if (type == kLogFile) {\n+            logs_.push_back(number);\n+          } else if (type == kTableFile) {\n+            table_numbers_.push_back(number);\n+          } else {\n+            // Ignore other files\n+          }\n+        }\n+      }\n+    }\n+    return status;\n+  }\n+\n+  void ConvertLogFilesToTables() {\n+    for (size_t i = 0; i < logs_.size(); i++) {\n+      std::string logname = LogFileName(dbname_, logs_[i]);\n+      Status status = ConvertLogToTable(logs_[i]);\n+      if (!status.ok()) {\n+        Log(options_.info_log, \"Log #%llu: ignoring conversion error: %s\",\n+            (unsigned long long) logs_[i],\n+            status.ToString().c_str());\n+      }\n+      ArchiveFile(logname);\n+    }\n+  }\n+\n+  Status ConvertLogToTable(uint64_t log) {\n+    struct LogReporter : public log::Reader::Reporter {\n+      Env* env;\n+      Logger* info_log;\n+      uint64_t lognum;\n+      virtual void Corruption(size_t bytes, const Status& s) {\n+        // We print error messages for corruption, but continue repairing.\n+        Log(info_log, \"Log #%llu: dropping %d bytes; %s\",\n+            (unsigned long long) lognum,\n+            static_cast<int>(bytes),\n+            s.ToString().c_str());\n+      }\n+    };\n+\n+    // Open the log file\n+    std::string logname = LogFileName(dbname_, log);\n+    SequentialFile* lfile;\n+    Status status = env_->NewSequentialFile(logname, &lfile);\n+    if (!status.ok()) {\n+      return status;\n+    }\n+\n+    // Create the log reader.\n+    LogReporter reporter;\n+    reporter.env = env_;\n+    reporter.info_log = options_.info_log;\n+    reporter.lognum = log;\n+    // We intentially make log::Reader do checksumming so that\n+    // corruptions cause entire commits to be skipped instead of\n+    // propagating bad information (like overly large sequence\n+    // numbers).\n+    log::Reader reader(lfile, &reporter, false/*do not checksum*/,\n+                       0/*initial_offset*/);\n+\n+    // Read all the records and add to a memtable\n+    std::string scratch;\n+    Slice record;\n+    WriteBatch batch;\n+    MemTable* mem = new MemTable(icmp_);\n+    mem->Ref();\n+    int counter = 0;\n+    while (reader.ReadRecord(&record, &scratch)) {\n+      if (record.size() < 12) {\n+        reporter.Corruption(\n+            record.size(), Status::Corruption(\"log record too small\"));\n+        continue;\n+      }\n+      WriteBatchInternal::SetContents(&batch, record);\n+      status = WriteBatchInternal::InsertInto(&batch, mem);\n+      if (status.ok()) {\n+        counter += WriteBatchInternal::Count(&batch);\n+      } else {\n+        Log(options_.info_log, \"Log #%llu: ignoring %s\",\n+            (unsigned long long) log,\n+            status.ToString().c_str());\n+        status = Status::OK();  // Keep going with rest of file\n+      }\n+    }\n+    delete lfile;\n+\n+    // Do not record a version edit for this conversion to a Table\n+    // since ExtractMetaData() will also generate edits.\n+    FileMetaData meta;\n+    meta.number = next_file_number_++;\n+    Iterator* iter = mem->NewIterator();\n+    status = BuildTable(dbname_, env_, options_, table_cache_, iter, &meta);\n+    delete iter;\n+    mem->Unref();\n+    mem = NULL;\n+    if (status.ok()) {\n+      if (meta.file_size > 0) {\n+        table_numbers_.push_back(meta.number);\n+      }\n+    }\n+    Log(options_.info_log, \"Log #%llu: %d ops saved to Table #%llu %s\",\n+        (unsigned long long) log,\n+        counter,\n+        (unsigned long long) meta.number,\n+        status.ToString().c_str());\n+    return status;\n+  }\n+\n+  void ExtractMetaData() {\n+    std::vector<TableInfo> kept;\n+    for (size_t i = 0; i < table_numbers_.size(); i++) {\n+      TableInfo t;\n+      t.meta.number = table_numbers_[i];\n+      Status status = ScanTable(&t);\n+      if (!status.ok()) {\n+        std::string fname = TableFileName(dbname_, table_numbers_[i]);\n+        Log(options_.info_log, \"Table #%llu: ignoring %s\",\n+            (unsigned long long) table_numbers_[i],\n+            status.ToString().c_str());\n+        ArchiveFile(fname);\n+      } else {\n+        tables_.push_back(t);\n+      }\n+    }\n+  }\n+\n+  Status ScanTable(TableInfo* t) {\n+    std::string fname = TableFileName(dbname_, t->meta.number);\n+    int counter = 0;\n+    Status status = env_->GetFileSize(fname, &t->meta.file_size);\n+    if (status.ok()) {\n+      Iterator* iter = table_cache_->NewIterator(\n+          ReadOptions(), t->meta.number, t->meta.file_size);\n+      bool empty = true;\n+      ParsedInternalKey parsed;\n+      t->max_sequence = 0;\n+      for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+        Slice key = iter->key();\n+        if (!ParseInternalKey(key, &parsed)) {\n+          Log(options_.info_log, \"Table #%llu: unparsable key %s\",\n+              (unsigned long long) t->meta.number,\n+              EscapeString(key).c_str());\n+          continue;\n+        }\n+\n+        counter++;\n+        if (empty) {\n+          empty = false;\n+          t->meta.smallest.DecodeFrom(key);\n+        }\n+        t->meta.largest.DecodeFrom(key);\n+        if (parsed.sequence > t->max_sequence) {\n+          t->max_sequence = parsed.sequence;\n+        }\n+      }\n+      if (!iter->status().ok()) {\n+        status = iter->status();\n+      }\n+      delete iter;\n+    }\n+    Log(options_.info_log, \"Table #%llu: %d entries %s\",\n+        (unsigned long long) t->meta.number,\n+        counter,\n+        status.ToString().c_str());\n+    return status;\n+  }\n+\n+  Status WriteDescriptor() {\n+    std::string tmp = TempFileName(dbname_, 1);\n+    WritableFile* file;\n+    Status status = env_->NewWritableFile(tmp, &file);\n+    if (!status.ok()) {\n+      return status;\n+    }\n+\n+    SequenceNumber max_sequence = 0;\n+    for (size_t i = 0; i < tables_.size(); i++) {\n+      if (max_sequence < tables_[i].max_sequence) {\n+        max_sequence = tables_[i].max_sequence;\n+      }\n+    }\n+\n+    edit_.SetComparatorName(icmp_.user_comparator()->Name());\n+    edit_.SetLogNumber(0);\n+    edit_.SetNextFile(next_file_number_);\n+    edit_.SetLastSequence(max_sequence);\n+\n+    for (size_t i = 0; i < tables_.size(); i++) {\n+      // TODO(opt): separate out into multiple levels\n+      const TableInfo& t = tables_[i];\n+      edit_.AddFile(0, t.meta.number, t.meta.file_size,\n+                    t.meta.smallest, t.meta.largest);\n+    }\n+\n+    //fprintf(stderr, \"NewDescriptor:\\n%s\\n\", edit_.DebugString().c_str());\n+    {\n+      log::Writer log(file);\n+      std::string record;\n+      edit_.EncodeTo(&record);\n+      status = log.AddRecord(record);\n+    }\n+    if (status.ok()) {\n+      status = file->Close();\n+    }\n+    delete file;\n+    file = NULL;\n+\n+    if (!status.ok()) {\n+      env_->DeleteFile(tmp);\n+    } else {\n+      // Discard older manifests\n+      for (size_t i = 0; i < manifests_.size(); i++) {\n+        ArchiveFile(dbname_ + \"/\" + manifests_[i]);\n+      }\n+\n+      // Install new manifest\n+      status = env_->RenameFile(tmp, DescriptorFileName(dbname_, 1));\n+      if (status.ok()) {\n+        status = SetCurrentFile(env_, dbname_, 1);\n+      } else {\n+        env_->DeleteFile(tmp);\n+      }\n+    }\n+    return status;\n+  }\n+\n+  void ArchiveFile(const std::string& fname) {\n+    // Move into another directory.  E.g., for\n+    //    dir/foo\n+    // rename to\n+    //    dir/lost/foo\n+    const char* slash = strrchr(fname.c_str(), '/');\n+    std::string new_dir;\n+    if (slash != NULL) {\n+      new_dir.assign(fname.data(), slash - fname.data());\n+    }\n+    new_dir.append(\"/lost\");\n+    env_->CreateDir(new_dir);  // Ignore error\n+    std::string new_file = new_dir;\n+    new_file.append(\"/\");\n+    new_file.append((slash == NULL) ? fname.c_str() : slash + 1);\n+    Status s = env_->RenameFile(fname, new_file);\n+    Log(options_.info_log, \"Archiving %s: %s\\n\",\n+        fname.c_str(), s.ToString().c_str());\n+  }\n+};\n+}  // namespace\n+\n+Status RepairDB(const std::string& dbname, const Options& options) {\n+  Repairer repairer(dbname, options);\n+  return repairer.Run();\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "af85be6d01621b249f1756b2af5fd6078dd2ee25",
        "filename": "src/leveldb/db/skiplist.h",
        "status": "added",
        "additions": 379,
        "deletions": 0,
        "changes": 379,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/skiplist.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/skiplist.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/skiplist.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,379 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// Thread safety\n+// -------------\n+//\n+// Writes require external synchronization, most likely a mutex.\n+// Reads require a guarantee that the SkipList will not be destroyed\n+// while the read is in progress.  Apart from that, reads progress\n+// without any internal locking or synchronization.\n+//\n+// Invariants:\n+//\n+// (1) Allocated nodes are never deleted until the SkipList is\n+// destroyed.  This is trivially guaranteed by the code since we\n+// never delete any skip list nodes.\n+//\n+// (2) The contents of a Node except for the next/prev pointers are\n+// immutable after the Node has been linked into the SkipList.\n+// Only Insert() modifies the list, and it is careful to initialize\n+// a node and use release-stores to publish the nodes in one or\n+// more lists.\n+//\n+// ... prev vs. next pointer ordering ...\n+\n+#include <assert.h>\n+#include <stdlib.h>\n+#include \"port/port.h\"\n+#include \"util/arena.h\"\n+#include \"util/random.h\"\n+\n+namespace leveldb {\n+\n+class Arena;\n+\n+template<typename Key, class Comparator>\n+class SkipList {\n+ private:\n+  struct Node;\n+\n+ public:\n+  // Create a new SkipList object that will use \"cmp\" for comparing keys,\n+  // and will allocate memory using \"*arena\".  Objects allocated in the arena\n+  // must remain allocated for the lifetime of the skiplist object.\n+  explicit SkipList(Comparator cmp, Arena* arena);\n+\n+  // Insert key into the list.\n+  // REQUIRES: nothing that compares equal to key is currently in the list.\n+  void Insert(const Key& key);\n+\n+  // Returns true iff an entry that compares equal to key is in the list.\n+  bool Contains(const Key& key) const;\n+\n+  // Iteration over the contents of a skip list\n+  class Iterator {\n+   public:\n+    // Initialize an iterator over the specified list.\n+    // The returned iterator is not valid.\n+    explicit Iterator(const SkipList* list);\n+\n+    // Returns true iff the iterator is positioned at a valid node.\n+    bool Valid() const;\n+\n+    // Returns the key at the current position.\n+    // REQUIRES: Valid()\n+    const Key& key() const;\n+\n+    // Advances to the next position.\n+    // REQUIRES: Valid()\n+    void Next();\n+\n+    // Advances to the previous position.\n+    // REQUIRES: Valid()\n+    void Prev();\n+\n+    // Advance to the first entry with a key >= target\n+    void Seek(const Key& target);\n+\n+    // Position at the first entry in list.\n+    // Final state of iterator is Valid() iff list is not empty.\n+    void SeekToFirst();\n+\n+    // Position at the last entry in list.\n+    // Final state of iterator is Valid() iff list is not empty.\n+    void SeekToLast();\n+\n+   private:\n+    const SkipList* list_;\n+    Node* node_;\n+    // Intentionally copyable\n+  };\n+\n+ private:\n+  enum { kMaxHeight = 12 };\n+\n+  // Immutable after construction\n+  Comparator const compare_;\n+  Arena* const arena_;    // Arena used for allocations of nodes\n+\n+  Node* const head_;\n+\n+  // Modified only by Insert().  Read racily by readers, but stale\n+  // values are ok.\n+  port::AtomicPointer max_height_;   // Height of the entire list\n+\n+  inline int GetMaxHeight() const {\n+    return static_cast<int>(\n+        reinterpret_cast<intptr_t>(max_height_.NoBarrier_Load()));\n+  }\n+\n+  // Read/written only by Insert().\n+  Random rnd_;\n+\n+  Node* NewNode(const Key& key, int height);\n+  int RandomHeight();\n+  bool Equal(const Key& a, const Key& b) const { return (compare_(a, b) == 0); }\n+\n+  // Return true if key is greater than the data stored in \"n\"\n+  bool KeyIsAfterNode(const Key& key, Node* n) const;\n+\n+  // Return the earliest node that comes at or after key.\n+  // Return NULL if there is no such node.\n+  //\n+  // If prev is non-NULL, fills prev[level] with pointer to previous\n+  // node at \"level\" for every level in [0..max_height_-1].\n+  Node* FindGreaterOrEqual(const Key& key, Node** prev) const;\n+\n+  // Return the latest node with a key < key.\n+  // Return head_ if there is no such node.\n+  Node* FindLessThan(const Key& key) const;\n+\n+  // Return the last node in the list.\n+  // Return head_ if list is empty.\n+  Node* FindLast() const;\n+\n+  // No copying allowed\n+  SkipList(const SkipList&);\n+  void operator=(const SkipList&);\n+};\n+\n+// Implementation details follow\n+template<typename Key, class Comparator>\n+struct SkipList<Key,Comparator>::Node {\n+  explicit Node(const Key& k) : key(k) { }\n+\n+  Key const key;\n+\n+  // Accessors/mutators for links.  Wrapped in methods so we can\n+  // add the appropriate barriers as necessary.\n+  Node* Next(int n) {\n+    assert(n >= 0);\n+    // Use an 'acquire load' so that we observe a fully initialized\n+    // version of the returned Node.\n+    return reinterpret_cast<Node*>(next_[n].Acquire_Load());\n+  }\n+  void SetNext(int n, Node* x) {\n+    assert(n >= 0);\n+    // Use a 'release store' so that anybody who reads through this\n+    // pointer observes a fully initialized version of the inserted node.\n+    next_[n].Release_Store(x);\n+  }\n+\n+  // No-barrier variants that can be safely used in a few locations.\n+  Node* NoBarrier_Next(int n) {\n+    assert(n >= 0);\n+    return reinterpret_cast<Node*>(next_[n].NoBarrier_Load());\n+  }\n+  void NoBarrier_SetNext(int n, Node* x) {\n+    assert(n >= 0);\n+    next_[n].NoBarrier_Store(x);\n+  }\n+\n+ private:\n+  // Array of length equal to the node height.  next_[0] is lowest level link.\n+  port::AtomicPointer next_[1];\n+};\n+\n+template<typename Key, class Comparator>\n+typename SkipList<Key,Comparator>::Node*\n+SkipList<Key,Comparator>::NewNode(const Key& key, int height) {\n+  char* mem = arena_->AllocateAligned(\n+      sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1));\n+  return new (mem) Node(key);\n+}\n+\n+template<typename Key, class Comparator>\n+inline SkipList<Key,Comparator>::Iterator::Iterator(const SkipList* list) {\n+  list_ = list;\n+  node_ = NULL;\n+}\n+\n+template<typename Key, class Comparator>\n+inline bool SkipList<Key,Comparator>::Iterator::Valid() const {\n+  return node_ != NULL;\n+}\n+\n+template<typename Key, class Comparator>\n+inline const Key& SkipList<Key,Comparator>::Iterator::key() const {\n+  assert(Valid());\n+  return node_->key;\n+}\n+\n+template<typename Key, class Comparator>\n+inline void SkipList<Key,Comparator>::Iterator::Next() {\n+  assert(Valid());\n+  node_ = node_->Next(0);\n+}\n+\n+template<typename Key, class Comparator>\n+inline void SkipList<Key,Comparator>::Iterator::Prev() {\n+  // Instead of using explicit \"prev\" links, we just search for the\n+  // last node that falls before key.\n+  assert(Valid());\n+  node_ = list_->FindLessThan(node_->key);\n+  if (node_ == list_->head_) {\n+    node_ = NULL;\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+inline void SkipList<Key,Comparator>::Iterator::Seek(const Key& target) {\n+  node_ = list_->FindGreaterOrEqual(target, NULL);\n+}\n+\n+template<typename Key, class Comparator>\n+inline void SkipList<Key,Comparator>::Iterator::SeekToFirst() {\n+  node_ = list_->head_->Next(0);\n+}\n+\n+template<typename Key, class Comparator>\n+inline void SkipList<Key,Comparator>::Iterator::SeekToLast() {\n+  node_ = list_->FindLast();\n+  if (node_ == list_->head_) {\n+    node_ = NULL;\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+int SkipList<Key,Comparator>::RandomHeight() {\n+  // Increase height with probability 1 in kBranching\n+  static const unsigned int kBranching = 4;\n+  int height = 1;\n+  while (height < kMaxHeight && ((rnd_.Next() % kBranching) == 0)) {\n+    height++;\n+  }\n+  assert(height > 0);\n+  assert(height <= kMaxHeight);\n+  return height;\n+}\n+\n+template<typename Key, class Comparator>\n+bool SkipList<Key,Comparator>::KeyIsAfterNode(const Key& key, Node* n) const {\n+  // NULL n is considered infinite\n+  return (n != NULL) && (compare_(n->key, key) < 0);\n+}\n+\n+template<typename Key, class Comparator>\n+typename SkipList<Key,Comparator>::Node* SkipList<Key,Comparator>::FindGreaterOrEqual(const Key& key, Node** prev)\n+    const {\n+  Node* x = head_;\n+  int level = GetMaxHeight() - 1;\n+  while (true) {\n+    Node* next = x->Next(level);\n+    if (KeyIsAfterNode(key, next)) {\n+      // Keep searching in this list\n+      x = next;\n+    } else {\n+      if (prev != NULL) prev[level] = x;\n+      if (level == 0) {\n+        return next;\n+      } else {\n+        // Switch to next list\n+        level--;\n+      }\n+    }\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+typename SkipList<Key,Comparator>::Node*\n+SkipList<Key,Comparator>::FindLessThan(const Key& key) const {\n+  Node* x = head_;\n+  int level = GetMaxHeight() - 1;\n+  while (true) {\n+    assert(x == head_ || compare_(x->key, key) < 0);\n+    Node* next = x->Next(level);\n+    if (next == NULL || compare_(next->key, key) >= 0) {\n+      if (level == 0) {\n+        return x;\n+      } else {\n+        // Switch to next list\n+        level--;\n+      }\n+    } else {\n+      x = next;\n+    }\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+typename SkipList<Key,Comparator>::Node* SkipList<Key,Comparator>::FindLast()\n+    const {\n+  Node* x = head_;\n+  int level = GetMaxHeight() - 1;\n+  while (true) {\n+    Node* next = x->Next(level);\n+    if (next == NULL) {\n+      if (level == 0) {\n+        return x;\n+      } else {\n+        // Switch to next list\n+        level--;\n+      }\n+    } else {\n+      x = next;\n+    }\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+SkipList<Key,Comparator>::SkipList(Comparator cmp, Arena* arena)\n+    : compare_(cmp),\n+      arena_(arena),\n+      head_(NewNode(0 /* any key will do */, kMaxHeight)),\n+      max_height_(reinterpret_cast<void*>(1)),\n+      rnd_(0xdeadbeef) {\n+  for (int i = 0; i < kMaxHeight; i++) {\n+    head_->SetNext(i, NULL);\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+void SkipList<Key,Comparator>::Insert(const Key& key) {\n+  // TODO(opt): We can use a barrier-free variant of FindGreaterOrEqual()\n+  // here since Insert() is externally synchronized.\n+  Node* prev[kMaxHeight];\n+  Node* x = FindGreaterOrEqual(key, prev);\n+\n+  // Our data structure does not allow duplicate insertion\n+  assert(x == NULL || !Equal(key, x->key));\n+\n+  int height = RandomHeight();\n+  if (height > GetMaxHeight()) {\n+    for (int i = GetMaxHeight(); i < height; i++) {\n+      prev[i] = head_;\n+    }\n+    //fprintf(stderr, \"Change height from %d to %d\\n\", max_height_, height);\n+\n+    // It is ok to mutate max_height_ without any synchronization\n+    // with concurrent readers.  A concurrent reader that observes\n+    // the new value of max_height_ will see either the old value of\n+    // new level pointers from head_ (NULL), or a new value set in\n+    // the loop below.  In the former case the reader will\n+    // immediately drop to the next level since NULL sorts after all\n+    // keys.  In the latter case the reader will use the new node.\n+    max_height_.NoBarrier_Store(reinterpret_cast<void*>(height));\n+  }\n+\n+  x = NewNode(key, height);\n+  for (int i = 0; i < height; i++) {\n+    // NoBarrier_SetNext() suffices since we will add a barrier when\n+    // we publish a pointer to \"x\" in prev[i].\n+    x->NoBarrier_SetNext(i, prev[i]->NoBarrier_Next(i));\n+    prev[i]->SetNext(i, x);\n+  }\n+}\n+\n+template<typename Key, class Comparator>\n+bool SkipList<Key,Comparator>::Contains(const Key& key) const {\n+  Node* x = FindGreaterOrEqual(key, NULL);\n+  if (x != NULL && Equal(key, x->key)) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "c78f4b4fb1a0fc7fb5aadbefa0f7d19eaf06ba1b",
        "filename": "src/leveldb/db/skiplist_test.cc",
        "status": "added",
        "additions": 378,
        "deletions": 0,
        "changes": 378,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/skiplist_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/skiplist_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/skiplist_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,378 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/skiplist.h\"\n+#include <set>\n+#include \"leveldb/env.h\"\n+#include \"util/arena.h\"\n+#include \"util/hash.h\"\n+#include \"util/random.h\"\n+#include \"util/testharness.h\"\n+\n+namespace leveldb {\n+\n+typedef uint64_t Key;\n+\n+struct Comparator {\n+  int operator()(const Key& a, const Key& b) const {\n+    if (a < b) {\n+      return -1;\n+    } else if (a > b) {\n+      return +1;\n+    } else {\n+      return 0;\n+    }\n+  }\n+};\n+\n+class SkipTest { };\n+\n+TEST(SkipTest, Empty) {\n+  Arena arena;\n+  Comparator cmp;\n+  SkipList<Key, Comparator> list(cmp, &arena);\n+  ASSERT_TRUE(!list.Contains(10));\n+\n+  SkipList<Key, Comparator>::Iterator iter(&list);\n+  ASSERT_TRUE(!iter.Valid());\n+  iter.SeekToFirst();\n+  ASSERT_TRUE(!iter.Valid());\n+  iter.Seek(100);\n+  ASSERT_TRUE(!iter.Valid());\n+  iter.SeekToLast();\n+  ASSERT_TRUE(!iter.Valid());\n+}\n+\n+TEST(SkipTest, InsertAndLookup) {\n+  const int N = 2000;\n+  const int R = 5000;\n+  Random rnd(1000);\n+  std::set<Key> keys;\n+  Arena arena;\n+  Comparator cmp;\n+  SkipList<Key, Comparator> list(cmp, &arena);\n+  for (int i = 0; i < N; i++) {\n+    Key key = rnd.Next() % R;\n+    if (keys.insert(key).second) {\n+      list.Insert(key);\n+    }\n+  }\n+\n+  for (int i = 0; i < R; i++) {\n+    if (list.Contains(i)) {\n+      ASSERT_EQ(keys.count(i), 1);\n+    } else {\n+      ASSERT_EQ(keys.count(i), 0);\n+    }\n+  }\n+\n+  // Simple iterator tests\n+  {\n+    SkipList<Key, Comparator>::Iterator iter(&list);\n+    ASSERT_TRUE(!iter.Valid());\n+\n+    iter.Seek(0);\n+    ASSERT_TRUE(iter.Valid());\n+    ASSERT_EQ(*(keys.begin()), iter.key());\n+\n+    iter.SeekToFirst();\n+    ASSERT_TRUE(iter.Valid());\n+    ASSERT_EQ(*(keys.begin()), iter.key());\n+\n+    iter.SeekToLast();\n+    ASSERT_TRUE(iter.Valid());\n+    ASSERT_EQ(*(keys.rbegin()), iter.key());\n+  }\n+\n+  // Forward iteration test\n+  for (int i = 0; i < R; i++) {\n+    SkipList<Key, Comparator>::Iterator iter(&list);\n+    iter.Seek(i);\n+\n+    // Compare against model iterator\n+    std::set<Key>::iterator model_iter = keys.lower_bound(i);\n+    for (int j = 0; j < 3; j++) {\n+      if (model_iter == keys.end()) {\n+        ASSERT_TRUE(!iter.Valid());\n+        break;\n+      } else {\n+        ASSERT_TRUE(iter.Valid());\n+        ASSERT_EQ(*model_iter, iter.key());\n+        ++model_iter;\n+        iter.Next();\n+      }\n+    }\n+  }\n+\n+  // Backward iteration test\n+  {\n+    SkipList<Key, Comparator>::Iterator iter(&list);\n+    iter.SeekToLast();\n+\n+    // Compare against model iterator\n+    for (std::set<Key>::reverse_iterator model_iter = keys.rbegin();\n+         model_iter != keys.rend();\n+         ++model_iter) {\n+      ASSERT_TRUE(iter.Valid());\n+      ASSERT_EQ(*model_iter, iter.key());\n+      iter.Prev();\n+    }\n+    ASSERT_TRUE(!iter.Valid());\n+  }\n+}\n+\n+// We want to make sure that with a single writer and multiple\n+// concurrent readers (with no synchronization other than when a\n+// reader's iterator is created), the reader always observes all the\n+// data that was present in the skip list when the iterator was\n+// constructor.  Because insertions are happening concurrently, we may\n+// also observe new values that were inserted since the iterator was\n+// constructed, but we should never miss any values that were present\n+// at iterator construction time.\n+//\n+// We generate multi-part keys:\n+//     <key,gen,hash>\n+// where:\n+//     key is in range [0..K-1]\n+//     gen is a generation number for key\n+//     hash is hash(key,gen)\n+//\n+// The insertion code picks a random key, sets gen to be 1 + the last\n+// generation number inserted for that key, and sets hash to Hash(key,gen).\n+//\n+// At the beginning of a read, we snapshot the last inserted\n+// generation number for each key.  We then iterate, including random\n+// calls to Next() and Seek().  For every key we encounter, we\n+// check that it is either expected given the initial snapshot or has\n+// been concurrently added since the iterator started.\n+class ConcurrentTest {\n+ private:\n+  static const uint32_t K = 4;\n+\n+  static uint64_t key(Key key) { return (key >> 40); }\n+  static uint64_t gen(Key key) { return (key >> 8) & 0xffffffffu; }\n+  static uint64_t hash(Key key) { return key & 0xff; }\n+\n+  static uint64_t HashNumbers(uint64_t k, uint64_t g) {\n+    uint64_t data[2] = { k, g };\n+    return Hash(reinterpret_cast<char*>(data), sizeof(data), 0);\n+  }\n+\n+  static Key MakeKey(uint64_t k, uint64_t g) {\n+    assert(sizeof(Key) == sizeof(uint64_t));\n+    assert(k <= K);  // We sometimes pass K to seek to the end of the skiplist\n+    assert(g <= 0xffffffffu);\n+    return ((k << 40) | (g << 8) | (HashNumbers(k, g) & 0xff));\n+  }\n+\n+  static bool IsValidKey(Key k) {\n+    return hash(k) == (HashNumbers(key(k), gen(k)) & 0xff);\n+  }\n+\n+  static Key RandomTarget(Random* rnd) {\n+    switch (rnd->Next() % 10) {\n+      case 0:\n+        // Seek to beginning\n+        return MakeKey(0, 0);\n+      case 1:\n+        // Seek to end\n+        return MakeKey(K, 0);\n+      default:\n+        // Seek to middle\n+        return MakeKey(rnd->Next() % K, 0);\n+    }\n+  }\n+\n+  // Per-key generation\n+  struct State {\n+    port::AtomicPointer generation[K];\n+    void Set(int k, intptr_t v) {\n+      generation[k].Release_Store(reinterpret_cast<void*>(v));\n+    }\n+    intptr_t Get(int k) {\n+      return reinterpret_cast<intptr_t>(generation[k].Acquire_Load());\n+    }\n+\n+    State() {\n+      for (int k = 0; k < K; k++) {\n+        Set(k, 0);\n+      }\n+    }\n+  };\n+\n+  // Current state of the test\n+  State current_;\n+\n+  Arena arena_;\n+\n+  // SkipList is not protected by mu_.  We just use a single writer\n+  // thread to modify it.\n+  SkipList<Key, Comparator> list_;\n+\n+ public:\n+  ConcurrentTest() : list_(Comparator(), &arena_) { }\n+\n+  // REQUIRES: External synchronization\n+  void WriteStep(Random* rnd) {\n+    const uint32_t k = rnd->Next() % K;\n+    const intptr_t g = current_.Get(k) + 1;\n+    const Key key = MakeKey(k, g);\n+    list_.Insert(key);\n+    current_.Set(k, g);\n+  }\n+\n+  void ReadStep(Random* rnd) {\n+    // Remember the initial committed state of the skiplist.\n+    State initial_state;\n+    for (int k = 0; k < K; k++) {\n+      initial_state.Set(k, current_.Get(k));\n+    }\n+\n+    Key pos = RandomTarget(rnd);\n+    SkipList<Key, Comparator>::Iterator iter(&list_);\n+    iter.Seek(pos);\n+    while (true) {\n+      Key current;\n+      if (!iter.Valid()) {\n+        current = MakeKey(K, 0);\n+      } else {\n+        current = iter.key();\n+        ASSERT_TRUE(IsValidKey(current)) << current;\n+      }\n+      ASSERT_LE(pos, current) << \"should not go backwards\";\n+\n+      // Verify that everything in [pos,current) was not present in\n+      // initial_state.\n+      while (pos < current) {\n+        ASSERT_LT(key(pos), K) << pos;\n+\n+        // Note that generation 0 is never inserted, so it is ok if\n+        // <*,0,*> is missing.\n+        ASSERT_TRUE((gen(pos) == 0) ||\n+                    (gen(pos) > initial_state.Get(key(pos)))\n+                    ) << \"key: \" << key(pos)\n+                      << \"; gen: \" << gen(pos)\n+                      << \"; initgen: \"\n+                      << initial_state.Get(key(pos));\n+\n+        // Advance to next key in the valid key space\n+        if (key(pos) < key(current)) {\n+          pos = MakeKey(key(pos) + 1, 0);\n+        } else {\n+          pos = MakeKey(key(pos), gen(pos) + 1);\n+        }\n+      }\n+\n+      if (!iter.Valid()) {\n+        break;\n+      }\n+\n+      if (rnd->Next() % 2) {\n+        iter.Next();\n+        pos = MakeKey(key(pos), gen(pos) + 1);\n+      } else {\n+        Key new_target = RandomTarget(rnd);\n+        if (new_target > pos) {\n+          pos = new_target;\n+          iter.Seek(new_target);\n+        }\n+      }\n+    }\n+  }\n+};\n+const uint32_t ConcurrentTest::K;\n+\n+// Simple test that does single-threaded testing of the ConcurrentTest\n+// scaffolding.\n+TEST(SkipTest, ConcurrentWithoutThreads) {\n+  ConcurrentTest test;\n+  Random rnd(test::RandomSeed());\n+  for (int i = 0; i < 10000; i++) {\n+    test.ReadStep(&rnd);\n+    test.WriteStep(&rnd);\n+  }\n+}\n+\n+class TestState {\n+ public:\n+  ConcurrentTest t_;\n+  int seed_;\n+  port::AtomicPointer quit_flag_;\n+\n+  enum ReaderState {\n+    STARTING,\n+    RUNNING,\n+    DONE\n+  };\n+\n+  explicit TestState(int s)\n+      : seed_(s),\n+        quit_flag_(NULL),\n+        state_(STARTING),\n+        state_cv_(&mu_) {}\n+\n+  void Wait(ReaderState s) {\n+    mu_.Lock();\n+    while (state_ != s) {\n+      state_cv_.Wait();\n+    }\n+    mu_.Unlock();\n+  }\n+\n+  void Change(ReaderState s) {\n+    mu_.Lock();\n+    state_ = s;\n+    state_cv_.Signal();\n+    mu_.Unlock();\n+  }\n+\n+ private:\n+  port::Mutex mu_;\n+  ReaderState state_;\n+  port::CondVar state_cv_;\n+};\n+\n+static void ConcurrentReader(void* arg) {\n+  TestState* state = reinterpret_cast<TestState*>(arg);\n+  Random rnd(state->seed_);\n+  int64_t reads = 0;\n+  state->Change(TestState::RUNNING);\n+  while (!state->quit_flag_.Acquire_Load()) {\n+    state->t_.ReadStep(&rnd);\n+    ++reads;\n+  }\n+  state->Change(TestState::DONE);\n+}\n+\n+static void RunConcurrent(int run) {\n+  const int seed = test::RandomSeed() + (run * 100);\n+  Random rnd(seed);\n+  const int N = 1000;\n+  const int kSize = 1000;\n+  for (int i = 0; i < N; i++) {\n+    if ((i % 100) == 0) {\n+      fprintf(stderr, \"Run %d of %d\\n\", i, N);\n+    }\n+    TestState state(seed + 1);\n+    Env::Default()->Schedule(ConcurrentReader, &state);\n+    state.Wait(TestState::RUNNING);\n+    for (int i = 0; i < kSize; i++) {\n+      state.t_.WriteStep(&rnd);\n+    }\n+    state.quit_flag_.Release_Store(&state);  // Any non-NULL arg will do\n+    state.Wait(TestState::DONE);\n+  }\n+}\n+\n+TEST(SkipTest, Concurrent1) { RunConcurrent(1); }\n+TEST(SkipTest, Concurrent2) { RunConcurrent(2); }\n+TEST(SkipTest, Concurrent3) { RunConcurrent(3); }\n+TEST(SkipTest, Concurrent4) { RunConcurrent(4); }\n+TEST(SkipTest, Concurrent5) { RunConcurrent(5); }\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "e7f8fd2c37cf8a482c45f5524aa729e74f7b3aa7",
        "filename": "src/leveldb/db/snapshot.h",
        "status": "added",
        "additions": 66,
        "deletions": 0,
        "changes": 66,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/snapshot.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/snapshot.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/snapshot.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,66 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_SNAPSHOT_H_\n+#define STORAGE_LEVELDB_DB_SNAPSHOT_H_\n+\n+#include \"leveldb/db.h\"\n+\n+namespace leveldb {\n+\n+class SnapshotList;\n+\n+// Snapshots are kept in a doubly-linked list in the DB.\n+// Each SnapshotImpl corresponds to a particular sequence number.\n+class SnapshotImpl : public Snapshot {\n+ public:\n+  SequenceNumber number_;  // const after creation\n+\n+ private:\n+  friend class SnapshotList;\n+\n+  // SnapshotImpl is kept in a doubly-linked circular list\n+  SnapshotImpl* prev_;\n+  SnapshotImpl* next_;\n+\n+  SnapshotList* list_;                 // just for sanity checks\n+};\n+\n+class SnapshotList {\n+ public:\n+  SnapshotList() {\n+    list_.prev_ = &list_;\n+    list_.next_ = &list_;\n+  }\n+\n+  bool empty() const { return list_.next_ == &list_; }\n+  SnapshotImpl* oldest() const { assert(!empty()); return list_.next_; }\n+  SnapshotImpl* newest() const { assert(!empty()); return list_.prev_; }\n+\n+  const SnapshotImpl* New(SequenceNumber seq) {\n+    SnapshotImpl* s = new SnapshotImpl;\n+    s->number_ = seq;\n+    s->list_ = this;\n+    s->next_ = &list_;\n+    s->prev_ = list_.prev_;\n+    s->prev_->next_ = s;\n+    s->next_->prev_ = s;\n+    return s;\n+  }\n+\n+  void Delete(const SnapshotImpl* s) {\n+    assert(s->list_ == this);\n+    s->prev_->next_ = s->next_;\n+    s->next_->prev_ = s->prev_;\n+    delete s;\n+  }\n+\n+ private:\n+  // Dummy head of doubly-linked list of snapshots\n+  SnapshotImpl list_;\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_SNAPSHOT_H_"
      },
      {
        "sha": "497db270766d8857ddb355ad09ed0892e4ab2daa",
        "filename": "src/leveldb/db/table_cache.cc",
        "status": "added",
        "additions": 121,
        "deletions": 0,
        "changes": 121,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/table_cache.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/table_cache.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/table_cache.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,121 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/table_cache.h\"\n+\n+#include \"db/filename.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/table.h\"\n+#include \"util/coding.h\"\n+\n+namespace leveldb {\n+\n+struct TableAndFile {\n+  RandomAccessFile* file;\n+  Table* table;\n+};\n+\n+static void DeleteEntry(const Slice& key, void* value) {\n+  TableAndFile* tf = reinterpret_cast<TableAndFile*>(value);\n+  delete tf->table;\n+  delete tf->file;\n+  delete tf;\n+}\n+\n+static void UnrefEntry(void* arg1, void* arg2) {\n+  Cache* cache = reinterpret_cast<Cache*>(arg1);\n+  Cache::Handle* h = reinterpret_cast<Cache::Handle*>(arg2);\n+  cache->Release(h);\n+}\n+\n+TableCache::TableCache(const std::string& dbname,\n+                       const Options* options,\n+                       int entries)\n+    : env_(options->env),\n+      dbname_(dbname),\n+      options_(options),\n+      cache_(NewLRUCache(entries)) {\n+}\n+\n+TableCache::~TableCache() {\n+  delete cache_;\n+}\n+\n+Status TableCache::FindTable(uint64_t file_number, uint64_t file_size,\n+                             Cache::Handle** handle) {\n+  Status s;\n+  char buf[sizeof(file_number)];\n+  EncodeFixed64(buf, file_number);\n+  Slice key(buf, sizeof(buf));\n+  *handle = cache_->Lookup(key);\n+  if (*handle == NULL) {\n+    std::string fname = TableFileName(dbname_, file_number);\n+    RandomAccessFile* file = NULL;\n+    Table* table = NULL;\n+    s = env_->NewRandomAccessFile(fname, &file);\n+    if (s.ok()) {\n+      s = Table::Open(*options_, file, file_size, &table);\n+    }\n+\n+    if (!s.ok()) {\n+      assert(table == NULL);\n+      delete file;\n+      // We do not cache error results so that if the error is transient,\n+      // or somebody repairs the file, we recover automatically.\n+    } else {\n+      TableAndFile* tf = new TableAndFile;\n+      tf->file = file;\n+      tf->table = table;\n+      *handle = cache_->Insert(key, tf, 1, &DeleteEntry);\n+    }\n+  }\n+  return s;\n+}\n+\n+Iterator* TableCache::NewIterator(const ReadOptions& options,\n+                                  uint64_t file_number,\n+                                  uint64_t file_size,\n+                                  Table** tableptr) {\n+  if (tableptr != NULL) {\n+    *tableptr = NULL;\n+  }\n+\n+  Cache::Handle* handle = NULL;\n+  Status s = FindTable(file_number, file_size, &handle);\n+  if (!s.ok()) {\n+    return NewErrorIterator(s);\n+  }\n+\n+  Table* table = reinterpret_cast<TableAndFile*>(cache_->Value(handle))->table;\n+  Iterator* result = table->NewIterator(options);\n+  result->RegisterCleanup(&UnrefEntry, cache_, handle);\n+  if (tableptr != NULL) {\n+    *tableptr = table;\n+  }\n+  return result;\n+}\n+\n+Status TableCache::Get(const ReadOptions& options,\n+                       uint64_t file_number,\n+                       uint64_t file_size,\n+                       const Slice& k,\n+                       void* arg,\n+                       void (*saver)(void*, const Slice&, const Slice&)) {\n+  Cache::Handle* handle = NULL;\n+  Status s = FindTable(file_number, file_size, &handle);\n+  if (s.ok()) {\n+    Table* t = reinterpret_cast<TableAndFile*>(cache_->Value(handle))->table;\n+    s = t->InternalGet(options, k, arg, saver);\n+    cache_->Release(handle);\n+  }\n+  return s;\n+}\n+\n+void TableCache::Evict(uint64_t file_number) {\n+  char buf[sizeof(file_number)];\n+  EncodeFixed64(buf, file_number);\n+  cache_->Erase(Slice(buf, sizeof(buf)));\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "8cf4aaf12d8ed1a02bd7d1962b79cc8506575b6f",
        "filename": "src/leveldb/db/table_cache.h",
        "status": "added",
        "additions": 61,
        "deletions": 0,
        "changes": 61,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/table_cache.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/table_cache.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/table_cache.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,61 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// Thread-safe (provides internal synchronization)\n+\n+#ifndef STORAGE_LEVELDB_DB_TABLE_CACHE_H_\n+#define STORAGE_LEVELDB_DB_TABLE_CACHE_H_\n+\n+#include <string>\n+#include <stdint.h>\n+#include \"db/dbformat.h\"\n+#include \"leveldb/cache.h\"\n+#include \"leveldb/table.h\"\n+#include \"port/port.h\"\n+\n+namespace leveldb {\n+\n+class Env;\n+\n+class TableCache {\n+ public:\n+  TableCache(const std::string& dbname, const Options* options, int entries);\n+  ~TableCache();\n+\n+  // Return an iterator for the specified file number (the corresponding\n+  // file length must be exactly \"file_size\" bytes).  If \"tableptr\" is\n+  // non-NULL, also sets \"*tableptr\" to point to the Table object\n+  // underlying the returned iterator, or NULL if no Table object underlies\n+  // the returned iterator.  The returned \"*tableptr\" object is owned by\n+  // the cache and should not be deleted, and is valid for as long as the\n+  // returned iterator is live.\n+  Iterator* NewIterator(const ReadOptions& options,\n+                        uint64_t file_number,\n+                        uint64_t file_size,\n+                        Table** tableptr = NULL);\n+\n+  // If a seek to internal key \"k\" in specified file finds an entry,\n+  // call (*handle_result)(arg, found_key, found_value).\n+  Status Get(const ReadOptions& options,\n+             uint64_t file_number,\n+             uint64_t file_size,\n+             const Slice& k,\n+             void* arg,\n+             void (*handle_result)(void*, const Slice&, const Slice&));\n+\n+  // Evict any entry for the specified file number\n+  void Evict(uint64_t file_number);\n+\n+ private:\n+  Env* const env_;\n+  const std::string dbname_;\n+  const Options* options_;\n+  Cache* cache_;\n+\n+  Status FindTable(uint64_t file_number, uint64_t file_size, Cache::Handle**);\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_TABLE_CACHE_H_"
      },
      {
        "sha": "f10a2d58b211cb16becb0ac0298210f0dacbd2a5",
        "filename": "src/leveldb/db/version_edit.cc",
        "status": "added",
        "additions": 266,
        "deletions": 0,
        "changes": 266,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/version_edit.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/version_edit.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_edit.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,266 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/version_edit.h\"\n+\n+#include \"db/version_set.h\"\n+#include \"util/coding.h\"\n+\n+namespace leveldb {\n+\n+// Tag numbers for serialized VersionEdit.  These numbers are written to\n+// disk and should not be changed.\n+enum Tag {\n+  kComparator           = 1,\n+  kLogNumber            = 2,\n+  kNextFileNumber       = 3,\n+  kLastSequence         = 4,\n+  kCompactPointer       = 5,\n+  kDeletedFile          = 6,\n+  kNewFile              = 7,\n+  // 8 was used for large value refs\n+  kPrevLogNumber        = 9\n+};\n+\n+void VersionEdit::Clear() {\n+  comparator_.clear();\n+  log_number_ = 0;\n+  prev_log_number_ = 0;\n+  last_sequence_ = 0;\n+  next_file_number_ = 0;\n+  has_comparator_ = false;\n+  has_log_number_ = false;\n+  has_prev_log_number_ = false;\n+  has_next_file_number_ = false;\n+  has_last_sequence_ = false;\n+  deleted_files_.clear();\n+  new_files_.clear();\n+}\n+\n+void VersionEdit::EncodeTo(std::string* dst) const {\n+  if (has_comparator_) {\n+    PutVarint32(dst, kComparator);\n+    PutLengthPrefixedSlice(dst, comparator_);\n+  }\n+  if (has_log_number_) {\n+    PutVarint32(dst, kLogNumber);\n+    PutVarint64(dst, log_number_);\n+  }\n+  if (has_prev_log_number_) {\n+    PutVarint32(dst, kPrevLogNumber);\n+    PutVarint64(dst, prev_log_number_);\n+  }\n+  if (has_next_file_number_) {\n+    PutVarint32(dst, kNextFileNumber);\n+    PutVarint64(dst, next_file_number_);\n+  }\n+  if (has_last_sequence_) {\n+    PutVarint32(dst, kLastSequence);\n+    PutVarint64(dst, last_sequence_);\n+  }\n+\n+  for (size_t i = 0; i < compact_pointers_.size(); i++) {\n+    PutVarint32(dst, kCompactPointer);\n+    PutVarint32(dst, compact_pointers_[i].first);  // level\n+    PutLengthPrefixedSlice(dst, compact_pointers_[i].second.Encode());\n+  }\n+\n+  for (DeletedFileSet::const_iterator iter = deleted_files_.begin();\n+       iter != deleted_files_.end();\n+       ++iter) {\n+    PutVarint32(dst, kDeletedFile);\n+    PutVarint32(dst, iter->first);   // level\n+    PutVarint64(dst, iter->second);  // file number\n+  }\n+\n+  for (size_t i = 0; i < new_files_.size(); i++) {\n+    const FileMetaData& f = new_files_[i].second;\n+    PutVarint32(dst, kNewFile);\n+    PutVarint32(dst, new_files_[i].first);  // level\n+    PutVarint64(dst, f.number);\n+    PutVarint64(dst, f.file_size);\n+    PutLengthPrefixedSlice(dst, f.smallest.Encode());\n+    PutLengthPrefixedSlice(dst, f.largest.Encode());\n+  }\n+}\n+\n+static bool GetInternalKey(Slice* input, InternalKey* dst) {\n+  Slice str;\n+  if (GetLengthPrefixedSlice(input, &str)) {\n+    dst->DecodeFrom(str);\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+static bool GetLevel(Slice* input, int* level) {\n+  uint32_t v;\n+  if (GetVarint32(input, &v) &&\n+      v < config::kNumLevels) {\n+    *level = v;\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+Status VersionEdit::DecodeFrom(const Slice& src) {\n+  Clear();\n+  Slice input = src;\n+  const char* msg = NULL;\n+  uint32_t tag;\n+\n+  // Temporary storage for parsing\n+  int level;\n+  uint64_t number;\n+  FileMetaData f;\n+  Slice str;\n+  InternalKey key;\n+\n+  while (msg == NULL && GetVarint32(&input, &tag)) {\n+    switch (tag) {\n+      case kComparator:\n+        if (GetLengthPrefixedSlice(&input, &str)) {\n+          comparator_ = str.ToString();\n+          has_comparator_ = true;\n+        } else {\n+          msg = \"comparator name\";\n+        }\n+        break;\n+\n+      case kLogNumber:\n+        if (GetVarint64(&input, &log_number_)) {\n+          has_log_number_ = true;\n+        } else {\n+          msg = \"log number\";\n+        }\n+        break;\n+\n+      case kPrevLogNumber:\n+        if (GetVarint64(&input, &prev_log_number_)) {\n+          has_prev_log_number_ = true;\n+        } else {\n+          msg = \"previous log number\";\n+        }\n+        break;\n+\n+      case kNextFileNumber:\n+        if (GetVarint64(&input, &next_file_number_)) {\n+          has_next_file_number_ = true;\n+        } else {\n+          msg = \"next file number\";\n+        }\n+        break;\n+\n+      case kLastSequence:\n+        if (GetVarint64(&input, &last_sequence_)) {\n+          has_last_sequence_ = true;\n+        } else {\n+          msg = \"last sequence number\";\n+        }\n+        break;\n+\n+      case kCompactPointer:\n+        if (GetLevel(&input, &level) &&\n+            GetInternalKey(&input, &key)) {\n+          compact_pointers_.push_back(std::make_pair(level, key));\n+        } else {\n+          msg = \"compaction pointer\";\n+        }\n+        break;\n+\n+      case kDeletedFile:\n+        if (GetLevel(&input, &level) &&\n+            GetVarint64(&input, &number)) {\n+          deleted_files_.insert(std::make_pair(level, number));\n+        } else {\n+          msg = \"deleted file\";\n+        }\n+        break;\n+\n+      case kNewFile:\n+        if (GetLevel(&input, &level) &&\n+            GetVarint64(&input, &f.number) &&\n+            GetVarint64(&input, &f.file_size) &&\n+            GetInternalKey(&input, &f.smallest) &&\n+            GetInternalKey(&input, &f.largest)) {\n+          new_files_.push_back(std::make_pair(level, f));\n+        } else {\n+          msg = \"new-file entry\";\n+        }\n+        break;\n+\n+      default:\n+        msg = \"unknown tag\";\n+        break;\n+    }\n+  }\n+\n+  if (msg == NULL && !input.empty()) {\n+    msg = \"invalid tag\";\n+  }\n+\n+  Status result;\n+  if (msg != NULL) {\n+    result = Status::Corruption(\"VersionEdit\", msg);\n+  }\n+  return result;\n+}\n+\n+std::string VersionEdit::DebugString() const {\n+  std::string r;\n+  r.append(\"VersionEdit {\");\n+  if (has_comparator_) {\n+    r.append(\"\\n  Comparator: \");\n+    r.append(comparator_);\n+  }\n+  if (has_log_number_) {\n+    r.append(\"\\n  LogNumber: \");\n+    AppendNumberTo(&r, log_number_);\n+  }\n+  if (has_prev_log_number_) {\n+    r.append(\"\\n  PrevLogNumber: \");\n+    AppendNumberTo(&r, prev_log_number_);\n+  }\n+  if (has_next_file_number_) {\n+    r.append(\"\\n  NextFile: \");\n+    AppendNumberTo(&r, next_file_number_);\n+  }\n+  if (has_last_sequence_) {\n+    r.append(\"\\n  LastSeq: \");\n+    AppendNumberTo(&r, last_sequence_);\n+  }\n+  for (size_t i = 0; i < compact_pointers_.size(); i++) {\n+    r.append(\"\\n  CompactPointer: \");\n+    AppendNumberTo(&r, compact_pointers_[i].first);\n+    r.append(\" \");\n+    r.append(compact_pointers_[i].second.DebugString());\n+  }\n+  for (DeletedFileSet::const_iterator iter = deleted_files_.begin();\n+       iter != deleted_files_.end();\n+       ++iter) {\n+    r.append(\"\\n  DeleteFile: \");\n+    AppendNumberTo(&r, iter->first);\n+    r.append(\" \");\n+    AppendNumberTo(&r, iter->second);\n+  }\n+  for (size_t i = 0; i < new_files_.size(); i++) {\n+    const FileMetaData& f = new_files_[i].second;\n+    r.append(\"\\n  AddFile: \");\n+    AppendNumberTo(&r, new_files_[i].first);\n+    r.append(\" \");\n+    AppendNumberTo(&r, f.number);\n+    r.append(\" \");\n+    AppendNumberTo(&r, f.file_size);\n+    r.append(\" \");\n+    r.append(f.smallest.DebugString());\n+    r.append(\" .. \");\n+    r.append(f.largest.DebugString());\n+  }\n+  r.append(\"\\n}\\n\");\n+  return r;\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "eaef77b327c64a3756a5f2512bc1786530907cbd",
        "filename": "src/leveldb/db/version_edit.h",
        "status": "added",
        "additions": 107,
        "deletions": 0,
        "changes": 107,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/version_edit.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/version_edit.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_edit.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,107 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_VERSION_EDIT_H_\n+#define STORAGE_LEVELDB_DB_VERSION_EDIT_H_\n+\n+#include <set>\n+#include <utility>\n+#include <vector>\n+#include \"db/dbformat.h\"\n+\n+namespace leveldb {\n+\n+class VersionSet;\n+\n+struct FileMetaData {\n+  int refs;\n+  int allowed_seeks;          // Seeks allowed until compaction\n+  uint64_t number;\n+  uint64_t file_size;         // File size in bytes\n+  InternalKey smallest;       // Smallest internal key served by table\n+  InternalKey largest;        // Largest internal key served by table\n+\n+  FileMetaData() : refs(0), allowed_seeks(1 << 30), file_size(0) { }\n+};\n+\n+class VersionEdit {\n+ public:\n+  VersionEdit() { Clear(); }\n+  ~VersionEdit() { }\n+\n+  void Clear();\n+\n+  void SetComparatorName(const Slice& name) {\n+    has_comparator_ = true;\n+    comparator_ = name.ToString();\n+  }\n+  void SetLogNumber(uint64_t num) {\n+    has_log_number_ = true;\n+    log_number_ = num;\n+  }\n+  void SetPrevLogNumber(uint64_t num) {\n+    has_prev_log_number_ = true;\n+    prev_log_number_ = num;\n+  }\n+  void SetNextFile(uint64_t num) {\n+    has_next_file_number_ = true;\n+    next_file_number_ = num;\n+  }\n+  void SetLastSequence(SequenceNumber seq) {\n+    has_last_sequence_ = true;\n+    last_sequence_ = seq;\n+  }\n+  void SetCompactPointer(int level, const InternalKey& key) {\n+    compact_pointers_.push_back(std::make_pair(level, key));\n+  }\n+\n+  // Add the specified file at the specified number.\n+  // REQUIRES: This version has not been saved (see VersionSet::SaveTo)\n+  // REQUIRES: \"smallest\" and \"largest\" are smallest and largest keys in file\n+  void AddFile(int level, uint64_t file,\n+               uint64_t file_size,\n+               const InternalKey& smallest,\n+               const InternalKey& largest) {\n+    FileMetaData f;\n+    f.number = file;\n+    f.file_size = file_size;\n+    f.smallest = smallest;\n+    f.largest = largest;\n+    new_files_.push_back(std::make_pair(level, f));\n+  }\n+\n+  // Delete the specified \"file\" from the specified \"level\".\n+  void DeleteFile(int level, uint64_t file) {\n+    deleted_files_.insert(std::make_pair(level, file));\n+  }\n+\n+  void EncodeTo(std::string* dst) const;\n+  Status DecodeFrom(const Slice& src);\n+\n+  std::string DebugString() const;\n+\n+ private:\n+  friend class VersionSet;\n+\n+  typedef std::set< std::pair<int, uint64_t> > DeletedFileSet;\n+\n+  std::string comparator_;\n+  uint64_t log_number_;\n+  uint64_t prev_log_number_;\n+  uint64_t next_file_number_;\n+  SequenceNumber last_sequence_;\n+  bool has_comparator_;\n+  bool has_log_number_;\n+  bool has_prev_log_number_;\n+  bool has_next_file_number_;\n+  bool has_last_sequence_;\n+\n+  std::vector< std::pair<int, InternalKey> > compact_pointers_;\n+  DeletedFileSet deleted_files_;\n+  std::vector< std::pair<int, FileMetaData> > new_files_;\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_VERSION_EDIT_H_"
      },
      {
        "sha": "280310b49d846e245df0d000ca1407724582daa2",
        "filename": "src/leveldb/db/version_edit_test.cc",
        "status": "added",
        "additions": 46,
        "deletions": 0,
        "changes": 46,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/version_edit_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/version_edit_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_edit_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,46 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/version_edit.h\"\n+#include \"util/testharness.h\"\n+\n+namespace leveldb {\n+\n+static void TestEncodeDecode(const VersionEdit& edit) {\n+  std::string encoded, encoded2;\n+  edit.EncodeTo(&encoded);\n+  VersionEdit parsed;\n+  Status s = parsed.DecodeFrom(encoded);\n+  ASSERT_TRUE(s.ok()) << s.ToString();\n+  parsed.EncodeTo(&encoded2);\n+  ASSERT_EQ(encoded, encoded2);\n+}\n+\n+class VersionEditTest { };\n+\n+TEST(VersionEditTest, EncodeDecode) {\n+  static const uint64_t kBig = 1ull << 50;\n+\n+  VersionEdit edit;\n+  for (int i = 0; i < 4; i++) {\n+    TestEncodeDecode(edit);\n+    edit.AddFile(3, kBig + 300 + i, kBig + 400 + i,\n+                 InternalKey(\"foo\", kBig + 500 + i, kTypeValue),\n+                 InternalKey(\"zoo\", kBig + 600 + i, kTypeDeletion));\n+    edit.DeleteFile(4, kBig + 700 + i);\n+    edit.SetCompactPointer(i, InternalKey(\"x\", kBig + 900 + i, kTypeValue));\n+  }\n+\n+  edit.SetComparatorName(\"foo\");\n+  edit.SetLogNumber(kBig + 100);\n+  edit.SetNextFile(kBig + 200);\n+  edit.SetLastSequence(kBig + 1000);\n+  TestEncodeDecode(edit);\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "4fd1ddef21011ffb379019b65ce64464ad8ca54c",
        "filename": "src/leveldb/db/version_set.cc",
        "status": "added",
        "additions": 1443,
        "deletions": 0,
        "changes": 1443,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/version_set.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/version_set.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_set.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,1443 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/version_set.h\"\n+\n+#include <algorithm>\n+#include <stdio.h>\n+#include \"db/filename.h\"\n+#include \"db/log_reader.h\"\n+#include \"db/log_writer.h\"\n+#include \"db/memtable.h\"\n+#include \"db/table_cache.h\"\n+#include \"leveldb/env.h\"\n+#include \"leveldb/table_builder.h\"\n+#include \"table/merger.h\"\n+#include \"table/two_level_iterator.h\"\n+#include \"util/coding.h\"\n+#include \"util/logging.h\"\n+\n+namespace leveldb {\n+\n+static const int kTargetFileSize = 2 * 1048576;\n+\n+// Maximum bytes of overlaps in grandparent (i.e., level+2) before we\n+// stop building a single file in a level->level+1 compaction.\n+static const int64_t kMaxGrandParentOverlapBytes = 10 * kTargetFileSize;\n+\n+// Maximum number of bytes in all compacted files.  We avoid expanding\n+// the lower level file set of a compaction if it would make the\n+// total compaction cover more than this many bytes.\n+static const int64_t kExpandedCompactionByteSizeLimit = 25 * kTargetFileSize;\n+\n+static double MaxBytesForLevel(int level) {\n+  // Note: the result for level zero is not really used since we set\n+  // the level-0 compaction threshold based on number of files.\n+  double result = 10 * 1048576.0;  // Result for both level-0 and level-1\n+  while (level > 1) {\n+    result *= 10;\n+    level--;\n+  }\n+  return result;\n+}\n+\n+static uint64_t MaxFileSizeForLevel(int level) {\n+  return kTargetFileSize;  // We could vary per level to reduce number of files?\n+}\n+\n+static int64_t TotalFileSize(const std::vector<FileMetaData*>& files) {\n+  int64_t sum = 0;\n+  for (size_t i = 0; i < files.size(); i++) {\n+    sum += files[i]->file_size;\n+  }\n+  return sum;\n+}\n+\n+namespace {\n+std::string IntSetToString(const std::set<uint64_t>& s) {\n+  std::string result = \"{\";\n+  for (std::set<uint64_t>::const_iterator it = s.begin();\n+       it != s.end();\n+       ++it) {\n+    result += (result.size() > 1) ? \",\" : \"\";\n+    result += NumberToString(*it);\n+  }\n+  result += \"}\";\n+  return result;\n+}\n+}  // namespace\n+\n+Version::~Version() {\n+  assert(refs_ == 0);\n+\n+  // Remove from linked list\n+  prev_->next_ = next_;\n+  next_->prev_ = prev_;\n+\n+  // Drop references to files\n+  for (int level = 0; level < config::kNumLevels; level++) {\n+    for (size_t i = 0; i < files_[level].size(); i++) {\n+      FileMetaData* f = files_[level][i];\n+      assert(f->refs > 0);\n+      f->refs--;\n+      if (f->refs <= 0) {\n+        delete f;\n+      }\n+    }\n+  }\n+}\n+\n+int FindFile(const InternalKeyComparator& icmp,\n+             const std::vector<FileMetaData*>& files,\n+             const Slice& key) {\n+  uint32_t left = 0;\n+  uint32_t right = files.size();\n+  while (left < right) {\n+    uint32_t mid = (left + right) / 2;\n+    const FileMetaData* f = files[mid];\n+    if (icmp.InternalKeyComparator::Compare(f->largest.Encode(), key) < 0) {\n+      // Key at \"mid.largest\" is < \"target\".  Therefore all\n+      // files at or before \"mid\" are uninteresting.\n+      left = mid + 1;\n+    } else {\n+      // Key at \"mid.largest\" is >= \"target\".  Therefore all files\n+      // after \"mid\" are uninteresting.\n+      right = mid;\n+    }\n+  }\n+  return right;\n+}\n+\n+static bool AfterFile(const Comparator* ucmp,\n+                      const Slice* user_key, const FileMetaData* f) {\n+  // NULL user_key occurs before all keys and is therefore never after *f\n+  return (user_key != NULL &&\n+          ucmp->Compare(*user_key, f->largest.user_key()) > 0);\n+}\n+\n+static bool BeforeFile(const Comparator* ucmp,\n+                       const Slice* user_key, const FileMetaData* f) {\n+  // NULL user_key occurs after all keys and is therefore never before *f\n+  return (user_key != NULL &&\n+          ucmp->Compare(*user_key, f->smallest.user_key()) < 0);\n+}\n+\n+bool SomeFileOverlapsRange(\n+    const InternalKeyComparator& icmp,\n+    bool disjoint_sorted_files,\n+    const std::vector<FileMetaData*>& files,\n+    const Slice* smallest_user_key,\n+    const Slice* largest_user_key) {\n+  const Comparator* ucmp = icmp.user_comparator();\n+  if (!disjoint_sorted_files) {\n+    // Need to check against all files\n+    for (size_t i = 0; i < files.size(); i++) {\n+      const FileMetaData* f = files[i];\n+      if (AfterFile(ucmp, smallest_user_key, f) ||\n+          BeforeFile(ucmp, largest_user_key, f)) {\n+        // No overlap\n+      } else {\n+        return true;  // Overlap\n+      }\n+    }\n+    return false;\n+  }\n+\n+  // Binary search over file list\n+  uint32_t index = 0;\n+  if (smallest_user_key != NULL) {\n+    // Find the earliest possible internal key for smallest_user_key\n+    InternalKey small(*smallest_user_key, kMaxSequenceNumber,kValueTypeForSeek);\n+    index = FindFile(icmp, files, small.Encode());\n+  }\n+\n+  if (index >= files.size()) {\n+    // beginning of range is after all files, so no overlap.\n+    return false;\n+  }\n+\n+  return !BeforeFile(ucmp, largest_user_key, files[index]);\n+}\n+\n+// An internal iterator.  For a given version/level pair, yields\n+// information about the files in the level.  For a given entry, key()\n+// is the largest key that occurs in the file, and value() is an\n+// 16-byte value containing the file number and file size, both\n+// encoded using EncodeFixed64.\n+class Version::LevelFileNumIterator : public Iterator {\n+ public:\n+  LevelFileNumIterator(const InternalKeyComparator& icmp,\n+                       const std::vector<FileMetaData*>* flist)\n+      : icmp_(icmp),\n+        flist_(flist),\n+        index_(flist->size()) {        // Marks as invalid\n+  }\n+  virtual bool Valid() const {\n+    return index_ < flist_->size();\n+  }\n+  virtual void Seek(const Slice& target) {\n+    index_ = FindFile(icmp_, *flist_, target);\n+  }\n+  virtual void SeekToFirst() { index_ = 0; }\n+  virtual void SeekToLast() {\n+    index_ = flist_->empty() ? 0 : flist_->size() - 1;\n+  }\n+  virtual void Next() {\n+    assert(Valid());\n+    index_++;\n+  }\n+  virtual void Prev() {\n+    assert(Valid());\n+    if (index_ == 0) {\n+      index_ = flist_->size();  // Marks as invalid\n+    } else {\n+      index_--;\n+    }\n+  }\n+  Slice key() const {\n+    assert(Valid());\n+    return (*flist_)[index_]->largest.Encode();\n+  }\n+  Slice value() const {\n+    assert(Valid());\n+    EncodeFixed64(value_buf_, (*flist_)[index_]->number);\n+    EncodeFixed64(value_buf_+8, (*flist_)[index_]->file_size);\n+    return Slice(value_buf_, sizeof(value_buf_));\n+  }\n+  virtual Status status() const { return Status::OK(); }\n+ private:\n+  const InternalKeyComparator icmp_;\n+  const std::vector<FileMetaData*>* const flist_;\n+  uint32_t index_;\n+\n+  // Backing store for value().  Holds the file number and size.\n+  mutable char value_buf_[16];\n+};\n+\n+static Iterator* GetFileIterator(void* arg,\n+                                 const ReadOptions& options,\n+                                 const Slice& file_value) {\n+  TableCache* cache = reinterpret_cast<TableCache*>(arg);\n+  if (file_value.size() != 16) {\n+    return NewErrorIterator(\n+        Status::Corruption(\"FileReader invoked with unexpected value\"));\n+  } else {\n+    return cache->NewIterator(options,\n+                              DecodeFixed64(file_value.data()),\n+                              DecodeFixed64(file_value.data() + 8));\n+  }\n+}\n+\n+Iterator* Version::NewConcatenatingIterator(const ReadOptions& options,\n+                                            int level) const {\n+  return NewTwoLevelIterator(\n+      new LevelFileNumIterator(vset_->icmp_, &files_[level]),\n+      &GetFileIterator, vset_->table_cache_, options);\n+}\n+\n+void Version::AddIterators(const ReadOptions& options,\n+                           std::vector<Iterator*>* iters) {\n+  // Merge all level zero files together since they may overlap\n+  for (size_t i = 0; i < files_[0].size(); i++) {\n+    iters->push_back(\n+        vset_->table_cache_->NewIterator(\n+            options, files_[0][i]->number, files_[0][i]->file_size));\n+  }\n+\n+  // For levels > 0, we can use a concatenating iterator that sequentially\n+  // walks through the non-overlapping files in the level, opening them\n+  // lazily.\n+  for (int level = 1; level < config::kNumLevels; level++) {\n+    if (!files_[level].empty()) {\n+      iters->push_back(NewConcatenatingIterator(options, level));\n+    }\n+  }\n+}\n+\n+// Callback from TableCache::Get()\n+namespace {\n+enum SaverState {\n+  kNotFound,\n+  kFound,\n+  kDeleted,\n+  kCorrupt,\n+};\n+struct Saver {\n+  SaverState state;\n+  const Comparator* ucmp;\n+  Slice user_key;\n+  std::string* value;\n+};\n+}\n+static void SaveValue(void* arg, const Slice& ikey, const Slice& v) {\n+  Saver* s = reinterpret_cast<Saver*>(arg);\n+  ParsedInternalKey parsed_key;\n+  if (!ParseInternalKey(ikey, &parsed_key)) {\n+    s->state = kCorrupt;\n+  } else {\n+    if (s->ucmp->Compare(parsed_key.user_key, s->user_key) == 0) {\n+      s->state = (parsed_key.type == kTypeValue) ? kFound : kDeleted;\n+      if (s->state == kFound) {\n+        s->value->assign(v.data(), v.size());\n+      }\n+    }\n+  }\n+}\n+\n+static bool NewestFirst(FileMetaData* a, FileMetaData* b) {\n+  return a->number > b->number;\n+}\n+\n+Status Version::Get(const ReadOptions& options,\n+                    const LookupKey& k,\n+                    std::string* value,\n+                    GetStats* stats) {\n+  Slice ikey = k.internal_key();\n+  Slice user_key = k.user_key();\n+  const Comparator* ucmp = vset_->icmp_.user_comparator();\n+  Status s;\n+\n+  stats->seek_file = NULL;\n+  stats->seek_file_level = -1;\n+  FileMetaData* last_file_read = NULL;\n+  int last_file_read_level = -1;\n+\n+  // We can search level-by-level since entries never hop across\n+  // levels.  Therefore we are guaranteed that if we find data\n+  // in an smaller level, later levels are irrelevant.\n+  std::vector<FileMetaData*> tmp;\n+  FileMetaData* tmp2;\n+  for (int level = 0; level < config::kNumLevels; level++) {\n+    size_t num_files = files_[level].size();\n+    if (num_files == 0) continue;\n+\n+    // Get the list of files to search in this level\n+    FileMetaData* const* files = &files_[level][0];\n+    if (level == 0) {\n+      // Level-0 files may overlap each other.  Find all files that\n+      // overlap user_key and process them in order from newest to oldest.\n+      tmp.reserve(num_files);\n+      for (uint32_t i = 0; i < num_files; i++) {\n+        FileMetaData* f = files[i];\n+        if (ucmp->Compare(user_key, f->smallest.user_key()) >= 0 &&\n+            ucmp->Compare(user_key, f->largest.user_key()) <= 0) {\n+          tmp.push_back(f);\n+        }\n+      }\n+      if (tmp.empty()) continue;\n+\n+      std::sort(tmp.begin(), tmp.end(), NewestFirst);\n+      files = &tmp[0];\n+      num_files = tmp.size();\n+    } else {\n+      // Binary search to find earliest index whose largest key >= ikey.\n+      uint32_t index = FindFile(vset_->icmp_, files_[level], ikey);\n+      if (index >= num_files) {\n+        files = NULL;\n+        num_files = 0;\n+      } else {\n+        tmp2 = files[index];\n+        if (ucmp->Compare(user_key, tmp2->smallest.user_key()) < 0) {\n+          // All of \"tmp2\" is past any data for user_key\n+          files = NULL;\n+          num_files = 0;\n+        } else {\n+          files = &tmp2;\n+          num_files = 1;\n+        }\n+      }\n+    }\n+\n+    for (uint32_t i = 0; i < num_files; ++i) {\n+      if (last_file_read != NULL && stats->seek_file == NULL) {\n+        // We have had more than one seek for this read.  Charge the 1st file.\n+        stats->seek_file = last_file_read;\n+        stats->seek_file_level = last_file_read_level;\n+      }\n+\n+      FileMetaData* f = files[i];\n+      last_file_read = f;\n+      last_file_read_level = level;\n+\n+      Saver saver;\n+      saver.state = kNotFound;\n+      saver.ucmp = ucmp;\n+      saver.user_key = user_key;\n+      saver.value = value;\n+      s = vset_->table_cache_->Get(options, f->number, f->file_size,\n+                                   ikey, &saver, SaveValue);\n+      if (!s.ok()) {\n+        return s;\n+      }\n+      switch (saver.state) {\n+        case kNotFound:\n+          break;      // Keep searching in other files\n+        case kFound:\n+          return s;\n+        case kDeleted:\n+          s = Status::NotFound(Slice());  // Use empty error message for speed\n+          return s;\n+        case kCorrupt:\n+          s = Status::Corruption(\"corrupted key for \", user_key);\n+          return s;\n+      }\n+    }\n+  }\n+\n+  return Status::NotFound(Slice());  // Use an empty error message for speed\n+}\n+\n+bool Version::UpdateStats(const GetStats& stats) {\n+  FileMetaData* f = stats.seek_file;\n+  if (f != NULL) {\n+    f->allowed_seeks--;\n+    if (f->allowed_seeks <= 0 && file_to_compact_ == NULL) {\n+      file_to_compact_ = f;\n+      file_to_compact_level_ = stats.seek_file_level;\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+void Version::Ref() {\n+  ++refs_;\n+}\n+\n+void Version::Unref() {\n+  assert(this != &vset_->dummy_versions_);\n+  assert(refs_ >= 1);\n+  --refs_;\n+  if (refs_ == 0) {\n+    delete this;\n+  }\n+}\n+\n+bool Version::OverlapInLevel(int level,\n+                             const Slice* smallest_user_key,\n+                             const Slice* largest_user_key) {\n+  return SomeFileOverlapsRange(vset_->icmp_, (level > 0), files_[level],\n+                               smallest_user_key, largest_user_key);\n+}\n+\n+int Version::PickLevelForMemTableOutput(\n+    const Slice& smallest_user_key,\n+    const Slice& largest_user_key) {\n+  int level = 0;\n+  if (!OverlapInLevel(0, &smallest_user_key, &largest_user_key)) {\n+    // Push to next level if there is no overlap in next level,\n+    // and the #bytes overlapping in the level after that are limited.\n+    InternalKey start(smallest_user_key, kMaxSequenceNumber, kValueTypeForSeek);\n+    InternalKey limit(largest_user_key, 0, static_cast<ValueType>(0));\n+    std::vector<FileMetaData*> overlaps;\n+    while (level < config::kMaxMemCompactLevel) {\n+      if (OverlapInLevel(level + 1, &smallest_user_key, &largest_user_key)) {\n+        break;\n+      }\n+      GetOverlappingInputs(level + 2, &start, &limit, &overlaps);\n+      const int64_t sum = TotalFileSize(overlaps);\n+      if (sum > kMaxGrandParentOverlapBytes) {\n+        break;\n+      }\n+      level++;\n+    }\n+  }\n+  return level;\n+}\n+\n+// Store in \"*inputs\" all files in \"level\" that overlap [begin,end]\n+void Version::GetOverlappingInputs(\n+    int level,\n+    const InternalKey* begin,\n+    const InternalKey* end,\n+    std::vector<FileMetaData*>* inputs) {\n+  inputs->clear();\n+  Slice user_begin, user_end;\n+  if (begin != NULL) {\n+    user_begin = begin->user_key();\n+  }\n+  if (end != NULL) {\n+    user_end = end->user_key();\n+  }\n+  const Comparator* user_cmp = vset_->icmp_.user_comparator();\n+  for (size_t i = 0; i < files_[level].size(); ) {\n+    FileMetaData* f = files_[level][i++];\n+    const Slice file_start = f->smallest.user_key();\n+    const Slice file_limit = f->largest.user_key();\n+    if (begin != NULL && user_cmp->Compare(file_limit, user_begin) < 0) {\n+      // \"f\" is completely before specified range; skip it\n+    } else if (end != NULL && user_cmp->Compare(file_start, user_end) > 0) {\n+      // \"f\" is completely after specified range; skip it\n+    } else {\n+      inputs->push_back(f);\n+      if (level == 0) {\n+        // Level-0 files may overlap each other.  So check if the newly\n+        // added file has expanded the range.  If so, restart search.\n+        if (begin != NULL && user_cmp->Compare(file_start, user_begin) < 0) {\n+          user_begin = file_start;\n+          inputs->clear();\n+          i = 0;\n+        } else if (end != NULL && user_cmp->Compare(file_limit, user_end) > 0) {\n+          user_end = file_limit;\n+          inputs->clear();\n+          i = 0;\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+std::string Version::DebugString() const {\n+  std::string r;\n+  for (int level = 0; level < config::kNumLevels; level++) {\n+    // E.g.,\n+    //   --- level 1 ---\n+    //   17:123['a' .. 'd']\n+    //   20:43['e' .. 'g']\n+    r.append(\"--- level \");\n+    AppendNumberTo(&r, level);\n+    r.append(\" ---\\n\");\n+    const std::vector<FileMetaData*>& files = files_[level];\n+    for (size_t i = 0; i < files.size(); i++) {\n+      r.push_back(' ');\n+      AppendNumberTo(&r, files[i]->number);\n+      r.push_back(':');\n+      AppendNumberTo(&r, files[i]->file_size);\n+      r.append(\"[\");\n+      r.append(files[i]->smallest.DebugString());\n+      r.append(\" .. \");\n+      r.append(files[i]->largest.DebugString());\n+      r.append(\"]\\n\");\n+    }\n+  }\n+  return r;\n+}\n+\n+// A helper class so we can efficiently apply a whole sequence\n+// of edits to a particular state without creating intermediate\n+// Versions that contain full copies of the intermediate state.\n+class VersionSet::Builder {\n+ private:\n+  // Helper to sort by v->files_[file_number].smallest\n+  struct BySmallestKey {\n+    const InternalKeyComparator* internal_comparator;\n+\n+    bool operator()(FileMetaData* f1, FileMetaData* f2) const {\n+      int r = internal_comparator->Compare(f1->smallest, f2->smallest);\n+      if (r != 0) {\n+        return (r < 0);\n+      } else {\n+        // Break ties by file number\n+        return (f1->number < f2->number);\n+      }\n+    }\n+  };\n+\n+  typedef std::set<FileMetaData*, BySmallestKey> FileSet;\n+  struct LevelState {\n+    std::set<uint64_t> deleted_files;\n+    FileSet* added_files;\n+  };\n+\n+  VersionSet* vset_;\n+  Version* base_;\n+  LevelState levels_[config::kNumLevels];\n+\n+ public:\n+  // Initialize a builder with the files from *base and other info from *vset\n+  Builder(VersionSet* vset, Version* base)\n+      : vset_(vset),\n+        base_(base) {\n+    base_->Ref();\n+    BySmallestKey cmp;\n+    cmp.internal_comparator = &vset_->icmp_;\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      levels_[level].added_files = new FileSet(cmp);\n+    }\n+  }\n+\n+  ~Builder() {\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      const FileSet* added = levels_[level].added_files;\n+      std::vector<FileMetaData*> to_unref;\n+      to_unref.reserve(added->size());\n+      for (FileSet::const_iterator it = added->begin();\n+          it != added->end(); ++it) {\n+        to_unref.push_back(*it);\n+      }\n+      delete added;\n+      for (uint32_t i = 0; i < to_unref.size(); i++) {\n+        FileMetaData* f = to_unref[i];\n+        f->refs--;\n+        if (f->refs <= 0) {\n+          delete f;\n+        }\n+      }\n+    }\n+    base_->Unref();\n+  }\n+\n+  // Apply all of the edits in *edit to the current state.\n+  void Apply(VersionEdit* edit) {\n+    // Update compaction pointers\n+    for (size_t i = 0; i < edit->compact_pointers_.size(); i++) {\n+      const int level = edit->compact_pointers_[i].first;\n+      vset_->compact_pointer_[level] =\n+          edit->compact_pointers_[i].second.Encode().ToString();\n+    }\n+\n+    // Delete files\n+    const VersionEdit::DeletedFileSet& del = edit->deleted_files_;\n+    for (VersionEdit::DeletedFileSet::const_iterator iter = del.begin();\n+         iter != del.end();\n+         ++iter) {\n+      const int level = iter->first;\n+      const uint64_t number = iter->second;\n+      levels_[level].deleted_files.insert(number);\n+    }\n+\n+    // Add new files\n+    for (size_t i = 0; i < edit->new_files_.size(); i++) {\n+      const int level = edit->new_files_[i].first;\n+      FileMetaData* f = new FileMetaData(edit->new_files_[i].second);\n+      f->refs = 1;\n+\n+      // We arrange to automatically compact this file after\n+      // a certain number of seeks.  Let's assume:\n+      //   (1) One seek costs 10ms\n+      //   (2) Writing or reading 1MB costs 10ms (100MB/s)\n+      //   (3) A compaction of 1MB does 25MB of IO:\n+      //         1MB read from this level\n+      //         10-12MB read from next level (boundaries may be misaligned)\n+      //         10-12MB written to next level\n+      // This implies that 25 seeks cost the same as the compaction\n+      // of 1MB of data.  I.e., one seek costs approximately the\n+      // same as the compaction of 40KB of data.  We are a little\n+      // conservative and allow approximately one seek for every 16KB\n+      // of data before triggering a compaction.\n+      f->allowed_seeks = (f->file_size / 16384);\n+      if (f->allowed_seeks < 100) f->allowed_seeks = 100;\n+\n+      levels_[level].deleted_files.erase(f->number);\n+      levels_[level].added_files->insert(f);\n+    }\n+  }\n+\n+  // Save the current state in *v.\n+  void SaveTo(Version* v) {\n+    BySmallestKey cmp;\n+    cmp.internal_comparator = &vset_->icmp_;\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      // Merge the set of added files with the set of pre-existing files.\n+      // Drop any deleted files.  Store the result in *v.\n+      const std::vector<FileMetaData*>& base_files = base_->files_[level];\n+      std::vector<FileMetaData*>::const_iterator base_iter = base_files.begin();\n+      std::vector<FileMetaData*>::const_iterator base_end = base_files.end();\n+      const FileSet* added = levels_[level].added_files;\n+      v->files_[level].reserve(base_files.size() + added->size());\n+      for (FileSet::const_iterator added_iter = added->begin();\n+           added_iter != added->end();\n+           ++added_iter) {\n+        // Add all smaller files listed in base_\n+        for (std::vector<FileMetaData*>::const_iterator bpos\n+                 = std::upper_bound(base_iter, base_end, *added_iter, cmp);\n+             base_iter != bpos;\n+             ++base_iter) {\n+          MaybeAddFile(v, level, *base_iter);\n+        }\n+\n+        MaybeAddFile(v, level, *added_iter);\n+      }\n+\n+      // Add remaining base files\n+      for (; base_iter != base_end; ++base_iter) {\n+        MaybeAddFile(v, level, *base_iter);\n+      }\n+\n+#ifndef NDEBUG\n+      // Make sure there is no overlap in levels > 0\n+      if (level > 0) {\n+        for (uint32_t i = 1; i < v->files_[level].size(); i++) {\n+          const InternalKey& prev_end = v->files_[level][i-1]->largest;\n+          const InternalKey& this_begin = v->files_[level][i]->smallest;\n+          if (vset_->icmp_.Compare(prev_end, this_begin) >= 0) {\n+            fprintf(stderr, \"overlapping ranges in same level %s vs. %s\\n\",\n+                    prev_end.DebugString().c_str(),\n+                    this_begin.DebugString().c_str());\n+            abort();\n+          }\n+        }\n+      }\n+#endif\n+    }\n+  }\n+\n+  void MaybeAddFile(Version* v, int level, FileMetaData* f) {\n+    if (levels_[level].deleted_files.count(f->number) > 0) {\n+      // File is deleted: do nothing\n+    } else {\n+      std::vector<FileMetaData*>* files = &v->files_[level];\n+      if (level > 0 && !files->empty()) {\n+        // Must not overlap\n+        assert(vset_->icmp_.Compare((*files)[files->size()-1]->largest,\n+                                    f->smallest) < 0);\n+      }\n+      f->refs++;\n+      files->push_back(f);\n+    }\n+  }\n+};\n+\n+VersionSet::VersionSet(const std::string& dbname,\n+                       const Options* options,\n+                       TableCache* table_cache,\n+                       const InternalKeyComparator* cmp)\n+    : env_(options->env),\n+      dbname_(dbname),\n+      options_(options),\n+      table_cache_(table_cache),\n+      icmp_(*cmp),\n+      next_file_number_(2),\n+      manifest_file_number_(0),  // Filled by Recover()\n+      last_sequence_(0),\n+      log_number_(0),\n+      prev_log_number_(0),\n+      descriptor_file_(NULL),\n+      descriptor_log_(NULL),\n+      dummy_versions_(this),\n+      current_(NULL) {\n+  AppendVersion(new Version(this));\n+}\n+\n+VersionSet::~VersionSet() {\n+  current_->Unref();\n+  assert(dummy_versions_.next_ == &dummy_versions_);  // List must be empty\n+  delete descriptor_log_;\n+  delete descriptor_file_;\n+}\n+\n+void VersionSet::AppendVersion(Version* v) {\n+  // Make \"v\" current\n+  assert(v->refs_ == 0);\n+  assert(v != current_);\n+  if (current_ != NULL) {\n+    current_->Unref();\n+  }\n+  current_ = v;\n+  v->Ref();\n+\n+  // Append to linked list\n+  v->prev_ = dummy_versions_.prev_;\n+  v->next_ = &dummy_versions_;\n+  v->prev_->next_ = v;\n+  v->next_->prev_ = v;\n+}\n+\n+Status VersionSet::LogAndApply(VersionEdit* edit, port::Mutex* mu) {\n+  if (edit->has_log_number_) {\n+    assert(edit->log_number_ >= log_number_);\n+    assert(edit->log_number_ < next_file_number_);\n+  } else {\n+    edit->SetLogNumber(log_number_);\n+  }\n+\n+  if (!edit->has_prev_log_number_) {\n+    edit->SetPrevLogNumber(prev_log_number_);\n+  }\n+\n+  edit->SetNextFile(next_file_number_);\n+  edit->SetLastSequence(last_sequence_);\n+\n+  Version* v = new Version(this);\n+  {\n+    Builder builder(this, current_);\n+    builder.Apply(edit);\n+    builder.SaveTo(v);\n+  }\n+  Finalize(v);\n+\n+  // Initialize new descriptor log file if necessary by creating\n+  // a temporary file that contains a snapshot of the current version.\n+  std::string new_manifest_file;\n+  Status s;\n+  if (descriptor_log_ == NULL) {\n+    // No reason to unlock *mu here since we only hit this path in the\n+    // first call to LogAndApply (when opening the database).\n+    assert(descriptor_file_ == NULL);\n+    new_manifest_file = DescriptorFileName(dbname_, manifest_file_number_);\n+    edit->SetNextFile(next_file_number_);\n+    s = env_->NewWritableFile(new_manifest_file, &descriptor_file_);\n+    if (s.ok()) {\n+      descriptor_log_ = new log::Writer(descriptor_file_);\n+      s = WriteSnapshot(descriptor_log_);\n+    }\n+  }\n+\n+  // Unlock during expensive MANIFEST log write\n+  {\n+    mu->Unlock();\n+\n+    // Write new record to MANIFEST log\n+    if (s.ok()) {\n+      std::string record;\n+      edit->EncodeTo(&record);\n+      s = descriptor_log_->AddRecord(record);\n+      if (s.ok()) {\n+        s = descriptor_file_->Sync();\n+      }\n+      if (!s.ok()) {\n+        Log(options_->info_log, \"MANIFEST write: %s\\n\", s.ToString().c_str());\n+        if (ManifestContains(record)) {\n+          Log(options_->info_log,\n+              \"MANIFEST contains log record despite error; advancing to new \"\n+              \"version to prevent mismatch between in-memory and logged state\");\n+          s = Status::OK();\n+        }\n+      }\n+    }\n+\n+    // If we just created a new descriptor file, install it by writing a\n+    // new CURRENT file that points to it.\n+    if (s.ok() && !new_manifest_file.empty()) {\n+      s = SetCurrentFile(env_, dbname_, manifest_file_number_);\n+      // No need to double-check MANIFEST in case of error since it\n+      // will be discarded below.\n+    }\n+\n+    mu->Lock();\n+  }\n+\n+  // Install the new version\n+  if (s.ok()) {\n+    AppendVersion(v);\n+    log_number_ = edit->log_number_;\n+    prev_log_number_ = edit->prev_log_number_;\n+  } else {\n+    delete v;\n+    if (!new_manifest_file.empty()) {\n+      delete descriptor_log_;\n+      delete descriptor_file_;\n+      descriptor_log_ = NULL;\n+      descriptor_file_ = NULL;\n+      env_->DeleteFile(new_manifest_file);\n+    }\n+  }\n+\n+  return s;\n+}\n+\n+Status VersionSet::Recover() {\n+  struct LogReporter : public log::Reader::Reporter {\n+    Status* status;\n+    virtual void Corruption(size_t bytes, const Status& s) {\n+      if (this->status->ok()) *this->status = s;\n+    }\n+  };\n+\n+  // Read \"CURRENT\" file, which contains a pointer to the current manifest file\n+  std::string current;\n+  Status s = ReadFileToString(env_, CurrentFileName(dbname_), &current);\n+  if (!s.ok()) {\n+    return s;\n+  }\n+  if (current.empty() || current[current.size()-1] != '\\n') {\n+    return Status::Corruption(\"CURRENT file does not end with newline\");\n+  }\n+  current.resize(current.size() - 1);\n+\n+  std::string dscname = dbname_ + \"/\" + current;\n+  SequentialFile* file;\n+  s = env_->NewSequentialFile(dscname, &file);\n+  if (!s.ok()) {\n+    return s;\n+  }\n+\n+  bool have_log_number = false;\n+  bool have_prev_log_number = false;\n+  bool have_next_file = false;\n+  bool have_last_sequence = false;\n+  uint64_t next_file = 0;\n+  uint64_t last_sequence = 0;\n+  uint64_t log_number = 0;\n+  uint64_t prev_log_number = 0;\n+  Builder builder(this, current_);\n+\n+  {\n+    LogReporter reporter;\n+    reporter.status = &s;\n+    log::Reader reader(file, &reporter, true/*checksum*/, 0/*initial_offset*/);\n+    Slice record;\n+    std::string scratch;\n+    while (reader.ReadRecord(&record, &scratch) && s.ok()) {\n+      VersionEdit edit;\n+      s = edit.DecodeFrom(record);\n+      if (s.ok()) {\n+        if (edit.has_comparator_ &&\n+            edit.comparator_ != icmp_.user_comparator()->Name()) {\n+          s = Status::InvalidArgument(\n+              edit.comparator_ + \" does not match existing comparator \",\n+              icmp_.user_comparator()->Name());\n+        }\n+      }\n+\n+      if (s.ok()) {\n+        builder.Apply(&edit);\n+      }\n+\n+      if (edit.has_log_number_) {\n+        log_number = edit.log_number_;\n+        have_log_number = true;\n+      }\n+\n+      if (edit.has_prev_log_number_) {\n+        prev_log_number = edit.prev_log_number_;\n+        have_prev_log_number = true;\n+      }\n+\n+      if (edit.has_next_file_number_) {\n+        next_file = edit.next_file_number_;\n+        have_next_file = true;\n+      }\n+\n+      if (edit.has_last_sequence_) {\n+        last_sequence = edit.last_sequence_;\n+        have_last_sequence = true;\n+      }\n+    }\n+  }\n+  delete file;\n+  file = NULL;\n+\n+  if (s.ok()) {\n+    if (!have_next_file) {\n+      s = Status::Corruption(\"no meta-nextfile entry in descriptor\");\n+    } else if (!have_log_number) {\n+      s = Status::Corruption(\"no meta-lognumber entry in descriptor\");\n+    } else if (!have_last_sequence) {\n+      s = Status::Corruption(\"no last-sequence-number entry in descriptor\");\n+    }\n+\n+    if (!have_prev_log_number) {\n+      prev_log_number = 0;\n+    }\n+\n+    MarkFileNumberUsed(prev_log_number);\n+    MarkFileNumberUsed(log_number);\n+  }\n+\n+  if (s.ok()) {\n+    Version* v = new Version(this);\n+    builder.SaveTo(v);\n+    // Install recovered version\n+    Finalize(v);\n+    AppendVersion(v);\n+    manifest_file_number_ = next_file;\n+    next_file_number_ = next_file + 1;\n+    last_sequence_ = last_sequence;\n+    log_number_ = log_number;\n+    prev_log_number_ = prev_log_number;\n+  }\n+\n+  return s;\n+}\n+\n+void VersionSet::MarkFileNumberUsed(uint64_t number) {\n+  if (next_file_number_ <= number) {\n+    next_file_number_ = number + 1;\n+  }\n+}\n+\n+void VersionSet::Finalize(Version* v) {\n+  // Precomputed best level for next compaction\n+  int best_level = -1;\n+  double best_score = -1;\n+\n+  for (int level = 0; level < config::kNumLevels-1; level++) {\n+    double score;\n+    if (level == 0) {\n+      // We treat level-0 specially by bounding the number of files\n+      // instead of number of bytes for two reasons:\n+      //\n+      // (1) With larger write-buffer sizes, it is nice not to do too\n+      // many level-0 compactions.\n+      //\n+      // (2) The files in level-0 are merged on every read and\n+      // therefore we wish to avoid too many files when the individual\n+      // file size is small (perhaps because of a small write-buffer\n+      // setting, or very high compression ratios, or lots of\n+      // overwrites/deletions).\n+      score = v->files_[level].size() /\n+          static_cast<double>(config::kL0_CompactionTrigger);\n+    } else {\n+      // Compute the ratio of current size to size limit.\n+      const uint64_t level_bytes = TotalFileSize(v->files_[level]);\n+      score = static_cast<double>(level_bytes) / MaxBytesForLevel(level);\n+    }\n+\n+    if (score > best_score) {\n+      best_level = level;\n+      best_score = score;\n+    }\n+  }\n+\n+  v->compaction_level_ = best_level;\n+  v->compaction_score_ = best_score;\n+}\n+\n+Status VersionSet::WriteSnapshot(log::Writer* log) {\n+  // TODO: Break up into multiple records to reduce memory usage on recovery?\n+\n+  // Save metadata\n+  VersionEdit edit;\n+  edit.SetComparatorName(icmp_.user_comparator()->Name());\n+\n+  // Save compaction pointers\n+  for (int level = 0; level < config::kNumLevels; level++) {\n+    if (!compact_pointer_[level].empty()) {\n+      InternalKey key;\n+      key.DecodeFrom(compact_pointer_[level]);\n+      edit.SetCompactPointer(level, key);\n+    }\n+  }\n+\n+  // Save files\n+  for (int level = 0; level < config::kNumLevels; level++) {\n+    const std::vector<FileMetaData*>& files = current_->files_[level];\n+    for (size_t i = 0; i < files.size(); i++) {\n+      const FileMetaData* f = files[i];\n+      edit.AddFile(level, f->number, f->file_size, f->smallest, f->largest);\n+    }\n+  }\n+\n+  std::string record;\n+  edit.EncodeTo(&record);\n+  return log->AddRecord(record);\n+}\n+\n+int VersionSet::NumLevelFiles(int level) const {\n+  assert(level >= 0);\n+  assert(level < config::kNumLevels);\n+  return current_->files_[level].size();\n+}\n+\n+const char* VersionSet::LevelSummary(LevelSummaryStorage* scratch) const {\n+  // Update code if kNumLevels changes\n+  assert(config::kNumLevels == 7);\n+  snprintf(scratch->buffer, sizeof(scratch->buffer),\n+           \"files[ %d %d %d %d %d %d %d ]\",\n+           int(current_->files_[0].size()),\n+           int(current_->files_[1].size()),\n+           int(current_->files_[2].size()),\n+           int(current_->files_[3].size()),\n+           int(current_->files_[4].size()),\n+           int(current_->files_[5].size()),\n+           int(current_->files_[6].size()));\n+  return scratch->buffer;\n+}\n+\n+// Return true iff the manifest contains the specified record.\n+bool VersionSet::ManifestContains(const std::string& record) const {\n+  std::string fname = DescriptorFileName(dbname_, manifest_file_number_);\n+  Log(options_->info_log, \"ManifestContains: checking %s\\n\", fname.c_str());\n+  SequentialFile* file = NULL;\n+  Status s = env_->NewSequentialFile(fname, &file);\n+  if (!s.ok()) {\n+    Log(options_->info_log, \"ManifestContains: %s\\n\", s.ToString().c_str());\n+    return false;\n+  }\n+  log::Reader reader(file, NULL, true/*checksum*/, 0);\n+  Slice r;\n+  std::string scratch;\n+  bool result = false;\n+  while (reader.ReadRecord(&r, &scratch)) {\n+    if (r == Slice(record)) {\n+      result = true;\n+      break;\n+    }\n+  }\n+  delete file;\n+  Log(options_->info_log, \"ManifestContains: result = %d\\n\", result ? 1 : 0);\n+  return result;\n+}\n+\n+uint64_t VersionSet::ApproximateOffsetOf(Version* v, const InternalKey& ikey) {\n+  uint64_t result = 0;\n+  for (int level = 0; level < config::kNumLevels; level++) {\n+    const std::vector<FileMetaData*>& files = v->files_[level];\n+    for (size_t i = 0; i < files.size(); i++) {\n+      if (icmp_.Compare(files[i]->largest, ikey) <= 0) {\n+        // Entire file is before \"ikey\", so just add the file size\n+        result += files[i]->file_size;\n+      } else if (icmp_.Compare(files[i]->smallest, ikey) > 0) {\n+        // Entire file is after \"ikey\", so ignore\n+        if (level > 0) {\n+          // Files other than level 0 are sorted by meta->smallest, so\n+          // no further files in this level will contain data for\n+          // \"ikey\".\n+          break;\n+        }\n+      } else {\n+        // \"ikey\" falls in the range for this table.  Add the\n+        // approximate offset of \"ikey\" within the table.\n+        Table* tableptr;\n+        Iterator* iter = table_cache_->NewIterator(\n+            ReadOptions(), files[i]->number, files[i]->file_size, &tableptr);\n+        if (tableptr != NULL) {\n+          result += tableptr->ApproximateOffsetOf(ikey.Encode());\n+        }\n+        delete iter;\n+      }\n+    }\n+  }\n+  return result;\n+}\n+\n+void VersionSet::AddLiveFiles(std::set<uint64_t>* live) {\n+  for (Version* v = dummy_versions_.next_;\n+       v != &dummy_versions_;\n+       v = v->next_) {\n+    for (int level = 0; level < config::kNumLevels; level++) {\n+      const std::vector<FileMetaData*>& files = v->files_[level];\n+      for (size_t i = 0; i < files.size(); i++) {\n+        live->insert(files[i]->number);\n+      }\n+    }\n+  }\n+}\n+\n+int64_t VersionSet::NumLevelBytes(int level) const {\n+  assert(level >= 0);\n+  assert(level < config::kNumLevels);\n+  return TotalFileSize(current_->files_[level]);\n+}\n+\n+int64_t VersionSet::MaxNextLevelOverlappingBytes() {\n+  int64_t result = 0;\n+  std::vector<FileMetaData*> overlaps;\n+  for (int level = 1; level < config::kNumLevels - 1; level++) {\n+    for (size_t i = 0; i < current_->files_[level].size(); i++) {\n+      const FileMetaData* f = current_->files_[level][i];\n+      current_->GetOverlappingInputs(level+1, &f->smallest, &f->largest,\n+                                     &overlaps);\n+      const int64_t sum = TotalFileSize(overlaps);\n+      if (sum > result) {\n+        result = sum;\n+      }\n+    }\n+  }\n+  return result;\n+}\n+\n+// Stores the minimal range that covers all entries in inputs in\n+// *smallest, *largest.\n+// REQUIRES: inputs is not empty\n+void VersionSet::GetRange(const std::vector<FileMetaData*>& inputs,\n+                          InternalKey* smallest,\n+                          InternalKey* largest) {\n+  assert(!inputs.empty());\n+  smallest->Clear();\n+  largest->Clear();\n+  for (size_t i = 0; i < inputs.size(); i++) {\n+    FileMetaData* f = inputs[i];\n+    if (i == 0) {\n+      *smallest = f->smallest;\n+      *largest = f->largest;\n+    } else {\n+      if (icmp_.Compare(f->smallest, *smallest) < 0) {\n+        *smallest = f->smallest;\n+      }\n+      if (icmp_.Compare(f->largest, *largest) > 0) {\n+        *largest = f->largest;\n+      }\n+    }\n+  }\n+}\n+\n+// Stores the minimal range that covers all entries in inputs1 and inputs2\n+// in *smallest, *largest.\n+// REQUIRES: inputs is not empty\n+void VersionSet::GetRange2(const std::vector<FileMetaData*>& inputs1,\n+                           const std::vector<FileMetaData*>& inputs2,\n+                           InternalKey* smallest,\n+                           InternalKey* largest) {\n+  std::vector<FileMetaData*> all = inputs1;\n+  all.insert(all.end(), inputs2.begin(), inputs2.end());\n+  GetRange(all, smallest, largest);\n+}\n+\n+Iterator* VersionSet::MakeInputIterator(Compaction* c) {\n+  ReadOptions options;\n+  options.verify_checksums = options_->paranoid_checks;\n+  options.fill_cache = false;\n+\n+  // Level-0 files have to be merged together.  For other levels,\n+  // we will make a concatenating iterator per level.\n+  // TODO(opt): use concatenating iterator for level-0 if there is no overlap\n+  const int space = (c->level() == 0 ? c->inputs_[0].size() + 1 : 2);\n+  Iterator** list = new Iterator*[space];\n+  int num = 0;\n+  for (int which = 0; which < 2; which++) {\n+    if (!c->inputs_[which].empty()) {\n+      if (c->level() + which == 0) {\n+        const std::vector<FileMetaData*>& files = c->inputs_[which];\n+        for (size_t i = 0; i < files.size(); i++) {\n+          list[num++] = table_cache_->NewIterator(\n+              options, files[i]->number, files[i]->file_size);\n+        }\n+      } else {\n+        // Create concatenating iterator for the files from this level\n+        list[num++] = NewTwoLevelIterator(\n+            new Version::LevelFileNumIterator(icmp_, &c->inputs_[which]),\n+            &GetFileIterator, table_cache_, options);\n+      }\n+    }\n+  }\n+  assert(num <= space);\n+  Iterator* result = NewMergingIterator(&icmp_, list, num);\n+  delete[] list;\n+  return result;\n+}\n+\n+Compaction* VersionSet::PickCompaction() {\n+  Compaction* c;\n+  int level;\n+\n+  // We prefer compactions triggered by too much data in a level over\n+  // the compactions triggered by seeks.\n+  const bool size_compaction = (current_->compaction_score_ >= 1);\n+  const bool seek_compaction = (current_->file_to_compact_ != NULL);\n+  if (size_compaction) {\n+    level = current_->compaction_level_;\n+    assert(level >= 0);\n+    assert(level+1 < config::kNumLevels);\n+    c = new Compaction(level);\n+\n+    // Pick the first file that comes after compact_pointer_[level]\n+    for (size_t i = 0; i < current_->files_[level].size(); i++) {\n+      FileMetaData* f = current_->files_[level][i];\n+      if (compact_pointer_[level].empty() ||\n+          icmp_.Compare(f->largest.Encode(), compact_pointer_[level]) > 0) {\n+        c->inputs_[0].push_back(f);\n+        break;\n+      }\n+    }\n+    if (c->inputs_[0].empty()) {\n+      // Wrap-around to the beginning of the key space\n+      c->inputs_[0].push_back(current_->files_[level][0]);\n+    }\n+  } else if (seek_compaction) {\n+    level = current_->file_to_compact_level_;\n+    c = new Compaction(level);\n+    c->inputs_[0].push_back(current_->file_to_compact_);\n+  } else {\n+    return NULL;\n+  }\n+\n+  c->input_version_ = current_;\n+  c->input_version_->Ref();\n+\n+  // Files in level 0 may overlap each other, so pick up all overlapping ones\n+  if (level == 0) {\n+    InternalKey smallest, largest;\n+    GetRange(c->inputs_[0], &smallest, &largest);\n+    // Note that the next call will discard the file we placed in\n+    // c->inputs_[0] earlier and replace it with an overlapping set\n+    // which will include the picked file.\n+    current_->GetOverlappingInputs(0, &smallest, &largest, &c->inputs_[0]);\n+    assert(!c->inputs_[0].empty());\n+  }\n+\n+  SetupOtherInputs(c);\n+\n+  return c;\n+}\n+\n+void VersionSet::SetupOtherInputs(Compaction* c) {\n+  const int level = c->level();\n+  InternalKey smallest, largest;\n+  GetRange(c->inputs_[0], &smallest, &largest);\n+\n+  current_->GetOverlappingInputs(level+1, &smallest, &largest, &c->inputs_[1]);\n+\n+  // Get entire range covered by compaction\n+  InternalKey all_start, all_limit;\n+  GetRange2(c->inputs_[0], c->inputs_[1], &all_start, &all_limit);\n+\n+  // See if we can grow the number of inputs in \"level\" without\n+  // changing the number of \"level+1\" files we pick up.\n+  if (!c->inputs_[1].empty()) {\n+    std::vector<FileMetaData*> expanded0;\n+    current_->GetOverlappingInputs(level, &all_start, &all_limit, &expanded0);\n+    const int64_t inputs0_size = TotalFileSize(c->inputs_[0]);\n+    const int64_t inputs1_size = TotalFileSize(c->inputs_[1]);\n+    const int64_t expanded0_size = TotalFileSize(expanded0);\n+    if (expanded0.size() > c->inputs_[0].size() &&\n+        inputs1_size + expanded0_size < kExpandedCompactionByteSizeLimit) {\n+      InternalKey new_start, new_limit;\n+      GetRange(expanded0, &new_start, &new_limit);\n+      std::vector<FileMetaData*> expanded1;\n+      current_->GetOverlappingInputs(level+1, &new_start, &new_limit,\n+                                     &expanded1);\n+      if (expanded1.size() == c->inputs_[1].size()) {\n+        Log(options_->info_log,\n+            \"Expanding@%d %d+%d (%ld+%ld bytes) to %d+%d (%ld+%ld bytes)\\n\",\n+            level,\n+            int(c->inputs_[0].size()),\n+            int(c->inputs_[1].size()),\n+            long(inputs0_size), long(inputs1_size),\n+            int(expanded0.size()),\n+            int(expanded1.size()),\n+            long(expanded0_size), long(inputs1_size));\n+        smallest = new_start;\n+        largest = new_limit;\n+        c->inputs_[0] = expanded0;\n+        c->inputs_[1] = expanded1;\n+        GetRange2(c->inputs_[0], c->inputs_[1], &all_start, &all_limit);\n+      }\n+    }\n+  }\n+\n+  // Compute the set of grandparent files that overlap this compaction\n+  // (parent == level+1; grandparent == level+2)\n+  if (level + 2 < config::kNumLevels) {\n+    current_->GetOverlappingInputs(level + 2, &all_start, &all_limit,\n+                                   &c->grandparents_);\n+  }\n+\n+  if (false) {\n+    Log(options_->info_log, \"Compacting %d '%s' .. '%s'\",\n+        level,\n+        smallest.DebugString().c_str(),\n+        largest.DebugString().c_str());\n+  }\n+\n+  // Update the place where we will do the next compaction for this level.\n+  // We update this immediately instead of waiting for the VersionEdit\n+  // to be applied so that if the compaction fails, we will try a different\n+  // key range next time.\n+  compact_pointer_[level] = largest.Encode().ToString();\n+  c->edit_.SetCompactPointer(level, largest);\n+}\n+\n+Compaction* VersionSet::CompactRange(\n+    int level,\n+    const InternalKey* begin,\n+    const InternalKey* end) {\n+  std::vector<FileMetaData*> inputs;\n+  current_->GetOverlappingInputs(level, begin, end, &inputs);\n+  if (inputs.empty()) {\n+    return NULL;\n+  }\n+\n+  // Avoid compacting too much in one shot in case the range is large.\n+  // But we cannot do this for level-0 since level-0 files can overlap\n+  // and we must not pick one file and drop another older file if the\n+  // two files overlap.\n+  if (level > 0) {\n+    const uint64_t limit = MaxFileSizeForLevel(level);\n+    uint64_t total = 0;\n+    for (size_t i = 0; i < inputs.size(); i++) {\n+      uint64_t s = inputs[i]->file_size;\n+      total += s;\n+      if (total >= limit) {\n+        inputs.resize(i + 1);\n+        break;\n+      }\n+    }\n+  }\n+\n+  Compaction* c = new Compaction(level);\n+  c->input_version_ = current_;\n+  c->input_version_->Ref();\n+  c->inputs_[0] = inputs;\n+  SetupOtherInputs(c);\n+  return c;\n+}\n+\n+Compaction::Compaction(int level)\n+    : level_(level),\n+      max_output_file_size_(MaxFileSizeForLevel(level)),\n+      input_version_(NULL),\n+      grandparent_index_(0),\n+      seen_key_(false),\n+      overlapped_bytes_(0) {\n+  for (int i = 0; i < config::kNumLevels; i++) {\n+    level_ptrs_[i] = 0;\n+  }\n+}\n+\n+Compaction::~Compaction() {\n+  if (input_version_ != NULL) {\n+    input_version_->Unref();\n+  }\n+}\n+\n+bool Compaction::IsTrivialMove() const {\n+  // Avoid a move if there is lots of overlapping grandparent data.\n+  // Otherwise, the move could create a parent file that will require\n+  // a very expensive merge later on.\n+  return (num_input_files(0) == 1 &&\n+          num_input_files(1) == 0 &&\n+          TotalFileSize(grandparents_) <= kMaxGrandParentOverlapBytes);\n+}\n+\n+void Compaction::AddInputDeletions(VersionEdit* edit) {\n+  for (int which = 0; which < 2; which++) {\n+    for (size_t i = 0; i < inputs_[which].size(); i++) {\n+      edit->DeleteFile(level_ + which, inputs_[which][i]->number);\n+    }\n+  }\n+}\n+\n+bool Compaction::IsBaseLevelForKey(const Slice& user_key) {\n+  // Maybe use binary search to find right entry instead of linear search?\n+  const Comparator* user_cmp = input_version_->vset_->icmp_.user_comparator();\n+  for (int lvl = level_ + 2; lvl < config::kNumLevels; lvl++) {\n+    const std::vector<FileMetaData*>& files = input_version_->files_[lvl];\n+    for (; level_ptrs_[lvl] < files.size(); ) {\n+      FileMetaData* f = files[level_ptrs_[lvl]];\n+      if (user_cmp->Compare(user_key, f->largest.user_key()) <= 0) {\n+        // We've advanced far enough\n+        if (user_cmp->Compare(user_key, f->smallest.user_key()) >= 0) {\n+          // Key falls in this file's range, so definitely not base level\n+          return false;\n+        }\n+        break;\n+      }\n+      level_ptrs_[lvl]++;\n+    }\n+  }\n+  return true;\n+}\n+\n+bool Compaction::ShouldStopBefore(const Slice& internal_key) {\n+  // Scan to find earliest grandparent file that contains key.\n+  const InternalKeyComparator* icmp = &input_version_->vset_->icmp_;\n+  while (grandparent_index_ < grandparents_.size() &&\n+      icmp->Compare(internal_key,\n+                    grandparents_[grandparent_index_]->largest.Encode()) > 0) {\n+    if (seen_key_) {\n+      overlapped_bytes_ += grandparents_[grandparent_index_]->file_size;\n+    }\n+    grandparent_index_++;\n+  }\n+  seen_key_ = true;\n+\n+  if (overlapped_bytes_ > kMaxGrandParentOverlapBytes) {\n+    // Too much overlap for current output; start new output\n+    overlapped_bytes_ = 0;\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+void Compaction::ReleaseInputs() {\n+  if (input_version_ != NULL) {\n+    input_version_->Unref();\n+    input_version_ = NULL;\n+  }\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "9d084fdb7d00574c627291b1a48e059a8093d5b4",
        "filename": "src/leveldb/db/version_set.h",
        "status": "added",
        "additions": 383,
        "deletions": 0,
        "changes": 383,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/version_set.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/version_set.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_set.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,383 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// The representation of a DBImpl consists of a set of Versions.  The\n+// newest version is called \"current\".  Older versions may be kept\n+// around to provide a consistent view to live iterators.\n+//\n+// Each Version keeps track of a set of Table files per level.  The\n+// entire set of versions is maintained in a VersionSet.\n+//\n+// Version,VersionSet are thread-compatible, but require external\n+// synchronization on all accesses.\n+\n+#ifndef STORAGE_LEVELDB_DB_VERSION_SET_H_\n+#define STORAGE_LEVELDB_DB_VERSION_SET_H_\n+\n+#include <map>\n+#include <set>\n+#include <vector>\n+#include \"db/dbformat.h\"\n+#include \"db/version_edit.h\"\n+#include \"port/port.h\"\n+#include \"port/thread_annotations.h\"\n+\n+namespace leveldb {\n+\n+namespace log { class Writer; }\n+\n+class Compaction;\n+class Iterator;\n+class MemTable;\n+class TableBuilder;\n+class TableCache;\n+class Version;\n+class VersionSet;\n+class WritableFile;\n+\n+// Return the smallest index i such that files[i]->largest >= key.\n+// Return files.size() if there is no such file.\n+// REQUIRES: \"files\" contains a sorted list of non-overlapping files.\n+extern int FindFile(const InternalKeyComparator& icmp,\n+                    const std::vector<FileMetaData*>& files,\n+                    const Slice& key);\n+\n+// Returns true iff some file in \"files\" overlaps the user key range\n+// [*smallest,*largest].\n+// smallest==NULL represents a key smaller than all keys in the DB.\n+// largest==NULL represents a key largest than all keys in the DB.\n+// REQUIRES: If disjoint_sorted_files, files[] contains disjoint ranges\n+//           in sorted order.\n+extern bool SomeFileOverlapsRange(\n+    const InternalKeyComparator& icmp,\n+    bool disjoint_sorted_files,\n+    const std::vector<FileMetaData*>& files,\n+    const Slice* smallest_user_key,\n+    const Slice* largest_user_key);\n+\n+class Version {\n+ public:\n+  // Append to *iters a sequence of iterators that will\n+  // yield the contents of this Version when merged together.\n+  // REQUIRES: This version has been saved (see VersionSet::SaveTo)\n+  void AddIterators(const ReadOptions&, std::vector<Iterator*>* iters);\n+\n+  // Lookup the value for key.  If found, store it in *val and\n+  // return OK.  Else return a non-OK status.  Fills *stats.\n+  // REQUIRES: lock is not held\n+  struct GetStats {\n+    FileMetaData* seek_file;\n+    int seek_file_level;\n+  };\n+  Status Get(const ReadOptions&, const LookupKey& key, std::string* val,\n+             GetStats* stats);\n+\n+  // Adds \"stats\" into the current state.  Returns true if a new\n+  // compaction may need to be triggered, false otherwise.\n+  // REQUIRES: lock is held\n+  bool UpdateStats(const GetStats& stats);\n+\n+  // Reference count management (so Versions do not disappear out from\n+  // under live iterators)\n+  void Ref();\n+  void Unref();\n+\n+  void GetOverlappingInputs(\n+      int level,\n+      const InternalKey* begin,         // NULL means before all keys\n+      const InternalKey* end,           // NULL means after all keys\n+      std::vector<FileMetaData*>* inputs);\n+\n+  // Returns true iff some file in the specified level overlaps\n+  // some part of [*smallest_user_key,*largest_user_key].\n+  // smallest_user_key==NULL represents a key smaller than all keys in the DB.\n+  // largest_user_key==NULL represents a key largest than all keys in the DB.\n+  bool OverlapInLevel(int level,\n+                      const Slice* smallest_user_key,\n+                      const Slice* largest_user_key);\n+\n+  // Return the level at which we should place a new memtable compaction\n+  // result that covers the range [smallest_user_key,largest_user_key].\n+  int PickLevelForMemTableOutput(const Slice& smallest_user_key,\n+                                 const Slice& largest_user_key);\n+\n+  int NumFiles(int level) const { return files_[level].size(); }\n+\n+  // Return a human readable string that describes this version's contents.\n+  std::string DebugString() const;\n+\n+ private:\n+  friend class Compaction;\n+  friend class VersionSet;\n+\n+  class LevelFileNumIterator;\n+  Iterator* NewConcatenatingIterator(const ReadOptions&, int level) const;\n+\n+  VersionSet* vset_;            // VersionSet to which this Version belongs\n+  Version* next_;               // Next version in linked list\n+  Version* prev_;               // Previous version in linked list\n+  int refs_;                    // Number of live refs to this version\n+\n+  // List of files per level\n+  std::vector<FileMetaData*> files_[config::kNumLevels];\n+\n+  // Next file to compact based on seek stats.\n+  FileMetaData* file_to_compact_;\n+  int file_to_compact_level_;\n+\n+  // Level that should be compacted next and its compaction score.\n+  // Score < 1 means compaction is not strictly needed.  These fields\n+  // are initialized by Finalize().\n+  double compaction_score_;\n+  int compaction_level_;\n+\n+  explicit Version(VersionSet* vset)\n+      : vset_(vset), next_(this), prev_(this), refs_(0),\n+        file_to_compact_(NULL),\n+        file_to_compact_level_(-1),\n+        compaction_score_(-1),\n+        compaction_level_(-1) {\n+  }\n+\n+  ~Version();\n+\n+  // No copying allowed\n+  Version(const Version&);\n+  void operator=(const Version&);\n+};\n+\n+class VersionSet {\n+ public:\n+  VersionSet(const std::string& dbname,\n+             const Options* options,\n+             TableCache* table_cache,\n+             const InternalKeyComparator*);\n+  ~VersionSet();\n+\n+  // Apply *edit to the current version to form a new descriptor that\n+  // is both saved to persistent state and installed as the new\n+  // current version.  Will release *mu while actually writing to the file.\n+  // REQUIRES: *mu is held on entry.\n+  // REQUIRES: no other thread concurrently calls LogAndApply()\n+  Status LogAndApply(VersionEdit* edit, port::Mutex* mu)\n+      EXCLUSIVE_LOCKS_REQUIRED(mu);\n+\n+  // Recover the last saved descriptor from persistent storage.\n+  Status Recover();\n+\n+  // Return the current version.\n+  Version* current() const { return current_; }\n+\n+  // Return the current manifest file number\n+  uint64_t ManifestFileNumber() const { return manifest_file_number_; }\n+\n+  // Allocate and return a new file number\n+  uint64_t NewFileNumber() { return next_file_number_++; }\n+\n+  // Arrange to reuse \"file_number\" unless a newer file number has\n+  // already been allocated.\n+  // REQUIRES: \"file_number\" was returned by a call to NewFileNumber().\n+  void ReuseFileNumber(uint64_t file_number) {\n+    if (next_file_number_ == file_number + 1) {\n+      next_file_number_ = file_number;\n+    }\n+  }\n+\n+  // Return the number of Table files at the specified level.\n+  int NumLevelFiles(int level) const;\n+\n+  // Return the combined file size of all files at the specified level.\n+  int64_t NumLevelBytes(int level) const;\n+\n+  // Return the last sequence number.\n+  uint64_t LastSequence() const { return last_sequence_; }\n+\n+  // Set the last sequence number to s.\n+  void SetLastSequence(uint64_t s) {\n+    assert(s >= last_sequence_);\n+    last_sequence_ = s;\n+  }\n+\n+  // Mark the specified file number as used.\n+  void MarkFileNumberUsed(uint64_t number);\n+\n+  // Return the current log file number.\n+  uint64_t LogNumber() const { return log_number_; }\n+\n+  // Return the log file number for the log file that is currently\n+  // being compacted, or zero if there is no such log file.\n+  uint64_t PrevLogNumber() const { return prev_log_number_; }\n+\n+  // Pick level and inputs for a new compaction.\n+  // Returns NULL if there is no compaction to be done.\n+  // Otherwise returns a pointer to a heap-allocated object that\n+  // describes the compaction.  Caller should delete the result.\n+  Compaction* PickCompaction();\n+\n+  // Return a compaction object for compacting the range [begin,end] in\n+  // the specified level.  Returns NULL if there is nothing in that\n+  // level that overlaps the specified range.  Caller should delete\n+  // the result.\n+  Compaction* CompactRange(\n+      int level,\n+      const InternalKey* begin,\n+      const InternalKey* end);\n+\n+  // Return the maximum overlapping data (in bytes) at next level for any\n+  // file at a level >= 1.\n+  int64_t MaxNextLevelOverlappingBytes();\n+\n+  // Create an iterator that reads over the compaction inputs for \"*c\".\n+  // The caller should delete the iterator when no longer needed.\n+  Iterator* MakeInputIterator(Compaction* c);\n+\n+  // Returns true iff some level needs a compaction.\n+  bool NeedsCompaction() const {\n+    Version* v = current_;\n+    return (v->compaction_score_ >= 1) || (v->file_to_compact_ != NULL);\n+  }\n+\n+  // Add all files listed in any live version to *live.\n+  // May also mutate some internal state.\n+  void AddLiveFiles(std::set<uint64_t>* live);\n+\n+  // Return the approximate offset in the database of the data for\n+  // \"key\" as of version \"v\".\n+  uint64_t ApproximateOffsetOf(Version* v, const InternalKey& key);\n+\n+  // Return a human-readable short (single-line) summary of the number\n+  // of files per level.  Uses *scratch as backing store.\n+  struct LevelSummaryStorage {\n+    char buffer[100];\n+  };\n+  const char* LevelSummary(LevelSummaryStorage* scratch) const;\n+\n+ private:\n+  class Builder;\n+\n+  friend class Compaction;\n+  friend class Version;\n+\n+  void Finalize(Version* v);\n+\n+  void GetRange(const std::vector<FileMetaData*>& inputs,\n+                InternalKey* smallest,\n+                InternalKey* largest);\n+\n+  void GetRange2(const std::vector<FileMetaData*>& inputs1,\n+                 const std::vector<FileMetaData*>& inputs2,\n+                 InternalKey* smallest,\n+                 InternalKey* largest);\n+\n+  void SetupOtherInputs(Compaction* c);\n+\n+  // Save current contents to *log\n+  Status WriteSnapshot(log::Writer* log);\n+\n+  void AppendVersion(Version* v);\n+\n+  bool ManifestContains(const std::string& record) const;\n+\n+  Env* const env_;\n+  const std::string dbname_;\n+  const Options* const options_;\n+  TableCache* const table_cache_;\n+  const InternalKeyComparator icmp_;\n+  uint64_t next_file_number_;\n+  uint64_t manifest_file_number_;\n+  uint64_t last_sequence_;\n+  uint64_t log_number_;\n+  uint64_t prev_log_number_;  // 0 or backing store for memtable being compacted\n+\n+  // Opened lazily\n+  WritableFile* descriptor_file_;\n+  log::Writer* descriptor_log_;\n+  Version dummy_versions_;  // Head of circular doubly-linked list of versions.\n+  Version* current_;        // == dummy_versions_.prev_\n+\n+  // Per-level key at which the next compaction at that level should start.\n+  // Either an empty string, or a valid InternalKey.\n+  std::string compact_pointer_[config::kNumLevels];\n+\n+  // No copying allowed\n+  VersionSet(const VersionSet&);\n+  void operator=(const VersionSet&);\n+};\n+\n+// A Compaction encapsulates information about a compaction.\n+class Compaction {\n+ public:\n+  ~Compaction();\n+\n+  // Return the level that is being compacted.  Inputs from \"level\"\n+  // and \"level+1\" will be merged to produce a set of \"level+1\" files.\n+  int level() const { return level_; }\n+\n+  // Return the object that holds the edits to the descriptor done\n+  // by this compaction.\n+  VersionEdit* edit() { return &edit_; }\n+\n+  // \"which\" must be either 0 or 1\n+  int num_input_files(int which) const { return inputs_[which].size(); }\n+\n+  // Return the ith input file at \"level()+which\" (\"which\" must be 0 or 1).\n+  FileMetaData* input(int which, int i) const { return inputs_[which][i]; }\n+\n+  // Maximum size of files to build during this compaction.\n+  uint64_t MaxOutputFileSize() const { return max_output_file_size_; }\n+\n+  // Is this a trivial compaction that can be implemented by just\n+  // moving a single input file to the next level (no merging or splitting)\n+  bool IsTrivialMove() const;\n+\n+  // Add all inputs to this compaction as delete operations to *edit.\n+  void AddInputDeletions(VersionEdit* edit);\n+\n+  // Returns true if the information we have available guarantees that\n+  // the compaction is producing data in \"level+1\" for which no data exists\n+  // in levels greater than \"level+1\".\n+  bool IsBaseLevelForKey(const Slice& user_key);\n+\n+  // Returns true iff we should stop building the current output\n+  // before processing \"internal_key\".\n+  bool ShouldStopBefore(const Slice& internal_key);\n+\n+  // Release the input version for the compaction, once the compaction\n+  // is successful.\n+  void ReleaseInputs();\n+\n+ private:\n+  friend class Version;\n+  friend class VersionSet;\n+\n+  explicit Compaction(int level);\n+\n+  int level_;\n+  uint64_t max_output_file_size_;\n+  Version* input_version_;\n+  VersionEdit edit_;\n+\n+  // Each compaction reads inputs from \"level_\" and \"level_+1\"\n+  std::vector<FileMetaData*> inputs_[2];      // The two sets of inputs\n+\n+  // State used to check for number of of overlapping grandparent files\n+  // (parent == level_ + 1, grandparent == level_ + 2)\n+  std::vector<FileMetaData*> grandparents_;\n+  size_t grandparent_index_;  // Index in grandparent_starts_\n+  bool seen_key_;             // Some output key has been seen\n+  int64_t overlapped_bytes_;  // Bytes of overlap between current output\n+                              // and grandparent files\n+\n+  // State for implementing IsBaseLevelForKey\n+\n+  // level_ptrs_ holds indices into input_version_->levels_: our state\n+  // is that we are positioned at one of the file ranges for each\n+  // higher level than the ones involved in this compaction (i.e. for\n+  // all L >= level_ + 2).\n+  size_t level_ptrs_[config::kNumLevels];\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_DB_VERSION_SET_H_"
      },
      {
        "sha": "501e34d1337d3917185b70369d9982db54e787aa",
        "filename": "src/leveldb/db/version_set_test.cc",
        "status": "added",
        "additions": 179,
        "deletions": 0,
        "changes": 179,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/version_set_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/version_set_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_set_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,179 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"db/version_set.h\"\n+#include \"util/logging.h\"\n+#include \"util/testharness.h\"\n+#include \"util/testutil.h\"\n+\n+namespace leveldb {\n+\n+class FindFileTest {\n+ public:\n+  std::vector<FileMetaData*> files_;\n+  bool disjoint_sorted_files_;\n+\n+  FindFileTest() : disjoint_sorted_files_(true) { }\n+\n+  ~FindFileTest() {\n+    for (int i = 0; i < files_.size(); i++) {\n+      delete files_[i];\n+    }\n+  }\n+\n+  void Add(const char* smallest, const char* largest,\n+           SequenceNumber smallest_seq = 100,\n+           SequenceNumber largest_seq = 100) {\n+    FileMetaData* f = new FileMetaData;\n+    f->number = files_.size() + 1;\n+    f->smallest = InternalKey(smallest, smallest_seq, kTypeValue);\n+    f->largest = InternalKey(largest, largest_seq, kTypeValue);\n+    files_.push_back(f);\n+  }\n+\n+  int Find(const char* key) {\n+    InternalKey target(key, 100, kTypeValue);\n+    InternalKeyComparator cmp(BytewiseComparator());\n+    return FindFile(cmp, files_, target.Encode());\n+  }\n+\n+  bool Overlaps(const char* smallest, const char* largest) {\n+    InternalKeyComparator cmp(BytewiseComparator());\n+    Slice s(smallest != NULL ? smallest : \"\");\n+    Slice l(largest != NULL ? largest : \"\");\n+    return SomeFileOverlapsRange(cmp, disjoint_sorted_files_, files_,\n+                                 (smallest != NULL ? &s : NULL),\n+                                 (largest != NULL ? &l : NULL));\n+  }\n+};\n+\n+TEST(FindFileTest, Empty) {\n+  ASSERT_EQ(0, Find(\"foo\"));\n+  ASSERT_TRUE(! Overlaps(\"a\", \"z\"));\n+  ASSERT_TRUE(! Overlaps(NULL, \"z\"));\n+  ASSERT_TRUE(! Overlaps(\"a\", NULL));\n+  ASSERT_TRUE(! Overlaps(NULL, NULL));\n+}\n+\n+TEST(FindFileTest, Single) {\n+  Add(\"p\", \"q\");\n+  ASSERT_EQ(0, Find(\"a\"));\n+  ASSERT_EQ(0, Find(\"p\"));\n+  ASSERT_EQ(0, Find(\"p1\"));\n+  ASSERT_EQ(0, Find(\"q\"));\n+  ASSERT_EQ(1, Find(\"q1\"));\n+  ASSERT_EQ(1, Find(\"z\"));\n+\n+  ASSERT_TRUE(! Overlaps(\"a\", \"b\"));\n+  ASSERT_TRUE(! Overlaps(\"z1\", \"z2\"));\n+  ASSERT_TRUE(Overlaps(\"a\", \"p\"));\n+  ASSERT_TRUE(Overlaps(\"a\", \"q\"));\n+  ASSERT_TRUE(Overlaps(\"a\", \"z\"));\n+  ASSERT_TRUE(Overlaps(\"p\", \"p1\"));\n+  ASSERT_TRUE(Overlaps(\"p\", \"q\"));\n+  ASSERT_TRUE(Overlaps(\"p\", \"z\"));\n+  ASSERT_TRUE(Overlaps(\"p1\", \"p2\"));\n+  ASSERT_TRUE(Overlaps(\"p1\", \"z\"));\n+  ASSERT_TRUE(Overlaps(\"q\", \"q\"));\n+  ASSERT_TRUE(Overlaps(\"q\", \"q1\"));\n+\n+  ASSERT_TRUE(! Overlaps(NULL, \"j\"));\n+  ASSERT_TRUE(! Overlaps(\"r\", NULL));\n+  ASSERT_TRUE(Overlaps(NULL, \"p\"));\n+  ASSERT_TRUE(Overlaps(NULL, \"p1\"));\n+  ASSERT_TRUE(Overlaps(\"q\", NULL));\n+  ASSERT_TRUE(Overlaps(NULL, NULL));\n+}\n+\n+\n+TEST(FindFileTest, Multiple) {\n+  Add(\"150\", \"200\");\n+  Add(\"200\", \"250\");\n+  Add(\"300\", \"350\");\n+  Add(\"400\", \"450\");\n+  ASSERT_EQ(0, Find(\"100\"));\n+  ASSERT_EQ(0, Find(\"150\"));\n+  ASSERT_EQ(0, Find(\"151\"));\n+  ASSERT_EQ(0, Find(\"199\"));\n+  ASSERT_EQ(0, Find(\"200\"));\n+  ASSERT_EQ(1, Find(\"201\"));\n+  ASSERT_EQ(1, Find(\"249\"));\n+  ASSERT_EQ(1, Find(\"250\"));\n+  ASSERT_EQ(2, Find(\"251\"));\n+  ASSERT_EQ(2, Find(\"299\"));\n+  ASSERT_EQ(2, Find(\"300\"));\n+  ASSERT_EQ(2, Find(\"349\"));\n+  ASSERT_EQ(2, Find(\"350\"));\n+  ASSERT_EQ(3, Find(\"351\"));\n+  ASSERT_EQ(3, Find(\"400\"));\n+  ASSERT_EQ(3, Find(\"450\"));\n+  ASSERT_EQ(4, Find(\"451\"));\n+\n+  ASSERT_TRUE(! Overlaps(\"100\", \"149\"));\n+  ASSERT_TRUE(! Overlaps(\"251\", \"299\"));\n+  ASSERT_TRUE(! Overlaps(\"451\", \"500\"));\n+  ASSERT_TRUE(! Overlaps(\"351\", \"399\"));\n+\n+  ASSERT_TRUE(Overlaps(\"100\", \"150\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"200\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"300\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"400\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"500\"));\n+  ASSERT_TRUE(Overlaps(\"375\", \"400\"));\n+  ASSERT_TRUE(Overlaps(\"450\", \"450\"));\n+  ASSERT_TRUE(Overlaps(\"450\", \"500\"));\n+}\n+\n+TEST(FindFileTest, MultipleNullBoundaries) {\n+  Add(\"150\", \"200\");\n+  Add(\"200\", \"250\");\n+  Add(\"300\", \"350\");\n+  Add(\"400\", \"450\");\n+  ASSERT_TRUE(! Overlaps(NULL, \"149\"));\n+  ASSERT_TRUE(! Overlaps(\"451\", NULL));\n+  ASSERT_TRUE(Overlaps(NULL, NULL));\n+  ASSERT_TRUE(Overlaps(NULL, \"150\"));\n+  ASSERT_TRUE(Overlaps(NULL, \"199\"));\n+  ASSERT_TRUE(Overlaps(NULL, \"200\"));\n+  ASSERT_TRUE(Overlaps(NULL, \"201\"));\n+  ASSERT_TRUE(Overlaps(NULL, \"400\"));\n+  ASSERT_TRUE(Overlaps(NULL, \"800\"));\n+  ASSERT_TRUE(Overlaps(\"100\", NULL));\n+  ASSERT_TRUE(Overlaps(\"200\", NULL));\n+  ASSERT_TRUE(Overlaps(\"449\", NULL));\n+  ASSERT_TRUE(Overlaps(\"450\", NULL));\n+}\n+\n+TEST(FindFileTest, OverlapSequenceChecks) {\n+  Add(\"200\", \"200\", 5000, 3000);\n+  ASSERT_TRUE(! Overlaps(\"199\", \"199\"));\n+  ASSERT_TRUE(! Overlaps(\"201\", \"300\"));\n+  ASSERT_TRUE(Overlaps(\"200\", \"200\"));\n+  ASSERT_TRUE(Overlaps(\"190\", \"200\"));\n+  ASSERT_TRUE(Overlaps(\"200\", \"210\"));\n+}\n+\n+TEST(FindFileTest, OverlappingFiles) {\n+  Add(\"150\", \"600\");\n+  Add(\"400\", \"500\");\n+  disjoint_sorted_files_ = false;\n+  ASSERT_TRUE(! Overlaps(\"100\", \"149\"));\n+  ASSERT_TRUE(! Overlaps(\"601\", \"700\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"150\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"200\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"300\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"400\"));\n+  ASSERT_TRUE(Overlaps(\"100\", \"500\"));\n+  ASSERT_TRUE(Overlaps(\"375\", \"400\"));\n+  ASSERT_TRUE(Overlaps(\"450\", \"450\"));\n+  ASSERT_TRUE(Overlaps(\"450\", \"500\"));\n+  ASSERT_TRUE(Overlaps(\"450\", \"700\"));\n+  ASSERT_TRUE(Overlaps(\"600\", \"700\"));\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "33f4a4257ea94e0105a9368de79d761ac7bf979a",
        "filename": "src/leveldb/db/write_batch.cc",
        "status": "added",
        "additions": 147,
        "deletions": 0,
        "changes": 147,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/write_batch.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/write_batch.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/write_batch.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,147 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// WriteBatch::rep_ :=\n+//    sequence: fixed64\n+//    count: fixed32\n+//    data: record[count]\n+// record :=\n+//    kTypeValue varstring varstring         |\n+//    kTypeDeletion varstring\n+// varstring :=\n+//    len: varint32\n+//    data: uint8[len]\n+\n+#include \"leveldb/write_batch.h\"\n+\n+#include \"leveldb/db.h\"\n+#include \"db/dbformat.h\"\n+#include \"db/memtable.h\"\n+#include \"db/write_batch_internal.h\"\n+#include \"util/coding.h\"\n+\n+namespace leveldb {\n+\n+// WriteBatch header has an 8-byte sequence number followed by a 4-byte count.\n+static const size_t kHeader = 12;\n+\n+WriteBatch::WriteBatch() {\n+  Clear();\n+}\n+\n+WriteBatch::~WriteBatch() { }\n+\n+WriteBatch::Handler::~Handler() { }\n+\n+void WriteBatch::Clear() {\n+  rep_.clear();\n+  rep_.resize(kHeader);\n+}\n+\n+Status WriteBatch::Iterate(Handler* handler) const {\n+  Slice input(rep_);\n+  if (input.size() < kHeader) {\n+    return Status::Corruption(\"malformed WriteBatch (too small)\");\n+  }\n+\n+  input.remove_prefix(kHeader);\n+  Slice key, value;\n+  int found = 0;\n+  while (!input.empty()) {\n+    found++;\n+    char tag = input[0];\n+    input.remove_prefix(1);\n+    switch (tag) {\n+      case kTypeValue:\n+        if (GetLengthPrefixedSlice(&input, &key) &&\n+            GetLengthPrefixedSlice(&input, &value)) {\n+          handler->Put(key, value);\n+        } else {\n+          return Status::Corruption(\"bad WriteBatch Put\");\n+        }\n+        break;\n+      case kTypeDeletion:\n+        if (GetLengthPrefixedSlice(&input, &key)) {\n+          handler->Delete(key);\n+        } else {\n+          return Status::Corruption(\"bad WriteBatch Delete\");\n+        }\n+        break;\n+      default:\n+        return Status::Corruption(\"unknown WriteBatch tag\");\n+    }\n+  }\n+  if (found != WriteBatchInternal::Count(this)) {\n+    return Status::Corruption(\"WriteBatch has wrong count\");\n+  } else {\n+    return Status::OK();\n+  }\n+}\n+\n+int WriteBatchInternal::Count(const WriteBatch* b) {\n+  return DecodeFixed32(b->rep_.data() + 8);\n+}\n+\n+void WriteBatchInternal::SetCount(WriteBatch* b, int n) {\n+  EncodeFixed32(&b->rep_[8], n);\n+}\n+\n+SequenceNumber WriteBatchInternal::Sequence(const WriteBatch* b) {\n+  return SequenceNumber(DecodeFixed64(b->rep_.data()));\n+}\n+\n+void WriteBatchInternal::SetSequence(WriteBatch* b, SequenceNumber seq) {\n+  EncodeFixed64(&b->rep_[0], seq);\n+}\n+\n+void WriteBatch::Put(const Slice& key, const Slice& value) {\n+  WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1);\n+  rep_.push_back(static_cast<char>(kTypeValue));\n+  PutLengthPrefixedSlice(&rep_, key);\n+  PutLengthPrefixedSlice(&rep_, value);\n+}\n+\n+void WriteBatch::Delete(const Slice& key) {\n+  WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1);\n+  rep_.push_back(static_cast<char>(kTypeDeletion));\n+  PutLengthPrefixedSlice(&rep_, key);\n+}\n+\n+namespace {\n+class MemTableInserter : public WriteBatch::Handler {\n+ public:\n+  SequenceNumber sequence_;\n+  MemTable* mem_;\n+\n+  virtual void Put(const Slice& key, const Slice& value) {\n+    mem_->Add(sequence_, kTypeValue, key, value);\n+    sequence_++;\n+  }\n+  virtual void Delete(const Slice& key) {\n+    mem_->Add(sequence_, kTypeDeletion, key, Slice());\n+    sequence_++;\n+  }\n+};\n+}  // namespace\n+\n+Status WriteBatchInternal::InsertInto(const WriteBatch* b,\n+                                      MemTable* memtable) {\n+  MemTableInserter inserter;\n+  inserter.sequence_ = WriteBatchInternal::Sequence(b);\n+  inserter.mem_ = memtable;\n+  return b->Iterate(&inserter);\n+}\n+\n+void WriteBatchInternal::SetContents(WriteBatch* b, const Slice& contents) {\n+  assert(contents.size() >= kHeader);\n+  b->rep_.assign(contents.data(), contents.size());\n+}\n+\n+void WriteBatchInternal::Append(WriteBatch* dst, const WriteBatch* src) {\n+  SetCount(dst, Count(dst) + Count(src));\n+  assert(src->rep_.size() >= kHeader);\n+  dst->rep_.append(src->rep_.data() + kHeader, src->rep_.size() - kHeader);\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "4423a7f31842457dea62d43547616b2f5ec852f8",
        "filename": "src/leveldb/db/write_batch_internal.h",
        "status": "added",
        "additions": 49,
        "deletions": 0,
        "changes": 49,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/write_batch_internal.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/write_batch_internal.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/write_batch_internal.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,49 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_DB_WRITE_BATCH_INTERNAL_H_\n+#define STORAGE_LEVELDB_DB_WRITE_BATCH_INTERNAL_H_\n+\n+#include \"leveldb/write_batch.h\"\n+\n+namespace leveldb {\n+\n+class MemTable;\n+\n+// WriteBatchInternal provides static methods for manipulating a\n+// WriteBatch that we don't want in the public WriteBatch interface.\n+class WriteBatchInternal {\n+ public:\n+  // Return the number of entries in the batch.\n+  static int Count(const WriteBatch* batch);\n+\n+  // Set the count for the number of entries in the batch.\n+  static void SetCount(WriteBatch* batch, int n);\n+\n+  // Return the seqeunce number for the start of this batch.\n+  static SequenceNumber Sequence(const WriteBatch* batch);\n+\n+  // Store the specified number as the seqeunce number for the start of\n+  // this batch.\n+  static void SetSequence(WriteBatch* batch, SequenceNumber seq);\n+\n+  static Slice Contents(const WriteBatch* batch) {\n+    return Slice(batch->rep_);\n+  }\n+\n+  static size_t ByteSize(const WriteBatch* batch) {\n+    return batch->rep_.size();\n+  }\n+\n+  static void SetContents(WriteBatch* batch, const Slice& contents);\n+\n+  static Status InsertInto(const WriteBatch* batch, MemTable* memtable);\n+\n+  static void Append(WriteBatch* dst, const WriteBatch* src);\n+};\n+\n+}  // namespace leveldb\n+\n+\n+#endif  // STORAGE_LEVELDB_DB_WRITE_BATCH_INTERNAL_H_"
      },
      {
        "sha": "9064e3d85eb35f32d20ef4c7456b0866d525aee8",
        "filename": "src/leveldb/db/write_batch_test.cc",
        "status": "added",
        "additions": 120,
        "deletions": 0,
        "changes": 120,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/write_batch_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/db/write_batch_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/write_batch_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,120 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"leveldb/db.h\"\n+\n+#include \"db/memtable.h\"\n+#include \"db/write_batch_internal.h\"\n+#include \"leveldb/env.h\"\n+#include \"util/logging.h\"\n+#include \"util/testharness.h\"\n+\n+namespace leveldb {\n+\n+static std::string PrintContents(WriteBatch* b) {\n+  InternalKeyComparator cmp(BytewiseComparator());\n+  MemTable* mem = new MemTable(cmp);\n+  mem->Ref();\n+  std::string state;\n+  Status s = WriteBatchInternal::InsertInto(b, mem);\n+  int count = 0;\n+  Iterator* iter = mem->NewIterator();\n+  for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+    ParsedInternalKey ikey;\n+    ASSERT_TRUE(ParseInternalKey(iter->key(), &ikey));\n+    switch (ikey.type) {\n+      case kTypeValue:\n+        state.append(\"Put(\");\n+        state.append(ikey.user_key.ToString());\n+        state.append(\", \");\n+        state.append(iter->value().ToString());\n+        state.append(\")\");\n+        count++;\n+        break;\n+      case kTypeDeletion:\n+        state.append(\"Delete(\");\n+        state.append(ikey.user_key.ToString());\n+        state.append(\")\");\n+        count++;\n+        break;\n+    }\n+    state.append(\"@\");\n+    state.append(NumberToString(ikey.sequence));\n+  }\n+  delete iter;\n+  if (!s.ok()) {\n+    state.append(\"ParseError()\");\n+  } else if (count != WriteBatchInternal::Count(b)) {\n+    state.append(\"CountMismatch()\");\n+  }\n+  mem->Unref();\n+  return state;\n+}\n+\n+class WriteBatchTest { };\n+\n+TEST(WriteBatchTest, Empty) {\n+  WriteBatch batch;\n+  ASSERT_EQ(\"\", PrintContents(&batch));\n+  ASSERT_EQ(0, WriteBatchInternal::Count(&batch));\n+}\n+\n+TEST(WriteBatchTest, Multiple) {\n+  WriteBatch batch;\n+  batch.Put(Slice(\"foo\"), Slice(\"bar\"));\n+  batch.Delete(Slice(\"box\"));\n+  batch.Put(Slice(\"baz\"), Slice(\"boo\"));\n+  WriteBatchInternal::SetSequence(&batch, 100);\n+  ASSERT_EQ(100, WriteBatchInternal::Sequence(&batch));\n+  ASSERT_EQ(3, WriteBatchInternal::Count(&batch));\n+  ASSERT_EQ(\"Put(baz, boo)@102\"\n+            \"Delete(box)@101\"\n+            \"Put(foo, bar)@100\",\n+            PrintContents(&batch));\n+}\n+\n+TEST(WriteBatchTest, Corruption) {\n+  WriteBatch batch;\n+  batch.Put(Slice(\"foo\"), Slice(\"bar\"));\n+  batch.Delete(Slice(\"box\"));\n+  WriteBatchInternal::SetSequence(&batch, 200);\n+  Slice contents = WriteBatchInternal::Contents(&batch);\n+  WriteBatchInternal::SetContents(&batch,\n+                                  Slice(contents.data(),contents.size()-1));\n+  ASSERT_EQ(\"Put(foo, bar)@200\"\n+            \"ParseError()\",\n+            PrintContents(&batch));\n+}\n+\n+TEST(WriteBatchTest, Append) {\n+  WriteBatch b1, b2;\n+  WriteBatchInternal::SetSequence(&b1, 200);\n+  WriteBatchInternal::SetSequence(&b2, 300);\n+  WriteBatchInternal::Append(&b1, &b2);\n+  ASSERT_EQ(\"\",\n+            PrintContents(&b1));\n+  b2.Put(\"a\", \"va\");\n+  WriteBatchInternal::Append(&b1, &b2);\n+  ASSERT_EQ(\"Put(a, va)@200\",\n+            PrintContents(&b1));\n+  b2.Clear();\n+  b2.Put(\"b\", \"vb\");\n+  WriteBatchInternal::Append(&b1, &b2);\n+  ASSERT_EQ(\"Put(a, va)@200\"\n+            \"Put(b, vb)@201\",\n+            PrintContents(&b1));\n+  b2.Delete(\"foo\");\n+  WriteBatchInternal::Append(&b1, &b2);\n+  ASSERT_EQ(\"Put(a, va)@200\"\n+            \"Put(b, vb)@202\"\n+            \"Put(b, vb)@201\"\n+            \"Delete(foo)@203\",\n+            PrintContents(&b1));\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "e63aaa8dcc289915176b12749dd9ec21b50aba02",
        "filename": "src/leveldb/doc/bench/db_bench_sqlite3.cc",
        "status": "added",
        "additions": 718,
        "deletions": 0,
        "changes": 718,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/bench/db_bench_sqlite3.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/bench/db_bench_sqlite3.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/bench/db_bench_sqlite3.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,718 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include <stdio.h>\n+#include <stdlib.h>\n+#include <sqlite3.h>\n+#include \"util/histogram.h\"\n+#include \"util/random.h\"\n+#include \"util/testutil.h\"\n+\n+// Comma-separated list of operations to run in the specified order\n+//   Actual benchmarks:\n+//\n+//   fillseq       -- write N values in sequential key order in async mode\n+//   fillseqsync   -- write N/100 values in sequential key order in sync mode\n+//   fillseqbatch  -- batch write N values in sequential key order in async mode\n+//   fillrandom    -- write N values in random key order in async mode\n+//   fillrandsync  -- write N/100 values in random key order in sync mode\n+//   fillrandbatch -- batch write N values in sequential key order in async mode\n+//   overwrite     -- overwrite N values in random key order in async mode\n+//   fillrand100K  -- write N/1000 100K values in random order in async mode\n+//   fillseq100K   -- write N/1000 100K values in sequential order in async mode\n+//   readseq       -- read N times sequentially\n+//   readrandom    -- read N times in random order\n+//   readrand100K  -- read N/1000 100K values in sequential order in async mode\n+static const char* FLAGS_benchmarks =\n+    \"fillseq,\"\n+    \"fillseqsync,\"\n+    \"fillseqbatch,\"\n+    \"fillrandom,\"\n+    \"fillrandsync,\"\n+    \"fillrandbatch,\"\n+    \"overwrite,\"\n+    \"overwritebatch,\"\n+    \"readrandom,\"\n+    \"readseq,\"\n+    \"fillrand100K,\"\n+    \"fillseq100K,\"\n+    \"readseq,\"\n+    \"readrand100K,\"\n+    ;\n+\n+// Number of key/values to place in database\n+static int FLAGS_num = 1000000;\n+\n+// Number of read operations to do.  If negative, do FLAGS_num reads.\n+static int FLAGS_reads = -1;\n+\n+// Size of each value\n+static int FLAGS_value_size = 100;\n+\n+// Print histogram of operation timings\n+static bool FLAGS_histogram = false;\n+\n+// Arrange to generate values that shrink to this fraction of\n+// their original size after compression\n+static double FLAGS_compression_ratio = 0.5;\n+\n+// Page size. Default 1 KB.\n+static int FLAGS_page_size = 1024;\n+\n+// Number of pages.\n+// Default cache size = FLAGS_page_size * FLAGS_num_pages = 4 MB.\n+static int FLAGS_num_pages = 4096;\n+\n+// If true, do not destroy the existing database.  If you set this\n+// flag and also specify a benchmark that wants a fresh database, that\n+// benchmark will fail.\n+static bool FLAGS_use_existing_db = false;\n+\n+// If true, we allow batch writes to occur\n+static bool FLAGS_transaction = true;\n+\n+// If true, we enable Write-Ahead Logging\n+static bool FLAGS_WAL_enabled = true;\n+\n+// Use the db with the following name.\n+static const char* FLAGS_db = NULL;\n+\n+inline\n+static void ExecErrorCheck(int status, char *err_msg) {\n+  if (status != SQLITE_OK) {\n+    fprintf(stderr, \"SQL error: %s\\n\", err_msg);\n+    sqlite3_free(err_msg);\n+    exit(1);\n+  }\n+}\n+\n+inline\n+static void StepErrorCheck(int status) {\n+  if (status != SQLITE_DONE) {\n+    fprintf(stderr, \"SQL step error: status = %d\\n\", status);\n+    exit(1);\n+  }\n+}\n+\n+inline\n+static void ErrorCheck(int status) {\n+  if (status != SQLITE_OK) {\n+    fprintf(stderr, \"sqlite3 error: status = %d\\n\", status);\n+    exit(1);\n+  }\n+}\n+\n+inline\n+static void WalCheckpoint(sqlite3* db_) {\n+  // Flush all writes to disk\n+  if (FLAGS_WAL_enabled) {\n+    sqlite3_wal_checkpoint_v2(db_, NULL, SQLITE_CHECKPOINT_FULL, NULL, NULL);\n+  }\n+}\n+\n+namespace leveldb {\n+\n+// Helper for quickly generating random data.\n+namespace {\n+class RandomGenerator {\n+ private:\n+  std::string data_;\n+  int pos_;\n+\n+ public:\n+  RandomGenerator() {\n+    // We use a limited amount of data over and over again and ensure\n+    // that it is larger than the compression window (32KB), and also\n+    // large enough to serve all typical value sizes we want to write.\n+    Random rnd(301);\n+    std::string piece;\n+    while (data_.size() < 1048576) {\n+      // Add a short fragment that is as compressible as specified\n+      // by FLAGS_compression_ratio.\n+      test::CompressibleString(&rnd, FLAGS_compression_ratio, 100, &piece);\n+      data_.append(piece);\n+    }\n+    pos_ = 0;\n+  }\n+\n+  Slice Generate(int len) {\n+    if (pos_ + len > data_.size()) {\n+      pos_ = 0;\n+      assert(len < data_.size());\n+    }\n+    pos_ += len;\n+    return Slice(data_.data() + pos_ - len, len);\n+  }\n+};\n+\n+static Slice TrimSpace(Slice s) {\n+  int start = 0;\n+  while (start < s.size() && isspace(s[start])) {\n+    start++;\n+  }\n+  int limit = s.size();\n+  while (limit > start && isspace(s[limit-1])) {\n+    limit--;\n+  }\n+  return Slice(s.data() + start, limit - start);\n+}\n+\n+}  // namespace\n+\n+class Benchmark {\n+ private:\n+  sqlite3* db_;\n+  int db_num_;\n+  int num_;\n+  int reads_;\n+  double start_;\n+  double last_op_finish_;\n+  int64_t bytes_;\n+  std::string message_;\n+  Histogram hist_;\n+  RandomGenerator gen_;\n+  Random rand_;\n+\n+  // State kept for progress messages\n+  int done_;\n+  int next_report_;     // When to report next\n+\n+  void PrintHeader() {\n+    const int kKeySize = 16;\n+    PrintEnvironment();\n+    fprintf(stdout, \"Keys:       %d bytes each\\n\", kKeySize);\n+    fprintf(stdout, \"Values:     %d bytes each\\n\", FLAGS_value_size);\n+    fprintf(stdout, \"Entries:    %d\\n\", num_);\n+    fprintf(stdout, \"RawSize:    %.1f MB (estimated)\\n\",\n+            ((static_cast<int64_t>(kKeySize + FLAGS_value_size) * num_)\n+             / 1048576.0));\n+    PrintWarnings();\n+    fprintf(stdout, \"------------------------------------------------\\n\");\n+  }\n+\n+  void PrintWarnings() {\n+#if defined(__GNUC__) && !defined(__OPTIMIZE__)\n+    fprintf(stdout,\n+            \"WARNING: Optimization is disabled: benchmarks unnecessarily slow\\n\"\n+            );\n+#endif\n+#ifndef NDEBUG\n+    fprintf(stdout,\n+            \"WARNING: Assertions are enabled; benchmarks unnecessarily slow\\n\");\n+#endif\n+  }\n+\n+  void PrintEnvironment() {\n+    fprintf(stderr, \"SQLite:     version %s\\n\", SQLITE_VERSION);\n+\n+#if defined(__linux)\n+    time_t now = time(NULL);\n+    fprintf(stderr, \"Date:       %s\", ctime(&now));  // ctime() adds newline\n+\n+    FILE* cpuinfo = fopen(\"/proc/cpuinfo\", \"r\");\n+    if (cpuinfo != NULL) {\n+      char line[1000];\n+      int num_cpus = 0;\n+      std::string cpu_type;\n+      std::string cache_size;\n+      while (fgets(line, sizeof(line), cpuinfo) != NULL) {\n+        const char* sep = strchr(line, ':');\n+        if (sep == NULL) {\n+          continue;\n+        }\n+        Slice key = TrimSpace(Slice(line, sep - 1 - line));\n+        Slice val = TrimSpace(Slice(sep + 1));\n+        if (key == \"model name\") {\n+          ++num_cpus;\n+          cpu_type = val.ToString();\n+        } else if (key == \"cache size\") {\n+          cache_size = val.ToString();\n+        }\n+      }\n+      fclose(cpuinfo);\n+      fprintf(stderr, \"CPU:        %d * %s\\n\", num_cpus, cpu_type.c_str());\n+      fprintf(stderr, \"CPUCache:   %s\\n\", cache_size.c_str());\n+    }\n+#endif\n+  }\n+\n+  void Start() {\n+    start_ = Env::Default()->NowMicros() * 1e-6;\n+    bytes_ = 0;\n+    message_.clear();\n+    last_op_finish_ = start_;\n+    hist_.Clear();\n+    done_ = 0;\n+    next_report_ = 100;\n+  }\n+\n+  void FinishedSingleOp() {\n+    if (FLAGS_histogram) {\n+      double now = Env::Default()->NowMicros() * 1e-6;\n+      double micros = (now - last_op_finish_) * 1e6;\n+      hist_.Add(micros);\n+      if (micros > 20000) {\n+        fprintf(stderr, \"long op: %.1f micros%30s\\r\", micros, \"\");\n+        fflush(stderr);\n+      }\n+      last_op_finish_ = now;\n+    }\n+\n+    done_++;\n+    if (done_ >= next_report_) {\n+      if      (next_report_ < 1000)   next_report_ += 100;\n+      else if (next_report_ < 5000)   next_report_ += 500;\n+      else if (next_report_ < 10000)  next_report_ += 1000;\n+      else if (next_report_ < 50000)  next_report_ += 5000;\n+      else if (next_report_ < 100000) next_report_ += 10000;\n+      else if (next_report_ < 500000) next_report_ += 50000;\n+      else                            next_report_ += 100000;\n+      fprintf(stderr, \"... finished %d ops%30s\\r\", done_, \"\");\n+      fflush(stderr);\n+    }\n+  }\n+\n+  void Stop(const Slice& name) {\n+    double finish = Env::Default()->NowMicros() * 1e-6;\n+\n+    // Pretend at least one op was done in case we are running a benchmark\n+    // that does not call FinishedSingleOp().\n+    if (done_ < 1) done_ = 1;\n+\n+    if (bytes_ > 0) {\n+      char rate[100];\n+      snprintf(rate, sizeof(rate), \"%6.1f MB/s\",\n+               (bytes_ / 1048576.0) / (finish - start_));\n+      if (!message_.empty()) {\n+        message_  = std::string(rate) + \" \" + message_;\n+      } else {\n+        message_ = rate;\n+      }\n+    }\n+\n+    fprintf(stdout, \"%-12s : %11.3f micros/op;%s%s\\n\",\n+            name.ToString().c_str(),\n+            (finish - start_) * 1e6 / done_,\n+            (message_.empty() ? \"\" : \" \"),\n+            message_.c_str());\n+    if (FLAGS_histogram) {\n+      fprintf(stdout, \"Microseconds per op:\\n%s\\n\", hist_.ToString().c_str());\n+    }\n+    fflush(stdout);\n+  }\n+\n+ public:\n+  enum Order {\n+    SEQUENTIAL,\n+    RANDOM\n+  };\n+  enum DBState {\n+    FRESH,\n+    EXISTING\n+  };\n+\n+  Benchmark()\n+  : db_(NULL),\n+    db_num_(0),\n+    num_(FLAGS_num),\n+    reads_(FLAGS_reads < 0 ? FLAGS_num : FLAGS_reads),\n+    bytes_(0),\n+    rand_(301) {\n+    std::vector<std::string> files;\n+    std::string test_dir;\n+    Env::Default()->GetTestDirectory(&test_dir);\n+    Env::Default()->GetChildren(test_dir, &files);\n+    if (!FLAGS_use_existing_db) {\n+      for (int i = 0; i < files.size(); i++) {\n+        if (Slice(files[i]).starts_with(\"dbbench_sqlite3\")) {\n+          std::string file_name(test_dir);\n+          file_name += \"/\";\n+          file_name += files[i];\n+          Env::Default()->DeleteFile(file_name.c_str());\n+        }\n+      }\n+    }\n+  }\n+\n+  ~Benchmark() {\n+    int status = sqlite3_close(db_);\n+    ErrorCheck(status);\n+  }\n+\n+  void Run() {\n+    PrintHeader();\n+    Open();\n+\n+    const char* benchmarks = FLAGS_benchmarks;\n+    while (benchmarks != NULL) {\n+      const char* sep = strchr(benchmarks, ',');\n+      Slice name;\n+      if (sep == NULL) {\n+        name = benchmarks;\n+        benchmarks = NULL;\n+      } else {\n+        name = Slice(benchmarks, sep - benchmarks);\n+        benchmarks = sep + 1;\n+      }\n+\n+      bytes_ = 0;\n+      Start();\n+\n+      bool known = true;\n+      bool write_sync = false;\n+      if (name == Slice(\"fillseq\")) {\n+        Write(write_sync, SEQUENTIAL, FRESH, num_, FLAGS_value_size, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillseqbatch\")) {\n+        Write(write_sync, SEQUENTIAL, FRESH, num_, FLAGS_value_size, 1000);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillrandom\")) {\n+        Write(write_sync, RANDOM, FRESH, num_, FLAGS_value_size, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillrandbatch\")) {\n+        Write(write_sync, RANDOM, FRESH, num_, FLAGS_value_size, 1000);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"overwrite\")) {\n+        Write(write_sync, RANDOM, EXISTING, num_, FLAGS_value_size, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"overwritebatch\")) {\n+        Write(write_sync, RANDOM, EXISTING, num_, FLAGS_value_size, 1000);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillrandsync\")) {\n+        write_sync = true;\n+        Write(write_sync, RANDOM, FRESH, num_ / 100, FLAGS_value_size, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillseqsync\")) {\n+        write_sync = true;\n+        Write(write_sync, SEQUENTIAL, FRESH, num_ / 100, FLAGS_value_size, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillrand100K\")) {\n+        Write(write_sync, RANDOM, FRESH, num_ / 1000, 100 * 1000, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"fillseq100K\")) {\n+        Write(write_sync, SEQUENTIAL, FRESH, num_ / 1000, 100 * 1000, 1);\n+        WalCheckpoint(db_);\n+      } else if (name == Slice(\"readseq\")) {\n+        ReadSequential();\n+      } else if (name == Slice(\"readrandom\")) {\n+        Read(RANDOM, 1);\n+      } else if (name == Slice(\"readrand100K\")) {\n+        int n = reads_;\n+        reads_ /= 1000;\n+        Read(RANDOM, 1);\n+        reads_ = n;\n+      } else {\n+        known = false;\n+        if (name != Slice()) {  // No error message for empty name\n+          fprintf(stderr, \"unknown benchmark '%s'\\n\", name.ToString().c_str());\n+        }\n+      }\n+      if (known) {\n+        Stop(name);\n+      }\n+    }\n+  }\n+\n+  void Open() {\n+    assert(db_ == NULL);\n+\n+    int status;\n+    char file_name[100];\n+    char* err_msg = NULL;\n+    db_num_++;\n+\n+    // Open database\n+    std::string tmp_dir;\n+    Env::Default()->GetTestDirectory(&tmp_dir);\n+    snprintf(file_name, sizeof(file_name),\n+             \"%s/dbbench_sqlite3-%d.db\",\n+             tmp_dir.c_str(),\n+             db_num_);\n+    status = sqlite3_open(file_name, &db_);\n+    if (status) {\n+      fprintf(stderr, \"open error: %s\\n\", sqlite3_errmsg(db_));\n+      exit(1);\n+    }\n+\n+    // Change SQLite cache size\n+    char cache_size[100];\n+    snprintf(cache_size, sizeof(cache_size), \"PRAGMA cache_size = %d\",\n+             FLAGS_num_pages);\n+    status = sqlite3_exec(db_, cache_size, NULL, NULL, &err_msg);\n+    ExecErrorCheck(status, err_msg);\n+\n+    // FLAGS_page_size is defaulted to 1024\n+    if (FLAGS_page_size != 1024) {\n+      char page_size[100];\n+      snprintf(page_size, sizeof(page_size), \"PRAGMA page_size = %d\",\n+               FLAGS_page_size);\n+      status = sqlite3_exec(db_, page_size, NULL, NULL, &err_msg);\n+      ExecErrorCheck(status, err_msg);\n+    }\n+\n+    // Change journal mode to WAL if WAL enabled flag is on\n+    if (FLAGS_WAL_enabled) {\n+      std::string WAL_stmt = \"PRAGMA journal_mode = WAL\";\n+\n+      // LevelDB's default cache size is a combined 4 MB\n+      std::string WAL_checkpoint = \"PRAGMA wal_autocheckpoint = 4096\";\n+      status = sqlite3_exec(db_, WAL_stmt.c_str(), NULL, NULL, &err_msg);\n+      ExecErrorCheck(status, err_msg);\n+      status = sqlite3_exec(db_, WAL_checkpoint.c_str(), NULL, NULL, &err_msg);\n+      ExecErrorCheck(status, err_msg);\n+    }\n+\n+    // Change locking mode to exclusive and create tables/index for database\n+    std::string locking_stmt = \"PRAGMA locking_mode = EXCLUSIVE\";\n+    std::string create_stmt =\n+          \"CREATE TABLE test (key blob, value blob, PRIMARY KEY(key))\";\n+    std::string stmt_array[] = { locking_stmt, create_stmt };\n+    int stmt_array_length = sizeof(stmt_array) / sizeof(std::string);\n+    for (int i = 0; i < stmt_array_length; i++) {\n+      status = sqlite3_exec(db_, stmt_array[i].c_str(), NULL, NULL, &err_msg);\n+      ExecErrorCheck(status, err_msg);\n+    }\n+  }\n+\n+  void Write(bool write_sync, Order order, DBState state,\n+             int num_entries, int value_size, int entries_per_batch) {\n+    // Create new database if state == FRESH\n+    if (state == FRESH) {\n+      if (FLAGS_use_existing_db) {\n+        message_ = \"skipping (--use_existing_db is true)\";\n+        return;\n+      }\n+      sqlite3_close(db_);\n+      db_ = NULL;\n+      Open();\n+      Start();\n+    }\n+\n+    if (num_entries != num_) {\n+      char msg[100];\n+      snprintf(msg, sizeof(msg), \"(%d ops)\", num_entries);\n+      message_ = msg;\n+    }\n+\n+    char* err_msg = NULL;\n+    int status;\n+\n+    sqlite3_stmt *replace_stmt, *begin_trans_stmt, *end_trans_stmt;\n+    std::string replace_str = \"REPLACE INTO test (key, value) VALUES (?, ?)\";\n+    std::string begin_trans_str = \"BEGIN TRANSACTION;\";\n+    std::string end_trans_str = \"END TRANSACTION;\";\n+\n+    // Check for synchronous flag in options\n+    std::string sync_stmt = (write_sync) ? \"PRAGMA synchronous = FULL\" :\n+                                           \"PRAGMA synchronous = OFF\";\n+    status = sqlite3_exec(db_, sync_stmt.c_str(), NULL, NULL, &err_msg);\n+    ExecErrorCheck(status, err_msg);\n+\n+    // Preparing sqlite3 statements\n+    status = sqlite3_prepare_v2(db_, replace_str.c_str(), -1,\n+                                &replace_stmt, NULL);\n+    ErrorCheck(status);\n+    status = sqlite3_prepare_v2(db_, begin_trans_str.c_str(), -1,\n+                                &begin_trans_stmt, NULL);\n+    ErrorCheck(status);\n+    status = sqlite3_prepare_v2(db_, end_trans_str.c_str(), -1,\n+                                &end_trans_stmt, NULL);\n+    ErrorCheck(status);\n+\n+    bool transaction = (entries_per_batch > 1);\n+    for (int i = 0; i < num_entries; i += entries_per_batch) {\n+      // Begin write transaction\n+      if (FLAGS_transaction && transaction) {\n+        status = sqlite3_step(begin_trans_stmt);\n+        StepErrorCheck(status);\n+        status = sqlite3_reset(begin_trans_stmt);\n+        ErrorCheck(status);\n+      }\n+\n+      // Create and execute SQL statements\n+      for (int j = 0; j < entries_per_batch; j++) {\n+        const char* value = gen_.Generate(value_size).data();\n+\n+        // Create values for key-value pair\n+        const int k = (order == SEQUENTIAL) ? i + j :\n+                      (rand_.Next() % num_entries);\n+        char key[100];\n+        snprintf(key, sizeof(key), \"%016d\", k);\n+\n+        // Bind KV values into replace_stmt\n+        status = sqlite3_bind_blob(replace_stmt, 1, key, 16, SQLITE_STATIC);\n+        ErrorCheck(status);\n+        status = sqlite3_bind_blob(replace_stmt, 2, value,\n+                                   value_size, SQLITE_STATIC);\n+        ErrorCheck(status);\n+\n+        // Execute replace_stmt\n+        bytes_ += value_size + strlen(key);\n+        status = sqlite3_step(replace_stmt);\n+        StepErrorCheck(status);\n+\n+        // Reset SQLite statement for another use\n+        status = sqlite3_clear_bindings(replace_stmt);\n+        ErrorCheck(status);\n+        status = sqlite3_reset(replace_stmt);\n+        ErrorCheck(status);\n+\n+        FinishedSingleOp();\n+      }\n+\n+      // End write transaction\n+      if (FLAGS_transaction && transaction) {\n+        status = sqlite3_step(end_trans_stmt);\n+        StepErrorCheck(status);\n+        status = sqlite3_reset(end_trans_stmt);\n+        ErrorCheck(status);\n+      }\n+    }\n+\n+    status = sqlite3_finalize(replace_stmt);\n+    ErrorCheck(status);\n+    status = sqlite3_finalize(begin_trans_stmt);\n+    ErrorCheck(status);\n+    status = sqlite3_finalize(end_trans_stmt);\n+    ErrorCheck(status);\n+  }\n+\n+  void Read(Order order, int entries_per_batch) {\n+    int status;\n+    sqlite3_stmt *read_stmt, *begin_trans_stmt, *end_trans_stmt;\n+\n+    std::string read_str = \"SELECT * FROM test WHERE key = ?\";\n+    std::string begin_trans_str = \"BEGIN TRANSACTION;\";\n+    std::string end_trans_str = \"END TRANSACTION;\";\n+\n+    // Preparing sqlite3 statements\n+    status = sqlite3_prepare_v2(db_, begin_trans_str.c_str(), -1,\n+                                &begin_trans_stmt, NULL);\n+    ErrorCheck(status);\n+    status = sqlite3_prepare_v2(db_, end_trans_str.c_str(), -1,\n+                                &end_trans_stmt, NULL);\n+    ErrorCheck(status);\n+    status = sqlite3_prepare_v2(db_, read_str.c_str(), -1, &read_stmt, NULL);\n+    ErrorCheck(status);\n+\n+    bool transaction = (entries_per_batch > 1);\n+    for (int i = 0; i < reads_; i += entries_per_batch) {\n+      // Begin read transaction\n+      if (FLAGS_transaction && transaction) {\n+        status = sqlite3_step(begin_trans_stmt);\n+        StepErrorCheck(status);\n+        status = sqlite3_reset(begin_trans_stmt);\n+        ErrorCheck(status);\n+      }\n+\n+      // Create and execute SQL statements\n+      for (int j = 0; j < entries_per_batch; j++) {\n+        // Create key value\n+        char key[100];\n+        int k = (order == SEQUENTIAL) ? i + j : (rand_.Next() % reads_);\n+        snprintf(key, sizeof(key), \"%016d\", k);\n+\n+        // Bind key value into read_stmt\n+        status = sqlite3_bind_blob(read_stmt, 1, key, 16, SQLITE_STATIC);\n+        ErrorCheck(status);\n+\n+        // Execute read statement\n+        while ((status = sqlite3_step(read_stmt)) == SQLITE_ROW) {}\n+        StepErrorCheck(status);\n+\n+        // Reset SQLite statement for another use\n+        status = sqlite3_clear_bindings(read_stmt);\n+        ErrorCheck(status);\n+        status = sqlite3_reset(read_stmt);\n+        ErrorCheck(status);\n+        FinishedSingleOp();\n+      }\n+\n+      // End read transaction\n+      if (FLAGS_transaction && transaction) {\n+        status = sqlite3_step(end_trans_stmt);\n+        StepErrorCheck(status);\n+        status = sqlite3_reset(end_trans_stmt);\n+        ErrorCheck(status);\n+      }\n+    }\n+\n+    status = sqlite3_finalize(read_stmt);\n+    ErrorCheck(status);\n+    status = sqlite3_finalize(begin_trans_stmt);\n+    ErrorCheck(status);\n+    status = sqlite3_finalize(end_trans_stmt);\n+    ErrorCheck(status);\n+  }\n+\n+  void ReadSequential() {\n+    int status;\n+    sqlite3_stmt *pStmt;\n+    std::string read_str = \"SELECT * FROM test ORDER BY key\";\n+\n+    status = sqlite3_prepare_v2(db_, read_str.c_str(), -1, &pStmt, NULL);\n+    ErrorCheck(status);\n+    for (int i = 0; i < reads_ && SQLITE_ROW == sqlite3_step(pStmt); i++) {\n+      bytes_ += sqlite3_column_bytes(pStmt, 1) + sqlite3_column_bytes(pStmt, 2);\n+      FinishedSingleOp();\n+    }\n+\n+    status = sqlite3_finalize(pStmt);\n+    ErrorCheck(status);\n+  }\n+\n+};\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  std::string default_db_path;\n+  for (int i = 1; i < argc; i++) {\n+    double d;\n+    int n;\n+    char junk;\n+    if (leveldb::Slice(argv[i]).starts_with(\"--benchmarks=\")) {\n+      FLAGS_benchmarks = argv[i] + strlen(\"--benchmarks=\");\n+    } else if (sscanf(argv[i], \"--histogram=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_histogram = n;\n+    } else if (sscanf(argv[i], \"--compression_ratio=%lf%c\", &d, &junk) == 1) {\n+      FLAGS_compression_ratio = d;\n+    } else if (sscanf(argv[i], \"--use_existing_db=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_use_existing_db = n;\n+    } else if (sscanf(argv[i], \"--num=%d%c\", &n, &junk) == 1) {\n+      FLAGS_num = n;\n+    } else if (sscanf(argv[i], \"--reads=%d%c\", &n, &junk) == 1) {\n+      FLAGS_reads = n;\n+    } else if (sscanf(argv[i], \"--value_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_value_size = n;\n+    } else if (leveldb::Slice(argv[i]) == leveldb::Slice(\"--no_transaction\")) {\n+      FLAGS_transaction = false;\n+    } else if (sscanf(argv[i], \"--page_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_page_size = n;\n+    } else if (sscanf(argv[i], \"--num_pages=%d%c\", &n, &junk) == 1) {\n+      FLAGS_num_pages = n;\n+    } else if (sscanf(argv[i], \"--WAL_enabled=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_WAL_enabled = n;\n+    } else if (strncmp(argv[i], \"--db=\", 5) == 0) {\n+      FLAGS_db = argv[i] + 5;\n+    } else {\n+      fprintf(stderr, \"Invalid flag '%s'\\n\", argv[i]);\n+      exit(1);\n+    }\n+  }\n+\n+  // Choose a location for the test database if none given with --db=<path>\n+  if (FLAGS_db == NULL) {\n+      leveldb::Env::Default()->GetTestDirectory(&default_db_path);\n+      default_db_path += \"/dbbench\";\n+      FLAGS_db = default_db_path.c_str();\n+  }\n+\n+  leveldb::Benchmark benchmark;\n+  benchmark.Run();\n+  return 0;\n+}"
      },
      {
        "sha": "ed86f031c25fe931b0e3a05f4501269afd233f02",
        "filename": "src/leveldb/doc/bench/db_bench_tree_db.cc",
        "status": "added",
        "additions": 528,
        "deletions": 0,
        "changes": 528,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/bench/db_bench_tree_db.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/bench/db_bench_tree_db.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/bench/db_bench_tree_db.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,528 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include <stdio.h>\n+#include <stdlib.h>\n+#include <kcpolydb.h>\n+#include \"util/histogram.h\"\n+#include \"util/random.h\"\n+#include \"util/testutil.h\"\n+\n+// Comma-separated list of operations to run in the specified order\n+//   Actual benchmarks:\n+//\n+//   fillseq       -- write N values in sequential key order in async mode\n+//   fillrandom    -- write N values in random key order in async mode\n+//   overwrite     -- overwrite N values in random key order in async mode\n+//   fillseqsync   -- write N/100 values in sequential key order in sync mode\n+//   fillrandsync  -- write N/100 values in random key order in sync mode\n+//   fillrand100K  -- write N/1000 100K values in random order in async mode\n+//   fillseq100K   -- write N/1000 100K values in seq order in async mode\n+//   readseq       -- read N times sequentially\n+//   readseq100K   -- read N/1000 100K values in sequential order in async mode\n+//   readrand100K  -- read N/1000 100K values in sequential order in async mode\n+//   readrandom    -- read N times in random order\n+static const char* FLAGS_benchmarks =\n+    \"fillseq,\"\n+    \"fillseqsync,\"\n+    \"fillrandsync,\"\n+    \"fillrandom,\"\n+    \"overwrite,\"\n+    \"readrandom,\"\n+    \"readseq,\"\n+    \"fillrand100K,\"\n+    \"fillseq100K,\"\n+    \"readseq100K,\"\n+    \"readrand100K,\"\n+    ;\n+\n+// Number of key/values to place in database\n+static int FLAGS_num = 1000000;\n+\n+// Number of read operations to do.  If negative, do FLAGS_num reads.\n+static int FLAGS_reads = -1;\n+\n+// Size of each value\n+static int FLAGS_value_size = 100;\n+\n+// Arrange to generate values that shrink to this fraction of\n+// their original size after compression\n+static double FLAGS_compression_ratio = 0.5;\n+\n+// Print histogram of operation timings\n+static bool FLAGS_histogram = false;\n+\n+// Cache size. Default 4 MB\n+static int FLAGS_cache_size = 4194304;\n+\n+// Page size. Default 1 KB\n+static int FLAGS_page_size = 1024;\n+\n+// If true, do not destroy the existing database.  If you set this\n+// flag and also specify a benchmark that wants a fresh database, that\n+// benchmark will fail.\n+static bool FLAGS_use_existing_db = false;\n+\n+// Compression flag. If true, compression is on. If false, compression\n+// is off.\n+static bool FLAGS_compression = true;\n+\n+// Use the db with the following name.\n+static const char* FLAGS_db = NULL;\n+\n+inline\n+static void DBSynchronize(kyotocabinet::TreeDB* db_)\n+{\n+  // Synchronize will flush writes to disk\n+  if (!db_->synchronize()) {\n+    fprintf(stderr, \"synchronize error: %s\\n\", db_->error().name());\n+  }\n+}\n+\n+namespace leveldb {\n+\n+// Helper for quickly generating random data.\n+namespace {\n+class RandomGenerator {\n+ private:\n+  std::string data_;\n+  int pos_;\n+\n+ public:\n+  RandomGenerator() {\n+    // We use a limited amount of data over and over again and ensure\n+    // that it is larger than the compression window (32KB), and also\n+    // large enough to serve all typical value sizes we want to write.\n+    Random rnd(301);\n+    std::string piece;\n+    while (data_.size() < 1048576) {\n+      // Add a short fragment that is as compressible as specified\n+      // by FLAGS_compression_ratio.\n+      test::CompressibleString(&rnd, FLAGS_compression_ratio, 100, &piece);\n+      data_.append(piece);\n+    }\n+    pos_ = 0;\n+  }\n+\n+  Slice Generate(int len) {\n+    if (pos_ + len > data_.size()) {\n+      pos_ = 0;\n+      assert(len < data_.size());\n+    }\n+    pos_ += len;\n+    return Slice(data_.data() + pos_ - len, len);\n+  }\n+};\n+\n+static Slice TrimSpace(Slice s) {\n+  int start = 0;\n+  while (start < s.size() && isspace(s[start])) {\n+    start++;\n+  }\n+  int limit = s.size();\n+  while (limit > start && isspace(s[limit-1])) {\n+    limit--;\n+  }\n+  return Slice(s.data() + start, limit - start);\n+}\n+\n+}  // namespace\n+\n+class Benchmark {\n+ private:\n+  kyotocabinet::TreeDB* db_;\n+  int db_num_;\n+  int num_;\n+  int reads_;\n+  double start_;\n+  double last_op_finish_;\n+  int64_t bytes_;\n+  std::string message_;\n+  Histogram hist_;\n+  RandomGenerator gen_;\n+  Random rand_;\n+  kyotocabinet::LZOCompressor<kyotocabinet::LZO::RAW> comp_;\n+\n+  // State kept for progress messages\n+  int done_;\n+  int next_report_;     // When to report next\n+\n+  void PrintHeader() {\n+    const int kKeySize = 16;\n+    PrintEnvironment();\n+    fprintf(stdout, \"Keys:       %d bytes each\\n\", kKeySize);\n+    fprintf(stdout, \"Values:     %d bytes each (%d bytes after compression)\\n\",\n+            FLAGS_value_size,\n+            static_cast<int>(FLAGS_value_size * FLAGS_compression_ratio + 0.5));\n+    fprintf(stdout, \"Entries:    %d\\n\", num_);\n+    fprintf(stdout, \"RawSize:    %.1f MB (estimated)\\n\",\n+            ((static_cast<int64_t>(kKeySize + FLAGS_value_size) * num_)\n+             / 1048576.0));\n+    fprintf(stdout, \"FileSize:   %.1f MB (estimated)\\n\",\n+            (((kKeySize + FLAGS_value_size * FLAGS_compression_ratio) * num_)\n+             / 1048576.0));\n+    PrintWarnings();\n+    fprintf(stdout, \"------------------------------------------------\\n\");\n+  }\n+\n+  void PrintWarnings() {\n+#if defined(__GNUC__) && !defined(__OPTIMIZE__)\n+    fprintf(stdout,\n+            \"WARNING: Optimization is disabled: benchmarks unnecessarily slow\\n\"\n+            );\n+#endif\n+#ifndef NDEBUG\n+    fprintf(stdout,\n+            \"WARNING: Assertions are enabled; benchmarks unnecessarily slow\\n\");\n+#endif\n+  }\n+\n+  void PrintEnvironment() {\n+    fprintf(stderr, \"Kyoto Cabinet:    version %s, lib ver %d, lib rev %d\\n\",\n+            kyotocabinet::VERSION, kyotocabinet::LIBVER, kyotocabinet::LIBREV);\n+\n+#if defined(__linux)\n+    time_t now = time(NULL);\n+    fprintf(stderr, \"Date:           %s\", ctime(&now));  // ctime() adds newline\n+\n+    FILE* cpuinfo = fopen(\"/proc/cpuinfo\", \"r\");\n+    if (cpuinfo != NULL) {\n+      char line[1000];\n+      int num_cpus = 0;\n+      std::string cpu_type;\n+      std::string cache_size;\n+      while (fgets(line, sizeof(line), cpuinfo) != NULL) {\n+        const char* sep = strchr(line, ':');\n+        if (sep == NULL) {\n+          continue;\n+        }\n+        Slice key = TrimSpace(Slice(line, sep - 1 - line));\n+        Slice val = TrimSpace(Slice(sep + 1));\n+        if (key == \"model name\") {\n+          ++num_cpus;\n+          cpu_type = val.ToString();\n+        } else if (key == \"cache size\") {\n+          cache_size = val.ToString();\n+        }\n+      }\n+      fclose(cpuinfo);\n+      fprintf(stderr, \"CPU:            %d * %s\\n\", num_cpus, cpu_type.c_str());\n+      fprintf(stderr, \"CPUCache:       %s\\n\", cache_size.c_str());\n+    }\n+#endif\n+  }\n+\n+  void Start() {\n+    start_ = Env::Default()->NowMicros() * 1e-6;\n+    bytes_ = 0;\n+    message_.clear();\n+    last_op_finish_ = start_;\n+    hist_.Clear();\n+    done_ = 0;\n+    next_report_ = 100;\n+  }\n+\n+  void FinishedSingleOp() {\n+    if (FLAGS_histogram) {\n+      double now = Env::Default()->NowMicros() * 1e-6;\n+      double micros = (now - last_op_finish_) * 1e6;\n+      hist_.Add(micros);\n+      if (micros > 20000) {\n+        fprintf(stderr, \"long op: %.1f micros%30s\\r\", micros, \"\");\n+        fflush(stderr);\n+      }\n+      last_op_finish_ = now;\n+    }\n+\n+    done_++;\n+    if (done_ >= next_report_) {\n+      if      (next_report_ < 1000)   next_report_ += 100;\n+      else if (next_report_ < 5000)   next_report_ += 500;\n+      else if (next_report_ < 10000)  next_report_ += 1000;\n+      else if (next_report_ < 50000)  next_report_ += 5000;\n+      else if (next_report_ < 100000) next_report_ += 10000;\n+      else if (next_report_ < 500000) next_report_ += 50000;\n+      else                            next_report_ += 100000;\n+      fprintf(stderr, \"... finished %d ops%30s\\r\", done_, \"\");\n+      fflush(stderr);\n+    }\n+  }\n+\n+  void Stop(const Slice& name) {\n+    double finish = Env::Default()->NowMicros() * 1e-6;\n+\n+    // Pretend at least one op was done in case we are running a benchmark\n+    // that does not call FinishedSingleOp().\n+    if (done_ < 1) done_ = 1;\n+\n+    if (bytes_ > 0) {\n+      char rate[100];\n+      snprintf(rate, sizeof(rate), \"%6.1f MB/s\",\n+               (bytes_ / 1048576.0) / (finish - start_));\n+      if (!message_.empty()) {\n+        message_  = std::string(rate) + \" \" + message_;\n+      } else {\n+        message_ = rate;\n+      }\n+    }\n+\n+    fprintf(stdout, \"%-12s : %11.3f micros/op;%s%s\\n\",\n+            name.ToString().c_str(),\n+            (finish - start_) * 1e6 / done_,\n+            (message_.empty() ? \"\" : \" \"),\n+            message_.c_str());\n+    if (FLAGS_histogram) {\n+      fprintf(stdout, \"Microseconds per op:\\n%s\\n\", hist_.ToString().c_str());\n+    }\n+    fflush(stdout);\n+  }\n+\n+ public:\n+  enum Order {\n+    SEQUENTIAL,\n+    RANDOM\n+  };\n+  enum DBState {\n+    FRESH,\n+    EXISTING\n+  };\n+\n+  Benchmark()\n+  : db_(NULL),\n+    num_(FLAGS_num),\n+    reads_(FLAGS_reads < 0 ? FLAGS_num : FLAGS_reads),\n+    bytes_(0),\n+    rand_(301) {\n+    std::vector<std::string> files;\n+    std::string test_dir;\n+    Env::Default()->GetTestDirectory(&test_dir);\n+    Env::Default()->GetChildren(test_dir.c_str(), &files);\n+    if (!FLAGS_use_existing_db) {\n+      for (int i = 0; i < files.size(); i++) {\n+        if (Slice(files[i]).starts_with(\"dbbench_polyDB\")) {\n+          std::string file_name(test_dir);\n+          file_name += \"/\";\n+          file_name += files[i];\n+          Env::Default()->DeleteFile(file_name.c_str());\n+        }\n+      }\n+    }\n+  }\n+\n+  ~Benchmark() {\n+    if (!db_->close()) {\n+      fprintf(stderr, \"close error: %s\\n\", db_->error().name());\n+    }\n+  }\n+\n+  void Run() {\n+    PrintHeader();\n+    Open(false);\n+\n+    const char* benchmarks = FLAGS_benchmarks;\n+    while (benchmarks != NULL) {\n+      const char* sep = strchr(benchmarks, ',');\n+      Slice name;\n+      if (sep == NULL) {\n+        name = benchmarks;\n+        benchmarks = NULL;\n+      } else {\n+        name = Slice(benchmarks, sep - benchmarks);\n+        benchmarks = sep + 1;\n+      }\n+\n+      Start();\n+\n+      bool known = true;\n+      bool write_sync = false;\n+      if (name == Slice(\"fillseq\")) {\n+        Write(write_sync, SEQUENTIAL, FRESH, num_, FLAGS_value_size, 1);\n+        \n+      } else if (name == Slice(\"fillrandom\")) {\n+        Write(write_sync, RANDOM, FRESH, num_, FLAGS_value_size, 1);\n+        DBSynchronize(db_);\n+      } else if (name == Slice(\"overwrite\")) {\n+        Write(write_sync, RANDOM, EXISTING, num_, FLAGS_value_size, 1);\n+        DBSynchronize(db_);\n+      } else if (name == Slice(\"fillrandsync\")) {\n+        write_sync = true;\n+        Write(write_sync, RANDOM, FRESH, num_ / 100, FLAGS_value_size, 1);\n+        DBSynchronize(db_);\n+      } else if (name == Slice(\"fillseqsync\")) {\n+        write_sync = true;\n+        Write(write_sync, SEQUENTIAL, FRESH, num_ / 100, FLAGS_value_size, 1);\n+        DBSynchronize(db_);\n+      } else if (name == Slice(\"fillrand100K\")) {\n+        Write(write_sync, RANDOM, FRESH, num_ / 1000, 100 * 1000, 1);\n+        DBSynchronize(db_);\n+      } else if (name == Slice(\"fillseq100K\")) {\n+        Write(write_sync, SEQUENTIAL, FRESH, num_ / 1000, 100 * 1000, 1);\n+        DBSynchronize(db_);\n+      } else if (name == Slice(\"readseq\")) {\n+        ReadSequential();\n+      } else if (name == Slice(\"readrandom\")) {\n+        ReadRandom();\n+      } else if (name == Slice(\"readrand100K\")) {\n+        int n = reads_;\n+        reads_ /= 1000;\n+        ReadRandom();\n+        reads_ = n;\n+      } else if (name == Slice(\"readseq100K\")) {\n+        int n = reads_;\n+        reads_ /= 1000;\n+        ReadSequential();\n+        reads_ = n;\n+      } else {\n+        known = false;\n+        if (name != Slice()) {  // No error message for empty name\n+          fprintf(stderr, \"unknown benchmark '%s'\\n\", name.ToString().c_str());\n+        }\n+      }\n+      if (known) {\n+        Stop(name);\n+      }\n+    }\n+  }\n+\n+ private:\n+    void Open(bool sync) {\n+    assert(db_ == NULL);\n+\n+    // Initialize db_\n+    db_ = new kyotocabinet::TreeDB();\n+    char file_name[100];\n+    db_num_++;\n+    std::string test_dir;\n+    Env::Default()->GetTestDirectory(&test_dir);\n+    snprintf(file_name, sizeof(file_name),\n+             \"%s/dbbench_polyDB-%d.kct\",\n+             test_dir.c_str(),\n+             db_num_);\n+\n+    // Create tuning options and open the database\n+    int open_options = kyotocabinet::PolyDB::OWRITER |\n+                       kyotocabinet::PolyDB::OCREATE;\n+    int tune_options = kyotocabinet::TreeDB::TSMALL |\n+        kyotocabinet::TreeDB::TLINEAR;\n+    if (FLAGS_compression) {\n+      tune_options |= kyotocabinet::TreeDB::TCOMPRESS;\n+      db_->tune_compressor(&comp_);\n+    }\n+    db_->tune_options(tune_options);\n+    db_->tune_page_cache(FLAGS_cache_size);\n+    db_->tune_page(FLAGS_page_size);\n+    db_->tune_map(256LL<<20);\n+    if (sync) {\n+      open_options |= kyotocabinet::PolyDB::OAUTOSYNC;\n+    }\n+    if (!db_->open(file_name, open_options)) {\n+      fprintf(stderr, \"open error: %s\\n\", db_->error().name());\n+    }\n+  }\n+\n+  void Write(bool sync, Order order, DBState state,\n+             int num_entries, int value_size, int entries_per_batch) {\n+    // Create new database if state == FRESH\n+    if (state == FRESH) {\n+      if (FLAGS_use_existing_db) {\n+        message_ = \"skipping (--use_existing_db is true)\";\n+        return;\n+      }\n+      delete db_;\n+      db_ = NULL;\n+      Open(sync);\n+      Start();  // Do not count time taken to destroy/open\n+    }\n+\n+    if (num_entries != num_) {\n+      char msg[100];\n+      snprintf(msg, sizeof(msg), \"(%d ops)\", num_entries);\n+      message_ = msg;\n+    }\n+\n+    // Write to database\n+    for (int i = 0; i < num_entries; i++)\n+    {\n+      const int k = (order == SEQUENTIAL) ? i : (rand_.Next() % num_entries);\n+      char key[100];\n+      snprintf(key, sizeof(key), \"%016d\", k);\n+      bytes_ += value_size + strlen(key);\n+      std::string cpp_key = key;\n+      if (!db_->set(cpp_key, gen_.Generate(value_size).ToString())) {\n+        fprintf(stderr, \"set error: %s\\n\", db_->error().name());\n+      }\n+      FinishedSingleOp();\n+    }\n+  }\n+\n+  void ReadSequential() {\n+    kyotocabinet::DB::Cursor* cur = db_->cursor();\n+    cur->jump();\n+    std::string ckey, cvalue;\n+    while (cur->get(&ckey, &cvalue, true)) {\n+      bytes_ += ckey.size() + cvalue.size();\n+      FinishedSingleOp();\n+    }\n+    delete cur;\n+  }\n+\n+  void ReadRandom() {\n+    std::string value;\n+    for (int i = 0; i < reads_; i++) {\n+      char key[100];\n+      const int k = rand_.Next() % reads_;\n+      snprintf(key, sizeof(key), \"%016d\", k);\n+      db_->get(key, &value);\n+      FinishedSingleOp();\n+    }\n+  }\n+};\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  std::string default_db_path;\n+  for (int i = 1; i < argc; i++) {\n+    double d;\n+    int n;\n+    char junk;\n+    if (leveldb::Slice(argv[i]).starts_with(\"--benchmarks=\")) {\n+      FLAGS_benchmarks = argv[i] + strlen(\"--benchmarks=\");\n+    } else if (sscanf(argv[i], \"--compression_ratio=%lf%c\", &d, &junk) == 1) {\n+      FLAGS_compression_ratio = d;\n+    } else if (sscanf(argv[i], \"--histogram=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_histogram = n;\n+    } else if (sscanf(argv[i], \"--num=%d%c\", &n, &junk) == 1) {\n+      FLAGS_num = n;\n+    } else if (sscanf(argv[i], \"--reads=%d%c\", &n, &junk) == 1) {\n+      FLAGS_reads = n;\n+    } else if (sscanf(argv[i], \"--value_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_value_size = n;\n+    } else if (sscanf(argv[i], \"--cache_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_cache_size = n;\n+    } else if (sscanf(argv[i], \"--page_size=%d%c\", &n, &junk) == 1) {\n+      FLAGS_page_size = n;\n+    } else if (sscanf(argv[i], \"--compression=%d%c\", &n, &junk) == 1 &&\n+               (n == 0 || n == 1)) {\n+      FLAGS_compression = (n == 1) ? true : false;\n+    } else if (strncmp(argv[i], \"--db=\", 5) == 0) {\n+      FLAGS_db = argv[i] + 5;\n+    } else {\n+      fprintf(stderr, \"Invalid flag '%s'\\n\", argv[i]);\n+      exit(1);\n+    }\n+  }\n+\n+  // Choose a location for the test database if none given with --db=<path>\n+  if (FLAGS_db == NULL) {\n+      leveldb::Env::Default()->GetTestDirectory(&default_db_path);\n+      default_db_path += \"/dbbench\";\n+      FLAGS_db = default_db_path.c_str();\n+  }\n+\n+  leveldb::Benchmark benchmark;\n+  benchmark.Run();\n+  return 0;\n+}"
      },
      {
        "sha": "c4639772c175b463f6f41eeb0cd0c73fe7c16d68",
        "filename": "src/leveldb/doc/benchmark.html",
        "status": "added",
        "additions": 459,
        "deletions": 0,
        "changes": 459,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/benchmark.html",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/benchmark.html",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/benchmark.html?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,459 @@\n+<!DOCTYPE html>\n+<html>\n+<head>\n+<title>LevelDB Benchmarks</title>\n+<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n+<style>\n+body {\n+  font-family:Helvetica,sans-serif;\n+  padding:20px;\n+}\n+\n+h2 {\n+  padding-top:30px;\n+}\n+\n+table.bn {\n+  width:800px;\n+  border-collapse:collapse;\n+  border:0;\n+  padding:0;\n+}\n+\n+table.bnbase {\n+  width:650px;\n+}\n+\n+table.bn td {\n+  padding:2px 0;\n+}\n+\n+table.bn td.c1 {\n+  font-weight:bold;\n+  width:150px;\n+}\n+\n+table.bn td.c1 div.e {\n+  float:right;\n+  font-weight:normal;\n+}\n+\n+table.bn td.c2 {\n+  width:150px;\n+  text-align:right;\n+  padding:2px;\n+}\n+\n+table.bn td.c3 {\n+  width:350px;\n+}\n+\n+table.bn td.c4 {\n+  width:150px;\n+  font-size:small;\n+  padding-left:4px;\n+}\n+\n+/* chart bars */\n+div.bldb {\n+  background-color:#0255df;\n+}\n+\n+div.bkct {\n+  background-color:#df5555;\n+}\n+\n+div.bsql {\n+  background-color:#aadf55;\n+}\n+\n+.code {\n+  font-family:monospace;\n+  font-size:large;\n+}\n+\n+.todo {\n+  color: red;\n+}\n+\n+</style>\n+</head>\n+<body>\n+<h1>LevelDB Benchmarks</h1>\n+<p>Google, July 2011</p>\n+<hr>\n+\n+<p>In order to test LevelDB's performance, we benchmark it against other well-established database implementations. We compare LevelDB (revision 39) against <a href=\"http://www.sqlite.org/\">SQLite3</a> (version 3.7.6.3) and <a href=\"http://fallabs.com/kyotocabinet/spex.html\">Kyoto Cabinet's</a> (version 1.2.67) TreeDB (a B+Tree based key-value store). We would like to acknowledge Scott Hess and Mikio Hirabayashi for their suggestions and contributions to the SQLite3 and Kyoto Cabinet benchmarks, respectively.</p>\n+\n+<p>Benchmarks were all performed on a six-core Intel(R) Xeon(R) CPU X5650 @ 2.67GHz, with 12288 KB of total L3 cache and 12 GB of DDR3 RAM at 1333 MHz. (Note that LevelDB uses at most two CPUs since the benchmarks are single threaded: one to run the benchmark, and one for background compactions.) We ran the benchmarks on two machines (with identical processors), one with an Ext3 file system and one with an Ext4 file system. The machine with the Ext3 file system has a SATA Hitachi HDS721050CLA362 hard drive. The machine with the Ext4 file system has a SATA Samsung HD502HJ hard drive. Both hard drives spin at 7200 RPM and have hard drive write-caching enabled (using `hdparm -W 1 [device]`). The numbers reported below are the median of three measurements.</p>\n+\n+<h4>Benchmark Source Code</h4>\n+<p>We wrote benchmark tools for SQLite and Kyoto TreeDB based on LevelDB's <span class=\"code\">db_bench</span>. The code for each of the benchmarks resides here:</p>\n+<ul>\n+\t<li> <b>LevelDB:</b> <a href=\"http://code.google.com/p/leveldb/source/browse/trunk/db/db_bench.cc\">db/db_bench.cc</a>.</li>\n+\t<li> <b>SQLite:</b> <a href=\"http://code.google.com/p/leveldb/source/browse/#svn%2Ftrunk%2Fdoc%2Fbench%2Fdb_bench_sqlite3.cc\">doc/bench/db_bench_sqlite3.cc</a>.</li>\n+\t<li> <b>Kyoto TreeDB:</b> <a href=\"http://code.google.com/p/leveldb/source/browse/#svn%2Ftrunk%2Fdoc%2Fbench%2Fdb_bench_tree_db.cc\">doc/bench/db_bench_tree_db.cc</a>.</li>\n+</ul>\n+\n+<h4>Custom Build Specifications</h4>\n+<ul>\n+<li>LevelDB: LevelDB was compiled with the <a href=\"http://code.google.com/p/google-perftools\">tcmalloc</a> library and the <a href=\"http://code.google.com/p/snappy/\">Snappy</a> compression library (revision 33).  Assertions were disabled.</li>\n+<li>TreeDB: TreeDB was compiled using the <a href=\"http://www.oberhumer.com/opensource/lzo/\">LZO</a> compression library (version 2.03). Furthermore, we enabled the TSMALL and TLINEAR options when opening the database in order to reduce the footprint of each record.</li>\n+<li>SQLite: We tuned SQLite's performance, by setting its locking mode to exclusive.  We also enabled SQLite's <a href=\"http://www.sqlite.org/draft/wal.html\">write-ahead logging</a>.</li>\n+</ul>\n+\n+<h2>1. Baseline Performance</h2>\n+<p>This section gives the baseline performance of all the\n+databases.  Following sections show how performance changes as various\n+parameters are varied.  For the baseline:</p>\n+<ul>\n+\t<li> Each database is allowed 4 MB of cache memory.</li>\n+        <li> Databases are opened in <em>asynchronous</em> write mode.\n+             (LevelDB's sync option, TreeDB's OAUTOSYNC option, and\n+             SQLite3's synchronous options are all turned off).  I.e.,\n+             every write is pushed to the operating system, but the\n+             benchmark does not wait for the write to reach the disk.</li>\n+\t<li> Keys are 16 bytes each.</li>\n+        <li> Value are 100 bytes each (with enough redundancy so that\n+             a simple compressor shrinks them to 50% of their original\n+             size).</li>\n+\t<li> Sequential reads/writes traverse the key space in increasing order.</li>\n+\t<li> Random reads/writes traverse the key space in random order.</li>\n+</ul>\n+\n+<h3>A. Sequential Reads</h3>\n+<table class=\"bn bnbase\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">4,030,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">1,010,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:95px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">383,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:33px\">&nbsp;</div></td>\n+</table>\n+<h3>B. Random Reads</h3>\n+<table class=\"bn bnbase\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">129,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:298px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">151,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:350px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">134,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:310px\">&nbsp;</div></td>\n+</table>\n+<h3>C. Sequential Writes</h3>\n+<table class=\"bn bnbase\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">779,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">342,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:154px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">48,600 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:22px\">&nbsp;</div></td>\n+</table>\n+<h3>D. Random Writes</h3>\n+<table class=\"bn bnbase\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">164,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">88,500 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:188px\">&nbsp;</div></td>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">9,860 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:21px\">&nbsp;</div></td>\n+</table>\n+\n+<p>LevelDB outperforms both SQLite3 and TreeDB in sequential and random write operations and sequential read operations. Kyoto Cabinet has the fastest random read operations.</p>\n+\n+<h2>2. Write Performance under Different Configurations</h2>\n+<h3>A. Large Values </h3>\n+<p>For this benchmark, we start with an empty database, and write 100,000 byte values (~50% compressible). To keep the benchmark running time reasonable, we stop after writing 1000 values.</p>\n+<h4>Sequential Writes</h4>\n+<table class=\"bn bnbase\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">1,100 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:234px\">&nbsp;</div></td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">1,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:224px\">&nbsp;</div></td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">1,600 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:350px\">&nbsp;</div></td></tr>\n+</table>\n+<h4>Random Writes</h4>\n+<table class=\"bn bnbase\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">480 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:105px\">&nbsp;</div></td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">1,100 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:240px\">&nbsp;</div></td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">1,600 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:350px\">&nbsp;</div></td></tr>\n+</table>\n+<p>LevelDB doesn't perform as well with large values of 100,000 bytes each. This is because LevelDB writes keys and values at least twice: first time to the transaction log, and second time (during a compaction) to a sorted file.\n+With larger values, LevelDB's per-operation efficiency is swamped by the\n+cost of extra copies of large values.</p>\n+<h3>B. Batch Writes</h3>\n+<p>A batch write is a set of writes that are applied atomically to the underlying database. A single batch of N writes may be significantly faster than N individual writes. The following benchmark writes one thousand batches where each batch contains one thousand 100-byte values. TreeDB does not support batch writes and is omitted from this benchmark.</p>\n+<h4>Sequential Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">840,000 entries/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.08x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">124,000 entries/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:52px\">&nbsp;</div></td>\n+    <td class=\"c4\">(2.55x baseline)</td></tr>\n+</table>\n+<h4>Random Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">221,000 entries/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.35x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">22,000 entries/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:34px\">&nbsp;</div></td>\n+    <td class=\"c4\">(2.23x baseline)</td></tr>\n+</table>\n+\n+<p>Because of the way LevelDB persistent storage is organized, batches of\n+random writes are not much slower (only a factor of 4x) than batches\n+of sequential writes.</p>\n+\n+<h3>C. Synchronous Writes</h3>\n+<p>In the following benchmark, we enable the synchronous writing modes\n+of all of the databases.  Since this change significantly slows down the\n+benchmark, we stop after 10,000 writes. For synchronous write tests, we've\n+disabled hard drive write-caching (using `hdparm -W 0 [device]`).</p>\n+<ul>\n+    <li>For LevelDB, we set WriteOptions.sync = true.</li>\n+    <li>In TreeDB, we enabled TreeDB's OAUTOSYNC option.</li>\n+    <li>For SQLite3, we set \"PRAGMA synchronous = FULL\".</li>\n+</ul>\n+<h4>Sequential Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">100 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.003x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">7 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:27px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.0004x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">88 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:315px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.002x baseline)</td></tr>\n+</table>\n+<h4>Random Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">100 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.015x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">8 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:29px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.001x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">88 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:314px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.009x baseline)</td></tr>\n+</table>\n+\n+<p>Also see the <code>ext4</code> performance numbers below\n+since synchronous writes behave significantly differently\n+on <code>ext3</code> and <code>ext4</code>.</p>\n+\n+<h3>D. Turning Compression Off</h3>\n+\n+<p>In the baseline measurements, LevelDB and TreeDB were using\n+light-weight compression\n+(<a href=\"http://code.google.com/p/snappy/\">Snappy</a> for LevelDB,\n+and <a href=\"http://www.oberhumer.com/opensource/lzo/\">LZO</a> for\n+TreeDB). SQLite3, by default does not use compression.  The\n+experiments below show what happens when compression is disabled in\n+all of the databases (the SQLite3 numbers are just a copy of\n+its baseline measurements):</p>\n+\n+<h4>Sequential Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">594,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.76x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">485,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:239px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.42x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">48,600 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:29px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.00x baseline)</td></tr>\n+</table>\n+<h4>Random Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">135,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:296px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.82x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">159,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.80x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">9,860 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:22px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.00x baseline)</td></tr>\n+</table>\n+\n+<p>LevelDB's write performance is better with compression than without\n+since compression decreases the amount of data that has to be written\n+to disk.  Therefore LevelDB users can leave compression enabled in\n+most scenarios without having worry about a tradeoff between space\n+usage and performance.  TreeDB's performance on the other hand is\n+better without compression than with compression.  Presumably this is\n+because TreeDB's compression library (LZO) is more expensive than\n+LevelDB's compression library (Snappy).<p>\n+\n+<h3>E. Using More Memory</h3>\n+<p>We increased the overall cache size for each database to 128 MB. For LevelDB, we partitioned 128 MB into a 120 MB write buffer and 8 MB of cache (up from 2 MB of write buffer and 2 MB of cache). For SQLite3, we kept the page size at 1024 bytes, but increased the number of pages to 131,072 (up from 4096). For TreeDB, we also kept the page size at 1024 bytes, but increased the cache size to 128 MB (up from 4 MB).</p>\n+<h4>Sequential Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">812,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.04x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">321,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:138px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.94x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">48,500 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:21px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.00x baseline)</td></tr>\n+</table>\n+<h4>Random Writes</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">355,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(2.16x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">284,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:280px\">&nbsp;</div></td>\n+    <td class=\"c4\">(3.21x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">9,670 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:10px\">&nbsp;</div></td>\n+    <td class=\"c4\">(0.98x baseline)</td></tr>\n+</table>\n+\n+<p>SQLite's performance does not change substantially when compared to\n+the baseline, but the random write performance for both LevelDB and\n+TreeDB increases significantly.  LevelDB's performance improves\n+because a larger write buffer reduces the need to merge sorted files\n+(since it creates a smaller number of larger sorted files).  TreeDB's\n+performance goes up because the entire database is available in memory\n+for fast in-place updates.</p>\n+\n+  <h2>3. Read Performance under Different Configurations</h2>\n+<h3>A. Larger Caches</h3>\n+<p>We increased the overall memory usage to 128 MB for each database.\n+For LevelDB, we allocated 8 MB to LevelDB's write buffer and 120 MB\n+to LevelDB's cache. The other databases don't differentiate between a\n+write buffer and a cache, so we simply set their cache size to 128\n+MB.</p>\n+<h4>Sequential Reads</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">5,210,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.29x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">1,070,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:72px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.06x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">609,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:41px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.59x baseline)</td></tr>\n+</table>\n+\n+<h4>Random Reads</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">190,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:144px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.47x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">463,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(3.07x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">186,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:141px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.39x baseline)</td></tr>\n+</table>\n+\n+<p>As expected, the read performance of all of the databases increases\n+when the caches are enlarged.  In particular, TreeDB seems to make\n+very effective use of a cache that is large enough to hold the entire\n+database.</p>\n+\n+<h3>B. No Compression Reads </h3>\n+<p>For this benchmark, we populated a database with 1 million entries consisting of 16 byte keys and 100 byte values. We compiled LevelDB and Kyoto Cabinet without compression support, so results that are read out from the database are already uncompressed. We've listed the SQLite3 baseline read performance as a point of comparison.</p>\n+<h4>Sequential Reads</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">4,880,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.21x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">1,230,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:88px\">&nbsp;</div></td>\n+    <td class=\"c4\">(3.60x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">383,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:27px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.00x baseline)</td></tr>\n+</table>\n+<h4>Random Reads</h4>\n+<table class=\"bn\">\n+<tr><td class=\"c1\">LevelDB</td>\n+    <td class=\"c2\">149,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bldb\" style=\"width:300px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.16x baseline)</td></tr>\n+<tr><td class=\"c1\">Kyoto TreeDB</td>\n+    <td class=\"c2\">175,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bkct\" style=\"width:350px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.16x baseline)</td></tr>\n+<tr><td class=\"c1\">SQLite3</td>\n+    <td class=\"c2\">134,000 ops/sec</td>\n+    <td class=\"c3\"><div class=\"bsql\" style=\"width:268px\">&nbsp;</div></td>\n+    <td class=\"c4\">(1.00x baseline)</td></tr>\n+</table>\n+\n+<p>Performance of both LevelDB and TreeDB improves a small amount when\n+compression is disabled.  Note however that under different workloads,\n+performance may very well be better with compression if it allows more\n+of the working set to fit in memory.</p>\n+\n+<h2>Note about Ext4 Filesystems</h2>\n+<p>The preceding numbers are for an ext3 file system. Synchronous writes are much slower under <a href=\"http://en.wikipedia.org/wiki/Ext4\">ext4</a> (LevelDB drops to ~31 writes / second and TreeDB drops to ~5 writes / second; SQLite3's synchronous writes do not noticeably drop) due to ext4's different handling of <span class=\"code\">fsync</span> / <span class=\"code\">msync</span> calls. Even LevelDB's asynchronous write performance drops somewhat since it spreads its storage across multiple files and issues <span class=\"code\">fsync</span> calls when switching to a new file.</p>\n+\n+<h2>Acknowledgements</h2>\n+<p>Jeff Dean and Sanjay Ghemawat wrote LevelDB. Kevin Tseng wrote and compiled these benchmarks. Mikio Hirabayashi, Scott Hess, and Gabor Cselle provided help and advice.</p>\n+</body>\n+</html>"
      },
      {
        "sha": "700c564e433827c311cd8a26896ceaa7c8260818",
        "filename": "src/leveldb/doc/doc.css",
        "status": "added",
        "additions": 89,
        "deletions": 0,
        "changes": 89,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/doc.css",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/doc.css",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/doc.css?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,89 @@\n+body {\n+  margin-left: 0.5in;\n+  margin-right: 0.5in;\n+  background: white;\n+  color: black;\n+}\n+\n+h1 {\n+  margin-left: -0.2in;\n+  font-size: 14pt;\n+}\n+h2 {\n+  margin-left: -0in;\n+  font-size: 12pt;\n+}\n+h3 {\n+  margin-left: -0in;\n+}\n+h4 {\n+  margin-left: -0in;\n+}\n+hr {\n+  margin-left: -0in;\n+}\n+\n+/* Definition lists: definition term bold */\n+dt {\n+  font-weight: bold;\n+}\n+\n+address {\n+  text-align: center;\n+}\n+code,samp,var {\n+  color: blue;\n+}\n+kbd {\n+  color: #600000;\n+}\n+div.note p {\n+  float: right;\n+  width: 3in;\n+  margin-right: 0%;\n+  padding: 1px;\n+  border: 2px solid #6060a0;\n+  background-color: #fffff0;\n+}\n+\n+ul {\n+  margin-top: -0em;\n+  margin-bottom: -0em;\n+}\n+\n+ol {\n+  margin-top: -0em;\n+  margin-bottom: -0em;\n+}\n+\n+UL.nobullets {\n+  list-style-type: none;\n+  list-style-image: none;\n+  margin-left: -1em;\n+}\n+\n+p {\n+  margin: 1em 0 1em 0;\n+  padding: 0 0 0 0;\n+}\n+\n+pre {\n+  line-height: 1.3em;\n+  padding: 0.4em 0 0.8em 0;\n+  margin:  0 0 0 0;\n+  border:  0 0 0 0;\n+  color: blue;\n+}\n+\n+.datatable {\n+  margin-left: auto;\n+  margin-right: auto;\n+  margin-top: 2em;\n+  margin-bottom: 2em;\n+  border: 1px solid;\n+}\n+\n+.datatable td,th {\n+  padding: 0 0.5em 0 0.5em;\n+  text-align: right;\n+}"
      },
      {
        "sha": "e870795d231463b167d5b79efdc16b80107de93e",
        "filename": "src/leveldb/doc/impl.html",
        "status": "added",
        "additions": 213,
        "deletions": 0,
        "changes": 213,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/impl.html",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/impl.html",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/impl.html?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,213 @@\n+<!DOCTYPE html>\n+<html>\n+<head>\n+<link rel=\"stylesheet\" type=\"text/css\" href=\"doc.css\" />\n+<title>Leveldb file layout and compactions</title>\n+</head>\n+\n+<body>\n+\n+<h1>Files</h1>\n+\n+The implementation of leveldb is similar in spirit to the\n+representation of a single\n+<a href=\"http://labs.google.com/papers/bigtable.html\">\n+Bigtable tablet (section 5.3)</a>.\n+However the organization of the files that make up the representation\n+is somewhat different and is explained below.\n+\n+<p>\n+Each database is represented by a set of files stored in a directory.\n+There are several different types of files as documented below:\n+<p>\n+<h2>Log files</h2>\n+<p>\n+A log file (*.log) stores a sequence of recent updates.  Each update\n+is appended to the current log file.  When the log file reaches a\n+pre-determined size (approximately 4MB by default), it is converted\n+to a sorted table (see below) and a new log file is created for future\n+updates.\n+<p>\n+A copy of the current log file is kept in an in-memory structure (the\n+<code>memtable</code>).  This copy is consulted on every read so that read\n+operations reflect all logged updates.\n+<p>\n+<h2>Sorted tables</h2>\n+<p>\n+A sorted table (*.sst) stores a sequence of entries sorted by key.\n+Each entry is either a value for the key, or a deletion marker for the\n+key.  (Deletion markers are kept around to hide obsolete values\n+present in older sorted tables).\n+<p>\n+The set of sorted tables are organized into a sequence of levels.  The\n+sorted table generated from a log file is placed in a special <code>young</code>\n+level (also called level-0).  When the number of young files exceeds a\n+certain threshold (currently four), all of the young files are merged\n+together with all of the overlapping level-1 files to produce a\n+sequence of new level-1 files (we create a new level-1 file for every\n+2MB of data.)\n+<p>\n+Files in the young level may contain overlapping keys.  However files\n+in other levels have distinct non-overlapping key ranges.  Consider\n+level number L where L >= 1.  When the combined size of files in\n+level-L exceeds (10^L) MB (i.e., 10MB for level-1, 100MB for level-2,\n+...), one file in level-L, and all of the overlapping files in\n+level-(L+1) are merged to form a set of new files for level-(L+1).\n+These merges have the effect of gradually migrating new updates from\n+the young level to the largest level using only bulk reads and writes\n+(i.e., minimizing expensive seeks).\n+\n+<h2>Manifest</h2>\n+<p>\n+A MANIFEST file lists the set of sorted tables that make up each\n+level, the corresponding key ranges, and other important metadata.\n+A new MANIFEST file (with a new number embedded in the file name)\n+is created whenever the database is reopened.  The MANIFEST file is\n+formatted as a log, and changes made to the serving state (as files\n+are added or removed) are appended to this log.\n+<p>\n+<h2>Current</h2>\n+<p>\n+CURRENT is a simple text file that contains the name of the latest\n+MANIFEST file.\n+<p>\n+<h2>Info logs</h2>\n+<p>\n+Informational messages are printed to files named LOG and LOG.old.\n+<p>\n+<h2>Others</h2>\n+<p>\n+Other files used for miscellaneous purposes may also be present\n+(LOCK, *.dbtmp).\n+\n+<h1>Level 0</h1>\n+When the log file grows above a certain size (1MB by default):\n+<ul>\n+<li>Create a brand new memtable and log file and direct future updates here\n+<li>In the background:\n+<ul>\n+<li>Write the contents of the previous memtable to an sstable\n+<li>Discard the memtable\n+<li>Delete the old log file and the old memtable\n+<li>Add the new sstable to the young (level-0) level.\n+</ul>\n+</ul>\n+\n+<h1>Compactions</h1>\n+\n+<p>\n+When the size of level L exceeds its limit, we compact it in a\n+background thread.  The compaction picks a file from level L and all\n+overlapping files from the next level L+1.  Note that if a level-L\n+file overlaps only part of a level-(L+1) file, the entire file at\n+level-(L+1) is used as an input to the compaction and will be\n+discarded after the compaction.  Aside: because level-0 is special\n+(files in it may overlap each other), we treat compactions from\n+level-0 to level-1 specially: a level-0 compaction may pick more than\n+one level-0 file in case some of these files overlap each other.\n+\n+<p>\n+A compaction merges the contents of the picked files to produce a\n+sequence of level-(L+1) files.  We switch to producing a new\n+level-(L+1) file after the current output file has reached the target\n+file size (2MB).  We also switch to a new output file when the key\n+range of the current output file has grown enough to overlap more then\n+ten level-(L+2) files.  This last rule ensures that a later compaction\n+of a level-(L+1) file will not pick up too much data from level-(L+2).\n+\n+<p>\n+The old files are discarded and the new files are added to the serving\n+state.\n+\n+<p>\n+Compactions for a particular level rotate through the key space.  In\n+more detail, for each level L, we remember the ending key of the last\n+compaction at level L.  The next compaction for level L will pick the\n+first file that starts after this key (wrapping around to the\n+beginning of the key space if there is no such file).\n+\n+<p>\n+Compactions drop overwritten values.  They also drop deletion markers\n+if there are no higher numbered levels that contain a file whose range\n+overlaps the current key.\n+\n+<h2>Timing</h2>\n+\n+Level-0 compactions will read up to four 1MB files from level-0, and\n+at worst all the level-1 files (10MB).  I.e., we will read 14MB and\n+write 14MB.\n+\n+<p>\n+Other than the special level-0 compactions, we will pick one 2MB file\n+from level L.  In the worst case, this will overlap ~ 12 files from\n+level L+1 (10 because level-(L+1) is ten times the size of level-L,\n+and another two at the boundaries since the file ranges at level-L\n+will usually not be aligned with the file ranges at level-L+1).  The\n+compaction will therefore read 26MB and write 26MB.  Assuming a disk\n+IO rate of 100MB/s (ballpark range for modern drives), the worst\n+compaction cost will be approximately 0.5 second.\n+\n+<p>\n+If we throttle the background writing to something small, say 10% of\n+the full 100MB/s speed, a compaction may take up to 5 seconds.  If the\n+user is writing at 10MB/s, we might build up lots of level-0 files\n+(~50 to hold the 5*10MB).  This may signficantly increase the cost of\n+reads due to the overhead of merging more files together on every\n+read.\n+\n+<p>\n+Solution 1: To reduce this problem, we might want to increase the log\n+switching threshold when the number of level-0 files is large.  Though\n+the downside is that the larger this threshold, the more memory we will\n+need to hold the corresponding memtable.\n+\n+<p>\n+Solution 2: We might want to decrease write rate artificially when the\n+number of level-0 files goes up.\n+\n+<p>\n+Solution 3: We work on reducing the cost of very wide merges.\n+Perhaps most of the level-0 files will have their blocks sitting\n+uncompressed in the cache and we will only need to worry about the\n+O(N) complexity in the merging iterator.\n+\n+<h2>Number of files</h2>\n+\n+Instead of always making 2MB files, we could make larger files for\n+larger levels to reduce the total file count, though at the expense of\n+more bursty compactions.  Alternatively, we could shard the set of\n+files into multiple directories.\n+\n+<p>\n+An experiment on an <code>ext3</code> filesystem on Feb 04, 2011 shows\n+the following timings to do 100K file opens in directories with\n+varying number of files:\n+<table class=\"datatable\">\n+<tr><th>Files in directory</th><th>Microseconds to open a file</th></tr>\n+<tr><td>1000</td><td>9</td>\n+<tr><td>10000</td><td>10</td>\n+<tr><td>100000</td><td>16</td>\n+</table>\n+So maybe even the sharding is not necessary on modern filesystems?\n+\n+<h1>Recovery</h1>\n+\n+<ul>\n+<li> Read CURRENT to find name of the latest committed MANIFEST\n+<li> Read the named MANIFEST file\n+<li> Clean up stale files\n+<li> We could open all sstables here, but it is probably better to be lazy...\n+<li> Convert log chunk to a new level-0 sstable\n+<li> Start directing new writes to a new log file with recovered sequence#\n+</ul>\n+\n+<h1>Garbage collection of files</h1>\n+\n+<code>DeleteObsoleteFiles()</code> is called at the end of every\n+compaction and at the end of recovery.  It finds the names of all\n+files in the database.  It deletes all log files that are not the\n+current log file.  It deletes all table files that are not referenced\n+from some level and are not the output of an active compaction.\n+\n+</body>\n+</html>"
      },
      {
        "sha": "3ed0ed9d9e305ed630f39a201e3034e00d815b43",
        "filename": "src/leveldb/doc/index.html",
        "status": "added",
        "additions": 549,
        "deletions": 0,
        "changes": 549,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/index.html",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/index.html",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/index.html?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,549 @@\n+<!DOCTYPE html>\n+<html>\n+<head>\n+<link rel=\"stylesheet\" type=\"text/css\" href=\"doc.css\" />\n+<title>Leveldb</title>\n+</head>\n+\n+<body>\n+<h1>Leveldb</h1>\n+<address>Jeff Dean, Sanjay Ghemawat</address>\n+<p>\n+The <code>leveldb</code> library provides a persistent key value store.  Keys and\n+values are arbitrary byte arrays.  The keys are ordered within the key\n+value store according to a user-specified comparator function.\n+\n+<p>\n+<h1>Opening A Database</h1>\n+<p>\n+A <code>leveldb</code> database has a name which corresponds to a file system\n+directory.  All of the contents of database are stored in this\n+directory.  The following example shows how to open a database,\n+creating it if necessary:\n+<p>\n+<pre>\n+  #include &lt;assert&gt;\n+  #include \"leveldb/db.h\"\n+\n+  leveldb::DB* db;\n+  leveldb::Options options;\n+  options.create_if_missing = true;\n+  leveldb::Status status = leveldb::DB::Open(options, \"/tmp/testdb\", &amp;db);\n+  assert(status.ok());\n+  ...\n+</pre>\n+If you want to raise an error if the database already exists, add\n+the following line before the <code>leveldb::DB::Open</code> call:\n+<pre>\n+  options.error_if_exists = true;\n+</pre>\n+<h1>Status</h1>\n+<p>\n+You may have noticed the <code>leveldb::Status</code> type above.  Values of this\n+type are returned by most functions in <code>leveldb</code> that may encounter an\n+error.  You can check if such a result is ok, and also print an\n+associated error message:\n+<p>\n+<pre>\n+   leveldb::Status s = ...;\n+   if (!s.ok()) cerr &lt;&lt; s.ToString() &lt;&lt; endl;\n+</pre>\n+<h1>Closing A Database</h1>\n+<p>\n+When you are done with a database, just delete the database object.\n+Example:\n+<p>\n+<pre>\n+  ... open the db as described above ...\n+  ... do something with db ...\n+  delete db;\n+</pre>\n+<h1>Reads And Writes</h1>\n+<p>\n+The database provides <code>Put</code>, <code>Delete</code>, and <code>Get</code> methods to\n+modify/query the database.  For example, the following code\n+moves the value stored under key1 to key2.\n+<pre>\n+  std::string value;\n+  leveldb::Status s = db-&gt;Get(leveldb::ReadOptions(), key1, &amp;value);\n+  if (s.ok()) s = db-&gt;Put(leveldb::WriteOptions(), key2, value);\n+  if (s.ok()) s = db-&gt;Delete(leveldb::WriteOptions(), key1);\n+</pre>\n+\n+<h1>Atomic Updates</h1>\n+<p>\n+Note that if the process dies after the Put of key2 but before the\n+delete of key1, the same value may be left stored under multiple keys.\n+Such problems can be avoided by using the <code>WriteBatch</code> class to\n+atomically apply a set of updates:\n+<p>\n+<pre>\n+  #include \"leveldb/write_batch.h\"\n+  ...\n+  std::string value;\n+  leveldb::Status s = db-&gt;Get(leveldb::ReadOptions(), key1, &amp;value);\n+  if (s.ok()) {\n+    leveldb::WriteBatch batch;\n+    batch.Delete(key1);\n+    batch.Put(key2, value);\n+    s = db-&gt;Write(leveldb::WriteOptions(), &amp;batch);\n+  }\n+</pre>\n+The <code>WriteBatch</code> holds a sequence of edits to be made to the database,\n+and these edits within the batch are applied in order.  Note that we\n+called <code>Delete</code> before <code>Put</code> so that if <code>key1</code> is identical to <code>key2</code>,\n+we do not end up erroneously dropping the value entirely.\n+<p>\n+Apart from its atomicity benefits, <code>WriteBatch</code> may also be used to\n+speed up bulk updates by placing lots of individual mutations into the\n+same batch.\n+\n+<h1>Synchronous Writes</h1>\n+By default, each write to <code>leveldb</code> is asynchronous: it\n+returns after pushing the write from the process into the operating\n+system.  The transfer from operating system memory to the underlying\n+persistent storage happens asynchronously.  The <code>sync</code> flag\n+can be turned on for a particular write to make the write operation\n+not return until the data being written has been pushed all the way to\n+persistent storage.  (On Posix systems, this is implemented by calling\n+either <code>fsync(...)</code> or <code>fdatasync(...)</code> or\n+<code>msync(..., MS_SYNC)</code> before the write operation returns.)\n+<pre>\n+  leveldb::WriteOptions write_options;\n+  write_options.sync = true;\n+  db-&gt;Put(write_options, ...);\n+</pre>\n+Asynchronous writes are often more than a thousand times as fast as\n+synchronous writes.  The downside of asynchronous writes is that a\n+crash of the machine may cause the last few updates to be lost.  Note\n+that a crash of just the writing process (i.e., not a reboot) will not\n+cause any loss since even when <code>sync</code> is false, an update\n+is pushed from the process memory into the operating system before it\n+is considered done.\n+\n+<p>\n+Asynchronous writes can often be used safely.  For example, when\n+loading a large amount of data into the database you can handle lost\n+updates by restarting the bulk load after a crash.  A hybrid scheme is\n+also possible where every Nth write is synchronous, and in the event\n+of a crash, the bulk load is restarted just after the last synchronous\n+write finished by the previous run.  (The synchronous write can update\n+a marker that describes where to restart on a crash.)\n+\n+<p>\n+<code>WriteBatch</code> provides an alternative to asynchronous writes.\n+Multiple updates may be placed in the same <code>WriteBatch</code> and\n+applied together using a synchronous write (i.e.,\n+<code>write_options.sync</code> is set to true).  The extra cost of\n+the synchronous write will be amortized across all of the writes in\n+the batch.\n+\n+<p>\n+<h1>Concurrency</h1>\n+<p>\n+A database may only be opened by one process at a time.\n+The <code>leveldb</code> implementation acquires a lock from the\n+operating system to prevent misuse.  Within a single process, the\n+same <code>leveldb::DB</code> object may be safely shared by multiple\n+concurrent threads.  I.e., different threads may write into or fetch\n+iterators or call <code>Get</code> on the same database without any\n+external synchronization (the leveldb implementation will\n+automatically do the required synchronization).  However other objects\n+(like Iterator and WriteBatch) may require external synchronization.\n+If two threads share such an object, they must protect access to it\n+using their own locking protocol.  More details are available in\n+the public header files.\n+<p>\n+<h1>Iteration</h1>\n+<p>\n+The following example demonstrates how to print all key,value pairs\n+in a database.\n+<p>\n+<pre>\n+  leveldb::Iterator* it = db-&gt;NewIterator(leveldb::ReadOptions());\n+  for (it-&gt;SeekToFirst(); it-&gt;Valid(); it-&gt;Next()) {\n+    cout &lt;&lt; it-&gt;key().ToString() &lt;&lt; \": \"  &lt;&lt; it-&gt;value().ToString() &lt;&lt; endl;\n+  }\n+  assert(it-&gt;status().ok());  // Check for any errors found during the scan\n+  delete it;\n+</pre>\n+The following variation shows how to process just the keys in the\n+range <code>[start,limit)</code>:\n+<p>\n+<pre>\n+  for (it-&gt;Seek(start);\n+       it-&gt;Valid() &amp;&amp; it-&gt;key().ToString() &lt; limit;\n+       it-&gt;Next()) {\n+    ...\n+  }\n+</pre>\n+You can also process entries in reverse order.  (Caveat: reverse\n+iteration may be somewhat slower than forward iteration.)\n+<p>\n+<pre>\n+  for (it-&gt;SeekToLast(); it-&gt;Valid(); it-&gt;Prev()) {\n+    ...\n+  }\n+</pre>\n+<h1>Snapshots</h1>\n+<p>\n+Snapshots provide consistent read-only views over the entire state of\n+the key-value store.  <code>ReadOptions::snapshot</code> may be non-NULL to indicate\n+that a read should operate on a particular version of the DB state.\n+If <code>ReadOptions::snapshot</code> is NULL, the read will operate on an\n+implicit snapshot of the current state.\n+<p>\n+Snapshots are created by the DB::GetSnapshot() method:\n+<p>\n+<pre>\n+  leveldb::ReadOptions options;\n+  options.snapshot = db-&gt;GetSnapshot();\n+  ... apply some updates to db ...\n+  leveldb::Iterator* iter = db-&gt;NewIterator(options);\n+  ... read using iter to view the state when the snapshot was created ...\n+  delete iter;\n+  db-&gt;ReleaseSnapshot(options.snapshot);\n+</pre>\n+Note that when a snapshot is no longer needed, it should be released\n+using the DB::ReleaseSnapshot interface.  This allows the\n+implementation to get rid of state that was being maintained just to\n+support reading as of that snapshot.\n+<h1>Slice</h1>\n+<p>\n+The return value of the <code>it->key()</code> and <code>it->value()</code> calls above\n+are instances of the <code>leveldb::Slice</code> type.  <code>Slice</code> is a simple\n+structure that contains a length and a pointer to an external byte\n+array.  Returning a <code>Slice</code> is a cheaper alternative to returning a\n+<code>std::string</code> since we do not need to copy potentially large keys and\n+values.  In addition, <code>leveldb</code> methods do not return null-terminated\n+C-style strings since <code>leveldb</code> keys and values are allowed to\n+contain '\\0' bytes.\n+<p>\n+C++ strings and null-terminated C-style strings can be easily converted\n+to a Slice:\n+<p>\n+<pre>\n+   leveldb::Slice s1 = \"hello\";\n+\n+   std::string str(\"world\");\n+   leveldb::Slice s2 = str;\n+</pre>\n+A Slice can be easily converted back to a C++ string:\n+<pre>\n+   std::string str = s1.ToString();\n+   assert(str == std::string(\"hello\"));\n+</pre>\n+Be careful when using Slices since it is up to the caller to ensure that\n+the external byte array into which the Slice points remains live while\n+the Slice is in use.  For example, the following is buggy:\n+<p>\n+<pre>\n+   leveldb::Slice slice;\n+   if (...) {\n+     std::string str = ...;\n+     slice = str;\n+   }\n+   Use(slice);\n+</pre>\n+When the <code>if</code> statement goes out of scope, <code>str</code> will be destroyed and the\n+backing storage for <code>slice</code> will disappear.\n+<p>\n+<h1>Comparators</h1>\n+<p>\n+The preceding examples used the default ordering function for key,\n+which orders bytes lexicographically.  You can however supply a custom\n+comparator when opening a database.  For example, suppose each\n+database key consists of two numbers and we should sort by the first\n+number, breaking ties by the second number.  First, define a proper\n+subclass of <code>leveldb::Comparator</code> that expresses these rules:\n+<p>\n+<pre>\n+  class TwoPartComparator : public leveldb::Comparator {\n+   public:\n+    // Three-way comparison function:\n+    //   if a &lt; b: negative result\n+    //   if a &gt; b: positive result\n+    //   else: zero result\n+    int Compare(const leveldb::Slice&amp; a, const leveldb::Slice&amp; b) const {\n+      int a1, a2, b1, b2;\n+      ParseKey(a, &amp;a1, &amp;a2);\n+      ParseKey(b, &amp;b1, &amp;b2);\n+      if (a1 &lt; b1) return -1;\n+      if (a1 &gt; b1) return +1;\n+      if (a2 &lt; b2) return -1;\n+      if (a2 &gt; b2) return +1;\n+      return 0;\n+    }\n+\n+    // Ignore the following methods for now:\n+    const char* Name() const { return \"TwoPartComparator\"; }\n+    void FindShortestSeparator(std::string*, const leveldb::Slice&amp;) const { }\n+    void FindShortSuccessor(std::string*) const { }\n+  };\n+</pre>\n+Now create a database using this custom comparator:\n+<p>\n+<pre>\n+  TwoPartComparator cmp;\n+  leveldb::DB* db;\n+  leveldb::Options options;\n+  options.create_if_missing = true;\n+  options.comparator = &amp;cmp;\n+  leveldb::Status status = leveldb::DB::Open(options, \"/tmp/testdb\", &amp;db);\n+  ...\n+</pre>\n+<h2>Backwards compatibility</h2>\n+<p>\n+The result of the comparator's <code>Name</code> method is attached to the\n+database when it is created, and is checked on every subsequent\n+database open.  If the name changes, the <code>leveldb::DB::Open</code> call will\n+fail.  Therefore, change the name if and only if the new key format\n+and comparison function are incompatible with existing databases, and\n+it is ok to discard the contents of all existing databases.\n+<p>\n+You can however still gradually evolve your key format over time with\n+a little bit of pre-planning.  For example, you could store a version\n+number at the end of each key (one byte should suffice for most uses).\n+When you wish to switch to a new key format (e.g., adding an optional\n+third part to the keys processed by <code>TwoPartComparator</code>),\n+(a) keep the same comparator name (b) increment the version number\n+for new keys (c) change the comparator function so it uses the\n+version numbers found in the keys to decide how to interpret them.\n+<p>\n+<h1>Performance</h1>\n+<p>\n+Performance can be tuned by changing the default values of the\n+types defined in <code>include/leveldb/options.h</code>.\n+\n+<p>\n+<h2>Block size</h2>\n+<p>\n+<code>leveldb</code> groups adjacent keys together into the same block and such a\n+block is the unit of transfer to and from persistent storage.  The\n+default block size is approximately 4096 uncompressed bytes.\n+Applications that mostly do bulk scans over the contents of the\n+database may wish to increase this size.  Applications that do a lot\n+of point reads of small values may wish to switch to a smaller block\n+size if performance measurements indicate an improvement.  There isn't\n+much benefit in using blocks smaller than one kilobyte, or larger than\n+a few megabytes.  Also note that compression will be more effective\n+with larger block sizes.\n+<p>\n+<h2>Compression</h2>\n+<p>\n+Each block is individually compressed before being written to\n+persistent storage.  Compression is on by default since the default\n+compression method is very fast, and is automatically disabled for\n+uncompressible data.  In rare cases, applications may want to disable\n+compression entirely, but should only do so if benchmarks show a\n+performance improvement:\n+<p>\n+<pre>\n+  leveldb::Options options;\n+  options.compression = leveldb::kNoCompression;\n+  ... leveldb::DB::Open(options, name, ...) ....\n+</pre>\n+<h2>Cache</h2>\n+<p>\n+The contents of the database are stored in a set of files in the\n+filesystem and each file stores a sequence of compressed blocks.  If\n+<code>options.cache</code> is non-NULL, it is used to cache frequently used\n+uncompressed block contents.\n+<p>\n+<pre>\n+  #include \"leveldb/cache.h\"\n+\n+  leveldb::Options options;\n+  options.cache = leveldb::NewLRUCache(100 * 1048576);  // 100MB cache\n+  leveldb::DB* db;\n+  leveldb::DB::Open(options, name, &db);\n+  ... use the db ...\n+  delete db\n+  delete options.cache;\n+</pre>\n+Note that the cache holds uncompressed data, and therefore it should\n+be sized according to application level data sizes, without any\n+reduction from compression.  (Caching of compressed blocks is left to\n+the operating system buffer cache, or any custom <code>Env</code>\n+implementation provided by the client.)\n+<p>\n+When performing a bulk read, the application may wish to disable\n+caching so that the data processed by the bulk read does not end up\n+displacing most of the cached contents.  A per-iterator option can be\n+used to achieve this:\n+<p>\n+<pre>\n+  leveldb::ReadOptions options;\n+  options.fill_cache = false;\n+  leveldb::Iterator* it = db-&gt;NewIterator(options);\n+  for (it-&gt;SeekToFirst(); it-&gt;Valid(); it-&gt;Next()) {\n+    ...\n+  }\n+</pre>\n+<h2>Key Layout</h2>\n+<p>\n+Note that the unit of disk transfer and caching is a block.  Adjacent\n+keys (according to the database sort order) will usually be placed in\n+the same block.  Therefore the application can improve its performance\n+by placing keys that are accessed together near each other and placing\n+infrequently used keys in a separate region of the key space.\n+<p>\n+For example, suppose we are implementing a simple file system on top\n+of <code>leveldb</code>.  The types of entries we might wish to store are:\n+<p>\n+<pre>\n+   filename -&gt; permission-bits, length, list of file_block_ids\n+   file_block_id -&gt; data\n+</pre>\n+We might want to prefix <code>filename</code> keys with one letter (say '/') and the\n+<code>file_block_id</code> keys with a different letter (say '0') so that scans\n+over just the metadata do not force us to fetch and cache bulky file\n+contents.\n+<p>\n+<h2>Filters</h2>\n+<p>\n+Because of the way <code>leveldb</code> data is organized on disk,\n+a single <code>Get()</code> call may involve multiple reads from disk.\n+The optional <code>FilterPolicy</code> mechanism can be used to reduce\n+the number of disk reads substantially.\n+<pre>\n+   leveldb::Options options;\n+   options.filter_policy = NewBloomFilterPolicy(10);\n+   leveldb::DB* db;\n+   leveldb::DB::Open(options, \"/tmp/testdb\", &amp;db);\n+   ... use the database ...\n+   delete db;\n+   delete options.filter_policy;\n+</pre>\n+The preceding code associates a\n+<a href=\"http://en.wikipedia.org/wiki/Bloom_filter\">Bloom filter</a>\n+based filtering policy with the database.  Bloom filter based\n+filtering relies on keeping some number of bits of data in memory per\n+key (in this case 10 bits per key since that is the argument we passed\n+to NewBloomFilterPolicy).  This filter will reduce the number of unnecessary\n+disk reads needed for <code>Get()</code> calls by a factor of\n+approximately a 100.  Increasing the bits per key will lead to a\n+larger reduction at the cost of more memory usage.  We recommend that\n+applications whose working set does not fit in memory and that do a\n+lot of random reads set a filter policy.\n+<p>\n+If you are using a custom comparator, you should ensure that the filter\n+policy you are using is compatible with your comparator.  For example,\n+consider a comparator that ignores trailing spaces when comparing keys.\n+<code>NewBloomFilterPolicy</code> must not be used with such a comparator.\n+Instead, the application should provide a custom filter policy that\n+also ignores trailing spaces.  For example:\n+<pre>\n+  class CustomFilterPolicy : public leveldb::FilterPolicy {\n+   private:\n+    FilterPolicy* builtin_policy_;\n+   public:\n+    CustomFilterPolicy() : builtin_policy_(NewBloomFilterPolicy(10)) { }\n+    ~CustomFilterPolicy() { delete builtin_policy_; }\n+\n+    const char* Name() const { return \"IgnoreTrailingSpacesFilter\"; }\n+\n+    void CreateFilter(const Slice* keys, int n, std::string* dst) const {\n+      // Use builtin bloom filter code after removing trailing spaces\n+      std::vector&lt;Slice&gt; trimmed(n);\n+      for (int i = 0; i &lt; n; i++) {\n+        trimmed[i] = RemoveTrailingSpaces(keys[i]);\n+      }\n+      return builtin_policy_-&gt;CreateFilter(&amp;trimmed[i], n, dst);\n+    }\n+\n+    bool KeyMayMatch(const Slice& key, const Slice& filter) const {\n+      // Use builtin bloom filter code after removing trailing spaces\n+      return builtin_policy_-&gt;KeyMayMatch(RemoveTrailingSpaces(key), filter);\n+    }\n+  };\n+</pre>\n+<p>\n+Advanced applications may provide a filter policy that does not use\n+a bloom filter but uses some other mechanism for summarizing a set\n+of keys.  See <code>leveldb/filter_policy.h</code> for detail.\n+<p>\n+<h1>Checksums</h1>\n+<p>\n+<code>leveldb</code> associates checksums with all data it stores in the file system.\n+There are two separate controls provided over how aggressively these\n+checksums are verified:\n+<p>\n+<ul>\n+<li> <code>ReadOptions::verify_checksums</code> may be set to true to force\n+  checksum verification of all data that is read from the file system on\n+  behalf of a particular read.  By default, no such verification is\n+  done.\n+<p>\n+<li> <code>Options::paranoid_checks</code> may be set to true before opening a\n+  database to make the database implementation raise an error as soon as\n+  it detects an internal corruption.  Depending on which portion of the\n+  database has been corrupted, the error may be raised when the database\n+  is opened, or later by another database operation.  By default,\n+  paranoid checking is off so that the database can be used even if\n+  parts of its persistent storage have been corrupted.\n+<p>\n+  If a database is corrupted (perhaps it cannot be opened when\n+  paranoid checking is turned on), the <code>leveldb::RepairDB</code> function\n+  may be used to recover as much of the data as possible\n+<p>\n+</ul>\n+<h1>Approximate Sizes</h1>\n+<p>\n+The <code>GetApproximateSizes</code> method can used to get the approximate\n+number of bytes of file system space used by one or more key ranges.\n+<p>\n+<pre>\n+   leveldb::Range ranges[2];\n+   ranges[0] = leveldb::Range(\"a\", \"c\");\n+   ranges[1] = leveldb::Range(\"x\", \"z\");\n+   uint64_t sizes[2];\n+   leveldb::Status s = db-&gt;GetApproximateSizes(ranges, 2, sizes);\n+</pre>\n+The preceding call will set <code>sizes[0]</code> to the approximate number of\n+bytes of file system space used by the key range <code>[a..c)</code> and\n+<code>sizes[1]</code> to the approximate number of bytes used by the key range\n+<code>[x..z)</code>.\n+<p>\n+<h1>Environment</h1>\n+<p>\n+All file operations (and other operating system calls) issued by the\n+<code>leveldb</code> implementation are routed through a <code>leveldb::Env</code> object.\n+Sophisticated clients may wish to provide their own <code>Env</code>\n+implementation to get better control.  For example, an application may\n+introduce artificial delays in the file IO paths to limit the impact\n+of <code>leveldb</code> on other activities in the system.\n+<p>\n+<pre>\n+  class SlowEnv : public leveldb::Env {\n+    .. implementation of the Env interface ...\n+  };\n+\n+  SlowEnv env;\n+  leveldb::Options options;\n+  options.env = &amp;env;\n+  Status s = leveldb::DB::Open(options, ...);\n+</pre>\n+<h1>Porting</h1>\n+<p>\n+<code>leveldb</code> may be ported to a new platform by providing platform\n+specific implementations of the types/methods/functions exported by\n+<code>leveldb/port/port.h</code>.  See <code>leveldb/port/port_example.h</code> for more\n+details.\n+<p>\n+In addition, the new platform may need a new default <code>leveldb::Env</code>\n+implementation.  See <code>leveldb/util/env_posix.h</code> for an example.\n+\n+<h1>Other Information</h1>\n+\n+<p>\n+Details about the <code>leveldb</code> implementation may be found in\n+the following documents:\n+<ul>\n+<li> <a href=\"impl.html\">Implementation notes</a>\n+<li> <a href=\"table_format.txt\">Format of an immutable Table file</a>\n+<li> <a href=\"log_format.txt\">Format of a log file</a>\n+</ul>\n+\n+</body>\n+</html>"
      },
      {
        "sha": "5228f624de4347c114b87520c12a1ae39425d411",
        "filename": "src/leveldb/doc/log_format.txt",
        "status": "added",
        "additions": 75,
        "deletions": 0,
        "changes": 75,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/log_format.txt",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/log_format.txt",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/log_format.txt?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,75 @@\n+The log file contents are a sequence of 32KB blocks.  The only\n+exception is that the tail of the file may contain a partial block.\n+\n+Each block consists of a sequence of records:\n+   block := record* trailer?\n+   record :=\n+\tchecksum: uint32\t// crc32c of type and data[] ; little-endian\n+\tlength: uint16\t\t// little-endian\n+\ttype: uint8\t\t// One of FULL, FIRST, MIDDLE, LAST\n+\tdata: uint8[length]\n+\n+A record never starts within the last six bytes of a block (since it\n+won't fit).  Any leftover bytes here form the trailer, which must\n+consist entirely of zero bytes and must be skipped by readers.  \n+\n+Aside: if exactly seven bytes are left in the current block, and a new\n+non-zero length record is added, the writer must emit a FIRST record\n+(which contains zero bytes of user data) to fill up the trailing seven\n+bytes of the block and then emit all of the user data in subsequent\n+blocks.\n+\n+More types may be added in the future.  Some Readers may skip record\n+types they do not understand, others may report that some data was\n+skipped.\n+\n+FULL == 1\n+FIRST == 2\n+MIDDLE == 3\n+LAST == 4\n+\n+The FULL record contains the contents of an entire user record.\n+\n+FIRST, MIDDLE, LAST are types used for user records that have been\n+split into multiple fragments (typically because of block boundaries).\n+FIRST is the type of the first fragment of a user record, LAST is the\n+type of the last fragment of a user record, and MID is the type of all\n+interior fragments of a user record.\n+\n+Example: consider a sequence of user records:\n+   A: length 1000\n+   B: length 97270\n+   C: length 8000\n+A will be stored as a FULL record in the first block.\n+\n+B will be split into three fragments: first fragment occupies the rest\n+of the first block, second fragment occupies the entirety of the\n+second block, and the third fragment occupies a prefix of the third\n+block.  This will leave six bytes free in the third block, which will\n+be left empty as the trailer.\n+\n+C will be stored as a FULL record in the fourth block.\n+\n+===================\n+\n+Some benefits over the recordio format:\n+\n+(1) We do not need any heuristics for resyncing - just go to next\n+block boundary and scan.  If there is a corruption, skip to the next\n+block.  As a side-benefit, we do not get confused when part of the\n+contents of one log file are embedded as a record inside another log\n+file.\n+\n+(2) Splitting at approximate boundaries (e.g., for mapreduce) is\n+simple: find the next block boundary and skip records until we\n+hit a FULL or FIRST record.\n+\n+(3) We do not need extra buffering for large records.\n+\n+Some downsides compared to recordio format:\n+\n+(1) No packing of tiny records.  This could be fixed by adding a new\n+record type, so it is a shortcoming of the current implementation,\n+not necessarily the format.\n+\n+(2) No compression.  Again, this could be fixed by adding new record types."
      },
      {
        "sha": "ca8f9b4460ad85d9e09f14a959ed47bd2812edb5",
        "filename": "src/leveldb/doc/table_format.txt",
        "status": "added",
        "additions": 104,
        "deletions": 0,
        "changes": 104,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/table_format.txt",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/doc/table_format.txt",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/doc/table_format.txt?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,104 @@\n+File format\n+===========\n+\n+  <beginning_of_file>\n+  [data block 1]\n+  [data block 2]\n+  ...\n+  [data block N]\n+  [meta block 1]\n+  ...\n+  [meta block K]\n+  [metaindex block]\n+  [index block]\n+  [Footer]        (fixed size; starts at file_size - sizeof(Footer))\n+  <end_of_file>\n+\n+The file contains internal pointers.  Each such pointer is called\n+a BlockHandle and contains the following information:\n+  offset:\t    varint64\n+  size:\t\t    varint64\n+See https://developers.google.com/protocol-buffers/docs/encoding#varints\n+for an explanation of varint64 format.\n+\n+(1) The sequence of key/value pairs in the file are stored in sorted\n+order and partitioned into a sequence of data blocks.  These blocks\n+come one after another at the beginning of the file.  Each data block\n+is formatted according to the code in block_builder.cc, and then\n+optionally compressed.\n+\n+(2) After the data blocks we store a bunch of meta blocks.  The\n+supported meta block types are described below.  More meta block types\n+may be added in the future.  Each meta block is again formatted using\n+block_builder.cc and then optionally compressed.\n+\n+(3) A \"metaindex\" block.  It contains one entry for every other meta\n+block where the key is the name of the meta block and the value is a\n+BlockHandle pointing to that meta block.\n+\n+(4) An \"index\" block.  This block contains one entry per data block,\n+where the key is a string >= last key in that data block and before\n+the first key in the successive data block.  The value is the\n+BlockHandle for the data block.\n+\n+(6) At the very end of the file is a fixed length footer that contains\n+the BlockHandle of the metaindex and index blocks as well as a magic number.\n+       metaindex_handle: char[p];    // Block handle for metaindex\n+       index_handle:     char[q];    // Block handle for index\n+       padding:          char[40-p-q]; // zeroed bytes to make fixed length\n+                                       // (40==2*BlockHandle::kMaxEncodedLength)\n+       magic:            fixed64;    // == 0xdb4775248b80fb57 (little-endian)\n+\n+\"filter\" Meta Block\n+-------------------\n+\n+If a \"FilterPolicy\" was specified when the database was opened, a\n+filter block is stored in each table.  The \"metaindex\" block contains\n+an entry that maps from \"filter.<N>\" to the BlockHandle for the filter\n+block where \"<N>\" is the string returned by the filter policy's\n+\"Name()\" method.\n+\n+The filter block stores a sequence of filters, where filter i contains\n+the output of FilterPolicy::CreateFilter() on all keys that are stored\n+in a block whose file offset falls within the range\n+\n+    [ i*base ... (i+1)*base-1 ]\n+\n+Currently, \"base\" is 2KB.  So for example, if blocks X and Y start in\n+the range [ 0KB .. 2KB-1 ], all of the keys in X and Y will be\n+converted to a filter by calling FilterPolicy::CreateFilter(), and the\n+resulting filter will be stored as the first filter in the filter\n+block.\n+\n+The filter block is formatted as follows:\n+\n+     [filter 0]\n+     [filter 1]\n+     [filter 2]\n+     ...\n+     [filter N-1]\n+\n+     [offset of filter 0]                  : 4 bytes\n+     [offset of filter 1]                  : 4 bytes\n+     [offset of filter 2]                  : 4 bytes\n+     ...\n+     [offset of filter N-1]                : 4 bytes\n+\n+     [offset of beginning of offset array] : 4 bytes\n+     lg(base)                              : 1 byte\n+\n+The offset array at the end of the filter block allows efficient\n+mapping from a data block offset to the corresponding filter.\n+\n+\"stats\" Meta Block\n+------------------\n+\n+This meta block contains a bunch of stats.  The key is the name\n+of the statistic.  The value contains the statistic.\n+TODO(postrelease): record following stats.\n+  data size\n+  index size\n+  key size (uncompressed)\n+  value size (uncompressed)\n+  number of entries\n+  number of data blocks"
      },
      {
        "sha": "5879de121456a7c5c16457eb36d85c64ad0a1b61",
        "filename": "src/leveldb/helpers/memenv/memenv.cc",
        "status": "added",
        "additions": 384,
        "deletions": 0,
        "changes": 384,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/helpers/memenv/memenv.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/helpers/memenv/memenv.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/helpers/memenv/memenv.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,384 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"helpers/memenv/memenv.h\"\n+\n+#include \"leveldb/env.h\"\n+#include \"leveldb/status.h\"\n+#include \"port/port.h\"\n+#include \"util/mutexlock.h\"\n+#include <map>\n+#include <string.h>\n+#include <string>\n+#include <vector>\n+\n+namespace leveldb {\n+\n+namespace {\n+\n+class FileState {\n+ public:\n+  // FileStates are reference counted. The initial reference count is zero\n+  // and the caller must call Ref() at least once.\n+  FileState() : refs_(0), size_(0) {}\n+\n+  // Increase the reference count.\n+  void Ref() {\n+    MutexLock lock(&refs_mutex_);\n+    ++refs_;\n+  }\n+\n+  // Decrease the reference count. Delete if this is the last reference.\n+  void Unref() {\n+    bool do_delete = false;\n+\n+    {\n+      MutexLock lock(&refs_mutex_);\n+      --refs_;\n+      assert(refs_ >= 0);\n+      if (refs_ <= 0) {\n+        do_delete = true;\n+      }\n+    }\n+\n+    if (do_delete) {\n+      delete this;\n+    }\n+  }\n+\n+  uint64_t Size() const { return size_; }\n+\n+  Status Read(uint64_t offset, size_t n, Slice* result, char* scratch) const {\n+    if (offset > size_) {\n+      return Status::IOError(\"Offset greater than file size.\");\n+    }\n+    const uint64_t available = size_ - offset;\n+    if (n > available) {\n+      n = available;\n+    }\n+    if (n == 0) {\n+      *result = Slice();\n+      return Status::OK();\n+    }\n+\n+    size_t block = offset / kBlockSize;\n+    size_t block_offset = offset % kBlockSize;\n+\n+    if (n <= kBlockSize - block_offset) {\n+      // The requested bytes are all in the first block.\n+      *result = Slice(blocks_[block] + block_offset, n);\n+      return Status::OK();\n+    }\n+\n+    size_t bytes_to_copy = n;\n+    char* dst = scratch;\n+\n+    while (bytes_to_copy > 0) {\n+      size_t avail = kBlockSize - block_offset;\n+      if (avail > bytes_to_copy) {\n+        avail = bytes_to_copy;\n+      }\n+      memcpy(dst, blocks_[block] + block_offset, avail);\n+\n+      bytes_to_copy -= avail;\n+      dst += avail;\n+      block++;\n+      block_offset = 0;\n+    }\n+\n+    *result = Slice(scratch, n);\n+    return Status::OK();\n+  }\n+\n+  Status Append(const Slice& data) {\n+    const char* src = data.data();\n+    size_t src_len = data.size();\n+\n+    while (src_len > 0) {\n+      size_t avail;\n+      size_t offset = size_ % kBlockSize;\n+\n+      if (offset != 0) {\n+        // There is some room in the last block.\n+        avail = kBlockSize - offset;\n+      } else {\n+        // No room in the last block; push new one.\n+        blocks_.push_back(new char[kBlockSize]);\n+        avail = kBlockSize;\n+      }\n+\n+      if (avail > src_len) {\n+        avail = src_len;\n+      }\n+      memcpy(blocks_.back() + offset, src, avail);\n+      src_len -= avail;\n+      src += avail;\n+      size_ += avail;\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+ private:\n+  // Private since only Unref() should be used to delete it.\n+  ~FileState() {\n+    for (std::vector<char*>::iterator i = blocks_.begin(); i != blocks_.end();\n+         ++i) {\n+      delete [] *i;\n+    }\n+  }\n+\n+  // No copying allowed.\n+  FileState(const FileState&);\n+  void operator=(const FileState&);\n+\n+  port::Mutex refs_mutex_;\n+  int refs_;  // Protected by refs_mutex_;\n+\n+  // The following fields are not protected by any mutex. They are only mutable\n+  // while the file is being written, and concurrent access is not allowed\n+  // to writable files.\n+  std::vector<char*> blocks_;\n+  uint64_t size_;\n+\n+  enum { kBlockSize = 8 * 1024 };\n+};\n+\n+class SequentialFileImpl : public SequentialFile {\n+ public:\n+  explicit SequentialFileImpl(FileState* file) : file_(file), pos_(0) {\n+    file_->Ref();\n+  }\n+\n+  ~SequentialFileImpl() {\n+    file_->Unref();\n+  }\n+\n+  virtual Status Read(size_t n, Slice* result, char* scratch) {\n+    Status s = file_->Read(pos_, n, result, scratch);\n+    if (s.ok()) {\n+      pos_ += result->size();\n+    }\n+    return s;\n+  }\n+\n+  virtual Status Skip(uint64_t n) {\n+    if (pos_ > file_->Size()) {\n+      return Status::IOError(\"pos_ > file_->Size()\");\n+    }\n+    const size_t available = file_->Size() - pos_;\n+    if (n > available) {\n+      n = available;\n+    }\n+    pos_ += n;\n+    return Status::OK();\n+  }\n+\n+ private:\n+  FileState* file_;\n+  size_t pos_;\n+};\n+\n+class RandomAccessFileImpl : public RandomAccessFile {\n+ public:\n+  explicit RandomAccessFileImpl(FileState* file) : file_(file) {\n+    file_->Ref();\n+  }\n+\n+  ~RandomAccessFileImpl() {\n+    file_->Unref();\n+  }\n+\n+  virtual Status Read(uint64_t offset, size_t n, Slice* result,\n+                      char* scratch) const {\n+    return file_->Read(offset, n, result, scratch);\n+  }\n+\n+ private:\n+  FileState* file_;\n+};\n+\n+class WritableFileImpl : public WritableFile {\n+ public:\n+  WritableFileImpl(FileState* file) : file_(file) {\n+    file_->Ref();\n+  }\n+\n+  ~WritableFileImpl() {\n+    file_->Unref();\n+  }\n+\n+  virtual Status Append(const Slice& data) {\n+    return file_->Append(data);\n+  }\n+\n+  virtual Status Close() { return Status::OK(); }\n+  virtual Status Flush() { return Status::OK(); }\n+  virtual Status Sync() { return Status::OK(); }\n+\n+ private:\n+  FileState* file_;\n+};\n+\n+class NoOpLogger : public Logger {\n+ public:\n+  virtual void Logv(const char* format, va_list ap) { }\n+};\n+\n+class InMemoryEnv : public EnvWrapper {\n+ public:\n+  explicit InMemoryEnv(Env* base_env) : EnvWrapper(base_env) { }\n+\n+  virtual ~InMemoryEnv() {\n+    for (FileSystem::iterator i = file_map_.begin(); i != file_map_.end(); ++i){\n+      i->second->Unref();\n+    }\n+  }\n+\n+  // Partial implementation of the Env interface.\n+  virtual Status NewSequentialFile(const std::string& fname,\n+                                   SequentialFile** result) {\n+    MutexLock lock(&mutex_);\n+    if (file_map_.find(fname) == file_map_.end()) {\n+      *result = NULL;\n+      return Status::IOError(fname, \"File not found\");\n+    }\n+\n+    *result = new SequentialFileImpl(file_map_[fname]);\n+    return Status::OK();\n+  }\n+\n+  virtual Status NewRandomAccessFile(const std::string& fname,\n+                                     RandomAccessFile** result) {\n+    MutexLock lock(&mutex_);\n+    if (file_map_.find(fname) == file_map_.end()) {\n+      *result = NULL;\n+      return Status::IOError(fname, \"File not found\");\n+    }\n+\n+    *result = new RandomAccessFileImpl(file_map_[fname]);\n+    return Status::OK();\n+  }\n+\n+  virtual Status NewWritableFile(const std::string& fname,\n+                                 WritableFile** result) {\n+    MutexLock lock(&mutex_);\n+    if (file_map_.find(fname) != file_map_.end()) {\n+      DeleteFileInternal(fname);\n+    }\n+\n+    FileState* file = new FileState();\n+    file->Ref();\n+    file_map_[fname] = file;\n+\n+    *result = new WritableFileImpl(file);\n+    return Status::OK();\n+  }\n+\n+  virtual bool FileExists(const std::string& fname) {\n+    MutexLock lock(&mutex_);\n+    return file_map_.find(fname) != file_map_.end();\n+  }\n+\n+  virtual Status GetChildren(const std::string& dir,\n+                             std::vector<std::string>* result) {\n+    MutexLock lock(&mutex_);\n+    result->clear();\n+\n+    for (FileSystem::iterator i = file_map_.begin(); i != file_map_.end(); ++i){\n+      const std::string& filename = i->first;\n+\n+      if (filename.size() >= dir.size() + 1 && filename[dir.size()] == '/' &&\n+          Slice(filename).starts_with(Slice(dir))) {\n+        result->push_back(filename.substr(dir.size() + 1));\n+      }\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  void DeleteFileInternal(const std::string& fname) {\n+    if (file_map_.find(fname) == file_map_.end()) {\n+      return;\n+    }\n+\n+    file_map_[fname]->Unref();\n+    file_map_.erase(fname);\n+  }\n+\n+  virtual Status DeleteFile(const std::string& fname) {\n+    MutexLock lock(&mutex_);\n+    if (file_map_.find(fname) == file_map_.end()) {\n+      return Status::IOError(fname, \"File not found\");\n+    }\n+\n+    DeleteFileInternal(fname);\n+    return Status::OK();\n+  }\n+\n+  virtual Status CreateDir(const std::string& dirname) {\n+    return Status::OK();\n+  }\n+\n+  virtual Status DeleteDir(const std::string& dirname) {\n+    return Status::OK();\n+  }\n+\n+  virtual Status GetFileSize(const std::string& fname, uint64_t* file_size) {\n+    MutexLock lock(&mutex_);\n+    if (file_map_.find(fname) == file_map_.end()) {\n+      return Status::IOError(fname, \"File not found\");\n+    }\n+\n+    *file_size = file_map_[fname]->Size();\n+    return Status::OK();\n+  }\n+\n+  virtual Status RenameFile(const std::string& src,\n+                            const std::string& target) {\n+    MutexLock lock(&mutex_);\n+    if (file_map_.find(src) == file_map_.end()) {\n+      return Status::IOError(src, \"File not found\");\n+    }\n+\n+    DeleteFileInternal(target);\n+    file_map_[target] = file_map_[src];\n+    file_map_.erase(src);\n+    return Status::OK();\n+  }\n+\n+  virtual Status LockFile(const std::string& fname, FileLock** lock) {\n+    *lock = new FileLock;\n+    return Status::OK();\n+  }\n+\n+  virtual Status UnlockFile(FileLock* lock) {\n+    delete lock;\n+    return Status::OK();\n+  }\n+\n+  virtual Status GetTestDirectory(std::string* path) {\n+    *path = \"/test\";\n+    return Status::OK();\n+  }\n+\n+  virtual Status NewLogger(const std::string& fname, Logger** result) {\n+    *result = new NoOpLogger;\n+    return Status::OK();\n+  }\n+\n+ private:\n+  // Map from filenames to FileState objects, representing a simple file system.\n+  typedef std::map<std::string, FileState*> FileSystem;\n+  port::Mutex mutex_;\n+  FileSystem file_map_;  // Protected by mutex_.\n+};\n+\n+}  // namespace\n+\n+Env* NewMemEnv(Env* base_env) {\n+  return new InMemoryEnv(base_env);\n+}\n+\n+}  // namespace leveldb"
      },
      {
        "sha": "03b88de761dc732e09dec54baddd34e94ee17613",
        "filename": "src/leveldb/helpers/memenv/memenv.h",
        "status": "added",
        "additions": 20,
        "deletions": 0,
        "changes": 20,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/helpers/memenv/memenv.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/helpers/memenv/memenv.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/helpers/memenv/memenv.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,20 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_HELPERS_MEMENV_MEMENV_H_\n+#define STORAGE_LEVELDB_HELPERS_MEMENV_MEMENV_H_\n+\n+namespace leveldb {\n+\n+class Env;\n+\n+// Returns a new environment that stores its data in memory and delegates\n+// all non-file-storage tasks to base_env. The caller must delete the result\n+// when it is no longer needed.\n+// *base_env must remain live while the result is in use.\n+Env* NewMemEnv(Env* base_env);\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_HELPERS_MEMENV_MEMENV_H_"
      },
      {
        "sha": "a44310fed80cd7f210d64b2c8e79ceb74284217a",
        "filename": "src/leveldb/helpers/memenv/memenv_test.cc",
        "status": "added",
        "additions": 232,
        "deletions": 0,
        "changes": 232,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/helpers/memenv/memenv_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/helpers/memenv/memenv_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/helpers/memenv/memenv_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,232 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"helpers/memenv/memenv.h\"\n+\n+#include \"db/db_impl.h\"\n+#include \"leveldb/db.h\"\n+#include \"leveldb/env.h\"\n+#include \"util/testharness.h\"\n+#include <string>\n+#include <vector>\n+\n+namespace leveldb {\n+\n+class MemEnvTest {\n+ public:\n+  Env* env_;\n+\n+  MemEnvTest()\n+      : env_(NewMemEnv(Env::Default())) {\n+  }\n+  ~MemEnvTest() {\n+    delete env_;\n+  }\n+};\n+\n+TEST(MemEnvTest, Basics) {\n+  uint64_t file_size;\n+  WritableFile* writable_file;\n+  std::vector<std::string> children;\n+\n+  ASSERT_OK(env_->CreateDir(\"/dir\"));\n+\n+  // Check that the directory is empty.\n+  ASSERT_TRUE(!env_->FileExists(\"/dir/non_existent\"));\n+  ASSERT_TRUE(!env_->GetFileSize(\"/dir/non_existent\", &file_size).ok());\n+  ASSERT_OK(env_->GetChildren(\"/dir\", &children));\n+  ASSERT_EQ(0, children.size());\n+\n+  // Create a file.\n+  ASSERT_OK(env_->NewWritableFile(\"/dir/f\", &writable_file));\n+  delete writable_file;\n+\n+  // Check that the file exists.\n+  ASSERT_TRUE(env_->FileExists(\"/dir/f\"));\n+  ASSERT_OK(env_->GetFileSize(\"/dir/f\", &file_size));\n+  ASSERT_EQ(0, file_size);\n+  ASSERT_OK(env_->GetChildren(\"/dir\", &children));\n+  ASSERT_EQ(1, children.size());\n+  ASSERT_EQ(\"f\", children[0]);\n+\n+  // Write to the file.\n+  ASSERT_OK(env_->NewWritableFile(\"/dir/f\", &writable_file));\n+  ASSERT_OK(writable_file->Append(\"abc\"));\n+  delete writable_file;\n+\n+  // Check for expected size.\n+  ASSERT_OK(env_->GetFileSize(\"/dir/f\", &file_size));\n+  ASSERT_EQ(3, file_size);\n+\n+  // Check that renaming works.\n+  ASSERT_TRUE(!env_->RenameFile(\"/dir/non_existent\", \"/dir/g\").ok());\n+  ASSERT_OK(env_->RenameFile(\"/dir/f\", \"/dir/g\"));\n+  ASSERT_TRUE(!env_->FileExists(\"/dir/f\"));\n+  ASSERT_TRUE(env_->FileExists(\"/dir/g\"));\n+  ASSERT_OK(env_->GetFileSize(\"/dir/g\", &file_size));\n+  ASSERT_EQ(3, file_size);\n+\n+  // Check that opening non-existent file fails.\n+  SequentialFile* seq_file;\n+  RandomAccessFile* rand_file;\n+  ASSERT_TRUE(!env_->NewSequentialFile(\"/dir/non_existent\", &seq_file).ok());\n+  ASSERT_TRUE(!seq_file);\n+  ASSERT_TRUE(!env_->NewRandomAccessFile(\"/dir/non_existent\", &rand_file).ok());\n+  ASSERT_TRUE(!rand_file);\n+\n+  // Check that deleting works.\n+  ASSERT_TRUE(!env_->DeleteFile(\"/dir/non_existent\").ok());\n+  ASSERT_OK(env_->DeleteFile(\"/dir/g\"));\n+  ASSERT_TRUE(!env_->FileExists(\"/dir/g\"));\n+  ASSERT_OK(env_->GetChildren(\"/dir\", &children));\n+  ASSERT_EQ(0, children.size());\n+  ASSERT_OK(env_->DeleteDir(\"/dir\"));\n+}\n+\n+TEST(MemEnvTest, ReadWrite) {\n+  WritableFile* writable_file;\n+  SequentialFile* seq_file;\n+  RandomAccessFile* rand_file;\n+  Slice result;\n+  char scratch[100];\n+\n+  ASSERT_OK(env_->CreateDir(\"/dir\"));\n+\n+  ASSERT_OK(env_->NewWritableFile(\"/dir/f\", &writable_file));\n+  ASSERT_OK(writable_file->Append(\"hello \"));\n+  ASSERT_OK(writable_file->Append(\"world\"));\n+  delete writable_file;\n+\n+  // Read sequentially.\n+  ASSERT_OK(env_->NewSequentialFile(\"/dir/f\", &seq_file));\n+  ASSERT_OK(seq_file->Read(5, &result, scratch)); // Read \"hello\".\n+  ASSERT_EQ(0, result.compare(\"hello\"));\n+  ASSERT_OK(seq_file->Skip(1));\n+  ASSERT_OK(seq_file->Read(1000, &result, scratch)); // Read \"world\".\n+  ASSERT_EQ(0, result.compare(\"world\"));\n+  ASSERT_OK(seq_file->Read(1000, &result, scratch)); // Try reading past EOF.\n+  ASSERT_EQ(0, result.size());\n+  ASSERT_OK(seq_file->Skip(100)); // Try to skip past end of file.\n+  ASSERT_OK(seq_file->Read(1000, &result, scratch));\n+  ASSERT_EQ(0, result.size());\n+  delete seq_file;\n+\n+  // Random reads.\n+  ASSERT_OK(env_->NewRandomAccessFile(\"/dir/f\", &rand_file));\n+  ASSERT_OK(rand_file->Read(6, 5, &result, scratch)); // Read \"world\".\n+  ASSERT_EQ(0, result.compare(\"world\"));\n+  ASSERT_OK(rand_file->Read(0, 5, &result, scratch)); // Read \"hello\".\n+  ASSERT_EQ(0, result.compare(\"hello\"));\n+  ASSERT_OK(rand_file->Read(10, 100, &result, scratch)); // Read \"d\".\n+  ASSERT_EQ(0, result.compare(\"d\"));\n+\n+  // Too high offset.\n+  ASSERT_TRUE(!rand_file->Read(1000, 5, &result, scratch).ok());\n+  delete rand_file;\n+}\n+\n+TEST(MemEnvTest, Locks) {\n+  FileLock* lock;\n+\n+  // These are no-ops, but we test they return success.\n+  ASSERT_OK(env_->LockFile(\"some file\", &lock));\n+  ASSERT_OK(env_->UnlockFile(lock));\n+}\n+\n+TEST(MemEnvTest, Misc) {\n+  std::string test_dir;\n+  ASSERT_OK(env_->GetTestDirectory(&test_dir));\n+  ASSERT_TRUE(!test_dir.empty());\n+\n+  WritableFile* writable_file;\n+  ASSERT_OK(env_->NewWritableFile(\"/a/b\", &writable_file));\n+\n+  // These are no-ops, but we test they return success.\n+  ASSERT_OK(writable_file->Sync());\n+  ASSERT_OK(writable_file->Flush());\n+  ASSERT_OK(writable_file->Close());\n+  delete writable_file;\n+}\n+\n+TEST(MemEnvTest, LargeWrite) {\n+  const size_t kWriteSize = 300 * 1024;\n+  char* scratch = new char[kWriteSize * 2];\n+\n+  std::string write_data;\n+  for (size_t i = 0; i < kWriteSize; ++i) {\n+    write_data.append(1, static_cast<char>(i));\n+  }\n+\n+  WritableFile* writable_file;\n+  ASSERT_OK(env_->NewWritableFile(\"/dir/f\", &writable_file));\n+  ASSERT_OK(writable_file->Append(\"foo\"));\n+  ASSERT_OK(writable_file->Append(write_data));\n+  delete writable_file;\n+\n+  SequentialFile* seq_file;\n+  Slice result;\n+  ASSERT_OK(env_->NewSequentialFile(\"/dir/f\", &seq_file));\n+  ASSERT_OK(seq_file->Read(3, &result, scratch)); // Read \"foo\".\n+  ASSERT_EQ(0, result.compare(\"foo\"));\n+\n+  size_t read = 0;\n+  std::string read_data;\n+  while (read < kWriteSize) {\n+    ASSERT_OK(seq_file->Read(kWriteSize - read, &result, scratch));\n+    read_data.append(result.data(), result.size());\n+    read += result.size();\n+  }\n+  ASSERT_TRUE(write_data == read_data);\n+  delete seq_file;\n+  delete [] scratch;\n+}\n+\n+TEST(MemEnvTest, DBTest) {\n+  Options options;\n+  options.create_if_missing = true;\n+  options.env = env_;\n+  DB* db;\n+\n+  const Slice keys[] = {Slice(\"aaa\"), Slice(\"bbb\"), Slice(\"ccc\")};\n+  const Slice vals[] = {Slice(\"foo\"), Slice(\"bar\"), Slice(\"baz\")};\n+\n+  ASSERT_OK(DB::Open(options, \"/dir/db\", &db));\n+  for (size_t i = 0; i < 3; ++i) {\n+    ASSERT_OK(db->Put(WriteOptions(), keys[i], vals[i]));\n+  }\n+\n+  for (size_t i = 0; i < 3; ++i) {\n+    std::string res;\n+    ASSERT_OK(db->Get(ReadOptions(), keys[i], &res));\n+    ASSERT_TRUE(res == vals[i]);\n+  }\n+\n+  Iterator* iterator = db->NewIterator(ReadOptions());\n+  iterator->SeekToFirst();\n+  for (size_t i = 0; i < 3; ++i) {\n+    ASSERT_TRUE(iterator->Valid());\n+    ASSERT_TRUE(keys[i] == iterator->key());\n+    ASSERT_TRUE(vals[i] == iterator->value());\n+    iterator->Next();\n+  }\n+  ASSERT_TRUE(!iterator->Valid());\n+  delete iterator;\n+\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db);\n+  ASSERT_OK(dbi->TEST_CompactMemTable());\n+\n+  for (size_t i = 0; i < 3; ++i) {\n+    std::string res;\n+    ASSERT_OK(db->Get(ReadOptions(), keys[i], &res));\n+    ASSERT_TRUE(res == vals[i]);\n+  }\n+\n+  delete db;\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "1fa58866c3958c66d7c68b264aa4ee4ccdf51c68",
        "filename": "src/leveldb/include/leveldb/c.h",
        "status": "added",
        "additions": 291,
        "deletions": 0,
        "changes": 291,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/c.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/c.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/c.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,291 @@\n+/* Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+  Use of this source code is governed by a BSD-style license that can be\n+  found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+  C bindings for leveldb.  May be useful as a stable ABI that can be\n+  used by programs that keep leveldb in a shared library, or for\n+  a JNI api.\n+\n+  Does not support:\n+  . getters for the option types\n+  . custom comparators that implement key shortening\n+  . capturing post-write-snapshot\n+  . custom iter, db, env, cache implementations using just the C bindings\n+\n+  Some conventions:\n+\n+  (1) We expose just opaque struct pointers and functions to clients.\n+  This allows us to change internal representations without having to\n+  recompile clients.\n+\n+  (2) For simplicity, there is no equivalent to the Slice type.  Instead,\n+  the caller has to pass the pointer and length as separate\n+  arguments.\n+\n+  (3) Errors are represented by a null-terminated c string.  NULL\n+  means no error.  All operations that can raise an error are passed\n+  a \"char** errptr\" as the last argument.  One of the following must\n+  be true on entry:\n+     *errptr == NULL\n+     *errptr points to a malloc()ed null-terminated error message\n+       (On Windows, *errptr must have been malloc()-ed by this library.)\n+  On success, a leveldb routine leaves *errptr unchanged.\n+  On failure, leveldb frees the old value of *errptr and\n+  set *errptr to a malloc()ed error message.\n+\n+  (4) Bools have the type unsigned char (0 == false; rest == true)\n+\n+  (5) All of the pointer arguments must be non-NULL.\n+*/\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_C_H_\n+#define STORAGE_LEVELDB_INCLUDE_C_H_\n+\n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n+\n+#include <stdarg.h>\n+#include <stddef.h>\n+#include <stdint.h>\n+\n+/* Exported types */\n+\n+typedef struct leveldb_t               leveldb_t;\n+typedef struct leveldb_cache_t         leveldb_cache_t;\n+typedef struct leveldb_comparator_t    leveldb_comparator_t;\n+typedef struct leveldb_env_t           leveldb_env_t;\n+typedef struct leveldb_filelock_t      leveldb_filelock_t;\n+typedef struct leveldb_filterpolicy_t  leveldb_filterpolicy_t;\n+typedef struct leveldb_iterator_t      leveldb_iterator_t;\n+typedef struct leveldb_logger_t        leveldb_logger_t;\n+typedef struct leveldb_options_t       leveldb_options_t;\n+typedef struct leveldb_randomfile_t    leveldb_randomfile_t;\n+typedef struct leveldb_readoptions_t   leveldb_readoptions_t;\n+typedef struct leveldb_seqfile_t       leveldb_seqfile_t;\n+typedef struct leveldb_snapshot_t      leveldb_snapshot_t;\n+typedef struct leveldb_writablefile_t  leveldb_writablefile_t;\n+typedef struct leveldb_writebatch_t    leveldb_writebatch_t;\n+typedef struct leveldb_writeoptions_t  leveldb_writeoptions_t;\n+\n+/* DB operations */\n+\n+extern leveldb_t* leveldb_open(\n+    const leveldb_options_t* options,\n+    const char* name,\n+    char** errptr);\n+\n+extern void leveldb_close(leveldb_t* db);\n+\n+extern void leveldb_put(\n+    leveldb_t* db,\n+    const leveldb_writeoptions_t* options,\n+    const char* key, size_t keylen,\n+    const char* val, size_t vallen,\n+    char** errptr);\n+\n+extern void leveldb_delete(\n+    leveldb_t* db,\n+    const leveldb_writeoptions_t* options,\n+    const char* key, size_t keylen,\n+    char** errptr);\n+\n+extern void leveldb_write(\n+    leveldb_t* db,\n+    const leveldb_writeoptions_t* options,\n+    leveldb_writebatch_t* batch,\n+    char** errptr);\n+\n+/* Returns NULL if not found.  A malloc()ed array otherwise.\n+   Stores the length of the array in *vallen. */\n+extern char* leveldb_get(\n+    leveldb_t* db,\n+    const leveldb_readoptions_t* options,\n+    const char* key, size_t keylen,\n+    size_t* vallen,\n+    char** errptr);\n+\n+extern leveldb_iterator_t* leveldb_create_iterator(\n+    leveldb_t* db,\n+    const leveldb_readoptions_t* options);\n+\n+extern const leveldb_snapshot_t* leveldb_create_snapshot(\n+    leveldb_t* db);\n+\n+extern void leveldb_release_snapshot(\n+    leveldb_t* db,\n+    const leveldb_snapshot_t* snapshot);\n+\n+/* Returns NULL if property name is unknown.\n+   Else returns a pointer to a malloc()-ed null-terminated value. */\n+extern char* leveldb_property_value(\n+    leveldb_t* db,\n+    const char* propname);\n+\n+extern void leveldb_approximate_sizes(\n+    leveldb_t* db,\n+    int num_ranges,\n+    const char* const* range_start_key, const size_t* range_start_key_len,\n+    const char* const* range_limit_key, const size_t* range_limit_key_len,\n+    uint64_t* sizes);\n+\n+extern void leveldb_compact_range(\n+    leveldb_t* db,\n+    const char* start_key, size_t start_key_len,\n+    const char* limit_key, size_t limit_key_len);\n+\n+/* Management operations */\n+\n+extern void leveldb_destroy_db(\n+    const leveldb_options_t* options,\n+    const char* name,\n+    char** errptr);\n+\n+extern void leveldb_repair_db(\n+    const leveldb_options_t* options,\n+    const char* name,\n+    char** errptr);\n+\n+/* Iterator */\n+\n+extern void leveldb_iter_destroy(leveldb_iterator_t*);\n+extern unsigned char leveldb_iter_valid(const leveldb_iterator_t*);\n+extern void leveldb_iter_seek_to_first(leveldb_iterator_t*);\n+extern void leveldb_iter_seek_to_last(leveldb_iterator_t*);\n+extern void leveldb_iter_seek(leveldb_iterator_t*, const char* k, size_t klen);\n+extern void leveldb_iter_next(leveldb_iterator_t*);\n+extern void leveldb_iter_prev(leveldb_iterator_t*);\n+extern const char* leveldb_iter_key(const leveldb_iterator_t*, size_t* klen);\n+extern const char* leveldb_iter_value(const leveldb_iterator_t*, size_t* vlen);\n+extern void leveldb_iter_get_error(const leveldb_iterator_t*, char** errptr);\n+\n+/* Write batch */\n+\n+extern leveldb_writebatch_t* leveldb_writebatch_create();\n+extern void leveldb_writebatch_destroy(leveldb_writebatch_t*);\n+extern void leveldb_writebatch_clear(leveldb_writebatch_t*);\n+extern void leveldb_writebatch_put(\n+    leveldb_writebatch_t*,\n+    const char* key, size_t klen,\n+    const char* val, size_t vlen);\n+extern void leveldb_writebatch_delete(\n+    leveldb_writebatch_t*,\n+    const char* key, size_t klen);\n+extern void leveldb_writebatch_iterate(\n+    leveldb_writebatch_t*,\n+    void* state,\n+    void (*put)(void*, const char* k, size_t klen, const char* v, size_t vlen),\n+    void (*deleted)(void*, const char* k, size_t klen));\n+\n+/* Options */\n+\n+extern leveldb_options_t* leveldb_options_create();\n+extern void leveldb_options_destroy(leveldb_options_t*);\n+extern void leveldb_options_set_comparator(\n+    leveldb_options_t*,\n+    leveldb_comparator_t*);\n+extern void leveldb_options_set_filter_policy(\n+    leveldb_options_t*,\n+    leveldb_filterpolicy_t*);\n+extern void leveldb_options_set_create_if_missing(\n+    leveldb_options_t*, unsigned char);\n+extern void leveldb_options_set_error_if_exists(\n+    leveldb_options_t*, unsigned char);\n+extern void leveldb_options_set_paranoid_checks(\n+    leveldb_options_t*, unsigned char);\n+extern void leveldb_options_set_env(leveldb_options_t*, leveldb_env_t*);\n+extern void leveldb_options_set_info_log(leveldb_options_t*, leveldb_logger_t*);\n+extern void leveldb_options_set_write_buffer_size(leveldb_options_t*, size_t);\n+extern void leveldb_options_set_max_open_files(leveldb_options_t*, int);\n+extern void leveldb_options_set_cache(leveldb_options_t*, leveldb_cache_t*);\n+extern void leveldb_options_set_block_size(leveldb_options_t*, size_t);\n+extern void leveldb_options_set_block_restart_interval(leveldb_options_t*, int);\n+\n+enum {\n+  leveldb_no_compression = 0,\n+  leveldb_snappy_compression = 1\n+};\n+extern void leveldb_options_set_compression(leveldb_options_t*, int);\n+\n+/* Comparator */\n+\n+extern leveldb_comparator_t* leveldb_comparator_create(\n+    void* state,\n+    void (*destructor)(void*),\n+    int (*compare)(\n+        void*,\n+        const char* a, size_t alen,\n+        const char* b, size_t blen),\n+    const char* (*name)(void*));\n+extern void leveldb_comparator_destroy(leveldb_comparator_t*);\n+\n+/* Filter policy */\n+\n+extern leveldb_filterpolicy_t* leveldb_filterpolicy_create(\n+    void* state,\n+    void (*destructor)(void*),\n+    char* (*create_filter)(\n+        void*,\n+        const char* const* key_array, const size_t* key_length_array,\n+        int num_keys,\n+        size_t* filter_length),\n+    unsigned char (*key_may_match)(\n+        void*,\n+        const char* key, size_t length,\n+        const char* filter, size_t filter_length),\n+    const char* (*name)(void*));\n+extern void leveldb_filterpolicy_destroy(leveldb_filterpolicy_t*);\n+\n+extern leveldb_filterpolicy_t* leveldb_filterpolicy_create_bloom(\n+    int bits_per_key);\n+\n+/* Read options */\n+\n+extern leveldb_readoptions_t* leveldb_readoptions_create();\n+extern void leveldb_readoptions_destroy(leveldb_readoptions_t*);\n+extern void leveldb_readoptions_set_verify_checksums(\n+    leveldb_readoptions_t*,\n+    unsigned char);\n+extern void leveldb_readoptions_set_fill_cache(\n+    leveldb_readoptions_t*, unsigned char);\n+extern void leveldb_readoptions_set_snapshot(\n+    leveldb_readoptions_t*,\n+    const leveldb_snapshot_t*);\n+\n+/* Write options */\n+\n+extern leveldb_writeoptions_t* leveldb_writeoptions_create();\n+extern void leveldb_writeoptions_destroy(leveldb_writeoptions_t*);\n+extern void leveldb_writeoptions_set_sync(\n+    leveldb_writeoptions_t*, unsigned char);\n+\n+/* Cache */\n+\n+extern leveldb_cache_t* leveldb_cache_create_lru(size_t capacity);\n+extern void leveldb_cache_destroy(leveldb_cache_t* cache);\n+\n+/* Env */\n+\n+extern leveldb_env_t* leveldb_create_default_env();\n+extern void leveldb_env_destroy(leveldb_env_t*);\n+\n+/* Utility */\n+\n+/* Calls free(ptr).\n+   REQUIRES: ptr was malloc()-ed and returned by one of the routines\n+   in this file.  Note that in certain cases (typically on Windows), you\n+   may need to call this routine instead of free(ptr) to dispose of\n+   malloc()-ed memory returned by this library. */\n+extern void leveldb_free(void* ptr);\n+\n+/* Return the major version number for this release. */\n+extern int leveldb_major_version();\n+\n+/* Return the minor version number for this release. */\n+extern int leveldb_minor_version();\n+\n+#ifdef __cplusplus\n+}  /* end extern \"C\" */\n+#endif\n+\n+#endif  /* STORAGE_LEVELDB_INCLUDE_C_H_ */"
      },
      {
        "sha": "5e3b47637d49e9f963b141dc5a011e1272615750",
        "filename": "src/leveldb/include/leveldb/cache.h",
        "status": "added",
        "additions": 99,
        "deletions": 0,
        "changes": 99,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/cache.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/cache.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/cache.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,99 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// A Cache is an interface that maps keys to values.  It has internal\n+// synchronization and may be safely accessed concurrently from\n+// multiple threads.  It may automatically evict entries to make room\n+// for new entries.  Values have a specified charge against the cache\n+// capacity.  For example, a cache where the values are variable\n+// length strings, may use the length of the string as the charge for\n+// the string.\n+//\n+// A builtin cache implementation with a least-recently-used eviction\n+// policy is provided.  Clients may use their own implementations if\n+// they want something more sophisticated (like scan-resistance, a\n+// custom eviction policy, variable cache sizing, etc.)\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_CACHE_H_\n+#define STORAGE_LEVELDB_INCLUDE_CACHE_H_\n+\n+#include <stdint.h>\n+#include \"leveldb/slice.h\"\n+\n+namespace leveldb {\n+\n+class Cache;\n+\n+// Create a new cache with a fixed size capacity.  This implementation\n+// of Cache uses a least-recently-used eviction policy.\n+extern Cache* NewLRUCache(size_t capacity);\n+\n+class Cache {\n+ public:\n+  Cache() { }\n+\n+  // Destroys all existing entries by calling the \"deleter\"\n+  // function that was passed to the constructor.\n+  virtual ~Cache();\n+\n+  // Opaque handle to an entry stored in the cache.\n+  struct Handle { };\n+\n+  // Insert a mapping from key->value into the cache and assign it\n+  // the specified charge against the total cache capacity.\n+  //\n+  // Returns a handle that corresponds to the mapping.  The caller\n+  // must call this->Release(handle) when the returned mapping is no\n+  // longer needed.\n+  //\n+  // When the inserted entry is no longer needed, the key and\n+  // value will be passed to \"deleter\".\n+  virtual Handle* Insert(const Slice& key, void* value, size_t charge,\n+                         void (*deleter)(const Slice& key, void* value)) = 0;\n+\n+  // If the cache has no mapping for \"key\", returns NULL.\n+  //\n+  // Else return a handle that corresponds to the mapping.  The caller\n+  // must call this->Release(handle) when the returned mapping is no\n+  // longer needed.\n+  virtual Handle* Lookup(const Slice& key) = 0;\n+\n+  // Release a mapping returned by a previous Lookup().\n+  // REQUIRES: handle must not have been released yet.\n+  // REQUIRES: handle must have been returned by a method on *this.\n+  virtual void Release(Handle* handle) = 0;\n+\n+  // Return the value encapsulated in a handle returned by a\n+  // successful Lookup().\n+  // REQUIRES: handle must not have been released yet.\n+  // REQUIRES: handle must have been returned by a method on *this.\n+  virtual void* Value(Handle* handle) = 0;\n+\n+  // If the cache contains entry for key, erase it.  Note that the\n+  // underlying entry will be kept around until all existing handles\n+  // to it have been released.\n+  virtual void Erase(const Slice& key) = 0;\n+\n+  // Return a new numeric id.  May be used by multiple clients who are\n+  // sharing the same cache to partition the key space.  Typically the\n+  // client will allocate a new id at startup and prepend the id to\n+  // its cache keys.\n+  virtual uint64_t NewId() = 0;\n+\n+ private:\n+  void LRU_Remove(Handle* e);\n+  void LRU_Append(Handle* e);\n+  void Unref(Handle* e);\n+\n+  struct Rep;\n+  Rep* rep_;\n+\n+  // No copying allowed\n+  Cache(const Cache&);\n+  void operator=(const Cache&);\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_UTIL_CACHE_H_"
      },
      {
        "sha": "556b984c7694f6520088754f3017bf58c7cafc9d",
        "filename": "src/leveldb/include/leveldb/comparator.h",
        "status": "added",
        "additions": 63,
        "deletions": 0,
        "changes": 63,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/comparator.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/comparator.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/comparator.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,63 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_COMPARATOR_H_\n+#define STORAGE_LEVELDB_INCLUDE_COMPARATOR_H_\n+\n+#include <string>\n+\n+namespace leveldb {\n+\n+class Slice;\n+\n+// A Comparator object provides a total order across slices that are\n+// used as keys in an sstable or a database.  A Comparator implementation\n+// must be thread-safe since leveldb may invoke its methods concurrently\n+// from multiple threads.\n+class Comparator {\n+ public:\n+  virtual ~Comparator();\n+\n+  // Three-way comparison.  Returns value:\n+  //   < 0 iff \"a\" < \"b\",\n+  //   == 0 iff \"a\" == \"b\",\n+  //   > 0 iff \"a\" > \"b\"\n+  virtual int Compare(const Slice& a, const Slice& b) const = 0;\n+\n+  // The name of the comparator.  Used to check for comparator\n+  // mismatches (i.e., a DB created with one comparator is\n+  // accessed using a different comparator.\n+  //\n+  // The client of this package should switch to a new name whenever\n+  // the comparator implementation changes in a way that will cause\n+  // the relative ordering of any two keys to change.\n+  //\n+  // Names starting with \"leveldb.\" are reserved and should not be used\n+  // by any clients of this package.\n+  virtual const char* Name() const = 0;\n+\n+  // Advanced functions: these are used to reduce the space requirements\n+  // for internal data structures like index blocks.\n+\n+  // If *start < limit, changes *start to a short string in [start,limit).\n+  // Simple comparator implementations may return with *start unchanged,\n+  // i.e., an implementation of this method that does nothing is correct.\n+  virtual void FindShortestSeparator(\n+      std::string* start,\n+      const Slice& limit) const = 0;\n+\n+  // Changes *key to a short string >= *key.\n+  // Simple comparator implementations may return with *key unchanged,\n+  // i.e., an implementation of this method that does nothing is correct.\n+  virtual void FindShortSuccessor(std::string* key) const = 0;\n+};\n+\n+// Return a builtin comparator that uses lexicographic byte-wise\n+// ordering.  The result remains the property of this module and\n+// must not be deleted.\n+extern const Comparator* BytewiseComparator();\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_COMPARATOR_H_"
      },
      {
        "sha": "da8b11a8c05b054ef231ba34d88ef51ec1e55eb0",
        "filename": "src/leveldb/include/leveldb/db.h",
        "status": "added",
        "additions": 161,
        "deletions": 0,
        "changes": 161,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/db.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/db.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/db.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,161 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_DB_H_\n+#define STORAGE_LEVELDB_INCLUDE_DB_H_\n+\n+#include <stdint.h>\n+#include <stdio.h>\n+#include \"leveldb/iterator.h\"\n+#include \"leveldb/options.h\"\n+\n+namespace leveldb {\n+\n+// Update Makefile if you change these\n+static const int kMajorVersion = 1;\n+static const int kMinorVersion = 12;\n+\n+struct Options;\n+struct ReadOptions;\n+struct WriteOptions;\n+class WriteBatch;\n+\n+// Abstract handle to particular state of a DB.\n+// A Snapshot is an immutable object and can therefore be safely\n+// accessed from multiple threads without any external synchronization.\n+class Snapshot {\n+ protected:\n+  virtual ~Snapshot();\n+};\n+\n+// A range of keys\n+struct Range {\n+  Slice start;          // Included in the range\n+  Slice limit;          // Not included in the range\n+\n+  Range() { }\n+  Range(const Slice& s, const Slice& l) : start(s), limit(l) { }\n+};\n+\n+// A DB is a persistent ordered map from keys to values.\n+// A DB is safe for concurrent access from multiple threads without\n+// any external synchronization.\n+class DB {\n+ public:\n+  // Open the database with the specified \"name\".\n+  // Stores a pointer to a heap-allocated database in *dbptr and returns\n+  // OK on success.\n+  // Stores NULL in *dbptr and returns a non-OK status on error.\n+  // Caller should delete *dbptr when it is no longer needed.\n+  static Status Open(const Options& options,\n+                     const std::string& name,\n+                     DB** dbptr);\n+\n+  DB() { }\n+  virtual ~DB();\n+\n+  // Set the database entry for \"key\" to \"value\".  Returns OK on success,\n+  // and a non-OK status on error.\n+  // Note: consider setting options.sync = true.\n+  virtual Status Put(const WriteOptions& options,\n+                     const Slice& key,\n+                     const Slice& value) = 0;\n+\n+  // Remove the database entry (if any) for \"key\".  Returns OK on\n+  // success, and a non-OK status on error.  It is not an error if \"key\"\n+  // did not exist in the database.\n+  // Note: consider setting options.sync = true.\n+  virtual Status Delete(const WriteOptions& options, const Slice& key) = 0;\n+\n+  // Apply the specified updates to the database.\n+  // Returns OK on success, non-OK on failure.\n+  // Note: consider setting options.sync = true.\n+  virtual Status Write(const WriteOptions& options, WriteBatch* updates) = 0;\n+\n+  // If the database contains an entry for \"key\" store the\n+  // corresponding value in *value and return OK.\n+  //\n+  // If there is no entry for \"key\" leave *value unchanged and return\n+  // a status for which Status::IsNotFound() returns true.\n+  //\n+  // May return some other Status on an error.\n+  virtual Status Get(const ReadOptions& options,\n+                     const Slice& key, std::string* value) = 0;\n+\n+  // Return a heap-allocated iterator over the contents of the database.\n+  // The result of NewIterator() is initially invalid (caller must\n+  // call one of the Seek methods on the iterator before using it).\n+  //\n+  // Caller should delete the iterator when it is no longer needed.\n+  // The returned iterator should be deleted before this db is deleted.\n+  virtual Iterator* NewIterator(const ReadOptions& options) = 0;\n+\n+  // Return a handle to the current DB state.  Iterators created with\n+  // this handle will all observe a stable snapshot of the current DB\n+  // state.  The caller must call ReleaseSnapshot(result) when the\n+  // snapshot is no longer needed.\n+  virtual const Snapshot* GetSnapshot() = 0;\n+\n+  // Release a previously acquired snapshot.  The caller must not\n+  // use \"snapshot\" after this call.\n+  virtual void ReleaseSnapshot(const Snapshot* snapshot) = 0;\n+\n+  // DB implementations can export properties about their state\n+  // via this method.  If \"property\" is a valid property understood by this\n+  // DB implementation, fills \"*value\" with its current value and returns\n+  // true.  Otherwise returns false.\n+  //\n+  //\n+  // Valid property names include:\n+  //\n+  //  \"leveldb.num-files-at-level<N>\" - return the number of files at level <N>,\n+  //     where <N> is an ASCII representation of a level number (e.g. \"0\").\n+  //  \"leveldb.stats\" - returns a multi-line string that describes statistics\n+  //     about the internal operation of the DB.\n+  //  \"leveldb.sstables\" - returns a multi-line string that describes all\n+  //     of the sstables that make up the db contents.\n+  virtual bool GetProperty(const Slice& property, std::string* value) = 0;\n+\n+  // For each i in [0,n-1], store in \"sizes[i]\", the approximate\n+  // file system space used by keys in \"[range[i].start .. range[i].limit)\".\n+  //\n+  // Note that the returned sizes measure file system space usage, so\n+  // if the user data compresses by a factor of ten, the returned\n+  // sizes will be one-tenth the size of the corresponding user data size.\n+  //\n+  // The results may not include the sizes of recently written data.\n+  virtual void GetApproximateSizes(const Range* range, int n,\n+                                   uint64_t* sizes) = 0;\n+\n+  // Compact the underlying storage for the key range [*begin,*end].\n+  // In particular, deleted and overwritten versions are discarded,\n+  // and the data is rearranged to reduce the cost of operations\n+  // needed to access the data.  This operation should typically only\n+  // be invoked by users who understand the underlying implementation.\n+  //\n+  // begin==NULL is treated as a key before all keys in the database.\n+  // end==NULL is treated as a key after all keys in the database.\n+  // Therefore the following call will compact the entire database:\n+  //    db->CompactRange(NULL, NULL);\n+  virtual void CompactRange(const Slice* begin, const Slice* end) = 0;\n+\n+ private:\n+  // No copying allowed\n+  DB(const DB&);\n+  void operator=(const DB&);\n+};\n+\n+// Destroy the contents of the specified database.\n+// Be very careful using this method.\n+Status DestroyDB(const std::string& name, const Options& options);\n+\n+// If a DB cannot be opened, you may attempt to call this method to\n+// resurrect as much of the contents of the database as possible.\n+// Some data may be lost, so be careful when calling this function\n+// on a database that contains important information.\n+Status RepairDB(const std::string& dbname, const Options& options);\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_DB_H_"
      },
      {
        "sha": "fa32289f581fd4d222dc74ea177a78138b71fbc2",
        "filename": "src/leveldb/include/leveldb/env.h",
        "status": "added",
        "additions": 333,
        "deletions": 0,
        "changes": 333,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/env.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/env.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/env.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,333 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// An Env is an interface used by the leveldb implementation to access\n+// operating system functionality like the filesystem etc.  Callers\n+// may wish to provide a custom Env object when opening a database to\n+// get fine gain control; e.g., to rate limit file system operations.\n+//\n+// All Env implementations are safe for concurrent access from\n+// multiple threads without any external synchronization.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_ENV_H_\n+#define STORAGE_LEVELDB_INCLUDE_ENV_H_\n+\n+#include <cstdarg>\n+#include <string>\n+#include <vector>\n+#include <stdint.h>\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+class FileLock;\n+class Logger;\n+class RandomAccessFile;\n+class SequentialFile;\n+class Slice;\n+class WritableFile;\n+\n+class Env {\n+ public:\n+  Env() { }\n+  virtual ~Env();\n+\n+  // Return a default environment suitable for the current operating\n+  // system.  Sophisticated users may wish to provide their own Env\n+  // implementation instead of relying on this default environment.\n+  //\n+  // The result of Default() belongs to leveldb and must never be deleted.\n+  static Env* Default();\n+\n+  // Create a brand new sequentially-readable file with the specified name.\n+  // On success, stores a pointer to the new file in *result and returns OK.\n+  // On failure stores NULL in *result and returns non-OK.  If the file does\n+  // not exist, returns a non-OK status.\n+  //\n+  // The returned file will only be accessed by one thread at a time.\n+  virtual Status NewSequentialFile(const std::string& fname,\n+                                   SequentialFile** result) = 0;\n+\n+  // Create a brand new random access read-only file with the\n+  // specified name.  On success, stores a pointer to the new file in\n+  // *result and returns OK.  On failure stores NULL in *result and\n+  // returns non-OK.  If the file does not exist, returns a non-OK\n+  // status.\n+  //\n+  // The returned file may be concurrently accessed by multiple threads.\n+  virtual Status NewRandomAccessFile(const std::string& fname,\n+                                     RandomAccessFile** result) = 0;\n+\n+  // Create an object that writes to a new file with the specified\n+  // name.  Deletes any existing file with the same name and creates a\n+  // new file.  On success, stores a pointer to the new file in\n+  // *result and returns OK.  On failure stores NULL in *result and\n+  // returns non-OK.\n+  //\n+  // The returned file will only be accessed by one thread at a time.\n+  virtual Status NewWritableFile(const std::string& fname,\n+                                 WritableFile** result) = 0;\n+\n+  // Returns true iff the named file exists.\n+  virtual bool FileExists(const std::string& fname) = 0;\n+\n+  // Store in *result the names of the children of the specified directory.\n+  // The names are relative to \"dir\".\n+  // Original contents of *results are dropped.\n+  virtual Status GetChildren(const std::string& dir,\n+                             std::vector<std::string>* result) = 0;\n+\n+  // Delete the named file.\n+  virtual Status DeleteFile(const std::string& fname) = 0;\n+\n+  // Create the specified directory.\n+  virtual Status CreateDir(const std::string& dirname) = 0;\n+\n+  // Delete the specified directory.\n+  virtual Status DeleteDir(const std::string& dirname) = 0;\n+\n+  // Store the size of fname in *file_size.\n+  virtual Status GetFileSize(const std::string& fname, uint64_t* file_size) = 0;\n+\n+  // Rename file src to target.\n+  virtual Status RenameFile(const std::string& src,\n+                            const std::string& target) = 0;\n+\n+  // Lock the specified file.  Used to prevent concurrent access to\n+  // the same db by multiple processes.  On failure, stores NULL in\n+  // *lock and returns non-OK.\n+  //\n+  // On success, stores a pointer to the object that represents the\n+  // acquired lock in *lock and returns OK.  The caller should call\n+  // UnlockFile(*lock) to release the lock.  If the process exits,\n+  // the lock will be automatically released.\n+  //\n+  // If somebody else already holds the lock, finishes immediately\n+  // with a failure.  I.e., this call does not wait for existing locks\n+  // to go away.\n+  //\n+  // May create the named file if it does not already exist.\n+  virtual Status LockFile(const std::string& fname, FileLock** lock) = 0;\n+\n+  // Release the lock acquired by a previous successful call to LockFile.\n+  // REQUIRES: lock was returned by a successful LockFile() call\n+  // REQUIRES: lock has not already been unlocked.\n+  virtual Status UnlockFile(FileLock* lock) = 0;\n+\n+  // Arrange to run \"(*function)(arg)\" once in a background thread.\n+  //\n+  // \"function\" may run in an unspecified thread.  Multiple functions\n+  // added to the same Env may run concurrently in different threads.\n+  // I.e., the caller may not assume that background work items are\n+  // serialized.\n+  virtual void Schedule(\n+      void (*function)(void* arg),\n+      void* arg) = 0;\n+\n+  // Start a new thread, invoking \"function(arg)\" within the new thread.\n+  // When \"function(arg)\" returns, the thread will be destroyed.\n+  virtual void StartThread(void (*function)(void* arg), void* arg) = 0;\n+\n+  // *path is set to a temporary directory that can be used for testing. It may\n+  // or many not have just been created. The directory may or may not differ\n+  // between runs of the same process, but subsequent calls will return the\n+  // same directory.\n+  virtual Status GetTestDirectory(std::string* path) = 0;\n+\n+  // Create and return a log file for storing informational messages.\n+  virtual Status NewLogger(const std::string& fname, Logger** result) = 0;\n+\n+  // Returns the number of micro-seconds since some fixed point in time. Only\n+  // useful for computing deltas of time.\n+  virtual uint64_t NowMicros() = 0;\n+\n+  // Sleep/delay the thread for the perscribed number of micro-seconds.\n+  virtual void SleepForMicroseconds(int micros) = 0;\n+\n+ private:\n+  // No copying allowed\n+  Env(const Env&);\n+  void operator=(const Env&);\n+};\n+\n+// A file abstraction for reading sequentially through a file\n+class SequentialFile {\n+ public:\n+  SequentialFile() { }\n+  virtual ~SequentialFile();\n+\n+  // Read up to \"n\" bytes from the file.  \"scratch[0..n-1]\" may be\n+  // written by this routine.  Sets \"*result\" to the data that was\n+  // read (including if fewer than \"n\" bytes were successfully read).\n+  // May set \"*result\" to point at data in \"scratch[0..n-1]\", so\n+  // \"scratch[0..n-1]\" must be live when \"*result\" is used.\n+  // If an error was encountered, returns a non-OK status.\n+  //\n+  // REQUIRES: External synchronization\n+  virtual Status Read(size_t n, Slice* result, char* scratch) = 0;\n+\n+  // Skip \"n\" bytes from the file. This is guaranteed to be no\n+  // slower that reading the same data, but may be faster.\n+  //\n+  // If end of file is reached, skipping will stop at the end of the\n+  // file, and Skip will return OK.\n+  //\n+  // REQUIRES: External synchronization\n+  virtual Status Skip(uint64_t n) = 0;\n+\n+ private:\n+  // No copying allowed\n+  SequentialFile(const SequentialFile&);\n+  void operator=(const SequentialFile&);\n+};\n+\n+// A file abstraction for randomly reading the contents of a file.\n+class RandomAccessFile {\n+ public:\n+  RandomAccessFile() { }\n+  virtual ~RandomAccessFile();\n+\n+  // Read up to \"n\" bytes from the file starting at \"offset\".\n+  // \"scratch[0..n-1]\" may be written by this routine.  Sets \"*result\"\n+  // to the data that was read (including if fewer than \"n\" bytes were\n+  // successfully read).  May set \"*result\" to point at data in\n+  // \"scratch[0..n-1]\", so \"scratch[0..n-1]\" must be live when\n+  // \"*result\" is used.  If an error was encountered, returns a non-OK\n+  // status.\n+  //\n+  // Safe for concurrent use by multiple threads.\n+  virtual Status Read(uint64_t offset, size_t n, Slice* result,\n+                      char* scratch) const = 0;\n+\n+ private:\n+  // No copying allowed\n+  RandomAccessFile(const RandomAccessFile&);\n+  void operator=(const RandomAccessFile&);\n+};\n+\n+// A file abstraction for sequential writing.  The implementation\n+// must provide buffering since callers may append small fragments\n+// at a time to the file.\n+class WritableFile {\n+ public:\n+  WritableFile() { }\n+  virtual ~WritableFile();\n+\n+  virtual Status Append(const Slice& data) = 0;\n+  virtual Status Close() = 0;\n+  virtual Status Flush() = 0;\n+  virtual Status Sync() = 0;\n+\n+ private:\n+  // No copying allowed\n+  WritableFile(const WritableFile&);\n+  void operator=(const WritableFile&);\n+};\n+\n+// An interface for writing log messages.\n+class Logger {\n+ public:\n+  Logger() { }\n+  virtual ~Logger();\n+\n+  // Write an entry to the log file with the specified format.\n+  virtual void Logv(const char* format, va_list ap) = 0;\n+\n+ private:\n+  // No copying allowed\n+  Logger(const Logger&);\n+  void operator=(const Logger&);\n+};\n+\n+\n+// Identifies a locked file.\n+class FileLock {\n+ public:\n+  FileLock() { }\n+  virtual ~FileLock();\n+ private:\n+  // No copying allowed\n+  FileLock(const FileLock&);\n+  void operator=(const FileLock&);\n+};\n+\n+// Log the specified data to *info_log if info_log is non-NULL.\n+extern void Log(Logger* info_log, const char* format, ...)\n+#   if defined(__GNUC__) || defined(__clang__)\n+    __attribute__((__format__ (__printf__, 2, 3)))\n+#   endif\n+    ;\n+\n+// A utility routine: write \"data\" to the named file.\n+extern Status WriteStringToFile(Env* env, const Slice& data,\n+                                const std::string& fname);\n+\n+// A utility routine: read contents of named file into *data\n+extern Status ReadFileToString(Env* env, const std::string& fname,\n+                               std::string* data);\n+\n+// An implementation of Env that forwards all calls to another Env.\n+// May be useful to clients who wish to override just part of the\n+// functionality of another Env.\n+class EnvWrapper : public Env {\n+ public:\n+  // Initialize an EnvWrapper that delegates all calls to *t\n+  explicit EnvWrapper(Env* t) : target_(t) { }\n+  virtual ~EnvWrapper();\n+\n+  // Return the target to which this Env forwards all calls\n+  Env* target() const { return target_; }\n+\n+  // The following text is boilerplate that forwards all methods to target()\n+  Status NewSequentialFile(const std::string& f, SequentialFile** r) {\n+    return target_->NewSequentialFile(f, r);\n+  }\n+  Status NewRandomAccessFile(const std::string& f, RandomAccessFile** r) {\n+    return target_->NewRandomAccessFile(f, r);\n+  }\n+  Status NewWritableFile(const std::string& f, WritableFile** r) {\n+    return target_->NewWritableFile(f, r);\n+  }\n+  bool FileExists(const std::string& f) { return target_->FileExists(f); }\n+  Status GetChildren(const std::string& dir, std::vector<std::string>* r) {\n+    return target_->GetChildren(dir, r);\n+  }\n+  Status DeleteFile(const std::string& f) { return target_->DeleteFile(f); }\n+  Status CreateDir(const std::string& d) { return target_->CreateDir(d); }\n+  Status DeleteDir(const std::string& d) { return target_->DeleteDir(d); }\n+  Status GetFileSize(const std::string& f, uint64_t* s) {\n+    return target_->GetFileSize(f, s);\n+  }\n+  Status RenameFile(const std::string& s, const std::string& t) {\n+    return target_->RenameFile(s, t);\n+  }\n+  Status LockFile(const std::string& f, FileLock** l) {\n+    return target_->LockFile(f, l);\n+  }\n+  Status UnlockFile(FileLock* l) { return target_->UnlockFile(l); }\n+  void Schedule(void (*f)(void*), void* a) {\n+    return target_->Schedule(f, a);\n+  }\n+  void StartThread(void (*f)(void*), void* a) {\n+    return target_->StartThread(f, a);\n+  }\n+  virtual Status GetTestDirectory(std::string* path) {\n+    return target_->GetTestDirectory(path);\n+  }\n+  virtual Status NewLogger(const std::string& fname, Logger** result) {\n+    return target_->NewLogger(fname, result);\n+  }\n+  uint64_t NowMicros() {\n+    return target_->NowMicros();\n+  }\n+  void SleepForMicroseconds(int micros) {\n+    target_->SleepForMicroseconds(micros);\n+  }\n+ private:\n+  Env* target_;\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_ENV_H_"
      },
      {
        "sha": "1fba08001fc335a14bde90fa5a1c5d58025ae038",
        "filename": "src/leveldb/include/leveldb/filter_policy.h",
        "status": "added",
        "additions": 70,
        "deletions": 0,
        "changes": 70,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/filter_policy.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/filter_policy.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/filter_policy.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,70 @@\n+// Copyright (c) 2012 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// A database can be configured with a custom FilterPolicy object.\n+// This object is responsible for creating a small filter from a set\n+// of keys.  These filters are stored in leveldb and are consulted\n+// automatically by leveldb to decide whether or not to read some\n+// information from disk. In many cases, a filter can cut down the\n+// number of disk seeks form a handful to a single disk seek per\n+// DB::Get() call.\n+//\n+// Most people will want to use the builtin bloom filter support (see\n+// NewBloomFilterPolicy() below).\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_FILTER_POLICY_H_\n+#define STORAGE_LEVELDB_INCLUDE_FILTER_POLICY_H_\n+\n+#include <string>\n+\n+namespace leveldb {\n+\n+class Slice;\n+\n+class FilterPolicy {\n+ public:\n+  virtual ~FilterPolicy();\n+\n+  // Return the name of this policy.  Note that if the filter encoding\n+  // changes in an incompatible way, the name returned by this method\n+  // must be changed.  Otherwise, old incompatible filters may be\n+  // passed to methods of this type.\n+  virtual const char* Name() const = 0;\n+\n+  // keys[0,n-1] contains a list of keys (potentially with duplicates)\n+  // that are ordered according to the user supplied comparator.\n+  // Append a filter that summarizes keys[0,n-1] to *dst.\n+  //\n+  // Warning: do not change the initial contents of *dst.  Instead,\n+  // append the newly constructed filter to *dst.\n+  virtual void CreateFilter(const Slice* keys, int n, std::string* dst)\n+      const = 0;\n+\n+  // \"filter\" contains the data appended by a preceding call to\n+  // CreateFilter() on this class.  This method must return true if\n+  // the key was in the list of keys passed to CreateFilter().\n+  // This method may return true or false if the key was not on the\n+  // list, but it should aim to return false with a high probability.\n+  virtual bool KeyMayMatch(const Slice& key, const Slice& filter) const = 0;\n+};\n+\n+// Return a new filter policy that uses a bloom filter with approximately\n+// the specified number of bits per key.  A good value for bits_per_key\n+// is 10, which yields a filter with ~ 1% false positive rate.\n+//\n+// Callers must delete the result after any database that is using the\n+// result has been closed.\n+//\n+// Note: if you are using a custom comparator that ignores some parts\n+// of the keys being compared, you must not use NewBloomFilterPolicy()\n+// and must provide your own FilterPolicy that also ignores the\n+// corresponding parts of the keys.  For example, if the comparator\n+// ignores trailing spaces, it would be incorrect to use a\n+// FilterPolicy (like NewBloomFilterPolicy) that does not ignore\n+// trailing spaces in keys.\n+extern const FilterPolicy* NewBloomFilterPolicy(int bits_per_key);\n+\n+}\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_FILTER_POLICY_H_"
      },
      {
        "sha": "ad543eb46cde9af30f9250ee2eaa7f0979cc2994",
        "filename": "src/leveldb/include/leveldb/iterator.h",
        "status": "added",
        "additions": 100,
        "deletions": 0,
        "changes": 100,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/iterator.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/iterator.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/iterator.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,100 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// An iterator yields a sequence of key/value pairs from a source.\n+// The following class defines the interface.  Multiple implementations\n+// are provided by this library.  In particular, iterators are provided\n+// to access the contents of a Table or a DB.\n+//\n+// Multiple threads can invoke const methods on an Iterator without\n+// external synchronization, but if any of the threads may call a\n+// non-const method, all threads accessing the same Iterator must use\n+// external synchronization.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_ITERATOR_H_\n+#define STORAGE_LEVELDB_INCLUDE_ITERATOR_H_\n+\n+#include \"leveldb/slice.h\"\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+class Iterator {\n+ public:\n+  Iterator();\n+  virtual ~Iterator();\n+\n+  // An iterator is either positioned at a key/value pair, or\n+  // not valid.  This method returns true iff the iterator is valid.\n+  virtual bool Valid() const = 0;\n+\n+  // Position at the first key in the source.  The iterator is Valid()\n+  // after this call iff the source is not empty.\n+  virtual void SeekToFirst() = 0;\n+\n+  // Position at the last key in the source.  The iterator is\n+  // Valid() after this call iff the source is not empty.\n+  virtual void SeekToLast() = 0;\n+\n+  // Position at the first key in the source that at or past target\n+  // The iterator is Valid() after this call iff the source contains\n+  // an entry that comes at or past target.\n+  virtual void Seek(const Slice& target) = 0;\n+\n+  // Moves to the next entry in the source.  After this call, Valid() is\n+  // true iff the iterator was not positioned at the last entry in the source.\n+  // REQUIRES: Valid()\n+  virtual void Next() = 0;\n+\n+  // Moves to the previous entry in the source.  After this call, Valid() is\n+  // true iff the iterator was not positioned at the first entry in source.\n+  // REQUIRES: Valid()\n+  virtual void Prev() = 0;\n+\n+  // Return the key for the current entry.  The underlying storage for\n+  // the returned slice is valid only until the next modification of\n+  // the iterator.\n+  // REQUIRES: Valid()\n+  virtual Slice key() const = 0;\n+\n+  // Return the value for the current entry.  The underlying storage for\n+  // the returned slice is valid only until the next modification of\n+  // the iterator.\n+  // REQUIRES: !AtEnd() && !AtStart()\n+  virtual Slice value() const = 0;\n+\n+  // If an error has occurred, return it.  Else return an ok status.\n+  virtual Status status() const = 0;\n+\n+  // Clients are allowed to register function/arg1/arg2 triples that\n+  // will be invoked when this iterator is destroyed.\n+  //\n+  // Note that unlike all of the preceding methods, this method is\n+  // not abstract and therefore clients should not override it.\n+  typedef void (*CleanupFunction)(void* arg1, void* arg2);\n+  void RegisterCleanup(CleanupFunction function, void* arg1, void* arg2);\n+\n+ private:\n+  struct Cleanup {\n+    CleanupFunction function;\n+    void* arg1;\n+    void* arg2;\n+    Cleanup* next;\n+  };\n+  Cleanup cleanup_;\n+\n+  // No copying allowed\n+  Iterator(const Iterator&);\n+  void operator=(const Iterator&);\n+};\n+\n+// Return an empty iterator (yields nothing).\n+extern Iterator* NewEmptyIterator();\n+\n+// Return an empty iterator with the specified status.\n+extern Iterator* NewErrorIterator(const Status& status);\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_ITERATOR_H_"
      },
      {
        "sha": "fdda718d3090638c7378f4418e4d024dd2e68bda",
        "filename": "src/leveldb/include/leveldb/options.h",
        "status": "added",
        "additions": 195,
        "deletions": 0,
        "changes": 195,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/options.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/options.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/options.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,195 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_OPTIONS_H_\n+#define STORAGE_LEVELDB_INCLUDE_OPTIONS_H_\n+\n+#include <stddef.h>\n+\n+namespace leveldb {\n+\n+class Cache;\n+class Comparator;\n+class Env;\n+class FilterPolicy;\n+class Logger;\n+class Snapshot;\n+\n+// DB contents are stored in a set of blocks, each of which holds a\n+// sequence of key,value pairs.  Each block may be compressed before\n+// being stored in a file.  The following enum describes which\n+// compression method (if any) is used to compress a block.\n+enum CompressionType {\n+  // NOTE: do not change the values of existing entries, as these are\n+  // part of the persistent format on disk.\n+  kNoCompression     = 0x0,\n+  kSnappyCompression = 0x1\n+};\n+\n+// Options to control the behavior of a database (passed to DB::Open)\n+struct Options {\n+  // -------------------\n+  // Parameters that affect behavior\n+\n+  // Comparator used to define the order of keys in the table.\n+  // Default: a comparator that uses lexicographic byte-wise ordering\n+  //\n+  // REQUIRES: The client must ensure that the comparator supplied\n+  // here has the same name and orders keys *exactly* the same as the\n+  // comparator provided to previous open calls on the same DB.\n+  const Comparator* comparator;\n+\n+  // If true, the database will be created if it is missing.\n+  // Default: false\n+  bool create_if_missing;\n+\n+  // If true, an error is raised if the database already exists.\n+  // Default: false\n+  bool error_if_exists;\n+\n+  // If true, the implementation will do aggressive checking of the\n+  // data it is processing and will stop early if it detects any\n+  // errors.  This may have unforeseen ramifications: for example, a\n+  // corruption of one DB entry may cause a large number of entries to\n+  // become unreadable or for the entire DB to become unopenable.\n+  // Default: false\n+  bool paranoid_checks;\n+\n+  // Use the specified object to interact with the environment,\n+  // e.g. to read/write files, schedule background work, etc.\n+  // Default: Env::Default()\n+  Env* env;\n+\n+  // Any internal progress/error information generated by the db will\n+  // be written to info_log if it is non-NULL, or to a file stored\n+  // in the same directory as the DB contents if info_log is NULL.\n+  // Default: NULL\n+  Logger* info_log;\n+\n+  // -------------------\n+  // Parameters that affect performance\n+\n+  // Amount of data to build up in memory (backed by an unsorted log\n+  // on disk) before converting to a sorted on-disk file.\n+  //\n+  // Larger values increase performance, especially during bulk loads.\n+  // Up to two write buffers may be held in memory at the same time,\n+  // so you may wish to adjust this parameter to control memory usage.\n+  // Also, a larger write buffer will result in a longer recovery time\n+  // the next time the database is opened.\n+  //\n+  // Default: 4MB\n+  size_t write_buffer_size;\n+\n+  // Number of open files that can be used by the DB.  You may need to\n+  // increase this if your database has a large working set (budget\n+  // one open file per 2MB of working set).\n+  //\n+  // Default: 1000\n+  int max_open_files;\n+\n+  // Control over blocks (user data is stored in a set of blocks, and\n+  // a block is the unit of reading from disk).\n+\n+  // If non-NULL, use the specified cache for blocks.\n+  // If NULL, leveldb will automatically create and use an 8MB internal cache.\n+  // Default: NULL\n+  Cache* block_cache;\n+\n+  // Approximate size of user data packed per block.  Note that the\n+  // block size specified here corresponds to uncompressed data.  The\n+  // actual size of the unit read from disk may be smaller if\n+  // compression is enabled.  This parameter can be changed dynamically.\n+  //\n+  // Default: 4K\n+  size_t block_size;\n+\n+  // Number of keys between restart points for delta encoding of keys.\n+  // This parameter can be changed dynamically.  Most clients should\n+  // leave this parameter alone.\n+  //\n+  // Default: 16\n+  int block_restart_interval;\n+\n+  // Compress blocks using the specified compression algorithm.  This\n+  // parameter can be changed dynamically.\n+  //\n+  // Default: kSnappyCompression, which gives lightweight but fast\n+  // compression.\n+  //\n+  // Typical speeds of kSnappyCompression on an Intel(R) Core(TM)2 2.4GHz:\n+  //    ~200-500MB/s compression\n+  //    ~400-800MB/s decompression\n+  // Note that these speeds are significantly faster than most\n+  // persistent storage speeds, and therefore it is typically never\n+  // worth switching to kNoCompression.  Even if the input data is\n+  // incompressible, the kSnappyCompression implementation will\n+  // efficiently detect that and will switch to uncompressed mode.\n+  CompressionType compression;\n+\n+  // If non-NULL, use the specified filter policy to reduce disk reads.\n+  // Many applications will benefit from passing the result of\n+  // NewBloomFilterPolicy() here.\n+  //\n+  // Default: NULL\n+  const FilterPolicy* filter_policy;\n+\n+  // Create an Options object with default values for all fields.\n+  Options();\n+};\n+\n+// Options that control read operations\n+struct ReadOptions {\n+  // If true, all data read from underlying storage will be\n+  // verified against corresponding checksums.\n+  // Default: false\n+  bool verify_checksums;\n+\n+  // Should the data read for this iteration be cached in memory?\n+  // Callers may wish to set this field to false for bulk scans.\n+  // Default: true\n+  bool fill_cache;\n+\n+  // If \"snapshot\" is non-NULL, read as of the supplied snapshot\n+  // (which must belong to the DB that is being read and which must\n+  // not have been released).  If \"snapshot\" is NULL, use an impliicit\n+  // snapshot of the state at the beginning of this read operation.\n+  // Default: NULL\n+  const Snapshot* snapshot;\n+\n+  ReadOptions()\n+      : verify_checksums(false),\n+        fill_cache(true),\n+        snapshot(NULL) {\n+  }\n+};\n+\n+// Options that control write operations\n+struct WriteOptions {\n+  // If true, the write will be flushed from the operating system\n+  // buffer cache (by calling WritableFile::Sync()) before the write\n+  // is considered complete.  If this flag is true, writes will be\n+  // slower.\n+  //\n+  // If this flag is false, and the machine crashes, some recent\n+  // writes may be lost.  Note that if it is just the process that\n+  // crashes (i.e., the machine does not reboot), no writes will be\n+  // lost even if sync==false.\n+  //\n+  // In other words, a DB write with sync==false has similar\n+  // crash semantics as the \"write()\" system call.  A DB write\n+  // with sync==true has similar crash semantics to a \"write()\"\n+  // system call followed by \"fsync()\".\n+  //\n+  // Default: false\n+  bool sync;\n+\n+  WriteOptions()\n+      : sync(false) {\n+  }\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_OPTIONS_H_"
      },
      {
        "sha": "74ea8fa49af6782b54ba07528844e665e8ea8095",
        "filename": "src/leveldb/include/leveldb/slice.h",
        "status": "added",
        "additions": 109,
        "deletions": 0,
        "changes": 109,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/slice.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/slice.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/slice.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,109 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// Slice is a simple structure containing a pointer into some external\n+// storage and a size.  The user of a Slice must ensure that the slice\n+// is not used after the corresponding external storage has been\n+// deallocated.\n+//\n+// Multiple threads can invoke const methods on a Slice without\n+// external synchronization, but if any of the threads may call a\n+// non-const method, all threads accessing the same Slice must use\n+// external synchronization.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_SLICE_H_\n+#define STORAGE_LEVELDB_INCLUDE_SLICE_H_\n+\n+#include <assert.h>\n+#include <stddef.h>\n+#include <string.h>\n+#include <string>\n+\n+namespace leveldb {\n+\n+class Slice {\n+ public:\n+  // Create an empty slice.\n+  Slice() : data_(\"\"), size_(0) { }\n+\n+  // Create a slice that refers to d[0,n-1].\n+  Slice(const char* d, size_t n) : data_(d), size_(n) { }\n+\n+  // Create a slice that refers to the contents of \"s\"\n+  Slice(const std::string& s) : data_(s.data()), size_(s.size()) { }\n+\n+  // Create a slice that refers to s[0,strlen(s)-1]\n+  Slice(const char* s) : data_(s), size_(strlen(s)) { }\n+\n+  // Return a pointer to the beginning of the referenced data\n+  const char* data() const { return data_; }\n+\n+  // Return the length (in bytes) of the referenced data\n+  size_t size() const { return size_; }\n+\n+  // Return true iff the length of the referenced data is zero\n+  bool empty() const { return size_ == 0; }\n+\n+  // Return the ith byte in the referenced data.\n+  // REQUIRES: n < size()\n+  char operator[](size_t n) const {\n+    assert(n < size());\n+    return data_[n];\n+  }\n+\n+  // Change this slice to refer to an empty array\n+  void clear() { data_ = \"\"; size_ = 0; }\n+\n+  // Drop the first \"n\" bytes from this slice.\n+  void remove_prefix(size_t n) {\n+    assert(n <= size());\n+    data_ += n;\n+    size_ -= n;\n+  }\n+\n+  // Return a string that contains the copy of the referenced data.\n+  std::string ToString() const { return std::string(data_, size_); }\n+\n+  // Three-way comparison.  Returns value:\n+  //   <  0 iff \"*this\" <  \"b\",\n+  //   == 0 iff \"*this\" == \"b\",\n+  //   >  0 iff \"*this\" >  \"b\"\n+  int compare(const Slice& b) const;\n+\n+  // Return true iff \"x\" is a prefix of \"*this\"\n+  bool starts_with(const Slice& x) const {\n+    return ((size_ >= x.size_) &&\n+            (memcmp(data_, x.data_, x.size_) == 0));\n+  }\n+\n+ private:\n+  const char* data_;\n+  size_t size_;\n+\n+  // Intentionally copyable\n+};\n+\n+inline bool operator==(const Slice& x, const Slice& y) {\n+  return ((x.size() == y.size()) &&\n+          (memcmp(x.data(), y.data(), x.size()) == 0));\n+}\n+\n+inline bool operator!=(const Slice& x, const Slice& y) {\n+  return !(x == y);\n+}\n+\n+inline int Slice::compare(const Slice& b) const {\n+  const int min_len = (size_ < b.size_) ? size_ : b.size_;\n+  int r = memcmp(data_, b.data_, min_len);\n+  if (r == 0) {\n+    if (size_ < b.size_) r = -1;\n+    else if (size_ > b.size_) r = +1;\n+  }\n+  return r;\n+}\n+\n+}  // namespace leveldb\n+\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_SLICE_H_"
      },
      {
        "sha": "11dbd4b47ed3883b7dd5092c21685441f6000c26",
        "filename": "src/leveldb/include/leveldb/status.h",
        "status": "added",
        "additions": 106,
        "deletions": 0,
        "changes": 106,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/status.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/status.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/status.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,106 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// A Status encapsulates the result of an operation.  It may indicate success,\n+// or it may indicate an error with an associated error message.\n+//\n+// Multiple threads can invoke const methods on a Status without\n+// external synchronization, but if any of the threads may call a\n+// non-const method, all threads accessing the same Status must use\n+// external synchronization.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_STATUS_H_\n+#define STORAGE_LEVELDB_INCLUDE_STATUS_H_\n+\n+#include <string>\n+#include \"leveldb/slice.h\"\n+\n+namespace leveldb {\n+\n+class Status {\n+ public:\n+  // Create a success status.\n+  Status() : state_(NULL) { }\n+  ~Status() { delete[] state_; }\n+\n+  // Copy the specified status.\n+  Status(const Status& s);\n+  void operator=(const Status& s);\n+\n+  // Return a success status.\n+  static Status OK() { return Status(); }\n+\n+  // Return error status of an appropriate type.\n+  static Status NotFound(const Slice& msg, const Slice& msg2 = Slice()) {\n+    return Status(kNotFound, msg, msg2);\n+  }\n+  static Status Corruption(const Slice& msg, const Slice& msg2 = Slice()) {\n+    return Status(kCorruption, msg, msg2);\n+  }\n+  static Status NotSupported(const Slice& msg, const Slice& msg2 = Slice()) {\n+    return Status(kNotSupported, msg, msg2);\n+  }\n+  static Status InvalidArgument(const Slice& msg, const Slice& msg2 = Slice()) {\n+    return Status(kInvalidArgument, msg, msg2);\n+  }\n+  static Status IOError(const Slice& msg, const Slice& msg2 = Slice()) {\n+    return Status(kIOError, msg, msg2);\n+  }\n+\n+  // Returns true iff the status indicates success.\n+  bool ok() const { return (state_ == NULL); }\n+\n+  // Returns true iff the status indicates a NotFound error.\n+  bool IsNotFound() const { return code() == kNotFound; }\n+\n+  // Returns true iff the status indicates a Corruption error.\n+  bool IsCorruption() const { return code() == kCorruption; }\n+\n+  // Returns true iff the status indicates an IOError.\n+  bool IsIOError() const { return code() == kIOError; }\n+\n+  // Return a string representation of this status suitable for printing.\n+  // Returns the string \"OK\" for success.\n+  std::string ToString() const;\n+\n+ private:\n+  // OK status has a NULL state_.  Otherwise, state_ is a new[] array\n+  // of the following form:\n+  //    state_[0..3] == length of message\n+  //    state_[4]    == code\n+  //    state_[5..]  == message\n+  const char* state_;\n+\n+  enum Code {\n+    kOk = 0,\n+    kNotFound = 1,\n+    kCorruption = 2,\n+    kNotSupported = 3,\n+    kInvalidArgument = 4,\n+    kIOError = 5\n+  };\n+\n+  Code code() const {\n+    return (state_ == NULL) ? kOk : static_cast<Code>(state_[4]);\n+  }\n+\n+  Status(Code code, const Slice& msg, const Slice& msg2);\n+  static const char* CopyState(const char* s);\n+};\n+\n+inline Status::Status(const Status& s) {\n+  state_ = (s.state_ == NULL) ? NULL : CopyState(s.state_);\n+}\n+inline void Status::operator=(const Status& s) {\n+  // The following condition catches both aliasing (when this == &s),\n+  // and the common case where both s and *this are ok.\n+  if (state_ != s.state_) {\n+    delete[] state_;\n+    state_ = (s.state_ == NULL) ? NULL : CopyState(s.state_);\n+  }\n+}\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_STATUS_H_"
      },
      {
        "sha": "a9746c3f5ea90250d8bde12d9ec7e9091fd5bd51",
        "filename": "src/leveldb/include/leveldb/table.h",
        "status": "added",
        "additions": 85,
        "deletions": 0,
        "changes": 85,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/table.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/table.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/table.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,85 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_TABLE_H_\n+#define STORAGE_LEVELDB_INCLUDE_TABLE_H_\n+\n+#include <stdint.h>\n+#include \"leveldb/iterator.h\"\n+\n+namespace leveldb {\n+\n+class Block;\n+class BlockHandle;\n+class Footer;\n+struct Options;\n+class RandomAccessFile;\n+struct ReadOptions;\n+class TableCache;\n+\n+// A Table is a sorted map from strings to strings.  Tables are\n+// immutable and persistent.  A Table may be safely accessed from\n+// multiple threads without external synchronization.\n+class Table {\n+ public:\n+  // Attempt to open the table that is stored in bytes [0..file_size)\n+  // of \"file\", and read the metadata entries necessary to allow\n+  // retrieving data from the table.\n+  //\n+  // If successful, returns ok and sets \"*table\" to the newly opened\n+  // table.  The client should delete \"*table\" when no longer needed.\n+  // If there was an error while initializing the table, sets \"*table\"\n+  // to NULL and returns a non-ok status.  Does not take ownership of\n+  // \"*source\", but the client must ensure that \"source\" remains live\n+  // for the duration of the returned table's lifetime.\n+  //\n+  // *file must remain live while this Table is in use.\n+  static Status Open(const Options& options,\n+                     RandomAccessFile* file,\n+                     uint64_t file_size,\n+                     Table** table);\n+\n+  ~Table();\n+\n+  // Returns a new iterator over the table contents.\n+  // The result of NewIterator() is initially invalid (caller must\n+  // call one of the Seek methods on the iterator before using it).\n+  Iterator* NewIterator(const ReadOptions&) const;\n+\n+  // Given a key, return an approximate byte offset in the file where\n+  // the data for that key begins (or would begin if the key were\n+  // present in the file).  The returned value is in terms of file\n+  // bytes, and so includes effects like compression of the underlying data.\n+  // E.g., the approximate offset of the last key in the table will\n+  // be close to the file length.\n+  uint64_t ApproximateOffsetOf(const Slice& key) const;\n+\n+ private:\n+  struct Rep;\n+  Rep* rep_;\n+\n+  explicit Table(Rep* rep) { rep_ = rep; }\n+  static Iterator* BlockReader(void*, const ReadOptions&, const Slice&);\n+\n+  // Calls (*handle_result)(arg, ...) with the entry found after a call\n+  // to Seek(key).  May not make such a call if filter policy says\n+  // that key is not present.\n+  friend class TableCache;\n+  Status InternalGet(\n+      const ReadOptions&, const Slice& key,\n+      void* arg,\n+      void (*handle_result)(void* arg, const Slice& k, const Slice& v));\n+\n+\n+  void ReadMeta(const Footer& footer);\n+  void ReadFilter(const Slice& filter_handle_value);\n+\n+  // No copying allowed\n+  Table(const Table&);\n+  void operator=(const Table&);\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_TABLE_H_"
      },
      {
        "sha": "5fd1dc71f1cb7541ef62397b6795946ad8c20652",
        "filename": "src/leveldb/include/leveldb/table_builder.h",
        "status": "added",
        "additions": 92,
        "deletions": 0,
        "changes": 92,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/table_builder.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/table_builder.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/table_builder.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,92 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// TableBuilder provides the interface used to build a Table\n+// (an immutable and sorted map from keys to values).\n+//\n+// Multiple threads can invoke const methods on a TableBuilder without\n+// external synchronization, but if any of the threads may call a\n+// non-const method, all threads accessing the same TableBuilder must use\n+// external synchronization.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_TABLE_BUILDER_H_\n+#define STORAGE_LEVELDB_INCLUDE_TABLE_BUILDER_H_\n+\n+#include <stdint.h>\n+#include \"leveldb/options.h\"\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+class BlockBuilder;\n+class BlockHandle;\n+class WritableFile;\n+\n+class TableBuilder {\n+ public:\n+  // Create a builder that will store the contents of the table it is\n+  // building in *file.  Does not close the file.  It is up to the\n+  // caller to close the file after calling Finish().\n+  TableBuilder(const Options& options, WritableFile* file);\n+\n+  // REQUIRES: Either Finish() or Abandon() has been called.\n+  ~TableBuilder();\n+\n+  // Change the options used by this builder.  Note: only some of the\n+  // option fields can be changed after construction.  If a field is\n+  // not allowed to change dynamically and its value in the structure\n+  // passed to the constructor is different from its value in the\n+  // structure passed to this method, this method will return an error\n+  // without changing any fields.\n+  Status ChangeOptions(const Options& options);\n+\n+  // Add key,value to the table being constructed.\n+  // REQUIRES: key is after any previously added key according to comparator.\n+  // REQUIRES: Finish(), Abandon() have not been called\n+  void Add(const Slice& key, const Slice& value);\n+\n+  // Advanced operation: flush any buffered key/value pairs to file.\n+  // Can be used to ensure that two adjacent entries never live in\n+  // the same data block.  Most clients should not need to use this method.\n+  // REQUIRES: Finish(), Abandon() have not been called\n+  void Flush();\n+\n+  // Return non-ok iff some error has been detected.\n+  Status status() const;\n+\n+  // Finish building the table.  Stops using the file passed to the\n+  // constructor after this function returns.\n+  // REQUIRES: Finish(), Abandon() have not been called\n+  Status Finish();\n+\n+  // Indicate that the contents of this builder should be abandoned.  Stops\n+  // using the file passed to the constructor after this function returns.\n+  // If the caller is not going to call Finish(), it must call Abandon()\n+  // before destroying this builder.\n+  // REQUIRES: Finish(), Abandon() have not been called\n+  void Abandon();\n+\n+  // Number of calls to Add() so far.\n+  uint64_t NumEntries() const;\n+\n+  // Size of the file generated so far.  If invoked after a successful\n+  // Finish() call, returns the size of the final generated file.\n+  uint64_t FileSize() const;\n+\n+ private:\n+  bool ok() const { return status().ok(); }\n+  void WriteBlock(BlockBuilder* block, BlockHandle* handle);\n+  void WriteRawBlock(const Slice& data, CompressionType, BlockHandle* handle);\n+\n+  struct Rep;\n+  Rep* rep_;\n+\n+  // No copying allowed\n+  TableBuilder(const TableBuilder&);\n+  void operator=(const TableBuilder&);\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_TABLE_BUILDER_H_"
      },
      {
        "sha": "ee9aab68e0d83dc4d94835ee21cf926c1ff0c0db",
        "filename": "src/leveldb/include/leveldb/write_batch.h",
        "status": "added",
        "additions": 64,
        "deletions": 0,
        "changes": 64,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/write_batch.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/include/leveldb/write_batch.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/write_batch.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,64 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// WriteBatch holds a collection of updates to apply atomically to a DB.\n+//\n+// The updates are applied in the order in which they are added\n+// to the WriteBatch.  For example, the value of \"key\" will be \"v3\"\n+// after the following batch is written:\n+//\n+//    batch.Put(\"key\", \"v1\");\n+//    batch.Delete(\"key\");\n+//    batch.Put(\"key\", \"v2\");\n+//    batch.Put(\"key\", \"v3\");\n+//\n+// Multiple threads can invoke const methods on a WriteBatch without\n+// external synchronization, but if any of the threads may call a\n+// non-const method, all threads accessing the same WriteBatch must use\n+// external synchronization.\n+\n+#ifndef STORAGE_LEVELDB_INCLUDE_WRITE_BATCH_H_\n+#define STORAGE_LEVELDB_INCLUDE_WRITE_BATCH_H_\n+\n+#include <string>\n+#include \"leveldb/status.h\"\n+\n+namespace leveldb {\n+\n+class Slice;\n+\n+class WriteBatch {\n+ public:\n+  WriteBatch();\n+  ~WriteBatch();\n+\n+  // Store the mapping \"key->value\" in the database.\n+  void Put(const Slice& key, const Slice& value);\n+\n+  // If the database contains a mapping for \"key\", erase it.  Else do nothing.\n+  void Delete(const Slice& key);\n+\n+  // Clear all updates buffered in this batch.\n+  void Clear();\n+\n+  // Support for iterating over the contents of a batch.\n+  class Handler {\n+   public:\n+    virtual ~Handler();\n+    virtual void Put(const Slice& key, const Slice& value) = 0;\n+    virtual void Delete(const Slice& key) = 0;\n+  };\n+  Status Iterate(Handler* handler) const;\n+\n+ private:\n+  friend class WriteBatchInternal;\n+\n+  std::string rep_;  // See comment in write_batch.cc for the format of rep_\n+\n+  // Intentionally copyable\n+};\n+\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_INCLUDE_WRITE_BATCH_H_"
      },
      {
        "sha": "1b1cf8bb28da5cf6a117ffdc4cb6527ef1585d6b",
        "filename": "src/leveldb/issues/issue178_test.cc",
        "status": "added",
        "additions": 92,
        "deletions": 0,
        "changes": 92,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/issues/issue178_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/issues/issue178_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/issues/issue178_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,92 @@\n+// Copyright (c) 2013 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+// Test for issue 178: a manual compaction causes deleted data to reappear.\n+#include <iostream>\n+#include <sstream>\n+#include <cstdlib>\n+\n+#include \"leveldb/db.h\"\n+#include \"leveldb/write_batch.h\"\n+#include \"util/testharness.h\"\n+\n+namespace {\n+\n+const int kNumKeys = 1100000;\n+\n+std::string Key1(int i) {\n+  char buf[100];\n+  snprintf(buf, sizeof(buf), \"my_key_%d\", i);\n+  return buf;\n+}\n+\n+std::string Key2(int i) {\n+  return Key1(i) + \"_xxx\";\n+}\n+\n+class Issue178 { };\n+\n+TEST(Issue178, Test) {\n+  // Get rid of any state from an old run.\n+  std::string dbpath = leveldb::test::TmpDir() + \"/leveldb_cbug_test\";\n+  DestroyDB(dbpath, leveldb::Options());\n+\n+  // Open database.  Disable compression since it affects the creation\n+  // of layers and the code below is trying to test against a very\n+  // specific scenario.\n+  leveldb::DB* db;\n+  leveldb::Options db_options;\n+  db_options.create_if_missing = true;\n+  db_options.compression = leveldb::kNoCompression;\n+  ASSERT_OK(leveldb::DB::Open(db_options, dbpath, &db));\n+\n+  // create first key range\n+  leveldb::WriteBatch batch;\n+  for (size_t i = 0; i < kNumKeys; i++) {\n+    batch.Put(Key1(i), \"value for range 1 key\");\n+  }\n+  ASSERT_OK(db->Write(leveldb::WriteOptions(), &batch));\n+\n+  // create second key range\n+  batch.Clear();\n+  for (size_t i = 0; i < kNumKeys; i++) {\n+    batch.Put(Key2(i), \"value for range 2 key\");\n+  }\n+  ASSERT_OK(db->Write(leveldb::WriteOptions(), &batch));\n+\n+  // delete second key range\n+  batch.Clear();\n+  for (size_t i = 0; i < kNumKeys; i++) {\n+    batch.Delete(Key2(i));\n+  }\n+  ASSERT_OK(db->Write(leveldb::WriteOptions(), &batch));\n+\n+  // compact database\n+  std::string start_key = Key1(0);\n+  std::string end_key = Key1(kNumKeys - 1);\n+  leveldb::Slice least(start_key.data(), start_key.size());\n+  leveldb::Slice greatest(end_key.data(), end_key.size());\n+\n+  // commenting out the line below causes the example to work correctly\n+  db->CompactRange(&least, &greatest);\n+\n+  // count the keys\n+  leveldb::Iterator* iter = db->NewIterator(leveldb::ReadOptions());\n+  size_t num_keys = 0;\n+  for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n+    num_keys++;\n+  }\n+  delete iter;\n+  ASSERT_EQ(kNumKeys, num_keys) << \"Bad number of keys\";\n+\n+  // close database\n+  delete db;\n+  DestroyDB(dbpath, leveldb::Options());\n+}\n+\n+}  // anonymous namespace\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "422563e25ce0230d92373637799da8d28ab60223",
        "filename": "src/leveldb/port/README",
        "status": "added",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/README",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/README",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/README?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,10 @@\n+This directory contains interfaces and implementations that isolate the\n+rest of the package from platform details.\n+\n+Code in the rest of the package includes \"port.h\" from this directory.\n+\"port.h\" in turn includes a platform specific \"port_<platform>.h\" file\n+that provides the platform specific implementation.\n+\n+See port_posix.h for an example of what must be provided in a platform\n+specific header file.\n+"
      },
      {
        "sha": "e17bf435eab33e55a87046e705a1ecb04dad7e24",
        "filename": "src/leveldb/port/atomic_pointer.h",
        "status": "added",
        "additions": 224,
        "deletions": 0,
        "changes": 224,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/atomic_pointer.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/atomic_pointer.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/atomic_pointer.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,224 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+// AtomicPointer provides storage for a lock-free pointer.\n+// Platform-dependent implementation of AtomicPointer:\n+// - If the platform provides a cheap barrier, we use it with raw pointers\n+// - If cstdatomic is present (on newer versions of gcc, it is), we use\n+//   a cstdatomic-based AtomicPointer.  However we prefer the memory\n+//   barrier based version, because at least on a gcc 4.4 32-bit build\n+//   on linux, we have encountered a buggy <cstdatomic>\n+//   implementation.  Also, some <cstdatomic> implementations are much\n+//   slower than a memory-barrier based implementation (~16ns for\n+//   <cstdatomic> based acquire-load vs. ~1ns for a barrier based\n+//   acquire-load).\n+// This code is based on atomicops-internals-* in Google's perftools:\n+// http://code.google.com/p/google-perftools/source/browse/#svn%2Ftrunk%2Fsrc%2Fbase\n+\n+#ifndef PORT_ATOMIC_POINTER_H_\n+#define PORT_ATOMIC_POINTER_H_\n+\n+#include <stdint.h>\n+#ifdef LEVELDB_CSTDATOMIC_PRESENT\n+#include <cstdatomic>\n+#endif\n+#ifdef OS_WIN\n+#include <windows.h>\n+#endif\n+#ifdef OS_MACOSX\n+#include <libkern/OSAtomic.h>\n+#endif\n+\n+#if defined(_M_X64) || defined(__x86_64__)\n+#define ARCH_CPU_X86_FAMILY 1\n+#elif defined(_M_IX86) || defined(__i386__) || defined(__i386)\n+#define ARCH_CPU_X86_FAMILY 1\n+#elif defined(__ARMEL__)\n+#define ARCH_CPU_ARM_FAMILY 1\n+#elif defined(__ppc__) || defined(__powerpc__) || defined(__powerpc64__)\n+#define ARCH_CPU_PPC_FAMILY 1\n+#endif\n+\n+namespace leveldb {\n+namespace port {\n+\n+// Define MemoryBarrier() if available\n+// Windows on x86\n+#if defined(OS_WIN) && defined(COMPILER_MSVC) && defined(ARCH_CPU_X86_FAMILY)\n+// windows.h already provides a MemoryBarrier(void) macro\n+// http://msdn.microsoft.com/en-us/library/ms684208(v=vs.85).aspx\n+#define LEVELDB_HAVE_MEMORY_BARRIER\n+\n+// Gcc on x86\n+#elif defined(ARCH_CPU_X86_FAMILY) && defined(__GNUC__)\n+inline void MemoryBarrier() {\n+  // See http://gcc.gnu.org/ml/gcc/2003-04/msg01180.html for a discussion on\n+  // this idiom. Also see http://en.wikipedia.org/wiki/Memory_ordering.\n+  __asm__ __volatile__(\"\" : : : \"memory\");\n+}\n+#define LEVELDB_HAVE_MEMORY_BARRIER\n+\n+// Sun Studio\n+#elif defined(ARCH_CPU_X86_FAMILY) && defined(__SUNPRO_CC)\n+inline void MemoryBarrier() {\n+  // See http://gcc.gnu.org/ml/gcc/2003-04/msg01180.html for a discussion on\n+  // this idiom. Also see http://en.wikipedia.org/wiki/Memory_ordering.\n+  asm volatile(\"\" : : : \"memory\");\n+}\n+#define LEVELDB_HAVE_MEMORY_BARRIER\n+\n+// Mac OS\n+#elif defined(OS_MACOSX)\n+inline void MemoryBarrier() {\n+  OSMemoryBarrier();\n+}\n+#define LEVELDB_HAVE_MEMORY_BARRIER\n+\n+// ARM Linux\n+#elif defined(ARCH_CPU_ARM_FAMILY) && defined(__linux__)\n+typedef void (*LinuxKernelMemoryBarrierFunc)(void);\n+// The Linux ARM kernel provides a highly optimized device-specific memory\n+// barrier function at a fixed memory address that is mapped in every\n+// user-level process.\n+//\n+// This beats using CPU-specific instructions which are, on single-core\n+// devices, un-necessary and very costly (e.g. ARMv7-A \"dmb\" takes more\n+// than 180ns on a Cortex-A8 like the one on a Nexus One). Benchmarking\n+// shows that the extra function call cost is completely negligible on\n+// multi-core devices.\n+//\n+inline void MemoryBarrier() {\n+  (*(LinuxKernelMemoryBarrierFunc)0xffff0fa0)();\n+}\n+#define LEVELDB_HAVE_MEMORY_BARRIER\n+\n+// PPC\n+#elif defined(ARCH_CPU_PPC_FAMILY) && defined(__GNUC__)\n+inline void MemoryBarrier() {\n+  // TODO for some powerpc expert: is there a cheaper suitable variant?\n+  // Perhaps by having separate barriers for acquire and release ops.\n+  asm volatile(\"sync\" : : : \"memory\");\n+}\n+#define LEVELDB_HAVE_MEMORY_BARRIER\n+\n+#endif\n+\n+// AtomicPointer built using platform-specific MemoryBarrier()\n+#if defined(LEVELDB_HAVE_MEMORY_BARRIER)\n+class AtomicPointer {\n+ private:\n+  void* rep_;\n+ public:\n+  AtomicPointer() { }\n+  explicit AtomicPointer(void* p) : rep_(p) {}\n+  inline void* NoBarrier_Load() const { return rep_; }\n+  inline void NoBarrier_Store(void* v) { rep_ = v; }\n+  inline void* Acquire_Load() const {\n+    void* result = rep_;\n+    MemoryBarrier();\n+    return result;\n+  }\n+  inline void Release_Store(void* v) {\n+    MemoryBarrier();\n+    rep_ = v;\n+  }\n+};\n+\n+// AtomicPointer based on <cstdatomic>\n+#elif defined(LEVELDB_CSTDATOMIC_PRESENT)\n+class AtomicPointer {\n+ private:\n+  std::atomic<void*> rep_;\n+ public:\n+  AtomicPointer() { }\n+  explicit AtomicPointer(void* v) : rep_(v) { }\n+  inline void* Acquire_Load() const {\n+    return rep_.load(std::memory_order_acquire);\n+  }\n+  inline void Release_Store(void* v) {\n+    rep_.store(v, std::memory_order_release);\n+  }\n+  inline void* NoBarrier_Load() const {\n+    return rep_.load(std::memory_order_relaxed);\n+  }\n+  inline void NoBarrier_Store(void* v) {\n+    rep_.store(v, std::memory_order_relaxed);\n+  }\n+};\n+\n+// Atomic pointer based on sparc memory barriers\n+#elif defined(__sparcv9) && defined(__GNUC__)\n+class AtomicPointer {\n+ private:\n+  void* rep_;\n+ public:\n+  AtomicPointer() { }\n+  explicit AtomicPointer(void* v) : rep_(v) { }\n+  inline void* Acquire_Load() const {\n+    void* val;\n+    __asm__ __volatile__ (\n+        \"ldx [%[rep_]], %[val] \\n\\t\"\n+         \"membar #LoadLoad|#LoadStore \\n\\t\"\n+        : [val] \"=r\" (val)\n+        : [rep_] \"r\" (&rep_)\n+        : \"memory\");\n+    return val;\n+  }\n+  inline void Release_Store(void* v) {\n+    __asm__ __volatile__ (\n+        \"membar #LoadStore|#StoreStore \\n\\t\"\n+        \"stx %[v], [%[rep_]] \\n\\t\"\n+        :\n+        : [rep_] \"r\" (&rep_), [v] \"r\" (v)\n+        : \"memory\");\n+  }\n+  inline void* NoBarrier_Load() const { return rep_; }\n+  inline void NoBarrier_Store(void* v) { rep_ = v; }\n+};\n+\n+// Atomic pointer based on ia64 acq/rel\n+#elif defined(__ia64) && defined(__GNUC__)\n+class AtomicPointer {\n+ private:\n+  void* rep_;\n+ public:\n+  AtomicPointer() { }\n+  explicit AtomicPointer(void* v) : rep_(v) { }\n+  inline void* Acquire_Load() const {\n+    void* val    ;\n+    __asm__ __volatile__ (\n+        \"ld8.acq %[val] = [%[rep_]] \\n\\t\"\n+        : [val] \"=r\" (val)\n+        : [rep_] \"r\" (&rep_)\n+        : \"memory\"\n+        );\n+    return val;\n+  }\n+  inline void Release_Store(void* v) {\n+    __asm__ __volatile__ (\n+        \"st8.rel [%[rep_]] = %[v]  \\n\\t\"\n+        :\n+        : [rep_] \"r\" (&rep_), [v] \"r\" (v)\n+        : \"memory\"\n+        );\n+  }\n+  inline void* NoBarrier_Load() const { return rep_; }\n+  inline void NoBarrier_Store(void* v) { rep_ = v; }\n+};\n+\n+// We have neither MemoryBarrier(), nor <cstdatomic>\n+#else\n+#error Please implement AtomicPointer for this platform.\n+\n+#endif\n+\n+#undef LEVELDB_HAVE_MEMORY_BARRIER\n+#undef ARCH_CPU_X86_FAMILY\n+#undef ARCH_CPU_ARM_FAMILY\n+#undef ARCH_CPU_PPC_FAMILY\n+\n+}  // namespace port\n+}  // namespace leveldb\n+\n+#endif  // PORT_ATOMIC_POINTER_H_"
      },
      {
        "sha": "4baafa8e22fd290cfd73ad4daf0b5245e0d109c1",
        "filename": "src/leveldb/port/port.h",
        "status": "added",
        "additions": 21,
        "deletions": 0,
        "changes": 21,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/port.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/port.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/port.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,21 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_PORT_PORT_H_\n+#define STORAGE_LEVELDB_PORT_PORT_H_\n+\n+#include <string.h>\n+\n+// Include the appropriate platform specific file below.  If you are\n+// porting to a new platform, see \"port_example.h\" for documentation\n+// of what the new port_<platform>.h file must provide.\n+#if defined(LEVELDB_PLATFORM_POSIX)\n+#  include \"port/port_posix.h\"\n+#elif defined(LEVELDB_PLATFORM_CHROMIUM)\n+#  include \"port/port_chromium.h\"\n+#elif defined(LEVELDB_PLATFORM_WINDOWS)\n+#  include \"port/port_win.h\"\n+#endif\n+\n+#endif  // STORAGE_LEVELDB_PORT_PORT_H_"
      },
      {
        "sha": "ab9e489b32d8eb4ec8a43da07a20ad917fb35a1b",
        "filename": "src/leveldb/port/port_example.h",
        "status": "added",
        "additions": 135,
        "deletions": 0,
        "changes": 135,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/port_example.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/port_example.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/port_example.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,135 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// This file contains the specification, but not the implementations,\n+// of the types/operations/etc. that should be defined by a platform\n+// specific port_<platform>.h file.  Use this file as a reference for\n+// how to port this package to a new platform.\n+\n+#ifndef STORAGE_LEVELDB_PORT_PORT_EXAMPLE_H_\n+#define STORAGE_LEVELDB_PORT_PORT_EXAMPLE_H_\n+\n+namespace leveldb {\n+namespace port {\n+\n+// TODO(jorlow): Many of these belong more in the environment class rather than\n+//               here. We should try moving them and see if it affects perf.\n+\n+// The following boolean constant must be true on a little-endian machine\n+// and false otherwise.\n+static const bool kLittleEndian = true /* or some other expression */;\n+\n+// ------------------ Threading -------------------\n+\n+// A Mutex represents an exclusive lock.\n+class Mutex {\n+ public:\n+  Mutex();\n+  ~Mutex();\n+\n+  // Lock the mutex.  Waits until other lockers have exited.\n+  // Will deadlock if the mutex is already locked by this thread.\n+  void Lock();\n+\n+  // Unlock the mutex.\n+  // REQUIRES: This mutex was locked by this thread.\n+  void Unlock();\n+\n+  // Optionally crash if this thread does not hold this mutex.\n+  // The implementation must be fast, especially if NDEBUG is\n+  // defined.  The implementation is allowed to skip all checks.\n+  void AssertHeld();\n+};\n+\n+class CondVar {\n+ public:\n+  explicit CondVar(Mutex* mu);\n+  ~CondVar();\n+\n+  // Atomically release *mu and block on this condition variable until\n+  // either a call to SignalAll(), or a call to Signal() that picks\n+  // this thread to wakeup.\n+  // REQUIRES: this thread holds *mu\n+  void Wait();\n+\n+  // If there are some threads waiting, wake up at least one of them.\n+  void Signal();\n+\n+  // Wake up all waiting threads.\n+  void SignallAll();\n+};\n+\n+// Thread-safe initialization.\n+// Used as follows:\n+//      static port::OnceType init_control = LEVELDB_ONCE_INIT;\n+//      static void Initializer() { ... do something ...; }\n+//      ...\n+//      port::InitOnce(&init_control, &Initializer);\n+typedef intptr_t OnceType;\n+#define LEVELDB_ONCE_INIT 0\n+extern void InitOnce(port::OnceType*, void (*initializer)());\n+\n+// A type that holds a pointer that can be read or written atomically\n+// (i.e., without word-tearing.)\n+class AtomicPointer {\n+ private:\n+  intptr_t rep_;\n+ public:\n+  // Initialize to arbitrary value\n+  AtomicPointer();\n+\n+  // Initialize to hold v\n+  explicit AtomicPointer(void* v) : rep_(v) { }\n+\n+  // Read and return the stored pointer with the guarantee that no\n+  // later memory access (read or write) by this thread can be\n+  // reordered ahead of this read.\n+  void* Acquire_Load() const;\n+\n+  // Set v as the stored pointer with the guarantee that no earlier\n+  // memory access (read or write) by this thread can be reordered\n+  // after this store.\n+  void Release_Store(void* v);\n+\n+  // Read the stored pointer with no ordering guarantees.\n+  void* NoBarrier_Load() const;\n+\n+  // Set va as the stored pointer with no ordering guarantees.\n+  void NoBarrier_Store(void* v);\n+};\n+\n+// ------------------ Compression -------------------\n+\n+// Store the snappy compression of \"input[0,input_length-1]\" in *output.\n+// Returns false if snappy is not supported by this port.\n+extern bool Snappy_Compress(const char* input, size_t input_length,\n+                            std::string* output);\n+\n+// If input[0,input_length-1] looks like a valid snappy compressed\n+// buffer, store the size of the uncompressed data in *result and\n+// return true.  Else return false.\n+extern bool Snappy_GetUncompressedLength(const char* input, size_t length,\n+                                         size_t* result);\n+\n+// Attempt to snappy uncompress input[0,input_length-1] into *output.\n+// Returns true if successful, false if the input is invalid lightweight\n+// compressed data.\n+//\n+// REQUIRES: at least the first \"n\" bytes of output[] must be writable\n+// where \"n\" is the result of a successful call to\n+// Snappy_GetUncompressedLength.\n+extern bool Snappy_Uncompress(const char* input_data, size_t input_length,\n+                              char* output);\n+\n+// ------------------ Miscellaneous -------------------\n+\n+// If heap profiling is not supported, returns false.\n+// Else repeatedly calls (*func)(arg, data, n) and then returns true.\n+// The concatenation of all \"data[0,n-1]\" fragments is the heap profile.\n+extern bool GetHeapProfile(void (*func)(void*, const char*, int), void* arg);\n+\n+}  // namespace port\n+}  // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_PORT_PORT_EXAMPLE_H_"
      },
      {
        "sha": "5ba127a5b91bfa036189aa29ee2aeb9b02a034d8",
        "filename": "src/leveldb/port/port_posix.cc",
        "status": "added",
        "additions": 54,
        "deletions": 0,
        "changes": 54,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/port_posix.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/port_posix.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/port_posix.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,54 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"port/port_posix.h\"\n+\n+#include <cstdlib>\n+#include <stdio.h>\n+#include <string.h>\n+#include \"util/logging.h\"\n+\n+namespace leveldb {\n+namespace port {\n+\n+static void PthreadCall(const char* label, int result) {\n+  if (result != 0) {\n+    fprintf(stderr, \"pthread %s: %s\\n\", label, strerror(result));\n+    abort();\n+  }\n+}\n+\n+Mutex::Mutex() { PthreadCall(\"init mutex\", pthread_mutex_init(&mu_, NULL)); }\n+\n+Mutex::~Mutex() { PthreadCall(\"destroy mutex\", pthread_mutex_destroy(&mu_)); }\n+\n+void Mutex::Lock() { PthreadCall(\"lock\", pthread_mutex_lock(&mu_)); }\n+\n+void Mutex::Unlock() { PthreadCall(\"unlock\", pthread_mutex_unlock(&mu_)); }\n+\n+CondVar::CondVar(Mutex* mu)\n+    : mu_(mu) {\n+    PthreadCall(\"init cv\", pthread_cond_init(&cv_, NULL));\n+}\n+\n+CondVar::~CondVar() { PthreadCall(\"destroy cv\", pthread_cond_destroy(&cv_)); }\n+\n+void CondVar::Wait() {\n+  PthreadCall(\"wait\", pthread_cond_wait(&cv_, &mu_->mu_));\n+}\n+\n+void CondVar::Signal() {\n+  PthreadCall(\"signal\", pthread_cond_signal(&cv_));\n+}\n+\n+void CondVar::SignalAll() {\n+  PthreadCall(\"broadcast\", pthread_cond_broadcast(&cv_));\n+}\n+\n+void InitOnce(OnceType* once, void (*initializer)()) {\n+  PthreadCall(\"once\", pthread_once(once, initializer));\n+}\n+\n+}  // namespace port\n+}  // namespace leveldb"
      },
      {
        "sha": "f2b89bffb99c232c03922806868b0ccf663bc111",
        "filename": "src/leveldb/port/port_posix.h",
        "status": "added",
        "additions": 157,
        "deletions": 0,
        "changes": 157,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/port_posix.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/port_posix.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/port_posix.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,157 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// See port_example.h for documentation for the following types/functions.\n+\n+#ifndef STORAGE_LEVELDB_PORT_PORT_POSIX_H_\n+#define STORAGE_LEVELDB_PORT_PORT_POSIX_H_\n+\n+#undef PLATFORM_IS_LITTLE_ENDIAN\n+#if defined(OS_MACOSX)\n+  #include <machine/endian.h>\n+  #if defined(__DARWIN_LITTLE_ENDIAN) && defined(__DARWIN_BYTE_ORDER)\n+    #define PLATFORM_IS_LITTLE_ENDIAN \\\n+        (__DARWIN_BYTE_ORDER == __DARWIN_LITTLE_ENDIAN)\n+  #endif\n+#elif defined(OS_SOLARIS)\n+  #include <sys/isa_defs.h>\n+  #ifdef _LITTLE_ENDIAN\n+    #define PLATFORM_IS_LITTLE_ENDIAN true\n+  #else\n+    #define PLATFORM_IS_LITTLE_ENDIAN false\n+  #endif\n+#elif defined(OS_FREEBSD)\n+  #include <sys/types.h>\n+  #include <sys/endian.h>\n+  #define PLATFORM_IS_LITTLE_ENDIAN (_BYTE_ORDER == _LITTLE_ENDIAN)\n+#elif defined(OS_OPENBSD) || defined(OS_NETBSD) ||\\\n+      defined(OS_DRAGONFLYBSD)\n+  #include <sys/types.h>\n+  #include <sys/endian.h>\n+#elif defined(OS_HPUX)\n+  #define PLATFORM_IS_LITTLE_ENDIAN false\n+#elif defined(OS_ANDROID)\n+  // Due to a bug in the NDK x86 <sys/endian.h> definition,\n+  // _BYTE_ORDER must be used instead of __BYTE_ORDER on Android.\n+  // See http://code.google.com/p/android/issues/detail?id=39824\n+  #include <endian.h>\n+  #define PLATFORM_IS_LITTLE_ENDIAN  (_BYTE_ORDER == _LITTLE_ENDIAN)\n+#else\n+  #include <endian.h>\n+#endif\n+\n+#include <pthread.h>\n+#ifdef SNAPPY\n+#include <snappy.h>\n+#endif\n+#include <stdint.h>\n+#include <string>\n+#include \"port/atomic_pointer.h\"\n+\n+#ifndef PLATFORM_IS_LITTLE_ENDIAN\n+#define PLATFORM_IS_LITTLE_ENDIAN (__BYTE_ORDER == __LITTLE_ENDIAN)\n+#endif\n+\n+#if defined(OS_MACOSX) || defined(OS_SOLARIS) || defined(OS_FREEBSD) ||\\\n+    defined(OS_NETBSD) || defined(OS_OPENBSD) || defined(OS_DRAGONFLYBSD) ||\\\n+    defined(OS_ANDROID) || defined(OS_HPUX)\n+// Use fread/fwrite/fflush on platforms without _unlocked variants\n+#define fread_unlocked fread\n+#define fwrite_unlocked fwrite\n+#define fflush_unlocked fflush\n+#endif\n+\n+#if defined(OS_MACOSX) || defined(OS_FREEBSD) ||\\\n+    defined(OS_OPENBSD) || defined(OS_DRAGONFLYBSD)\n+// Use fsync() on platforms without fdatasync()\n+#define fdatasync fsync\n+#endif\n+\n+#if defined(OS_ANDROID) && __ANDROID_API__ < 9\n+// fdatasync() was only introduced in API level 9 on Android. Use fsync()\n+// when targetting older platforms.\n+#define fdatasync fsync\n+#endif\n+\n+namespace leveldb {\n+namespace port {\n+\n+static const bool kLittleEndian = PLATFORM_IS_LITTLE_ENDIAN;\n+#undef PLATFORM_IS_LITTLE_ENDIAN\n+\n+class CondVar;\n+\n+class Mutex {\n+ public:\n+  Mutex();\n+  ~Mutex();\n+\n+  void Lock();\n+  void Unlock();\n+  void AssertHeld() { }\n+\n+ private:\n+  friend class CondVar;\n+  pthread_mutex_t mu_;\n+\n+  // No copying\n+  Mutex(const Mutex&);\n+  void operator=(const Mutex&);\n+};\n+\n+class CondVar {\n+ public:\n+  explicit CondVar(Mutex* mu);\n+  ~CondVar();\n+  void Wait();\n+  void Signal();\n+  void SignalAll();\n+ private:\n+  pthread_cond_t cv_;\n+  Mutex* mu_;\n+};\n+\n+typedef pthread_once_t OnceType;\n+#define LEVELDB_ONCE_INIT PTHREAD_ONCE_INIT\n+extern void InitOnce(OnceType* once, void (*initializer)());\n+\n+inline bool Snappy_Compress(const char* input, size_t length,\n+                            ::std::string* output) {\n+#ifdef SNAPPY\n+  output->resize(snappy::MaxCompressedLength(length));\n+  size_t outlen;\n+  snappy::RawCompress(input, length, &(*output)[0], &outlen);\n+  output->resize(outlen);\n+  return true;\n+#endif\n+\n+  return false;\n+}\n+\n+inline bool Snappy_GetUncompressedLength(const char* input, size_t length,\n+                                         size_t* result) {\n+#ifdef SNAPPY\n+  return snappy::GetUncompressedLength(input, length, result);\n+#else\n+  return false;\n+#endif\n+}\n+\n+inline bool Snappy_Uncompress(const char* input, size_t length,\n+                              char* output) {\n+#ifdef SNAPPY\n+  return snappy::RawUncompress(input, length, output);\n+#else\n+  return false;\n+#endif\n+}\n+\n+inline bool GetHeapProfile(void (*func)(void*, const char*, int), void* arg) {\n+  return false;\n+}\n+\n+} // namespace port\n+} // namespace leveldb\n+\n+#endif  // STORAGE_LEVELDB_PORT_PORT_POSIX_H_"
      },
      {
        "sha": "1b0f060a19caabfbb5dffe8ef87f1d90536d44f2",
        "filename": "src/leveldb/port/port_win.cc",
        "status": "added",
        "additions": 147,
        "deletions": 0,
        "changes": 147,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/port_win.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/port_win.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/port_win.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,147 @@\n+// LevelDB Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// See port_example.h for documentation for the following types/functions.\n+\n+// Redistribution and use in source and binary forms, with or without\n+// modification, are permitted provided that the following conditions are met:\n+// \n+//  * Redistributions of source code must retain the above copyright\n+//    notice, this list of conditions and the following disclaimer.\n+//  * Redistributions in binary form must reproduce the above copyright\n+//    notice, this list of conditions and the following disclaimer in the\n+//    documentation and/or other materials provided with the distribution.\n+//  * Neither the name of the University of California, Berkeley nor the\n+//    names of its contributors may be used to endorse or promote products\n+//    derived from this software without specific prior written permission.\n+// \n+// THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND ANY\n+// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n+// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n+// DISCLAIMED. IN NO EVENT SHALL THE REGENTS AND CONTRIBUTORS BE LIABLE FOR ANY\n+// DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n+// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n+// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n+// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+//\n+\n+#include \"port/port_win.h\"\n+\n+#include <windows.h>\n+#include <cassert>\n+\n+namespace leveldb {\n+namespace port {\n+\n+Mutex::Mutex() :\n+    cs_(NULL) {\n+  assert(!cs_);\n+  cs_ = static_cast<void *>(new CRITICAL_SECTION());\n+  ::InitializeCriticalSection(static_cast<CRITICAL_SECTION *>(cs_));\n+  assert(cs_);\n+}\n+\n+Mutex::~Mutex() {\n+  assert(cs_);\n+  ::DeleteCriticalSection(static_cast<CRITICAL_SECTION *>(cs_));\n+  delete static_cast<CRITICAL_SECTION *>(cs_);\n+  cs_ = NULL;\n+  assert(!cs_);\n+}\n+\n+void Mutex::Lock() {\n+  assert(cs_);\n+  ::EnterCriticalSection(static_cast<CRITICAL_SECTION *>(cs_));\n+}\n+\n+void Mutex::Unlock() {\n+  assert(cs_);\n+  ::LeaveCriticalSection(static_cast<CRITICAL_SECTION *>(cs_));\n+}\n+\n+void Mutex::AssertHeld() {\n+  assert(cs_);\n+  assert(1);\n+}\n+\n+CondVar::CondVar(Mutex* mu) :\n+    waiting_(0), \n+    mu_(mu), \n+    sem1_(::CreateSemaphore(NULL, 0, 10000, NULL)), \n+    sem2_(::CreateSemaphore(NULL, 0, 10000, NULL)) {\n+  assert(mu_);\n+}\n+\n+CondVar::~CondVar() {\n+  ::CloseHandle(sem1_);\n+  ::CloseHandle(sem2_);\n+}\n+\n+void CondVar::Wait() {\n+  mu_->AssertHeld();\n+\n+  wait_mtx_.Lock();\n+  ++waiting_;\n+  wait_mtx_.Unlock();\n+\n+  mu_->Unlock();\n+\n+  // initiate handshake\n+  ::WaitForSingleObject(sem1_, INFINITE);\n+  ::ReleaseSemaphore(sem2_, 1, NULL);\n+  mu_->Lock();\n+}\n+\n+void CondVar::Signal() {\n+  wait_mtx_.Lock();\n+  if (waiting_ > 0) {\n+    --waiting_;\n+\n+    // finalize handshake\n+    ::ReleaseSemaphore(sem1_, 1, NULL);\n+    ::WaitForSingleObject(sem2_, INFINITE);\n+  }\n+  wait_mtx_.Unlock();\n+}\n+\n+void CondVar::SignalAll() {\n+  wait_mtx_.Lock();\n+  ::ReleaseSemaphore(sem1_, waiting_, NULL);\n+  while(waiting_ > 0) {\n+    --waiting_;\n+    ::WaitForSingleObject(sem2_, INFINITE);\n+  }\n+  wait_mtx_.Unlock();\n+}\n+\n+AtomicPointer::AtomicPointer(void* v) {\n+  Release_Store(v);\n+}\n+\n+void InitOnce(OnceType* once, void (*initializer)()) {\n+  once->InitOnce(initializer);\n+}\n+\n+void* AtomicPointer::Acquire_Load() const {\n+  void * p = NULL;\n+  InterlockedExchangePointer(&p, rep_);\n+  return p;\n+}\n+\n+void AtomicPointer::Release_Store(void* v) {\n+  InterlockedExchangePointer(&rep_, v);\n+}\n+\n+void* AtomicPointer::NoBarrier_Load() const {\n+  return rep_;\n+}\n+\n+void AtomicPointer::NoBarrier_Store(void* v) {\n+  rep_ = v;\n+}\n+\n+}\n+}"
      },
      {
        "sha": "45bf2f0ea749d60abb2de3d7b357e7a70eac94de",
        "filename": "src/leveldb/port/port_win.h",
        "status": "added",
        "additions": 174,
        "deletions": 0,
        "changes": 174,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/port_win.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/port_win.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/port_win.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,174 @@\n+// LevelDB Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+//\n+// See port_example.h for documentation for the following types/functions.\n+\n+// Redistribution and use in source and binary forms, with or without\n+// modification, are permitted provided that the following conditions are met:\n+// \n+//  * Redistributions of source code must retain the above copyright\n+//    notice, this list of conditions and the following disclaimer.\n+//  * Redistributions in binary form must reproduce the above copyright\n+//    notice, this list of conditions and the following disclaimer in the\n+//    documentation and/or other materials provided with the distribution.\n+//  * Neither the name of the University of California, Berkeley nor the\n+//    names of its contributors may be used to endorse or promote products\n+//    derived from this software without specific prior written permission.\n+// \n+// THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND ANY\n+// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n+// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n+// DISCLAIMED. IN NO EVENT SHALL THE REGENTS AND CONTRIBUTORS BE LIABLE FOR ANY\n+// DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n+// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n+// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n+// ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+//\n+\n+#ifndef STORAGE_LEVELDB_PORT_PORT_WIN_H_\n+#define STORAGE_LEVELDB_PORT_PORT_WIN_H_\n+\n+#ifdef _MSC_VER\n+#define snprintf _snprintf\n+#define close _close\n+#define fread_unlocked _fread_nolock\n+#endif\n+\n+#include <string>\n+#include <stdint.h>\n+#ifdef SNAPPY\n+#include <snappy.h>\n+#endif\n+\n+namespace leveldb {\n+namespace port {\n+\n+// Windows is little endian (for now :p)\n+static const bool kLittleEndian = true;\n+\n+class CondVar;\n+\n+class Mutex {\n+ public:\n+  Mutex();\n+  ~Mutex();\n+\n+  void Lock();\n+  void Unlock();\n+  void AssertHeld();\n+\n+ private:\n+  friend class CondVar;\n+  // critical sections are more efficient than mutexes\n+  // but they are not recursive and can only be used to synchronize threads within the same process\n+  // we use opaque void * to avoid including windows.h in port_win.h\n+  void * cs_;\n+\n+  // No copying\n+  Mutex(const Mutex&);\n+  void operator=(const Mutex&);\n+};\n+\n+// the Win32 API offers a dependable condition variable mechanism, but only starting with\n+// Windows 2008 and Vista\n+// no matter what we will implement our own condition variable with a semaphore\n+// implementation as described in a paper written by Andrew D. Birrell in 2003\n+class CondVar {\n+ public:\n+  explicit CondVar(Mutex* mu);\n+  ~CondVar();\n+  void Wait();\n+  void Signal();\n+  void SignalAll();\n+ private:\n+  Mutex* mu_;\n+  \n+  Mutex wait_mtx_;\n+  long waiting_;\n+  \n+  void * sem1_;\n+  void * sem2_;\n+  \n+  \n+};\n+\n+class OnceType {\n+public:\n+//    OnceType() : init_(false) {}\n+    OnceType(const OnceType &once) : init_(once.init_) {}\n+    OnceType(bool f) : init_(f) {}\n+    void InitOnce(void (*initializer)()) {\n+        mutex_.Lock();\n+        if (!init_) {\n+            init_ = true;\n+            initializer();\n+        }\n+        mutex_.Unlock();\n+    }\n+\n+private:\n+    bool init_;\n+    Mutex mutex_;\n+};\n+\n+#define LEVELDB_ONCE_INIT false\n+extern void InitOnce(port::OnceType*, void (*initializer)());\n+\n+// Storage for a lock-free pointer\n+class AtomicPointer {\n+ private:\n+  void * rep_;\n+ public:\n+  AtomicPointer() : rep_(NULL) { }\n+  explicit AtomicPointer(void* v); \n+  void* Acquire_Load() const;\n+\n+  void Release_Store(void* v);\n+\n+  void* NoBarrier_Load() const;\n+\n+  void NoBarrier_Store(void* v);\n+};\n+\n+inline bool Snappy_Compress(const char* input, size_t length,\n+                            ::std::string* output) {\n+#ifdef SNAPPY\n+  output->resize(snappy::MaxCompressedLength(length));\n+  size_t outlen;\n+  snappy::RawCompress(input, length, &(*output)[0], &outlen);\n+  output->resize(outlen);\n+  return true;\n+#endif\n+\n+  return false;\n+}\n+\n+inline bool Snappy_GetUncompressedLength(const char* input, size_t length,\n+                                         size_t* result) {\n+#ifdef SNAPPY\n+  return snappy::GetUncompressedLength(input, length, result);\n+#else\n+  return false;\n+#endif\n+}\n+\n+inline bool Snappy_Uncompress(const char* input, size_t length,\n+                              char* output) {\n+#ifdef SNAPPY\n+  return snappy::RawUncompress(input, length, output);\n+#else\n+  return false;\n+#endif\n+}\n+\n+inline bool GetHeapProfile(void (*func)(void*, const char*, int), void* arg) {\n+  return false;\n+}\n+\n+}\n+}\n+\n+#endif  // STORAGE_LEVELDB_PORT_PORT_WIN_H_"
      },
      {
        "sha": "6f9b6a7924d68e16940254d6d21888eaae2a240d",
        "filename": "src/leveldb/port/thread_annotations.h",
        "status": "added",
        "additions": 59,
        "deletions": 0,
        "changes": 59,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/thread_annotations.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/thread_annotations.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/thread_annotations.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,59 @@\n+// Copyright (c) 2012 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#ifndef STORAGE_LEVELDB_PORT_THREAD_ANNOTATIONS_H\n+\n+// Some environments provide custom macros to aid in static thread-safety\n+// analysis.  Provide empty definitions of such macros unless they are already\n+// defined.\n+\n+#ifndef EXCLUSIVE_LOCKS_REQUIRED\n+#define EXCLUSIVE_LOCKS_REQUIRED(...)\n+#endif\n+\n+#ifndef SHARED_LOCKS_REQUIRED\n+#define SHARED_LOCKS_REQUIRED(...)\n+#endif\n+\n+#ifndef LOCKS_EXCLUDED\n+#define LOCKS_EXCLUDED(...)\n+#endif\n+\n+#ifndef LOCK_RETURNED\n+#define LOCK_RETURNED(x)\n+#endif\n+\n+#ifndef LOCKABLE\n+#define LOCKABLE\n+#endif\n+\n+#ifndef SCOPED_LOCKABLE\n+#define SCOPED_LOCKABLE\n+#endif\n+\n+#ifndef EXCLUSIVE_LOCK_FUNCTION\n+#define EXCLUSIVE_LOCK_FUNCTION(...)\n+#endif\n+\n+#ifndef SHARED_LOCK_FUNCTION\n+#define SHARED_LOCK_FUNCTION(...)\n+#endif\n+\n+#ifndef EXCLUSIVE_TRYLOCK_FUNCTION\n+#define EXCLUSIVE_TRYLOCK_FUNCTION(...)\n+#endif\n+\n+#ifndef SHARED_TRYLOCK_FUNCTION\n+#define SHARED_TRYLOCK_FUNCTION(...)\n+#endif\n+\n+#ifndef UNLOCK_FUNCTION\n+#define UNLOCK_FUNCTION(...)\n+#endif\n+\n+#ifndef NO_THREAD_SAFETY_ANALYSIS\n+#define NO_THREAD_SAFETY_ANALYSIS\n+#endif\n+\n+#endif  // STORAGE_LEVELDB_PORT_THREAD_ANNOTATIONS_H"
      },
      {
        "sha": "39edd0db13f436dc57dd28ed4013ab4d55a28a31",
        "filename": "src/leveldb/port/win/stdint.h",
        "status": "added",
        "additions": 24,
        "deletions": 0,
        "changes": 24,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/win/stdint.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/port/win/stdint.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/win/stdint.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "patch": "@@ -0,0 +1,24 @@\n+// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+// MSVC didn't ship with this file until the 2010 version.\n+\n+#ifndef STORAGE_LEVELDB_PORT_WIN_STDINT_H_\n+#define STORAGE_LEVELDB_PORT_WIN_STDINT_H_\n+\n+#if !defined(_MSC_VER)\n+#error This file should only be included when compiling with MSVC.\n+#endif\n+\n+// Define C99 equivalent types.\n+typedef signed char           int8_t;\n+typedef signed short          int16_t;\n+typedef signed int            int32_t;\n+typedef signed long long      int64_t;\n+typedef unsigned char         uint8_t;\n+typedef unsigned short        uint16_t;\n+typedef unsigned int          uint32_t;\n+typedef unsigned long long    uint64_t;\n+\n+#endif  // STORAGE_LEVELDB_PORT_WIN_STDINT_H_"
      },
      {
        "sha": "79ea9d9ee5fe8af28eb07f48b9534c2979fe842a",
        "filename": "src/leveldb/table/block.cc",
        "status": "added",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/block.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/block.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/block.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "2493eb9f9fd9da41aafcac80180c3f831928a32d",
        "filename": "src/leveldb/table/block.h",
        "status": "added",
        "additions": 44,
        "deletions": 0,
        "changes": 44,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/block.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/block.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/block.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "db660cd07cf50ad1b54310c21b22108ab6994802",
        "filename": "src/leveldb/table/block_builder.cc",
        "status": "added",
        "additions": 109,
        "deletions": 0,
        "changes": 109,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/block_builder.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/block_builder.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/block_builder.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "5b545bd1afb4f4e65c36d8430dde09e0f543259f",
        "filename": "src/leveldb/table/block_builder.h",
        "status": "added",
        "additions": 57,
        "deletions": 0,
        "changes": 57,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/block_builder.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/block_builder.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/block_builder.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "203e15c8bcb13b8776842052a725ad2a3909fcf5",
        "filename": "src/leveldb/table/filter_block.cc",
        "status": "added",
        "additions": 111,
        "deletions": 0,
        "changes": 111,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/filter_block.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/filter_block.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/filter_block.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "c67d010bd106756c456dea013c5babdf0a18494f",
        "filename": "src/leveldb/table/filter_block.h",
        "status": "added",
        "additions": 68,
        "deletions": 0,
        "changes": 68,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/filter_block.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/filter_block.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/filter_block.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "3a2a07cf53ca606b2d0e9890e6b9cfa02a678398",
        "filename": "src/leveldb/table/filter_block_test.cc",
        "status": "added",
        "additions": 128,
        "deletions": 0,
        "changes": 128,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/filter_block_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/filter_block_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/filter_block_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "cda1decdf35476ecd5c44d7f3a8e69111e620124",
        "filename": "src/leveldb/table/format.cc",
        "status": "added",
        "additions": 145,
        "deletions": 0,
        "changes": 145,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/format.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/format.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/format.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "6c0b80c0179c7fffbf6ee2af802a10ec02d73998",
        "filename": "src/leveldb/table/format.h",
        "status": "added",
        "additions": 108,
        "deletions": 0,
        "changes": 108,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/format.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/format.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/format.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "3d1c87fdece73d4c1ef16a0a762f70059b9443e6",
        "filename": "src/leveldb/table/iterator.cc",
        "status": "added",
        "additions": 67,
        "deletions": 0,
        "changes": 67,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/iterator.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/iterator.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/iterator.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "9e16b3dbedebe65f36fbbfa9e180fd1fa80e84a3",
        "filename": "src/leveldb/table/iterator_wrapper.h",
        "status": "added",
        "additions": 63,
        "deletions": 0,
        "changes": 63,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/iterator_wrapper.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/iterator_wrapper.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/iterator_wrapper.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "2dde4dc21fde9d86e98f5a3f3b493745d07a22f7",
        "filename": "src/leveldb/table/merger.cc",
        "status": "added",
        "additions": 197,
        "deletions": 0,
        "changes": 197,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/merger.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/merger.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/merger.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "91ddd80faa35bfcf7edb81ee2f22ed3f29b58f98",
        "filename": "src/leveldb/table/merger.h",
        "status": "added",
        "additions": 26,
        "deletions": 0,
        "changes": 26,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/merger.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/merger.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/merger.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "71c1756e5f440c38c3e712e0359886fe22d6d5a0",
        "filename": "src/leveldb/table/table.cc",
        "status": "added",
        "additions": 275,
        "deletions": 0,
        "changes": 275,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/table.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/table.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/table.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "62002c84f2b18f479fdb1cd3ab142c179e1f3a6f",
        "filename": "src/leveldb/table/table_builder.cc",
        "status": "added",
        "additions": 270,
        "deletions": 0,
        "changes": 270,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/table_builder.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/table_builder.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/table_builder.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "c723bf84cf5f55fae9b56ab324ebfdc83ba9be52",
        "filename": "src/leveldb/table/table_test.cc",
        "status": "added",
        "additions": 868,
        "deletions": 0,
        "changes": 868,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/table_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/table_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/table_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "7822ebab9c32ce579c42f9621545d7283e8332b9",
        "filename": "src/leveldb/table/two_level_iterator.cc",
        "status": "added",
        "additions": 182,
        "deletions": 0,
        "changes": 182,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/two_level_iterator.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/two_level_iterator.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/two_level_iterator.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "629ca34525414582e39df8ad7a48eff72e0e450f",
        "filename": "src/leveldb/table/two_level_iterator.h",
        "status": "added",
        "additions": 34,
        "deletions": 0,
        "changes": 34,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/two_level_iterator.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/table/two_level_iterator.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/table/two_level_iterator.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "9551d6a3a27c8c2bd13cc7e48882aaecdb75ba20",
        "filename": "src/leveldb/util/arena.cc",
        "status": "added",
        "additions": 68,
        "deletions": 0,
        "changes": 68,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/arena.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/arena.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/arena.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "8f7dde226c450745646f29986d0302630db6f1f5",
        "filename": "src/leveldb/util/arena.h",
        "status": "added",
        "additions": 68,
        "deletions": 0,
        "changes": 68,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/arena.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/arena.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/arena.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "63d1778034550a7394029df41d6b24f9053f5663",
        "filename": "src/leveldb/util/arena_test.cc",
        "status": "added",
        "additions": 68,
        "deletions": 0,
        "changes": 68,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/arena_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/arena_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/arena_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "d7941cd21fab64079cbef1f62060a48f0d86c74d",
        "filename": "src/leveldb/util/bloom.cc",
        "status": "added",
        "additions": 95,
        "deletions": 0,
        "changes": 95,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/bloom.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/bloom.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/bloom.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "0bf8e8d6ebeae860d7d553538511ef2ac6e80f22",
        "filename": "src/leveldb/util/bloom_test.cc",
        "status": "added",
        "additions": 160,
        "deletions": 0,
        "changes": 160,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/bloom_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/bloom_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/bloom_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "8b197bc02a98e639dc72a58bd7de59603ab77cbc",
        "filename": "src/leveldb/util/cache.cc",
        "status": "added",
        "additions": 325,
        "deletions": 0,
        "changes": 325,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/cache.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/cache.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/cache.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "43716715a89f476700845c303e1651448ecef62c",
        "filename": "src/leveldb/util/cache_test.cc",
        "status": "added",
        "additions": 186,
        "deletions": 0,
        "changes": 186,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/cache_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/cache_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/cache_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "21e3186d5dcff984a11563fd0d09c714426a29c3",
        "filename": "src/leveldb/util/coding.cc",
        "status": "added",
        "additions": 194,
        "deletions": 0,
        "changes": 194,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/coding.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/coding.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/coding.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "3993c4a755dfa5a0f8b966f1a698a371fa08556d",
        "filename": "src/leveldb/util/coding.h",
        "status": "added",
        "additions": 104,
        "deletions": 0,
        "changes": 104,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/coding.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/coding.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/coding.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "fb5726e33521d01270b481b93d9c5490c6f0b53f",
        "filename": "src/leveldb/util/coding_test.cc",
        "status": "added",
        "additions": 196,
        "deletions": 0,
        "changes": 196,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/coding_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/coding_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/coding_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "4b7b5724ef3be5f6c7ed9f95dcb47a99f2ee2f9b",
        "filename": "src/leveldb/util/comparator.cc",
        "status": "added",
        "additions": 81,
        "deletions": 0,
        "changes": 81,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/comparator.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/comparator.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/comparator.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "6db9e770774d7ebc0b0c3bc4a230b5b8254cd1f4",
        "filename": "src/leveldb/util/crc32c.cc",
        "status": "added",
        "additions": 332,
        "deletions": 0,
        "changes": 332,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/crc32c.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/crc32c.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/crc32c.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "1d7e5c075d86d6cf2551cfcd0fd9ac5adce6fc38",
        "filename": "src/leveldb/util/crc32c.h",
        "status": "added",
        "additions": 45,
        "deletions": 0,
        "changes": 45,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/crc32c.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/crc32c.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/crc32c.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "4b957ee120c8f805c0240d3d90eaf16a759437ba",
        "filename": "src/leveldb/util/crc32c_test.cc",
        "status": "added",
        "additions": 72,
        "deletions": 0,
        "changes": 72,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/crc32c_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/crc32c_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/crc32c_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "c2600e964a260c87f22afc8c5a3e7788b4c7e350",
        "filename": "src/leveldb/util/env.cc",
        "status": "added",
        "additions": 96,
        "deletions": 0,
        "changes": 96,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/env.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/env.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/env.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "6badfdc230f52f235ec7f7523e213dba9d628451",
        "filename": "src/leveldb/util/env_posix.cc",
        "status": "added",
        "additions": 701,
        "deletions": 0,
        "changes": 701,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/env_posix.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/env_posix.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/env_posix.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "b72cb4438425bca83d9a6ca0d207dbc8590efb2e",
        "filename": "src/leveldb/util/env_test.cc",
        "status": "added",
        "additions": 104,
        "deletions": 0,
        "changes": 104,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/env_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/env_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/env_test.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "ef2ecae8306623e613419782daed98044a117435",
        "filename": "src/leveldb/util/env_win.cc",
        "status": "added",
        "additions": 1031,
        "deletions": 0,
        "changes": 1031,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/env_win.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/env_win.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/env_win.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "7b045c8c91d6f6d600308e50966ccf56e53638bf",
        "filename": "src/leveldb/util/filter_policy.cc",
        "status": "added",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/filter_policy.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/filter_policy.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/filter_policy.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "07cf022060d41ea2139a438886d268a92cb586af",
        "filename": "src/leveldb/util/hash.cc",
        "status": "added",
        "additions": 52,
        "deletions": 0,
        "changes": 52,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/hash.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/hash.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/hash.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "8889d56be80e2f6342a1a292c6a0075d2481de80",
        "filename": "src/leveldb/util/hash.h",
        "status": "added",
        "additions": 19,
        "deletions": 0,
        "changes": 19,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/hash.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/hash.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/hash.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "bb95f583eac6a6f916a83e409045f0a226bd9ae7",
        "filename": "src/leveldb/util/histogram.cc",
        "status": "added",
        "additions": 139,
        "deletions": 0,
        "changes": 139,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/histogram.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/histogram.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/histogram.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "1ef9f3c8abdfc50858be433110611086bb3c0da6",
        "filename": "src/leveldb/util/histogram.h",
        "status": "added",
        "additions": 42,
        "deletions": 0,
        "changes": 42,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/histogram.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/histogram.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/histogram.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "22cf2785123c45ab77fa158256a45d8e700c1463",
        "filename": "src/leveldb/util/logging.cc",
        "status": "added",
        "additions": 81,
        "deletions": 0,
        "changes": 81,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/logging.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/logging.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/logging.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "b0c5da813e8658c9712b5529f4b219cb1a945508",
        "filename": "src/leveldb/util/logging.h",
        "status": "added",
        "additions": 47,
        "deletions": 0,
        "changes": 47,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/logging.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/logging.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/logging.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "1ff5a9efa15efa166b427ef9611ccc58c96a3984",
        "filename": "src/leveldb/util/mutexlock.h",
        "status": "added",
        "additions": 41,
        "deletions": 0,
        "changes": 41,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/mutexlock.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/mutexlock.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/mutexlock.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "76af5b9302d437d9847b2a93c87697232d027cac",
        "filename": "src/leveldb/util/options.cc",
        "status": "added",
        "additions": 29,
        "deletions": 0,
        "changes": 29,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/options.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/options.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/options.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "c063c2b7cb200dffa3c253fe76ee851910cfbe2f",
        "filename": "src/leveldb/util/posix_logger.h",
        "status": "added",
        "additions": 98,
        "deletions": 0,
        "changes": 98,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/posix_logger.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/posix_logger.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/posix_logger.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "07538242ea559ad31396b994a5172f13ecb3d775",
        "filename": "src/leveldb/util/random.h",
        "status": "added",
        "additions": 59,
        "deletions": 0,
        "changes": 59,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/random.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/random.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/random.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "a44f35b3149fa8fe88d9ca32dbf92fbb9f6d534c",
        "filename": "src/leveldb/util/status.cc",
        "status": "added",
        "additions": 75,
        "deletions": 0,
        "changes": 75,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/status.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/status.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/status.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "eb1bdd554a3ea2f06cd45053ab3df8c71c9610a8",
        "filename": "src/leveldb/util/testharness.cc",
        "status": "added",
        "additions": 77,
        "deletions": 0,
        "changes": 77,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/testharness.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/testharness.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/testharness.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "da4fe68bb4e76ee69af136d76f9417d349fa9605",
        "filename": "src/leveldb/util/testharness.h",
        "status": "added",
        "additions": 138,
        "deletions": 0,
        "changes": 138,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/testharness.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/testharness.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/testharness.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "538d09516d217d614ac45cefe38bb1503d3b1d5c",
        "filename": "src/leveldb/util/testutil.cc",
        "status": "added",
        "additions": 51,
        "deletions": 0,
        "changes": 51,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/testutil.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/testutil.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/testutil.cc?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "824e655bd2c5d7955aa83887966ad0f54fb77192",
        "filename": "src/leveldb/util/testutil.h",
        "status": "added",
        "additions": 53,
        "deletions": 0,
        "changes": 53,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/testutil.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e6552eed63f34fd68cf6c785dd23a241049fa73f/src/leveldb/util/testutil.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/testutil.h?ref=e6552eed63f34fd68cf6c785dd23a241049fa73f"
      }
    ]
  },
  {
    "sha": "6519339fe849bdbbff047d9eef8ab6c5fbb3e2a4",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo2NTE5MzM5ZmU4NDliZGJiZmYwNDdkOWVlZjhhYjZjNWZiYjNlMmE0",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "sipa@ulyssis.org",
        "date": "2013-08-20T22:43:09Z"
      },
      "committer": {
        "name": "Pieter Wuille",
        "email": "sipa@ulyssis.org",
        "date": "2013-08-20T22:43:09Z"
      },
      "message": "Squashed 'src/leveldb/' changes from a02ddf9..be1b0ff\n\nbe1b0ff On Mac OS X fsync does not guarantee write to disk. Use fcntl F_FULLFSYNC instead.\n\ngit-subtree-dir: src/leveldb\ngit-subtree-split: be1b0ff1fcd6ad820a7fd111ac671fb51cc68001",
      "tree": {
        "sha": "bc515a2d36881dde9dc3c5200b729ec0c67304c8",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/bc515a2d36881dde9dc3c5200b729ec0c67304c8"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/6519339fe849bdbbff047d9eef8ab6c5fbb3e2a4",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6519339fe849bdbbff047d9eef8ab6c5fbb3e2a4",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/6519339fe849bdbbff047d9eef8ab6c5fbb3e2a4",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6519339fe849bdbbff047d9eef8ab6c5fbb3e2a4/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/cb1e39f0a35cc2b36fb748c26f69cd27e0ed5332"
      }
    ],
    "stats": {
      "total": 6,
      "additions": 5,
      "deletions": 1
    },
    "files": [
      {
        "sha": "21c845e211ff06389ffbe7fef65db44079706134",
        "filename": "port/port_posix.h",
        "status": "modified",
        "additions": 5,
        "deletions": 1,
        "changes": 6,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/6519339fe849bdbbff047d9eef8ab6c5fbb3e2a4/port/port_posix.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/6519339fe849bdbbff047d9eef8ab6c5fbb3e2a4/port/port_posix.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/port/port_posix.h?ref=6519339fe849bdbbff047d9eef8ab6c5fbb3e2a4",
        "patch": "@@ -62,12 +62,16 @@\n #define fflush_unlocked fflush\n #endif\n \n-#if defined(OS_MACOSX) || defined(OS_FREEBSD) ||\\\n+#if defined(OS_FREEBSD) ||\\\n     defined(OS_OPENBSD) || defined(OS_DRAGONFLYBSD)\n // Use fsync() on platforms without fdatasync()\n #define fdatasync fsync\n #endif\n \n+#if defined(OS_MACOSX)\n+#define fdatasync(fd) fcntl(fd, F_FULLFSYNC, 0)\n+#endif\n+\n #if defined(OS_ANDROID) && __ANDROID_API__ < 9\n // fdatasync() was only introduced in API level 9 on Android. Use fsync()\n // when targetting older platforms."
      }
    ]
  },
  {
    "sha": "980b1c3571788841c0725a2708f05b8079106783",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo5ODBiMWMzNTcxNzg4ODQxYzA3MjVhMjcwOGYwNWI4MDc5MTA2Nzgz",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "sipa@ulyssis.org",
        "date": "2013-08-20T22:43:09Z"
      },
      "committer": {
        "name": "Pieter Wuille",
        "email": "sipa@ulyssis.org",
        "date": "2013-08-20T22:43:20Z"
      },
      "message": "Merge commit '6519339fe849bdbbff047d9eef8ab6c5fbb3e2a4'",
      "tree": {
        "sha": "36d1f83f48bfaafbb821af68dc6bbf2d8aa2420b",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/36d1f83f48bfaafbb821af68dc6bbf2d8aa2420b"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/980b1c3571788841c0725a2708f05b8079106783",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/980b1c3571788841c0725a2708f05b8079106783",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/980b1c3571788841c0725a2708f05b8079106783",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/980b1c3571788841c0725a2708f05b8079106783/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/e6552eed63f34fd68cf6c785dd23a241049fa73f",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/e6552eed63f34fd68cf6c785dd23a241049fa73f"
      },
      {
        "sha": "6519339fe849bdbbff047d9eef8ab6c5fbb3e2a4",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6519339fe849bdbbff047d9eef8ab6c5fbb3e2a4",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/6519339fe849bdbbff047d9eef8ab6c5fbb3e2a4"
      }
    ],
    "stats": {
      "total": 6,
      "additions": 5,
      "deletions": 1
    },
    "files": [
      {
        "sha": "21c845e211ff06389ffbe7fef65db44079706134",
        "filename": "src/leveldb/port/port_posix.h",
        "status": "modified",
        "additions": 5,
        "deletions": 1,
        "changes": 6,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/980b1c3571788841c0725a2708f05b8079106783/src/leveldb/port/port_posix.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/980b1c3571788841c0725a2708f05b8079106783/src/leveldb/port/port_posix.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/port_posix.h?ref=980b1c3571788841c0725a2708f05b8079106783",
        "patch": "@@ -62,12 +62,16 @@\n #define fflush_unlocked fflush\n #endif\n \n-#if defined(OS_MACOSX) || defined(OS_FREEBSD) ||\\\n+#if defined(OS_FREEBSD) ||\\\n     defined(OS_OPENBSD) || defined(OS_DRAGONFLYBSD)\n // Use fsync() on platforms without fdatasync()\n #define fdatasync fsync\n #endif\n \n+#if defined(OS_MACOSX)\n+#define fdatasync(fd) fcntl(fd, F_FULLFSYNC, 0)\n+#endif\n+\n #if defined(OS_ANDROID) && __ANDROID_API__ < 9\n // fdatasync() was only introduced in API level 9 on Android. Use fsync()\n // when targetting older platforms."
      }
    ]
  },
  {
    "sha": "ac7c9600671322847fcce2089d43736112ff3dc2",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzphYzdjOTYwMDY3MTMyMjg0N2ZjY2UyMDg5ZDQzNzM2MTEyZmYzZGMy",
    "commit": {
      "author": {
        "name": "Gregory Maxwell",
        "email": "greg@xiph.org",
        "date": "2013-08-19T03:21:06Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T22:49:00Z"
      },
      "message": "Performance optimization for bloom filters.\n\nThis reduces a peer's ability to attack network resources by\n using a full bloom filter, but without reducing the usability\n of bloom filters.  It sets a default match everything filter\n for peers and it generalizes a prior optimization to\n cover more cases.",
      "tree": {
        "sha": "71d69be0d33a73c30aacdd57a93b7141d75f0893",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/71d69be0d33a73c30aacdd57a93b7141d75f0893"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/ac7c9600671322847fcce2089d43736112ff3dc2",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/ac7c9600671322847fcce2089d43736112ff3dc2",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/ac7c9600671322847fcce2089d43736112ff3dc2",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/ac7c9600671322847fcce2089d43736112ff3dc2/comments",
    "author": {
      "login": "gmaxwell",
      "id": 858454,
      "node_id": "MDQ6VXNlcjg1ODQ1NA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/858454?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gmaxwell",
      "html_url": "https://github.com/gmaxwell",
      "followers_url": "https://api.github.com/users/gmaxwell/followers",
      "following_url": "https://api.github.com/users/gmaxwell/following{/other_user}",
      "gists_url": "https://api.github.com/users/gmaxwell/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gmaxwell/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gmaxwell/subscriptions",
      "organizations_url": "https://api.github.com/users/gmaxwell/orgs",
      "repos_url": "https://api.github.com/users/gmaxwell/repos",
      "events_url": "https://api.github.com/users/gmaxwell/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gmaxwell/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "980b1c3571788841c0725a2708f05b8079106783",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/980b1c3571788841c0725a2708f05b8079106783",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/980b1c3571788841c0725a2708f05b8079106783"
      }
    ],
    "stats": {
      "total": 40,
      "additions": 33,
      "deletions": 7
    },
    "files": [
      {
        "sha": "6bdffdbb38c2c97bafc8398a50a5a76a7d333bbd",
        "filename": "src/bloom.cpp",
        "status": "modified",
        "additions": 24,
        "deletions": 2,
        "changes": 26,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/ac7c9600671322847fcce2089d43736112ff3dc2/src/bloom.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/ac7c9600671322847fcce2089d43736112ff3dc2/src/bloom.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/bloom.cpp?ref=ac7c9600671322847fcce2089d43736112ff3dc2",
        "patch": "@@ -23,6 +23,8 @@ vData(min((unsigned int)(-1  / LN2SQUARED * nElements * log(nFPRate)), MAX_BLOOM\n // The ideal number of hash functions is filter size * ln(2) / number of elements\n // Again, we ignore filter parameters which will create a bloom filter with more hash functions than the protocol limits\n // See http://en.wikipedia.org/wiki/Bloom_filter for an explanation of these formulas\n+isFull(false),\n+isEmpty(false),\n nHashFuncs(min((unsigned int)(vData.size() * 8 / nElements * LN2), MAX_HASH_FUNCS)),\n nTweak(nTweakIn),\n nFlags(nFlagsIn)\n@@ -37,14 +39,15 @@ inline unsigned int CBloomFilter::Hash(unsigned int nHashNum, const std::vector<\n \n void CBloomFilter::insert(const vector<unsigned char>& vKey)\n {\n-    if (vData.size() == 1 && vData[0] == 0xff)\n+    if (isFull)\n         return;\n     for (unsigned int i = 0; i < nHashFuncs; i++)\n     {\n         unsigned int nIndex = Hash(i, vKey);\n         // Sets bit nIndex of vData\n         vData[nIndex >> 3] |= bit_mask[7 & nIndex];\n     }\n+    isEmpty = false;\n }\n \n void CBloomFilter::insert(const COutPoint& outpoint)\n@@ -63,8 +66,10 @@ void CBloomFilter::insert(const uint256& hash)\n \n bool CBloomFilter::contains(const vector<unsigned char>& vKey) const\n {\n-    if (vData.size() == 1 && vData[0] == 0xff)\n+    if (isFull)\n         return true;\n+    if (isEmpty)\n+        return false;\n     for (unsigned int i = 0; i < nHashFuncs; i++)\n     {\n         unsigned int nIndex = Hash(i, vKey);\n@@ -99,6 +104,10 @@ bool CBloomFilter::IsRelevantAndUpdate(const CTransaction& tx, const uint256& ha\n     bool fFound = false;\n     // Match if the filter contains the hash of tx\n     //  for finding tx when they appear in a block\n+    if (isFull)\n+        return true;\n+    if (isEmpty)\n+        return false;\n     if (contains(hash))\n         fFound = true;\n \n@@ -158,3 +167,16 @@ bool CBloomFilter::IsRelevantAndUpdate(const CTransaction& tx, const uint256& ha\n \n     return false;\n }\n+\n+void CBloomFilter::UpdateEmptyFull()\n+{\n+    bool full = true;\n+    bool empty = true;\n+    for (unsigned int i = 0; i < vData.size(); i++)\n+    {\n+        full &= vData[i] == 0xff;\n+        empty &= vData[i] == 0;\n+    }\n+    isFull = full;\n+    isEmpty = empty;\n+}"
      },
      {
        "sha": "f482bfcc10d2932467f815d4c9acaadceebebdf7",
        "filename": "src/bloom.h",
        "status": "modified",
        "additions": 6,
        "deletions": 3,
        "changes": 9,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/ac7c9600671322847fcce2089d43736112ff3dc2/src/bloom.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/ac7c9600671322847fcce2089d43736112ff3dc2/src/bloom.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/bloom.h?ref=ac7c9600671322847fcce2089d43736112ff3dc2",
        "patch": "@@ -42,6 +42,8 @@ class CBloomFilter\n {\n private:\n     std::vector<unsigned char> vData;\n+    bool isFull;\n+    bool isEmpty;\n     unsigned int nHashFuncs;\n     unsigned int nTweak;\n     unsigned char nFlags;\n@@ -57,9 +59,7 @@ class CBloomFilter\n     // It should generally always be a random value (and is largely only exposed for unit testing)\n     // nFlags should be one of the BLOOM_UPDATE_* enums (not _MASK)\n     CBloomFilter(unsigned int nElements, double nFPRate, unsigned int nTweak, unsigned char nFlagsIn);\n-    // Using a filter initialized with this results in undefined behavior\n-    // Should only be used for deserialization\n-    CBloomFilter() {}\n+    CBloomFilter() : isFull(true) {}\n \n     IMPLEMENT_SERIALIZE\n     (\n@@ -83,6 +83,9 @@ class CBloomFilter\n \n     // Also adds any outputs which match the filter to the filter (to match their spending txes)\n     bool IsRelevantAndUpdate(const CTransaction& tx, const uint256& hash);\n+\n+    // Checks for empty and full filters to avoid wasting cpu\n+    void UpdateEmptyFull();\n };\n \n #endif /* BITCOIN_BLOOM_H */"
      },
      {
        "sha": "36471d7d98f8d0c43ca5b47bd5bc6464b3ba3b40",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/ac7c9600671322847fcce2089d43736112ff3dc2/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/ac7c9600671322847fcce2089d43736112ff3dc2/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=ac7c9600671322847fcce2089d43736112ff3dc2",
        "patch": "@@ -3675,6 +3675,7 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n             LOCK(pfrom->cs_filter);\n             delete pfrom->pfilter;\n             pfrom->pfilter = new CBloomFilter(filter);\n+            filter.UpdateEmptyFull();\n         }\n         pfrom->fRelayTxes = true;\n     }\n@@ -3704,7 +3705,7 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n     {\n         LOCK(pfrom->cs_filter);\n         delete pfrom->pfilter;\n-        pfrom->pfilter = NULL;\n+        pfrom->pfilter = new CBloomFilter();\n         pfrom->fRelayTxes = true;\n     }\n "
      },
      {
        "sha": "6f315f74fd2d6e3890058f148d4bdf4cfb959a69",
        "filename": "src/net.h",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/ac7c9600671322847fcce2089d43736112ff3dc2/src/net.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/ac7c9600671322847fcce2089d43736112ff3dc2/src/net.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/net.h?ref=ac7c9600671322847fcce2089d43736112ff3dc2",
        "patch": "@@ -253,7 +253,7 @@ class CNode\n         nMisbehavior = 0;\n         fRelayTxes = false;\n         setInventoryKnown.max_size(SendBufferSize() / 1000);\n-        pfilter = NULL;\n+        pfilter = new CBloomFilter();\n \n         // Be shy and don't send version until we hear\n         if (hSocket != INVALID_SOCKET && !fInbound)"
      }
    ]
  },
  {
    "sha": "708c75c0eeafb6363936233e6042e9227038c39f",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo3MDhjNzVjMGVlYWZiNjM2MzkzNjIzM2U2MDQyZTkyMjcwMzhjMzlm",
    "commit": {
      "author": {
        "name": "theuni",
        "email": "theuni-nospam@xbmc.org",
        "date": "2013-06-14T03:39:54Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T22:49:00Z"
      },
      "message": "fixed: include boost header as necessary\n\nWithout this include, sometimes BOOST_VERSION was defined and sometimes\nit was not, depending on which includes came before it. The result was a\nrandom mix of sleep or sleep_for for boost versions >= 1.50.",
      "tree": {
        "sha": "a632fdf8837cbbe26429918bb4959d5c30adb850",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/a632fdf8837cbbe26429918bb4959d5c30adb850"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/708c75c0eeafb6363936233e6042e9227038c39f",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/708c75c0eeafb6363936233e6042e9227038c39f",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/708c75c0eeafb6363936233e6042e9227038c39f",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/708c75c0eeafb6363936233e6042e9227038c39f/comments",
    "author": null,
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "ac7c9600671322847fcce2089d43736112ff3dc2",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/ac7c9600671322847fcce2089d43736112ff3dc2",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/ac7c9600671322847fcce2089d43736112ff3dc2"
      }
    ],
    "stats": {
      "total": 1,
      "additions": 1,
      "deletions": 0
    },
    "files": [
      {
        "sha": "4ff128bb42f370f5b1613cc6dd3f7b19d9f64a68",
        "filename": "src/util.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/708c75c0eeafb6363936233e6042e9227038c39f/src/util.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/708c75c0eeafb6363936233e6042e9227038c39f/src/util.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/util.h?ref=708c75c0eeafb6363936233e6042e9227038c39f",
        "patch": "@@ -22,6 +22,7 @@ typedef int pid_t; /* define for Windows compatibility */\n #include <vector>\n #include <string>\n \n+#include <boost/version.hpp>\n #include <boost/thread.hpp>\n #include <boost/filesystem.hpp>\n #include <boost/filesystem/path.hpp>"
      }
    ]
  },
  {
    "sha": "5e18c6ccbcf98e242d534b8ef3f166e079ad3c99",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo1ZTE4YzZjY2JjZjk4ZTI0MmQ1MzRiOGVmM2YxNjZlMDc5YWQzYzk5",
    "commit": {
      "author": {
        "name": "theuni",
        "email": "theuni-nospam@xbmc.org",
        "date": "2013-06-14T03:46:08Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T22:49:00Z"
      },
      "message": "fixed: don't use thread::sleep_for where it's known to be broken\n\nFixes #2690.",
      "tree": {
        "sha": "471cafbfa5009526fa3e71587f1ce9ca0e457dd5",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/471cafbfa5009526fa3e71587f1ce9ca0e457dd5"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/5e18c6ccbcf98e242d534b8ef3f166e079ad3c99",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5e18c6ccbcf98e242d534b8ef3f166e079ad3c99",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/5e18c6ccbcf98e242d534b8ef3f166e079ad3c99",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5e18c6ccbcf98e242d534b8ef3f166e079ad3c99/comments",
    "author": null,
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "708c75c0eeafb6363936233e6042e9227038c39f",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/708c75c0eeafb6363936233e6042e9227038c39f",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/708c75c0eeafb6363936233e6042e9227038c39f"
      }
    ],
    "stats": {
      "total": 6,
      "additions": 5,
      "deletions": 1
    },
    "files": [
      {
        "sha": "33832261e130195338595370f8335c23da6ecb14",
        "filename": "src/util.h",
        "status": "modified",
        "additions": 5,
        "deletions": 1,
        "changes": 6,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/5e18c6ccbcf98e242d534b8ef3f166e079ad3c99/src/util.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/5e18c6ccbcf98e242d534b8ef3f166e079ad3c99/src/util.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/util.h?ref=5e18c6ccbcf98e242d534b8ef3f166e079ad3c99",
        "patch": "@@ -107,7 +107,11 @@ T* alignup(T* p)\n \n inline void MilliSleep(int64 n)\n {\n-#if BOOST_VERSION >= 105000\n+// Boost's sleep_for was uninterruptable when backed by nanosleep from 1.50\n+// until fixed in 1.52. Use the deprecated sleep method for the broken case.\n+// See: https://svn.boost.org/trac/boost/ticket/7238\n+\n+#if BOOST_VERSION >= 105000 && (!defined(BOOST_HAS_NANOSLEEP) || BOOST_VERSION >= 105200)\n     boost::this_thread::sleep_for(boost::chrono::milliseconds(n));\n #else\n     boost::this_thread::sleep(boost::posix_time::milliseconds(n));"
      }
    ]
  },
  {
    "sha": "d7fdc5fac34814b71b2b539ebf6d9b8fdae5b50c",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpkN2ZkYzVmYWMzNDgxNGI3MWIyYjUzOWViZjZkOWI4ZmRhZTViNTBj",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2013-08-02T00:47:22Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-20T23:13:49Z"
      },
      "message": "Fix non-standard disconnected transactions causing mempool orphans\n\nConflicts:\n\tsrc/main.cpp",
      "tree": {
        "sha": "cb83bcd3be159b1ebe0c9fef5c4ac85bec583f96",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/cb83bcd3be159b1ebe0c9fef5c4ac85bec583f96"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/d7fdc5fac34814b71b2b539ebf6d9b8fdae5b50c",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/d7fdc5fac34814b71b2b539ebf6d9b8fdae5b50c",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/d7fdc5fac34814b71b2b539ebf6d9b8fdae5b50c",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/d7fdc5fac34814b71b2b539ebf6d9b8fdae5b50c/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "5e18c6ccbcf98e242d534b8ef3f166e079ad3c99",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5e18c6ccbcf98e242d534b8ef3f166e079ad3c99",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/5e18c6ccbcf98e242d534b8ef3f166e079ad3c99"
      }
    ],
    "stats": {
      "total": 17,
      "additions": 9,
      "deletions": 8
    },
    "files": [
      {
        "sha": "4584fbab19f3b769b2fde530c3020bff946bab9c",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 9,
        "deletions": 8,
        "changes": 17,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/d7fdc5fac34814b71b2b539ebf6d9b8fdae5b50c/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/d7fdc5fac34814b71b2b539ebf6d9b8fdae5b50c/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=d7fdc5fac34814b71b2b539ebf6d9b8fdae5b50c",
        "patch": "@@ -837,15 +837,15 @@ bool CTxMemPool::remove(const CTransaction &tx, bool fRecursive)\n     {\n         LOCK(cs);\n         uint256 hash = tx.GetHash();\n+        if (fRecursive) {\n+            for (unsigned int i = 0; i < tx.vout.size(); i++) {\n+                std::map<COutPoint, CInPoint>::iterator it = mapNextTx.find(COutPoint(hash, i));\n+                if (it != mapNextTx.end())\n+                    remove(*it->second.ptx, true);\n+            }\n+        }\n         if (mapTx.count(hash))\n         {\n-            if (fRecursive) {\n-                for (unsigned int i = 0; i < tx.vout.size(); i++) {\n-                    std::map<COutPoint, CInPoint>::iterator it = mapNextTx.find(COutPoint(hash, i));\n-                    if (it != mapNextTx.end())\n-                        remove(*it->second.ptx, true);\n-                }\n-            }\n             BOOST_FOREACH(const CTxIn& txin, tx.vin)\n                 mapNextTx.erase(txin.prevout);\n             mapTx.erase(hash);\n@@ -1853,7 +1853,8 @@ bool SetBestChain(CValidationState &state, CBlockIndex* pindexNew)\n     BOOST_FOREACH(CTransaction& tx, vResurrect) {\n         // ignore validation errors in resurrected transactions\n         CValidationState stateDummy;\n-        tx.AcceptToMemoryPool(stateDummy, true, false);\n+        if (!tx.AcceptToMemoryPool(stateDummy, true, false))\n+            mempool.remove(tx, true);\n     }\n \n     // Delete redundant memory transactions that are in the connected branch"
      }
    ]
  },
  {
    "sha": "839c7d1fa8bf939e313d78d2fae65bcf0174875c",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo4MzljN2QxZmE4YmY5MzllMzEzZDc4ZDJmYWU2NWJjZjAxNzQ4NzVj",
    "commit": {
      "author": {
        "name": "Gregory Maxwell",
        "email": "greg@xiph.org",
        "date": "2013-08-21T00:41:42Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-08-21T01:07:28Z"
      },
      "message": "Update the bloom state on the real object, not the temporary one.\n\nThis resulted in just passing all transactions to filtered wallets\nwhich worked surprisingly well, except where it didn't.",
      "tree": {
        "sha": "d07db6211510882ea7317652a466cf368ff3ca4f",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/d07db6211510882ea7317652a466cf368ff3ca4f"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/839c7d1fa8bf939e313d78d2fae65bcf0174875c",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/839c7d1fa8bf939e313d78d2fae65bcf0174875c",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/839c7d1fa8bf939e313d78d2fae65bcf0174875c",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/839c7d1fa8bf939e313d78d2fae65bcf0174875c/comments",
    "author": {
      "login": "gmaxwell",
      "id": 858454,
      "node_id": "MDQ6VXNlcjg1ODQ1NA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/858454?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gmaxwell",
      "html_url": "https://github.com/gmaxwell",
      "followers_url": "https://api.github.com/users/gmaxwell/followers",
      "following_url": "https://api.github.com/users/gmaxwell/following{/other_user}",
      "gists_url": "https://api.github.com/users/gmaxwell/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gmaxwell/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gmaxwell/subscriptions",
      "organizations_url": "https://api.github.com/users/gmaxwell/orgs",
      "repos_url": "https://api.github.com/users/gmaxwell/repos",
      "events_url": "https://api.github.com/users/gmaxwell/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gmaxwell/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "d7fdc5fac34814b71b2b539ebf6d9b8fdae5b50c",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/d7fdc5fac34814b71b2b539ebf6d9b8fdae5b50c",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/d7fdc5fac34814b71b2b539ebf6d9b8fdae5b50c"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 1,
      "deletions": 1
    },
    "files": [
      {
        "sha": "cb0ecdc671dc7acdc4a1d14607a5abcb784a4fc2",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/839c7d1fa8bf939e313d78d2fae65bcf0174875c/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/839c7d1fa8bf939e313d78d2fae65bcf0174875c/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=839c7d1fa8bf939e313d78d2fae65bcf0174875c",
        "patch": "@@ -3676,7 +3676,7 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n             LOCK(pfrom->cs_filter);\n             delete pfrom->pfilter;\n             pfrom->pfilter = new CBloomFilter(filter);\n-            filter.UpdateEmptyFull();\n+            pfrom->pfilter->UpdateEmptyFull();\n         }\n         pfrom->fRelayTxes = true;\n     }"
      }
    ]
  },
  {
    "sha": "f0a1d87b00e666a7028c2b517d786bc7511eaf5d",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpmMGExZDg3YjAwZTY2NmE3MDI4YzJiNTE3ZDc4NmJjNzUxMWVhZjVk",
    "commit": {
      "author": {
        "name": "Gregory Maxwell",
        "email": "greg@xiph.org",
        "date": "2013-09-09T09:11:11Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-09-10T00:03:06Z"
      },
      "message": "Longer term workaround for chainstate corruption from negative versions.\n\nThis also makes negative transaction versions non-standard.\n\nThis avoids an issue triggered in block 256818 where transactions with\nnegative version numbers were incorrectly serialized into the UTXO set.\n\nOn restart nodes detect the inconsistency and refuse to start so long as\na block with these transactions is inside the self-consistency check\nwindow, logging \"coin database inconsistencies found\". The software\nrecommends reindexing, but reindexing does not correct the problem.\n\nThis should be fixed by changing the chainstate serialization, but\nworking around it seems harmless for now because the version is not\nused by any network rule currently.\n\nA patch free workaround is to start with -checklevel=2 which skips\nthe consistency checks, but the IsStandard change is important for\nminers in order to protect unpatched nodes.",
      "tree": {
        "sha": "a58e3b2cc3f6f62590eae3a53fd0eed626ebb395",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/a58e3b2cc3f6f62590eae3a53fd0eed626ebb395"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/f0a1d87b00e666a7028c2b517d786bc7511eaf5d",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f0a1d87b00e666a7028c2b517d786bc7511eaf5d",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/f0a1d87b00e666a7028c2b517d786bc7511eaf5d",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f0a1d87b00e666a7028c2b517d786bc7511eaf5d/comments",
    "author": {
      "login": "gmaxwell",
      "id": 858454,
      "node_id": "MDQ6VXNlcjg1ODQ1NA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/858454?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gmaxwell",
      "html_url": "https://github.com/gmaxwell",
      "followers_url": "https://api.github.com/users/gmaxwell/followers",
      "following_url": "https://api.github.com/users/gmaxwell/following{/other_user}",
      "gists_url": "https://api.github.com/users/gmaxwell/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gmaxwell/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gmaxwell/subscriptions",
      "organizations_url": "https://api.github.com/users/gmaxwell/orgs",
      "repos_url": "https://api.github.com/users/gmaxwell/repos",
      "events_url": "https://api.github.com/users/gmaxwell/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gmaxwell/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "839c7d1fa8bf939e313d78d2fae65bcf0174875c",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/839c7d1fa8bf939e313d78d2fae65bcf0174875c",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/839c7d1fa8bf939e313d78d2fae65bcf0174875c"
      }
    ],
    "stats": {
      "total": 7,
      "additions": 6,
      "deletions": 1
    },
    "files": [
      {
        "sha": "a7501a1b6886bcbd38ce6c53a8227c09511c885e",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 6,
        "deletions": 1,
        "changes": 7,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/f0a1d87b00e666a7028c2b517d786bc7511eaf5d/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/f0a1d87b00e666a7028c2b517d786bc7511eaf5d/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=f0a1d87b00e666a7028c2b517d786bc7511eaf5d",
        "patch": "@@ -368,7 +368,7 @@ bool CTxOut::IsDust() const\n \n bool CTransaction::IsStandard() const\n {\n-    if (nVersion > CTransaction::CURRENT_VERSION)\n+    if (nVersion > CTransaction::CURRENT_VERSION || nVersion < 1)\n         return false;\n \n     if (!IsFinal())\n@@ -1499,6 +1499,11 @@ bool CBlock::DisconnectBlock(CValidationState &state, CBlockIndex *pindex, CCoin\n         CCoins &outs = view.GetCoins(hash);\n \n         CCoins outsBlock = CCoins(tx, pindex->nHeight);\n+        // The CCoins serialization does not serialize negative numbers.\n+        // No network rules currently depend on the version here, so an inconsistency is harmless\n+        // but it must be corrected before txout nversion ever influences a network rule.\n+        if (outsBlock.nVersion < 0)\n+            outs.nVersion = outsBlock.nVersion;\n         if (outs != outsBlock)\n             fClean = fClean && error(\"DisconnectBlock() : added transaction mismatch? database corrupted\");\n "
      }
    ]
  },
  {
    "sha": "27fefeac7151fe362e9a5c059a9c70f909e602fb",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzoyN2ZlZmVhYzcxNTFmZTM2MmU5YTVjMDU5YTljNzBmOTA5ZTYwMmZi",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2013-08-24T21:22:13Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-09-12T02:41:17Z"
      },
      "message": "Fix out-of-bounds check",
      "tree": {
        "sha": "2594804e41754d3fffbb7a7f1f75b9dc8e2e2e4b",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/2594804e41754d3fffbb7a7f1f75b9dc8e2e2e4b"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/27fefeac7151fe362e9a5c059a9c70f909e602fb",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/27fefeac7151fe362e9a5c059a9c70f909e602fb",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/27fefeac7151fe362e9a5c059a9c70f909e602fb",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/27fefeac7151fe362e9a5c059a9c70f909e602fb/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "f0a1d87b00e666a7028c2b517d786bc7511eaf5d",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f0a1d87b00e666a7028c2b517d786bc7511eaf5d",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/f0a1d87b00e666a7028c2b517d786bc7511eaf5d"
      }
    ],
    "stats": {
      "total": 3,
      "additions": 2,
      "deletions": 1
    },
    "files": [
      {
        "sha": "ce45133a9fbbc2266c30974dcaf603d01a921ae4",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/27fefeac7151fe362e9a5c059a9c70f909e602fb/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/27fefeac7151fe362e9a5c059a9c70f909e602fb/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=27fefeac7151fe362e9a5c059a9c70f909e602fb",
        "patch": "@@ -2191,7 +2191,8 @@ bool CBlock::AcceptBlock(CValidationState &state, CDiskBlockPos *dbp)\n                 (fTestNet && CBlockIndex::IsSuperMajority(2, pindexPrev, 51, 100)))\n             {\n                 CScript expect = CScript() << nHeight;\n-                if (!std::equal(expect.begin(), expect.end(), vtx[0].vin[0].scriptSig.begin()))\n+                if (vtx[0].vin[0].scriptSig.size() < expect.size() ||\n+                    !std::equal(expect.begin(), expect.end(), vtx[0].vin[0].scriptSig.begin()))\n                     return state.DoS(100, error(\"AcceptBlock() : block height mismatch in coinbase\"));\n             }\n         }"
      }
    ]
  },
  {
    "sha": "ef14a26b12ad63cbe109e24043b0b1ce5c07447a",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzplZjE0YTI2YjEyYWQ2M2NiZTEwOWUyNDA0M2IwYjFjZTVjMDc0NDdh",
    "commit": {
      "author": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-09-12T02:46:38Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-09-12T03:35:18Z"
      },
      "message": "Bump version numbers for 0.8.5 release",
      "tree": {
        "sha": "62c945e901c646a4931a60c3c0a248e311ba39f3",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/62c945e901c646a4931a60c3c0a248e311ba39f3"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/ef14a26b12ad63cbe109e24043b0b1ce5c07447a",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/ef14a26b12ad63cbe109e24043b0b1ce5c07447a",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/ef14a26b12ad63cbe109e24043b0b1ce5c07447a",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/comments",
    "author": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "27fefeac7151fe362e9a5c059a9c70f909e602fb",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/27fefeac7151fe362e9a5c059a9c70f909e602fb",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/27fefeac7151fe362e9a5c059a9c70f909e602fb"
      }
    ],
    "stats": {
      "total": 19,
      "additions": 10,
      "deletions": 9
    },
    "files": [
      {
        "sha": "996b53cd1199913f510d171021091cdc69ddfbf3",
        "filename": "bitcoin-qt.pro",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/bitcoin-qt.pro",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/bitcoin-qt.pro",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/bitcoin-qt.pro?ref=ef14a26b12ad63cbe109e24043b0b1ce5c07447a",
        "patch": "@@ -1,7 +1,7 @@\n TEMPLATE = app\n TARGET = bitcoin-qt\n macx:TARGET = \"Bitcoin-Qt\"\n-VERSION = 0.8.4\n+VERSION = 0.8.5\n INCLUDEPATH += src src/json src/qt\n QT += network\n DEFINES += QT_GUI BOOST_THREAD_USE_LIB BOOST_SPIRIT_THREADSAFE"
      },
      {
        "sha": "d0c975bddd2ab83ef8cc1c744a0d7c1e72448e0b",
        "filename": "contrib/verifysfbinaries/verify.sh",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/contrib/verifysfbinaries/verify.sh",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/contrib/verifysfbinaries/verify.sh",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/contrib/verifysfbinaries/verify.sh?ref=ef14a26b12ad63cbe109e24043b0b1ce5c07447a",
        "patch": "@@ -18,7 +18,7 @@ WORKINGDIR=\"/tmp/bitcoin\"\n TMPFILE=\"hashes.tmp\"\n \n #this URL is used if a version number is not specified as an argument to the script\n-SIGNATUREFILE=\"http://downloads.sourceforge.net/project/bitcoin/Bitcoin/bitcoin-0.8.4/SHA256SUMS.asc\"\n+SIGNATUREFILE=\"http://downloads.sourceforge.net/project/bitcoin/Bitcoin/bitcoin-0.8.5/SHA256SUMS.asc\"\n \n SIGNATUREFILENAME=\"SHA256SUMS.asc\"\n RCSUBDIR=\"test/\""
      },
      {
        "sha": "1a7b068687df30debf0295eb9f5a782fb543ee9a",
        "filename": "doc/README.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/doc/README.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/doc/README.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/README.md?ref=ef14a26b12ad63cbe109e24043b0b1ce5c07447a",
        "patch": "@@ -1,4 +1,4 @@\n-Bitcoin 0.8.4 BETA\n+Bitcoin 0.8.5 BETA\n ====================\n \n Copyright (c) 2009-2013 Bitcoin Developers"
      },
      {
        "sha": "f0eb21150435315a9c2a6bc62c31f1c38f216e58",
        "filename": "doc/README_windows.txt",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/doc/README_windows.txt",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/doc/README_windows.txt",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/README_windows.txt?ref=ef14a26b12ad63cbe109e24043b0b1ce5c07447a",
        "patch": "@@ -1,4 +1,4 @@\n-Bitcoin 0.8.4 BETA\r\n+Bitcoin 0.8.5 BETA\r\n \r\n Copyright (c) 2009-2013 Bitcoin Developers\r\n Distributed under the MIT/X11 software license, see the accompanying\r"
      },
      {
        "sha": "bc605cf8bdb3aae91ac817ec2f5f6a879ad94b9a",
        "filename": "doc/release-notes.md",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/doc/release-notes.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/doc/release-notes.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/release-notes.md?ref=ef14a26b12ad63cbe109e24043b0b1ce5c07447a",
        "patch": "@@ -1,6 +1,7 @@\n (note: this is a temporary file, to be added-to by anybody, and deleted at\n release time)\n \n-0.8.4 changes\n+0.8.5 changes\n =============\n \n+Workaround negative version numbers serialization bug."
      },
      {
        "sha": "8005bf73b84bfad664bb359b7cdf444ecb7d0fa2",
        "filename": "share/setup.nsi",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/share/setup.nsi",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/share/setup.nsi",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/share/setup.nsi?ref=ef14a26b12ad63cbe109e24043b0b1ce5c07447a",
        "patch": "@@ -5,7 +5,7 @@ SetCompressor /SOLID lzma\n \r\n # General Symbol Definitions\r\n !define REGKEY \"SOFTWARE\\$(^Name)\"\r\n-!define VERSION 0.8.4\r\n+!define VERSION 0.8.5\r\n !define COMPANY \"Bitcoin project\"\r\n !define URL http://www.bitcoin.org/\r\n \r\n@@ -45,13 +45,13 @@ Var StartMenuGroup\n !insertmacro MUI_LANGUAGE English\r\n \r\n # Installer attributes\r\n-OutFile bitcoin-0.8.4-win32-setup.exe\r\n+OutFile bitcoin-0.8.5-win32-setup.exe\r\n InstallDir $PROGRAMFILES\\Bitcoin\r\n CRCCheck on\r\n XPStyle on\r\n BrandingText \" \"\r\n ShowInstDetails show\r\n-VIProductVersion 0.8.4.0\r\n+VIProductVersion 0.8.5.0\r\n VIAddVersionKey ProductName Bitcoin\r\n VIAddVersionKey ProductVersion \"${VERSION}\"\r\n VIAddVersionKey CompanyName \"${COMPANY}\"\r"
      },
      {
        "sha": "5381983f4e5e56f1a18a8fb8cdbd272a11419e32",
        "filename": "src/clientversion.h",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/src/clientversion.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/ef14a26b12ad63cbe109e24043b0b1ce5c07447a/src/clientversion.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/clientversion.h?ref=ef14a26b12ad63cbe109e24043b0b1ce5c07447a",
        "patch": "@@ -8,7 +8,7 @@\n // These need to be macros, as version.cpp's and bitcoin-qt.rc's voodoo requires it\n #define CLIENT_VERSION_MAJOR       0\n #define CLIENT_VERSION_MINOR       8\n-#define CLIENT_VERSION_REVISION    4\n+#define CLIENT_VERSION_REVISION    5\n #define CLIENT_VERSION_BUILD       0\n \n // Set to true for release, false for prerelease or test build"
      }
    ]
  },
  {
    "sha": "7084756f4f4e34818659a10f62933fcfc77fe27f",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo3MDg0NzU2ZjRmNGUzNDgxODY1OWExMGY2MjkzM2ZjZmM3N2ZlMjdm",
    "commit": {
      "author": {
        "name": "Patrick Strateman",
        "email": "patrick.strateman@gmail.com",
        "date": "2013-10-28T20:20:21Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T09:15:49Z"
      },
      "message": "process received messages one at a time without sleeping between messages",
      "tree": {
        "sha": "8e3112cdd38a6fafba4492df09ecf7be5269b136",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/8e3112cdd38a6fafba4492df09ecf7be5269b136"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/7084756f4f4e34818659a10f62933fcfc77fe27f",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7084756f4f4e34818659a10f62933fcfc77fe27f",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/7084756f4f4e34818659a10f62933fcfc77fe27f",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7084756f4f4e34818659a10f62933fcfc77fe27f/comments",
    "author": {
      "login": "pstratem",
      "id": 620611,
      "node_id": "MDQ6VXNlcjYyMDYxMQ==",
      "avatar_url": "https://avatars.githubusercontent.com/u/620611?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/pstratem",
      "html_url": "https://github.com/pstratem",
      "followers_url": "https://api.github.com/users/pstratem/followers",
      "following_url": "https://api.github.com/users/pstratem/following{/other_user}",
      "gists_url": "https://api.github.com/users/pstratem/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/pstratem/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/pstratem/subscriptions",
      "organizations_url": "https://api.github.com/users/pstratem/orgs",
      "repos_url": "https://api.github.com/users/pstratem/repos",
      "events_url": "https://api.github.com/users/pstratem/events{/privacy}",
      "received_events_url": "https://api.github.com/users/pstratem/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "ef14a26b12ad63cbe109e24043b0b1ce5c07447a",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/ef14a26b12ad63cbe109e24043b0b1ce5c07447a",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/ef14a26b12ad63cbe109e24043b0b1ce5c07447a"
      }
    ],
    "stats": {
      "total": 24,
      "additions": 23,
      "deletions": 1
    },
    "files": [
      {
        "sha": "d4816ca22f299951859cf938aae2b8763ae8f666",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/7084756f4f4e34818659a10f62933fcfc77fe27f/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/7084756f4f4e34818659a10f62933fcfc77fe27f/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=7084756f4f4e34818659a10f62933fcfc77fe27f",
        "patch": "@@ -3141,6 +3141,9 @@ void static ProcessGetData(CNode* pfrom)\n \n             // Track requests for our stuff.\n             Inventory(inv.hash);\n+\n+            if (inv.type == MSG_BLOCK || inv.type == MSG_FILTERED_BLOCK)\n+                break;\n         }\n     }\n \n@@ -3751,6 +3754,9 @@ bool ProcessMessages(CNode* pfrom)\n     if (!pfrom->vRecvGetData.empty())\n         ProcessGetData(pfrom);\n \n+    // this maintains the order of responses\n+    if (!pfrom->vRecvGetData.empty()) return fOk;\n+\n     std::deque<CNetMessage>::iterator it = pfrom->vRecvMsg.begin();\n     while (!pfrom->fDisconnect && it != pfrom->vRecvMsg.end()) {\n         // Don't bother if send buffer is too full to respond anyway\n@@ -3841,6 +3847,8 @@ bool ProcessMessages(CNode* pfrom)\n \n         if (!fRet)\n             printf(\"ProcessMessage(%s, %u bytes) FAILED\\n\", strCommand.c_str(), nMessageSize);\n+\n+        break;\n     }\n \n     // In case the connection got shut down, its receive buffer was wiped"
      },
      {
        "sha": "e5f85d329001a2249449eb0dd4776bacb988b441",
        "filename": "src/net.cpp",
        "status": "modified",
        "additions": 15,
        "deletions": 1,
        "changes": 16,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/7084756f4f4e34818659a10f62933fcfc77fe27f/src/net.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/7084756f4f4e34818659a10f62933fcfc77fe27f/src/net.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/net.cpp?ref=7084756f4f4e34818659a10f62933fcfc77fe27f",
        "patch": "@@ -1629,6 +1629,9 @@ void ThreadMessageHandler()\n         CNode* pnodeTrickle = NULL;\n         if (!vNodesCopy.empty())\n             pnodeTrickle = vNodesCopy[GetRand(vNodesCopy.size())];\n+\n+        bool fSleep = true;\n+\n         BOOST_FOREACH(CNode* pnode, vNodesCopy)\n         {\n             if (pnode->fDisconnect)\n@@ -1638,8 +1641,18 @@ void ThreadMessageHandler()\n             {\n                 TRY_LOCK(pnode->cs_vRecvMsg, lockRecv);\n                 if (lockRecv)\n+                {\n                     if (!ProcessMessages(pnode))\n                         pnode->CloseSocketDisconnect();\n+\n+                    if (pnode->nSendSize < SendBufferSize())\n+                    {\n+                        if (!pnode->vRecvGetData.empty() || (!pnode->vRecvMsg.empty() && pnode->vRecvMsg[0].complete()))\n+                        {\n+                            fSleep = false;\n+                        }\n+                    }\n+                }\n             }\n             boost::this_thread::interruption_point();\n \n@@ -1658,7 +1671,8 @@ void ThreadMessageHandler()\n                 pnode->Release();\n         }\n \n-        MilliSleep(100);\n+        if (fSleep)\n+            MilliSleep(100);\n     }\n }\n "
      }
    ]
  },
  {
    "sha": "5c55bf5af00aa9b5888b0469501fd113a144967f",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo1YzU1YmY1YWYwMGFhOWI1ODg4YjA0Njk1MDFmZDExM2ExNDQ5Njdm",
    "commit": {
      "author": {
        "name": "Matt Corallo",
        "email": "git@bluematt.me",
        "date": "2013-10-25T07:52:53Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T09:15:57Z"
      },
      "message": "Fix comparison tool by asking for blocks more aggressively\n\nRebased-from: b33b9a6fefbe832bf45a6c7717d0537f27597bff",
      "tree": {
        "sha": "c453c8cfb707e82fd6bcfc4b18c2768666eaca8c",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/c453c8cfb707e82fd6bcfc4b18c2768666eaca8c"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/5c55bf5af00aa9b5888b0469501fd113a144967f",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5c55bf5af00aa9b5888b0469501fd113a144967f",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/5c55bf5af00aa9b5888b0469501fd113a144967f",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5c55bf5af00aa9b5888b0469501fd113a144967f/comments",
    "author": {
      "login": "TheBlueMatt",
      "id": 649246,
      "node_id": "MDQ6VXNlcjY0OTI0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/TheBlueMatt",
      "html_url": "https://github.com/TheBlueMatt",
      "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
      "following_url": "https://api.github.com/users/TheBlueMatt/following{/other_user}",
      "gists_url": "https://api.github.com/users/TheBlueMatt/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/TheBlueMatt/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
      "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
      "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
      "events_url": "https://api.github.com/users/TheBlueMatt/events{/privacy}",
      "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "7084756f4f4e34818659a10f62933fcfc77fe27f",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7084756f4f4e34818659a10f62933fcfc77fe27f",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/7084756f4f4e34818659a10f62933fcfc77fe27f"
      }
    ],
    "stats": {
      "total": 11,
      "additions": 8,
      "deletions": 3
    },
    "files": [
      {
        "sha": "f8d8697a3e89a7477c82f403649b3930dd588fbd",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/5c55bf5af00aa9b5888b0469501fd113a144967f/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/5c55bf5af00aa9b5888b0469501fd113a144967f/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=5c55bf5af00aa9b5888b0469501fd113a144967f",
        "patch": "@@ -2123,7 +2123,7 @@ bool CBlock::CheckBlock(CValidationState &state, bool fCheckPOW, bool fCheckMerk\n         uniqueTx.insert(GetTxHash(i));\n     }\n     if (uniqueTx.size() != vtx.size())\n-        return state.DoS(100, error(\"CheckBlock() : duplicate transaction\"));\n+        return state.DoS(100, error(\"CheckBlock() : duplicate transaction\"), true);\n \n     unsigned int nSigOps = 0;\n     BOOST_FOREACH(const CTransaction& tx, vtx)\n@@ -3583,7 +3583,7 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n         pfrom->AddInventoryKnown(inv);\n \n         CValidationState state;\n-        if (ProcessBlock(state, pfrom, &block))\n+        if (ProcessBlock(state, pfrom, &block) || state.CorruptionPossible())\n             mapAlreadyAskedFor.erase(inv);\n         int nDoS;\n         if (state.IsInvalid(nDoS))"
      },
      {
        "sha": "e5226b69421aa01563a182982ac3b5c471fd872d",
        "filename": "src/main.h",
        "status": "modified",
        "additions": 6,
        "deletions": 1,
        "changes": 7,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/5c55bf5af00aa9b5888b0469501fd113a144967f/src/main.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/5c55bf5af00aa9b5888b0469501fd113a144967f/src/main.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.h?ref=5c55bf5af00aa9b5888b0469501fd113a144967f",
        "patch": "@@ -1881,13 +1881,15 @@ class CValidationState {\n         MODE_ERROR,   // run-time error\n     } mode;\n     int nDoS;\n+    bool corruptionPossible;\n public:\n     CValidationState() : mode(MODE_VALID), nDoS(0) {}\n-    bool DoS(int level, bool ret = false) {\n+    bool DoS(int level, bool ret = false, bool corruptionIn = false) {\n         if (mode == MODE_ERROR)\n             return ret;\n         nDoS += level;\n         mode = MODE_INVALID;\n+        corruptionPossible = corruptionIn;\n         return ret;\n     }\n     bool Invalid(bool ret = false) {\n@@ -1917,6 +1919,9 @@ class CValidationState {\n         }\n         return false;\n     }\n+    bool CorruptionPossible() {\n+        return corruptionPossible;\n+    }\n };\n \n "
      }
    ]
  },
  {
    "sha": "3e89dbb132a211ebb94bdc3b626f3c6c1d0b29dc",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzozZTg5ZGJiMTMyYTIxMWViYjk0YmRjM2I2MjZmM2M2YzFkMGIyOWRj",
    "commit": {
      "author": {
        "name": "theuni",
        "email": "theuni-nospam@xbmc.org",
        "date": "2013-11-26T00:48:14Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T09:32:16Z"
      },
      "message": "Fix uninitialized variable added in 5c55bf5\n\nAfter discussing with BlueMatt, this appears to be harmless in its\ncurrent state since it's always set before it's used. Initialize it\nanyway for readability and future safety.\n\nRebased-By: Wladimir J. van der Laan <laanwj@gmail.com>\nRebased-From: 106f133de6bdb577c4135847fd703d08f525ba46",
      "tree": {
        "sha": "5ef697b10cbb14b939b87c261fe09c39ad3b2921",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/5ef697b10cbb14b939b87c261fe09c39ad3b2921"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/3e89dbb132a211ebb94bdc3b626f3c6c1d0b29dc",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/3e89dbb132a211ebb94bdc3b626f3c6c1d0b29dc",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/3e89dbb132a211ebb94bdc3b626f3c6c1d0b29dc",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/3e89dbb132a211ebb94bdc3b626f3c6c1d0b29dc/comments",
    "author": null,
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "5c55bf5af00aa9b5888b0469501fd113a144967f",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5c55bf5af00aa9b5888b0469501fd113a144967f",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/5c55bf5af00aa9b5888b0469501fd113a144967f"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 1,
      "deletions": 1
    },
    "files": [
      {
        "sha": "76020420cbff73d49edbf6734c533eab067c262c",
        "filename": "src/main.h",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/3e89dbb132a211ebb94bdc3b626f3c6c1d0b29dc/src/main.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/3e89dbb132a211ebb94bdc3b626f3c6c1d0b29dc/src/main.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.h?ref=3e89dbb132a211ebb94bdc3b626f3c6c1d0b29dc",
        "patch": "@@ -1883,7 +1883,7 @@ class CValidationState {\n     int nDoS;\n     bool corruptionPossible;\n public:\n-    CValidationState() : mode(MODE_VALID), nDoS(0) {}\n+    CValidationState() : mode(MODE_VALID), nDoS(0), corruptionPossible(false) {}\n     bool DoS(int level, bool ret = false, bool corruptionIn = false) {\n         if (mode == MODE_ERROR)\n             return ret;"
      }
    ]
  },
  {
    "sha": "633d95ec222db340242410a581e65805eb863af9",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo2MzNkOTVlYzIyMmRiMzQwMjQyNDEwYTU4MWU2NTgwNWViODYzYWY5",
    "commit": {
      "author": {
        "name": "Philip Kaufmann",
        "email": "phil.kaufmann@t-online.de",
        "date": "2013-10-01T07:44:56Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T13:00:47Z"
      },
      "message": "log start and end of Shutdown()\n\n- could be helpful when debugging shutdown related problems\n\nRebased-by:   Warren Togami <wtogami@gmail.com>\nRebased-from: ced3c248168941fbbd42d5a3807657a88be6a54e",
      "tree": {
        "sha": "e4e0da4e7bc4b9ddf2b60e332c01e5e2fae68744",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/e4e0da4e7bc4b9ddf2b60e332c01e5e2fae68744"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/633d95ec222db340242410a581e65805eb863af9",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/633d95ec222db340242410a581e65805eb863af9",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/633d95ec222db340242410a581e65805eb863af9",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/633d95ec222db340242410a581e65805eb863af9/comments",
    "author": null,
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "3e89dbb132a211ebb94bdc3b626f3c6c1d0b29dc",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/3e89dbb132a211ebb94bdc3b626f3c6c1d0b29dc",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/3e89dbb132a211ebb94bdc3b626f3c6c1d0b29dc"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 2,
      "deletions": 0
    },
    "files": [
      {
        "sha": "91e13349231b4feb084be15c5c1e1184fcc6323f",
        "filename": "src/init.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/633d95ec222db340242410a581e65805eb863af9/src/init.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/633d95ec222db340242410a581e65805eb863af9/src/init.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/init.cpp?ref=633d95ec222db340242410a581e65805eb863af9",
        "patch": "@@ -89,6 +89,7 @@ static CCoinsViewDB *pcoinsdbview;\n \n void Shutdown()\n {\n+    printf(\"Shutdown : In progress...\\n\");\n     static CCriticalSection cs_Shutdown;\n     TRY_LOCK(cs_Shutdown, lockShutdown);\n     if (!lockShutdown) return;\n@@ -114,6 +115,7 @@ void Shutdown()\n     boost::filesystem::remove(GetPidFile());\n     UnregisterWallet(pwalletMain);\n     delete pwalletMain;\n+    printf(\"Shutdown : done\\n\");\n }\n \n //"
      }
    ]
  },
  {
    "sha": "c4892eb4b3fd7f263f3d8584029a24340837fa66",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpjNDg5MmViNGIzZmQ3ZjI2M2YzZDg1ODQwMjlhMjQzNDA4MzdmYTY2",
    "commit": {
      "author": {
        "name": "Jeff Garzik",
        "email": "jgarzik@bitpay.com",
        "date": "2013-06-05T18:52:06Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T13:00:47Z"
      },
      "message": "Log reason for non-standard transaction rejection\n\nConflicts:\n\tsrc/main.cpp\n\nRebased-from: cb3076a3daa68eebf19c681ab48a0c0d9f0ce7a5",
      "tree": {
        "sha": "84911c94d9d336af5eafd1a0641e66607e7a6a37",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/84911c94d9d336af5eafd1a0641e66607e7a6a37"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/c4892eb4b3fd7f263f3d8584029a24340837fa66",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/c4892eb4b3fd7f263f3d8584029a24340837fa66",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/c4892eb4b3fd7f263f3d8584029a24340837fa66",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/c4892eb4b3fd7f263f3d8584029a24340837fa66/comments",
    "author": null,
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "633d95ec222db340242410a581e65805eb863af9",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/633d95ec222db340242410a581e65805eb863af9",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/633d95ec222db340242410a581e65805eb863af9"
      }
    ],
    "stats": {
      "total": 43,
      "additions": 32,
      "deletions": 11
    },
    "files": [
      {
        "sha": "726a49c611f9b231570c1392a58cdd21e5560fd8",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 26,
        "deletions": 10,
        "changes": 36,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/c4892eb4b3fd7f263f3d8584029a24340837fa66/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/c4892eb4b3fd7f263f3d8584029a24340837fa66/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=c4892eb4b3fd7f263f3d8584029a24340837fa66",
        "patch": "@@ -366,37 +366,51 @@ bool CTxOut::IsDust() const\n     return ((nValue*1000)/(3*((int)GetSerializeSize(SER_DISK,0)+148)) < CTransaction::nMinRelayTxFee);\n }\n \n-bool CTransaction::IsStandard() const\n+bool CTransaction::IsStandard(string& strReason) const\n {\n-    if (nVersion > CTransaction::CURRENT_VERSION || nVersion < 1)\n+    if (nVersion > CTransaction::CURRENT_VERSION || nVersion < 1) {\n+        strReason = \"version\";\n         return false;\n+    }\n \n-    if (!IsFinal())\n+    if (!IsFinal()) {\n+        strReason = \"not-final\";\n         return false;\n+    }\n \n     // Extremely large transactions with lots of inputs can cost the network\n     // almost as much to process as they cost the sender in fees, because\n     // computing signature hashes is O(ninputs*txsize). Limiting transactions\n     // to MAX_STANDARD_TX_SIZE mitigates CPU exhaustion attacks.\n     unsigned int sz = this->GetSerializeSize(SER_NETWORK, CTransaction::CURRENT_VERSION);\n-    if (sz >= MAX_STANDARD_TX_SIZE)\n+    if (sz >= MAX_STANDARD_TX_SIZE) {\n+        strReason = \"tx-size\";\n         return false;\n+    }\n \n     BOOST_FOREACH(const CTxIn& txin, vin)\n     {\n         // Biggest 'standard' txin is a 3-signature 3-of-3 CHECKMULTISIG\n         // pay-to-script-hash, which is 3 ~80-byte signatures, 3\n         // ~65-byte public keys, plus a few script ops.\n-        if (txin.scriptSig.size() > 500)\n+        if (txin.scriptSig.size() > 500) {\n+            strReason = \"scriptsig-size\";\n             return false;\n-        if (!txin.scriptSig.IsPushOnly())\n+        }\n+        if (!txin.scriptSig.IsPushOnly()) {\n+            strReason = \"scriptsig-not-pushonly\";\n             return false;\n+        }\n     }\n     BOOST_FOREACH(const CTxOut& txout, vout) {\n-        if (!::IsStandard(txout.scriptPubKey))\n+        if (!::IsStandard(txout.scriptPubKey)) {\n+            strReason = \"scriptpubkey\";\n             return false;\n-        if (txout.IsDust())\n+        }\n+        if (txout.IsDust()) {\n+            strReason = \"dust\";\n             return false;\n+        }\n     }\n     return true;\n }\n@@ -661,8 +675,10 @@ bool CTxMemPool::accept(CValidationState &state, CTransaction &tx, bool fCheckIn\n         return error(\"CTxMemPool::accept() : not accepting nLockTime beyond 2038 yet\");\n \n     // Rather not work on nonstandard transactions (unless -testnet)\n-    if (!fTestNet && !tx.IsStandard())\n-        return error(\"CTxMemPool::accept() : nonstandard transaction type\");\n+    string strNonStd;\n+    if (!fTestNet && !tx.IsStandard(strNonStd))\n+        return error(\"CTxMemPool::accept() : nonstandard transaction (%s)\",\n+                     strNonStd.c_str());\n \n     // is it already in the memory pool?\n     uint256 hash = tx.GetHash();"
      },
      {
        "sha": "94260dc2e4b30e99f18d9558a076a391d5abd960",
        "filename": "src/main.h",
        "status": "modified",
        "additions": 6,
        "deletions": 1,
        "changes": 7,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/c4892eb4b3fd7f263f3d8584029a24340837fa66/src/main.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/c4892eb4b3fd7f263f3d8584029a24340837fa66/src/main.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.h?ref=c4892eb4b3fd7f263f3d8584029a24340837fa66",
        "patch": "@@ -559,7 +559,12 @@ class CTransaction\n     /** Check for standard transaction types\n         @return True if all outputs (scriptPubKeys) use only standard transaction forms\n     */\n-    bool IsStandard() const;\n+    bool IsStandard(std::string& strReason) const;\n+    bool IsStandard() const\n+    {\n+        std::string strReason;\n+        return IsStandard(strReason);\n+    }\n \n     /** Check for standard transaction types\n         @param[in] mapInputs\tMap of previous transactions that have outputs we're spending"
      }
    ]
  },
  {
    "sha": "05ea79052c1cc88f901d570cdded32b25a361759",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzowNWVhNzkwNTJjMWNjODhmOTAxZDU3MGNkZGVkMzJiMjVhMzYxNzU5",
    "commit": {
      "author": {
        "name": "Philip Kaufmann",
        "email": "phil.kaufmann@t-online.de",
        "date": "2013-10-30T10:36:51Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T13:00:47Z"
      },
      "message": "make -logtimestamps default on and rework help-message\n\nRebased-By: Wladimir J. van der Laan <laanwj@gmail.com>\nRebased-From: 959e62f",
      "tree": {
        "sha": "3340e040477ebe053197a01f6854d76c1294b49c",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/3340e040477ebe053197a01f6854d76c1294b49c"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/05ea79052c1cc88f901d570cdded32b25a361759",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/05ea79052c1cc88f901d570cdded32b25a361759",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/05ea79052c1cc88f901d570cdded32b25a361759",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/05ea79052c1cc88f901d570cdded32b25a361759/comments",
    "author": null,
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "c4892eb4b3fd7f263f3d8584029a24340837fa66",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/c4892eb4b3fd7f263f3d8584029a24340837fa66",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/c4892eb4b3fd7f263f3d8584029a24340837fa66"
      }
    ],
    "stats": {
      "total": 4,
      "additions": 2,
      "deletions": 2
    },
    "files": [
      {
        "sha": "9c5386779d11e9de059bc0b3d1dba71601aff61d",
        "filename": "src/init.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/05ea79052c1cc88f901d570cdded32b25a361759/src/init.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/05ea79052c1cc88f901d570cdded32b25a361759/src/init.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/init.cpp?ref=05ea79052c1cc88f901d570cdded32b25a361759",
        "patch": "@@ -335,7 +335,7 @@ std::string HelpMessage()\n         \"  -testnet               \" + _(\"Use the test network\") + \"\\n\" +\n         \"  -debug                 \" + _(\"Output extra debugging information. Implies all other -debug* options\") + \"\\n\" +\n         \"  -debugnet              \" + _(\"Output extra network debugging information\") + \"\\n\" +\n-        \"  -logtimestamps         \" + _(\"Prepend debug output with timestamp\") + \"\\n\" +\n+        \"  -logtimestamps         \" + _(\"Prepend debug output with timestamp (default: 1)\") + \"\\n\" +\n         \"  -shrinkdebugfile       \" + _(\"Shrink debug.log file on client startup (default: 1 when no -debug)\") + \"\\n\" +\n         \"  -printtoconsole        \" + _(\"Send trace/debug info to console instead of debug.log file\") + \"\\n\" +\n #ifdef WIN32\n@@ -571,7 +571,7 @@ bool AppInit2(boost::thread_group& threadGroup)\n #endif\n     fPrintToConsole = GetBoolArg(\"-printtoconsole\");\n     fPrintToDebugger = GetBoolArg(\"-printtodebugger\");\n-    fLogTimestamps = GetBoolArg(\"-logtimestamps\");\n+    fLogTimestamps = GetBoolArg(\"-logtimestamps\", true);\n \n     if (mapArgs.count(\"-timeout\"))\n     {"
      }
    ]
  },
  {
    "sha": "f46f128b9a60765c68dd9fbe66d7f7778914900b",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpmNDZmMTI4YjlhNjA3NjVjNjhkZDlmYmU2NmQ3Zjc3Nzg5MTQ5MDBi",
    "commit": {
      "author": {
        "name": "Gregory Maxwell",
        "email": "greg@xiph.org",
        "date": "2013-09-16T03:14:06Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T13:00:47Z"
      },
      "message": "More fixes for blockchain corruption on OSX.\n\nAs we'd previously learned, OSX's fsync is a data eating lie.\n\nSince 0.8.4 we're still getting some reports of disk corruption on\n OSX but now all of it looks like the block files have gotten out of\n sync with the database. It turns out that we were still using fsync()\n on the block files, so this isn't surprising.",
      "tree": {
        "sha": "3359ba1bb5ab9baf75b7b4b108a1f1a4753a82ef",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/3359ba1bb5ab9baf75b7b4b108a1f1a4753a82ef"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/f46f128b9a60765c68dd9fbe66d7f7778914900b",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f46f128b9a60765c68dd9fbe66d7f7778914900b",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/f46f128b9a60765c68dd9fbe66d7f7778914900b",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f46f128b9a60765c68dd9fbe66d7f7778914900b/comments",
    "author": {
      "login": "gmaxwell",
      "id": 858454,
      "node_id": "MDQ6VXNlcjg1ODQ1NA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/858454?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gmaxwell",
      "html_url": "https://github.com/gmaxwell",
      "followers_url": "https://api.github.com/users/gmaxwell/followers",
      "following_url": "https://api.github.com/users/gmaxwell/following{/other_user}",
      "gists_url": "https://api.github.com/users/gmaxwell/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gmaxwell/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gmaxwell/subscriptions",
      "organizations_url": "https://api.github.com/users/gmaxwell/orgs",
      "repos_url": "https://api.github.com/users/gmaxwell/repos",
      "events_url": "https://api.github.com/users/gmaxwell/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gmaxwell/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "05ea79052c1cc88f901d570cdded32b25a361759",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/05ea79052c1cc88f901d570cdded32b25a361759",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/05ea79052c1cc88f901d570cdded32b25a361759"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 2,
      "deletions": 0
    },
    "files": [
      {
        "sha": "481fc763cca0d1935b0fd55c6ab0c6fe62a100d7",
        "filename": "src/util.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/f46f128b9a60765c68dd9fbe66d7f7778914900b/src/util.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/f46f128b9a60765c68dd9fbe66d7f7778914900b/src/util.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/util.cpp?ref=f46f128b9a60765c68dd9fbe66d7f7778914900b",
        "patch": "@@ -1147,6 +1147,8 @@ void FileCommit(FILE *fileout)\n #else\n     #if defined(__linux__) || defined(__NetBSD__)\n     fdatasync(fileno(fileout));\n+    #elif defined(__APPLE__) && defined(F_FULLFSYNC)\n+    fcntl(fileno(fileout), F_FULLFSYNC, 0);\n     #else\n     fsync(fileno(fileout));\n     #endif"
      }
    ]
  },
  {
    "sha": "0119e0c592018d5c9e83e301408858ab04b7d674",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzowMTE5ZTBjNTkyMDE4ZDVjOWU4M2UzMDE0MDg4NThhYjA0YjdkNjc0",
    "commit": {
      "author": {
        "name": "David Hill",
        "email": "dhill@conformal.com",
        "date": "2013-10-04T12:46:45Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T13:00:48Z"
      },
      "message": "Hurricane Electric uses block 2001:470::, not 2011:470::",
      "tree": {
        "sha": "07a08ab04d10b7f4b5a71333d4377dfa8ba267b1",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/07a08ab04d10b7f4b5a71333d4377dfa8ba267b1"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/0119e0c592018d5c9e83e301408858ab04b7d674",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/0119e0c592018d5c9e83e301408858ab04b7d674",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/0119e0c592018d5c9e83e301408858ab04b7d674",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/0119e0c592018d5c9e83e301408858ab04b7d674/comments",
    "author": {
      "login": "dajohi",
      "id": 3308193,
      "node_id": "MDQ6VXNlcjMzMDgxOTM=",
      "avatar_url": "https://avatars.githubusercontent.com/u/3308193?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dajohi",
      "html_url": "https://github.com/dajohi",
      "followers_url": "https://api.github.com/users/dajohi/followers",
      "following_url": "https://api.github.com/users/dajohi/following{/other_user}",
      "gists_url": "https://api.github.com/users/dajohi/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dajohi/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dajohi/subscriptions",
      "organizations_url": "https://api.github.com/users/dajohi/orgs",
      "repos_url": "https://api.github.com/users/dajohi/repos",
      "events_url": "https://api.github.com/users/dajohi/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dajohi/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "f46f128b9a60765c68dd9fbe66d7f7778914900b",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f46f128b9a60765c68dd9fbe66d7f7778914900b",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/f46f128b9a60765c68dd9fbe66d7f7778914900b"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 1,
      "deletions": 1
    },
    "files": [
      {
        "sha": "d9695167502d9701694608295a16cc3f5315b927",
        "filename": "src/netbase.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/0119e0c592018d5c9e83e301408858ab04b7d674/src/netbase.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/0119e0c592018d5c9e83e301408858ab04b7d674/src/netbase.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/netbase.cpp?ref=0119e0c592018d5c9e83e301408858ab04b7d674",
        "patch": "@@ -860,7 +860,7 @@ std::vector<unsigned char> CNetAddr::GetGroup() const\n         nBits = 4;\n     }\n     // for he.net, use /36 groups\n-    else if (GetByte(15) == 0x20 && GetByte(14) == 0x11 && GetByte(13) == 0x04 && GetByte(12) == 0x70)\n+    else if (GetByte(15) == 0x20 && GetByte(14) == 0x01 && GetByte(13) == 0x04 && GetByte(12) == 0x70)\n         nBits = 36;\n     // for the rest of the IPv6 network, use /32 groups\n     else"
      }
    ]
  },
  {
    "sha": "e88eb41b6c94d72331ebbce5729d91d5788d14da",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzplODhlYjQxYjZjOTRkNzIzMzFlYmJjZTU3MjlkOTFkNTc4OGQxNGRh",
    "commit": {
      "author": {
        "name": "phelixbtc",
        "email": "github@blockchained.com",
        "date": "2013-10-21T17:09:12Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T13:00:48Z"
      },
      "message": "Porting MinGW multithreading bugfix to makefile.mingw\n\nOriginal fix:\nhttps://github.com/bitcoin/bitcoin/commit\n/8864019f6d88b13d3442843d9e6ebeb8dd938831",
      "tree": {
        "sha": "d854ac8c796eac14339a32e35eeb90106828383d",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/d854ac8c796eac14339a32e35eeb90106828383d"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/e88eb41b6c94d72331ebbce5729d91d5788d14da",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/e88eb41b6c94d72331ebbce5729d91d5788d14da",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/e88eb41b6c94d72331ebbce5729d91d5788d14da",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/e88eb41b6c94d72331ebbce5729d91d5788d14da/comments",
    "author": null,
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "0119e0c592018d5c9e83e301408858ab04b7d674",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/0119e0c592018d5c9e83e301408858ab04b7d674",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/0119e0c592018d5c9e83e301408858ab04b7d674"
      }
    ],
    "stats": {
      "total": 4,
      "additions": 2,
      "deletions": 2
    },
    "files": [
      {
        "sha": "5904d3dd020f632f32a6c89f1697457199c72ad2",
        "filename": "src/makefile.mingw",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e88eb41b6c94d72331ebbce5729d91d5788d14da/src/makefile.mingw",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e88eb41b6c94d72331ebbce5729d91d5788d14da/src/makefile.mingw",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/makefile.mingw?ref=e88eb41b6c94d72331ebbce5729d91d5788d14da",
        "patch": "@@ -43,7 +43,7 @@ LIBS= \\\n  -l ssl \\\n  -l crypto\n \n-DEFS=-DWIN32 -D_WINDOWS -DBOOST_THREAD_USE_LIB -DBOOST_SPIRIT_THREADSAFE\n+DEFS=-D_MT -DWIN32 -D_WINDOWS -DBOOST_THREAD_USE_LIB -DBOOST_SPIRIT_THREADSAFE\n DEBUGFLAGS=-g\n CFLAGS=-mthreads -O2 -w -Wall -Wextra -Wformat -Wformat-security -Wno-unused-parameter $(DEBUGFLAGS) $(DEFS) $(INCLUDEPATHS)\n # enable: ASLR, DEP and large address aware\n@@ -63,7 +63,7 @@ ifneq (${USE_IPV6}, -)\n \tDEFS += -DUSE_IPV6=$(USE_IPV6)\n endif\n \n-LIBS += -l kernel32 -l user32 -l gdi32 -l comdlg32 -l winspool -l winmm -l shell32 -l comctl32 -l ole32 -l oleaut32 -l uuid -l rpcrt4 -l advapi32 -l ws2_32 -l mswsock -l shlwapi\n+LIBS += -l mingwthrd -l kernel32 -l user32 -l gdi32 -l comdlg32 -l winspool -l winmm -l shell32 -l comctl32 -l ole32 -l oleaut32 -l uuid -l rpcrt4 -l advapi32 -l ws2_32 -l mswsock -l shlwapi\n \n # TODO: make the mingw builds smarter about dependencies, like the linux/osx builds are\n HEADERS = $(wildcard *.h)"
      }
    ]
  },
  {
    "sha": "f607729b26813f549284c8e90a8606565e972d12",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpmNjA3NzI5YjI2ODEzZjU0OTI4NGM4ZTkwYTg2MDY1NjVlOTcyZDEy",
    "commit": {
      "author": {
        "name": "Philip Kaufmann",
        "email": "phil.kaufmann@t-online.de",
        "date": "2013-07-24T07:30:09Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T13:00:48Z"
      },
      "message": "exclude CreatePidFile() function on WIN32 as it is unused",
      "tree": {
        "sha": "0aeb0f7d42658c054ac2c337ede0ab4b1f9a63be",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/0aeb0f7d42658c054ac2c337ede0ab4b1f9a63be"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/f607729b26813f549284c8e90a8606565e972d12",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f607729b26813f549284c8e90a8606565e972d12",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/f607729b26813f549284c8e90a8606565e972d12",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f607729b26813f549284c8e90a8606565e972d12/comments",
    "author": null,
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "e88eb41b6c94d72331ebbce5729d91d5788d14da",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/e88eb41b6c94d72331ebbce5729d91d5788d14da",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/e88eb41b6c94d72331ebbce5729d91d5788d14da"
      }
    ],
    "stats": {
      "total": 4,
      "additions": 4,
      "deletions": 0
    },
    "files": [
      {
        "sha": "ea10273f852f90dfc5c80fba9930f72795237050",
        "filename": "src/util.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/f607729b26813f549284c8e90a8606565e972d12/src/util.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/f607729b26813f549284c8e90a8606565e972d12/src/util.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/util.cpp?ref=f607729b26813f549284c8e90a8606565e972d12",
        "patch": "@@ -1118,6 +1118,7 @@ boost::filesystem::path GetPidFile()\n     return pathPidFile;\n }\n \n+#ifndef WIN32\n void CreatePidFile(const boost::filesystem::path &path, pid_t pid)\n {\n     FILE* file = fopen(path.string().c_str(), \"w\");\n@@ -1127,6 +1128,7 @@ void CreatePidFile(const boost::filesystem::path &path, pid_t pid)\n         fclose(file);\n     }\n }\n+#endif\n \n bool RenameOver(boost::filesystem::path src, boost::filesystem::path dest)\n {"
      },
      {
        "sha": "8c6c4bd2050d856b7c9007108bd5957603a15f00",
        "filename": "src/util.h",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/f607729b26813f549284c8e90a8606565e972d12/src/util.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/f607729b26813f549284c8e90a8606565e972d12/src/util.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/util.h?ref=f607729b26813f549284c8e90a8606565e972d12",
        "patch": "@@ -211,7 +211,9 @@ boost::filesystem::path GetDefaultDataDir();\n const boost::filesystem::path &GetDataDir(bool fNetSpecific = true);\n boost::filesystem::path GetConfigFile();\n boost::filesystem::path GetPidFile();\n+#ifndef WIN32\n void CreatePidFile(const boost::filesystem::path &path, pid_t pid);\n+#endif\n void ReadConfigFile(std::map<std::string, std::string>& mapSettingsRet, std::map<std::string, std::vector<std::string> >& mapMultiSettingsRet);\n #ifdef WIN32\n boost::filesystem::path GetSpecialFolderPath(int nFolder, bool fCreate = true);"
      }
    ]
  },
  {
    "sha": "99f73bd2fda6fa27a65d9559f9ba35bf11100f5d",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo5OWY3M2JkMmZkYTZmYTI3YTY1ZDk1NTlmOWJhMzViZjExMTAwZjVk",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2013-06-29T12:16:50Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T13:00:48Z"
      },
      "message": "Fix build date for from-tarball builds",
      "tree": {
        "sha": "3380fc39ac4b52dce4545a912d0f658a0be44c35",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/3380fc39ac4b52dce4545a912d0f658a0be44c35"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/99f73bd2fda6fa27a65d9559f9ba35bf11100f5d",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/99f73bd2fda6fa27a65d9559f9ba35bf11100f5d",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/99f73bd2fda6fa27a65d9559f9ba35bf11100f5d",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/99f73bd2fda6fa27a65d9559f9ba35bf11100f5d/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "f607729b26813f549284c8e90a8606565e972d12",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f607729b26813f549284c8e90a8606565e972d12",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/f607729b26813f549284c8e90a8606565e972d12"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 1,
      "deletions": 1
    },
    "files": [
      {
        "sha": "d9d6724a02962a37772c7cfdaf99687ae51f1dfd",
        "filename": "src/version.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/99f73bd2fda6fa27a65d9559f9ba35bf11100f5d/src/version.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/99f73bd2fda6fa27a65d9559f9ba35bf11100f5d/src/version.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/version.cpp?ref=99f73bd2fda6fa27a65d9559f9ba35bf11100f5d",
        "patch": "@@ -36,7 +36,7 @@ const std::string CLIENT_NAME(\"Satoshi\");\n // git will put \"#define GIT_ARCHIVE 1\" on the next line inside archives. $Format:%n#define GIT_ARCHIVE 1$\n #ifdef GIT_ARCHIVE\n #    define GIT_COMMIT_ID \"$Format:%h$\"\n-#    define GIT_COMMIT_DATE \"$Format:%cD\"\n+#    define GIT_COMMIT_DATE \"$Format:%cD$\"\n #endif\n \n #define BUILD_DESC_FROM_COMMIT(maj,min,rev,build,commit) \\"
      }
    ]
  },
  {
    "sha": "068996daa55e5ac89aaead6afb7b393550ec0933",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzowNjg5OTZkYWE1NWU1YWM4OWFhZWFkNmFmYjdiMzkzNTUwZWMwOTMz",
    "commit": {
      "author": {
        "name": "Cory Fields",
        "email": "theuni-nospam-@xbmc.org",
        "date": "2013-06-05T03:44:53Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T13:00:48Z"
      },
      "message": "osx: fix bitcoin-qt startup crash when clicking dock icon\n\nCrash probably introduced by 4d17a1b0.\nInialize the window to NULL and verify it before use.\n\nRebased-By: Wladimir J. van der Laan <laanwj@gmail.com>",
      "tree": {
        "sha": "29c0d237a9eb335563e2f580a31fe03973bb825f",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/29c0d237a9eb335563e2f580a31fe03973bb825f"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/068996daa55e5ac89aaead6afb7b393550ec0933",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/068996daa55e5ac89aaead6afb7b393550ec0933",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/068996daa55e5ac89aaead6afb7b393550ec0933",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/068996daa55e5ac89aaead6afb7b393550ec0933/comments",
    "author": null,
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "99f73bd2fda6fa27a65d9559f9ba35bf11100f5d",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/99f73bd2fda6fa27a65d9559f9ba35bf11100f5d",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/99f73bd2fda6fa27a65d9559f9ba35bf11100f5d"
      }
    ],
    "stats": {
      "total": 9,
      "additions": 7,
      "deletions": 2
    },
    "files": [
      {
        "sha": "02117536cbdef772e093ab8e492cc60e0df98f01",
        "filename": "src/qt/macdockiconhandler.mm",
        "status": "modified",
        "additions": 7,
        "deletions": 2,
        "changes": 9,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/068996daa55e5ac89aaead6afb7b393550ec0933/src/qt/macdockiconhandler.mm",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/068996daa55e5ac89aaead6afb7b393550ec0933/src/qt/macdockiconhandler.mm",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/qt/macdockiconhandler.mm?ref=068996daa55e5ac89aaead6afb7b393550ec0933",
        "patch": "@@ -52,6 +52,8 @@ - (void)handleDockClickEvent:(NSAppleEventDescriptor*)event withReplyEvent:(NSAp\n     this->m_dummyWidget = new QWidget();\n     this->m_dockMenu = new QMenu(this->m_dummyWidget);\n     qt_mac_set_dock_menu(this->m_dockMenu);\n+    this->setMainWindow(NULL);\n+\n     [pool release];\n }\n \n@@ -100,8 +102,11 @@ - (void)handleDockClickEvent:(NSAppleEventDescriptor*)event withReplyEvent:(NSAp\n \n void MacDockIconHandler::handleDockIconClickEvent()\n {\n-    this->mainWindow->activateWindow();\n-    this->mainWindow->show();\n+    if (this->mainWindow)\n+    {\n+        this->mainWindow->activateWindow();\n+        this->mainWindow->show();\n+    }\n \n     emit this->dockIconClicked();\n }"
      }
    ]
  },
  {
    "sha": "e4be09cfaea25eb47fa0c40b4efef6e4072c933c",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzplNGJlMDljZmFlYTI1ZWI0N2ZhMGM0MGI0ZWZlZjZlNDA3MmM5MzNj",
    "commit": {
      "author": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T09:38:13Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T13:00:48Z"
      },
      "message": "Bump version numbers for 0.8.6 release",
      "tree": {
        "sha": "77a70769c830461500807f0f4e6ead0361ff8f52",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/77a70769c830461500807f0f4e6ead0361ff8f52"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/e4be09cfaea25eb47fa0c40b4efef6e4072c933c",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/e4be09cfaea25eb47fa0c40b4efef6e4072c933c",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/e4be09cfaea25eb47fa0c40b4efef6e4072c933c",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/comments",
    "author": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "068996daa55e5ac89aaead6afb7b393550ec0933",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/068996daa55e5ac89aaead6afb7b393550ec0933",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/068996daa55e5ac89aaead6afb7b393550ec0933"
      }
    ],
    "stats": {
      "total": 21,
      "additions": 11,
      "deletions": 10
    },
    "files": [
      {
        "sha": "69c2478184d040d92f3b727754ae1f583feb7375",
        "filename": "bitcoin-qt.pro",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/bitcoin-qt.pro",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/bitcoin-qt.pro",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/bitcoin-qt.pro?ref=e4be09cfaea25eb47fa0c40b4efef6e4072c933c",
        "patch": "@@ -1,7 +1,7 @@\n TEMPLATE = app\n TARGET = bitcoin-qt\n macx:TARGET = \"Bitcoin-Qt\"\n-VERSION = 0.8.5\n+VERSION = 0.8.6\n INCLUDEPATH += src src/json src/qt\n QT += network\n DEFINES += QT_GUI BOOST_THREAD_USE_LIB BOOST_SPIRIT_THREADSAFE"
      },
      {
        "sha": "cd9d2d0d43f9f55c1f22542034811336b0e88ad8",
        "filename": "contrib/verifysfbinaries/verify.sh",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/contrib/verifysfbinaries/verify.sh",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/contrib/verifysfbinaries/verify.sh",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/contrib/verifysfbinaries/verify.sh?ref=e4be09cfaea25eb47fa0c40b4efef6e4072c933c",
        "patch": "@@ -18,7 +18,7 @@ WORKINGDIR=\"/tmp/bitcoin\"\n TMPFILE=\"hashes.tmp\"\n \n #this URL is used if a version number is not specified as an argument to the script\n-SIGNATUREFILE=\"http://downloads.sourceforge.net/project/bitcoin/Bitcoin/bitcoin-0.8.5/SHA256SUMS.asc\"\n+SIGNATUREFILE=\"http://downloads.sourceforge.net/project/bitcoin/Bitcoin/bitcoin-0.8.6/SHA256SUMS.asc\"\n \n SIGNATUREFILENAME=\"SHA256SUMS.asc\"\n RCSUBDIR=\"test/\""
      },
      {
        "sha": "7040bc6c5d73a355eb48f12288339a11df49a40e",
        "filename": "doc/Doxyfile",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/doc/Doxyfile",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/doc/Doxyfile",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/Doxyfile?ref=e4be09cfaea25eb47fa0c40b4efef6e4072c933c",
        "patch": "@@ -34,7 +34,7 @@ PROJECT_NAME           = Bitcoin\n # This could be handy for archiving the generated documentation or \n # if some version control system is used.\n \n-PROJECT_NUMBER         = 0.5.0\n+PROJECT_NUMBER         = 0.8.6\n \n # Using the PROJECT_BRIEF tag one can provide an optional one line description \n # for a project that appears at the top of each page and should give viewer "
      },
      {
        "sha": "ce094f15b11aa72d5e31685343dfb8f1783a2124",
        "filename": "doc/README.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/doc/README.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/doc/README.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/README.md?ref=e4be09cfaea25eb47fa0c40b4efef6e4072c933c",
        "patch": "@@ -1,4 +1,4 @@\n-Bitcoin 0.8.5 BETA\n+Bitcoin 0.8.6 BETA\n ====================\n \n Copyright (c) 2009-2013 Bitcoin Developers"
      },
      {
        "sha": "f620d8b3451b6f4713c8379dc7be867ff569b2f7",
        "filename": "doc/release-notes.md",
        "status": "modified",
        "additions": 3,
        "deletions": 2,
        "changes": 5,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/doc/release-notes.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/doc/release-notes.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/release-notes.md?ref=e4be09cfaea25eb47fa0c40b4efef6e4072c933c",
        "patch": "@@ -1,7 +1,8 @@\n (note: this is a temporary file, to be added-to by anybody, and deleted at\n release time)\n \n-0.8.5 changes\n+0.8.6 changes\n =============\n \n-Workaround negative version numbers serialization bug.\n+TODO\n+"
      },
      {
        "sha": "ee92feddcaa62049a7dbb8f32666b325bfb88de6",
        "filename": "share/setup.nsi",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/share/setup.nsi",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/share/setup.nsi",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/share/setup.nsi?ref=e4be09cfaea25eb47fa0c40b4efef6e4072c933c",
        "patch": "@@ -5,7 +5,7 @@ SetCompressor /SOLID lzma\n \r\n # General Symbol Definitions\r\n !define REGKEY \"SOFTWARE\\$(^Name)\"\r\n-!define VERSION 0.8.5\r\n+!define VERSION 0.8.6\r\n !define COMPANY \"Bitcoin project\"\r\n !define URL http://www.bitcoin.org/\r\n \r\n@@ -45,13 +45,13 @@ Var StartMenuGroup\n !insertmacro MUI_LANGUAGE English\r\n \r\n # Installer attributes\r\n-OutFile bitcoin-0.8.5-win32-setup.exe\r\n+OutFile bitcoin-0.8.6-win32-setup.exe\r\n InstallDir $PROGRAMFILES\\Bitcoin\r\n CRCCheck on\r\n XPStyle on\r\n BrandingText \" \"\r\n ShowInstDetails show\r\n-VIProductVersion 0.8.5.0\r\n+VIProductVersion 0.8.6.0\r\n VIAddVersionKey ProductName Bitcoin\r\n VIAddVersionKey ProductVersion \"${VERSION}\"\r\n VIAddVersionKey CompanyName \"${COMPANY}\"\r"
      },
      {
        "sha": "2387c8fe41eb373c8029c44af1beb593639aa4f6",
        "filename": "src/clientversion.h",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/src/clientversion.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e4be09cfaea25eb47fa0c40b4efef6e4072c933c/src/clientversion.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/clientversion.h?ref=e4be09cfaea25eb47fa0c40b4efef6e4072c933c",
        "patch": "@@ -8,7 +8,7 @@\n // These need to be macros, as version.cpp's and bitcoin-qt.rc's voodoo requires it\n #define CLIENT_VERSION_MAJOR       0\n #define CLIENT_VERSION_MINOR       8\n-#define CLIENT_VERSION_REVISION    5\n+#define CLIENT_VERSION_REVISION    6\n #define CLIENT_VERSION_BUILD       0\n \n // Set to true for release, false for prerelease or test build"
      }
    ]
  },
  {
    "sha": "83efc9104fa664aae67d152f996f828343cae4a1",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo4M2VmYzkxMDRmYTY2NGFhZTY3ZDE1MmY5OTZmODI4MzQzY2FlNGEx",
    "commit": {
      "author": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-26T11:43:27Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-11-28T01:03:32Z"
      },
      "message": "Squashed 'src/leveldb/' changes from be1b0ff..be91006\n\n936b461 Merge upstream LevelDB 1.13.\n748539c LevelDB 1.13\n\ngit-subtree-dir: src/leveldb\ngit-subtree-split: be9100673b05cec1662a54d0b7a59e4317fdda86",
      "tree": {
        "sha": "e9b70702bf2bd67d5bad5ff319596d18395a3c13",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/e9b70702bf2bd67d5bad5ff319596d18395a3c13"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/83efc9104fa664aae67d152f996f828343cae4a1",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/83efc9104fa664aae67d152f996f828343cae4a1",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/83efc9104fa664aae67d152f996f828343cae4a1",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/83efc9104fa664aae67d152f996f828343cae4a1/comments",
    "author": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "e4be09cfaea25eb47fa0c40b4efef6e4072c933c",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/e4be09cfaea25eb47fa0c40b4efef6e4072c933c",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/e4be09cfaea25eb47fa0c40b4efef6e4072c933c"
      }
    ],
    "stats": {
      "total": 430,
      "additions": 363,
      "deletions": 67
    },
    "files": [
      {
        "sha": "d20a2362c30c49d9cc2b1e2f65a2f829905eb3f8",
        "filename": "db/autocompact_test.cc",
        "status": "added",
        "additions": 118,
        "deletions": 0,
        "changes": 118,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/83efc9104fa664aae67d152f996f828343cae4a1/db/autocompact_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/83efc9104fa664aae67d152f996f828343cae4a1/db/autocompact_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/db/autocompact_test.cc?ref=83efc9104fa664aae67d152f996f828343cae4a1",
        "patch": "@@ -0,0 +1,118 @@\n+// Copyright (c) 2013 The LevelDB Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style license that can be\n+// found in the LICENSE file. See the AUTHORS file for names of contributors.\n+\n+#include \"leveldb/db.h\"\n+#include \"db/db_impl.h\"\n+#include \"leveldb/cache.h\"\n+#include \"util/testharness.h\"\n+#include \"util/testutil.h\"\n+\n+namespace leveldb {\n+\n+class AutoCompactTest {\n+ public:\n+  std::string dbname_;\n+  Cache* tiny_cache_;\n+  Options options_;\n+  DB* db_;\n+\n+  AutoCompactTest() {\n+    dbname_ = test::TmpDir() + \"/autocompact_test\";\n+    tiny_cache_ = NewLRUCache(100);\n+    options_.block_cache = tiny_cache_;\n+    DestroyDB(dbname_, options_);\n+    options_.create_if_missing = true;\n+    options_.compression = kNoCompression;\n+    ASSERT_OK(DB::Open(options_, dbname_, &db_));\n+  }\n+\n+  ~AutoCompactTest() {\n+    delete db_;\n+    DestroyDB(dbname_, Options());\n+    delete tiny_cache_;\n+  }\n+\n+  std::string Key(int i) {\n+    char buf[100];\n+    snprintf(buf, sizeof(buf), \"key%06d\", i);\n+    return std::string(buf);\n+  }\n+\n+  uint64_t Size(const Slice& start, const Slice& limit) {\n+    Range r(start, limit);\n+    uint64_t size;\n+    db_->GetApproximateSizes(&r, 1, &size);\n+    return size;\n+  }\n+\n+  void DoReads(int n);\n+};\n+\n+static const int kValueSize = 200 * 1024;\n+static const int kTotalSize = 100 * 1024 * 1024;\n+static const int kCount = kTotalSize / kValueSize;\n+\n+// Read through the first n keys repeatedly and check that they get\n+// compacted (verified by checking the size of the key space).\n+void AutoCompactTest::DoReads(int n) {\n+  std::string value(kValueSize, 'x');\n+  DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n+\n+  // Fill database\n+  for (int i = 0; i < kCount; i++) {\n+    ASSERT_OK(db_->Put(WriteOptions(), Key(i), value));\n+  }\n+  ASSERT_OK(dbi->TEST_CompactMemTable());\n+\n+  // Delete everything\n+  for (int i = 0; i < kCount; i++) {\n+    ASSERT_OK(db_->Delete(WriteOptions(), Key(i)));\n+  }\n+  ASSERT_OK(dbi->TEST_CompactMemTable());\n+\n+  // Get initial measurement of the space we will be reading.\n+  const int64_t initial_size = Size(Key(0), Key(n));\n+  const int64_t initial_other_size = Size(Key(n), Key(kCount));\n+\n+  // Read until size drops significantly.\n+  std::string limit_key = Key(n);\n+  for (int read = 0; true; read++) {\n+    ASSERT_LT(read, 100) << \"Taking too long to compact\";\n+    Iterator* iter = db_->NewIterator(ReadOptions());\n+    for (iter->SeekToFirst();\n+         iter->Valid() && iter->key().ToString() < limit_key;\n+         iter->Next()) {\n+      // Drop data\n+    }\n+    delete iter;\n+    // Wait a little bit to allow any triggered compactions to complete.\n+    Env::Default()->SleepForMicroseconds(1000000);\n+    uint64_t size = Size(Key(0), Key(n));\n+    fprintf(stderr, \"iter %3d => %7.3f MB [other %7.3f MB]\\n\",\n+            read+1, size/1048576.0, Size(Key(n), Key(kCount))/1048576.0);\n+    if (size <= initial_size/10) {\n+      break;\n+    }\n+  }\n+\n+  // Verify that the size of the key space not touched by the reads\n+  // is pretty much unchanged.\n+  const int64_t final_other_size = Size(Key(n), Key(kCount));\n+  ASSERT_LE(final_other_size, initial_other_size + 1048576);\n+  ASSERT_GE(final_other_size, initial_other_size/5 - 1048576);\n+}\n+\n+TEST(AutoCompactTest, ReadAll) {\n+  DoReads(kCount);\n+}\n+\n+TEST(AutoCompactTest, ReadHalf) {\n+  DoReads(kCount/2);\n+}\n+\n+}  // namespace leveldb\n+\n+int main(int argc, char** argv) {\n+  return leveldb::test::RunAllTests();\n+}"
      },
      {
        "sha": "20c9c4f28751a716e6279940d723e357ba1f088c",
        "filename": "src/leveldb/Makefile",
        "status": "modified",
        "additions": 5,
        "deletions": 1,
        "changes": 6,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/Makefile",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/Makefile",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/Makefile?ref=83efc9104fa664aae67d152f996f828343cae4a1",
        "patch": "@@ -31,6 +31,7 @@ TESTHARNESS = ./util/testharness.o $(TESTUTIL)\n \n TESTS = \\\n \tarena_test \\\n+\tautocompact_test \\\n \tbloom_test \\\n \tc_test \\\n \tcache_test \\\n@@ -70,7 +71,7 @@ SHARED = $(SHARED1)\n else\n # Update db.h if you change these.\n SHARED_MAJOR = 1\n-SHARED_MINOR = 12\n+SHARED_MINOR = 13\n SHARED1 = libleveldb.$(PLATFORM_SHARED_EXT)\n SHARED2 = $(SHARED1).$(SHARED_MAJOR)\n SHARED3 = $(SHARED1).$(SHARED_MAJOR).$(SHARED_MINOR)\n@@ -114,6 +115,9 @@ leveldbutil: db/leveldb_main.o $(LIBOBJECTS)\n arena_test: util/arena_test.o $(LIBOBJECTS) $(TESTHARNESS)\n \t$(CXX) $(LDFLAGS) util/arena_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n \n+autocompact_test: db/autocompact_test.o $(LIBOBJECTS) $(TESTHARNESS)\n+\t$(CXX) $(LDFLAGS) db/autocompact_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n+\n bloom_test: util/bloom_test.o $(LIBOBJECTS) $(TESTHARNESS)\n \t$(CXX) $(LDFLAGS) util/bloom_test.o $(LIBOBJECTS) $(TESTHARNESS) -o $@ $(LIBS)\n "
      },
      {
        "sha": "b37ffdfe645cdb7c65e6f4680776b38c668394a9",
        "filename": "src/leveldb/db/corruption_test.cc",
        "status": "modified",
        "additions": 22,
        "deletions": 29,
        "changes": 51,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/corruption_test.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/corruption_test.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/corruption_test.cc?ref=83efc9104fa664aae67d152f996f828343cae4a1",
        "patch": "@@ -35,6 +35,7 @@ class CorruptionTest {\n   CorruptionTest() {\n     tiny_cache_ = NewLRUCache(100);\n     options_.env = &env_;\n+    options_.block_cache = tiny_cache_;\n     dbname_ = test::TmpDir() + \"/db_test\";\n     DestroyDB(dbname_, options_);\n \n@@ -50,17 +51,14 @@ class CorruptionTest {\n      delete tiny_cache_;\n   }\n \n-  Status TryReopen(Options* options = NULL) {\n+  Status TryReopen() {\n     delete db_;\n     db_ = NULL;\n-    Options opt = (options ? *options : options_);\n-    opt.env = &env_;\n-    opt.block_cache = tiny_cache_;\n-    return DB::Open(opt, dbname_, &db_);\n+    return DB::Open(options_, dbname_, &db_);\n   }\n \n-  void Reopen(Options* options = NULL) {\n-    ASSERT_OK(TryReopen(options));\n+  void Reopen() {\n+    ASSERT_OK(TryReopen());\n   }\n \n   void RepairDB() {\n@@ -92,6 +90,10 @@ class CorruptionTest {\n     for (iter->SeekToFirst(); iter->Valid(); iter->Next()) {\n       uint64_t key;\n       Slice in(iter->key());\n+      if (in == \"\" || in == \"~\") {\n+        // Ignore boundary keys.\n+        continue;\n+      }\n       if (!ConsumeDecimalNumber(&in, &key) ||\n           !in.empty() ||\n           key < next_expected) {\n@@ -233,7 +235,7 @@ TEST(CorruptionTest, TableFile) {\n   dbi->TEST_CompactRange(1, NULL, NULL);\n \n   Corrupt(kTableFile, 100, 1);\n-  Check(99, 99);\n+  Check(90, 99);\n }\n \n TEST(CorruptionTest, TableFileIndexData) {\n@@ -299,40 +301,31 @@ TEST(CorruptionTest, CompactionInputError) {\n   ASSERT_EQ(1, Property(\"leveldb.num-files-at-level\" + NumberToString(last)));\n \n   Corrupt(kTableFile, 100, 1);\n-  Check(9, 9);\n+  Check(5, 9);\n \n   // Force compactions by writing lots of values\n   Build(10000);\n   Check(10000, 10000);\n }\n \n TEST(CorruptionTest, CompactionInputErrorParanoid) {\n-  Options options;\n-  options.paranoid_checks = true;\n-  options.write_buffer_size = 1048576;\n-  Reopen(&options);\n+  options_.paranoid_checks = true;\n+  options_.write_buffer_size = 512 << 10;\n+  Reopen();\n   DBImpl* dbi = reinterpret_cast<DBImpl*>(db_);\n \n-  // Fill levels >= 1 so memtable compaction outputs to level 1\n-  for (int level = 1; level < config::kNumLevels; level++) {\n-    dbi->Put(WriteOptions(), \"\", \"begin\");\n-    dbi->Put(WriteOptions(), \"~\", \"end\");\n+  // Make multiple inputs so we need to compact.\n+  for (int i = 0; i < 2; i++) {\n+    Build(10);\n     dbi->TEST_CompactMemTable();\n+    Corrupt(kTableFile, 100, 1);\n+    env_.SleepForMicroseconds(100000);\n   }\n+  dbi->CompactRange(NULL, NULL);\n \n-  Build(10);\n-  dbi->TEST_CompactMemTable();\n-  ASSERT_EQ(1, Property(\"leveldb.num-files-at-level0\"));\n-\n-  Corrupt(kTableFile, 100, 1);\n-  Check(9, 9);\n-\n-  // Write must eventually fail because of corrupted table\n-  Status s;\n+  // Write must fail because of corrupted table\n   std::string tmp1, tmp2;\n-  for (int i = 0; i < 10000 && s.ok(); i++) {\n-    s = db_->Put(WriteOptions(), Key(i, &tmp1), Value(i, &tmp2));\n-  }\n+  Status s = db_->Put(WriteOptions(), Key(5, &tmp1), Value(5, &tmp2));\n   ASSERT_TRUE(!s.ok()) << \"write did not fail in corrupted paranoid db\";\n }\n "
      },
      {
        "sha": "fa1351038bcb0b1c224dc566fb16d6806966cc62",
        "filename": "src/leveldb/db/db_impl.cc",
        "status": "modified",
        "additions": 27,
        "deletions": 14,
        "changes": 41,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/db_impl.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/db_impl.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_impl.cc?ref=83efc9104fa664aae67d152f996f828343cae4a1",
        "patch": "@@ -113,14 +113,14 @@ Options SanitizeOptions(const std::string& dbname,\n   return result;\n }\n \n-DBImpl::DBImpl(const Options& options, const std::string& dbname)\n-    : env_(options.env),\n-      internal_comparator_(options.comparator),\n-      internal_filter_policy_(options.filter_policy),\n-      options_(SanitizeOptions(\n-          dbname, &internal_comparator_, &internal_filter_policy_, options)),\n-      owns_info_log_(options_.info_log != options.info_log),\n-      owns_cache_(options_.block_cache != options.block_cache),\n+DBImpl::DBImpl(const Options& raw_options, const std::string& dbname)\n+    : env_(raw_options.env),\n+      internal_comparator_(raw_options.comparator),\n+      internal_filter_policy_(raw_options.filter_policy),\n+      options_(SanitizeOptions(dbname, &internal_comparator_,\n+                               &internal_filter_policy_, raw_options)),\n+      owns_info_log_(options_.info_log != raw_options.info_log),\n+      owns_cache_(options_.block_cache != raw_options.block_cache),\n       dbname_(dbname),\n       db_lock_(NULL),\n       shutting_down_(NULL),\n@@ -130,6 +130,7 @@ DBImpl::DBImpl(const Options& options, const std::string& dbname)\n       logfile_(NULL),\n       logfile_number_(0),\n       log_(NULL),\n+      seed_(0),\n       tmp_batch_(new WriteBatch),\n       bg_compaction_scheduled_(false),\n       manual_compaction_(NULL),\n@@ -138,7 +139,7 @@ DBImpl::DBImpl(const Options& options, const std::string& dbname)\n   has_imm_.Release_Store(NULL);\n \n   // Reserve ten files or so for other uses and give the rest to TableCache.\n-  const int table_cache_size = options.max_open_files - kNumNonTableCacheFiles;\n+  const int table_cache_size = options_.max_open_files - kNumNonTableCacheFiles;\n   table_cache_ = new TableCache(dbname_, &options_, table_cache_size);\n \n   versions_ = new VersionSet(dbname_, &options_, table_cache_,\n@@ -1027,7 +1028,8 @@ static void CleanupIteratorState(void* arg1, void* arg2) {\n }  // namespace\n \n Iterator* DBImpl::NewInternalIterator(const ReadOptions& options,\n-                                      SequenceNumber* latest_snapshot) {\n+                                      SequenceNumber* latest_snapshot,\n+                                      uint32_t* seed) {\n   IterState* cleanup = new IterState;\n   mutex_.Lock();\n   *latest_snapshot = versions_->LastSequence();\n@@ -1051,13 +1053,15 @@ Iterator* DBImpl::NewInternalIterator(const ReadOptions& options,\n   cleanup->version = versions_->current();\n   internal_iter->RegisterCleanup(CleanupIteratorState, cleanup, NULL);\n \n+  *seed = ++seed_;\n   mutex_.Unlock();\n   return internal_iter;\n }\n \n Iterator* DBImpl::TEST_NewInternalIterator() {\n   SequenceNumber ignored;\n-  return NewInternalIterator(ReadOptions(), &ignored);\n+  uint32_t ignored_seed;\n+  return NewInternalIterator(ReadOptions(), &ignored, &ignored_seed);\n }\n \n int64_t DBImpl::TEST_MaxNextLevelOverlappingBytes() {\n@@ -1114,12 +1118,21 @@ Status DBImpl::Get(const ReadOptions& options,\n \n Iterator* DBImpl::NewIterator(const ReadOptions& options) {\n   SequenceNumber latest_snapshot;\n-  Iterator* internal_iter = NewInternalIterator(options, &latest_snapshot);\n+  uint32_t seed;\n+  Iterator* iter = NewInternalIterator(options, &latest_snapshot, &seed);\n   return NewDBIterator(\n-      &dbname_, env_, user_comparator(), internal_iter,\n+      this, user_comparator(), iter,\n       (options.snapshot != NULL\n        ? reinterpret_cast<const SnapshotImpl*>(options.snapshot)->number_\n-       : latest_snapshot));\n+       : latest_snapshot),\n+      seed);\n+}\n+\n+void DBImpl::RecordReadSample(Slice key) {\n+  MutexLock l(&mutex_);\n+  if (versions_->current()->RecordReadSample(key)) {\n+    MaybeScheduleCompaction();\n+  }\n }\n \n const Snapshot* DBImpl::GetSnapshot() {"
      },
      {
        "sha": "75fd30abe9a56bd663537baa20b2104b349f645f",
        "filename": "src/leveldb/db/db_impl.h",
        "status": "modified",
        "additions": 8,
        "deletions": 1,
        "changes": 9,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/db_impl.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/db_impl.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_impl.h?ref=83efc9104fa664aae67d152f996f828343cae4a1",
        "patch": "@@ -59,13 +59,19 @@ class DBImpl : public DB {\n   // file at a level >= 1.\n   int64_t TEST_MaxNextLevelOverlappingBytes();\n \n+  // Record a sample of bytes read at the specified internal key.\n+  // Samples are taken approximately once every config::kReadBytesPeriod\n+  // bytes.\n+  void RecordReadSample(Slice key);\n+\n  private:\n   friend class DB;\n   struct CompactionState;\n   struct Writer;\n \n   Iterator* NewInternalIterator(const ReadOptions&,\n-                                SequenceNumber* latest_snapshot);\n+                                SequenceNumber* latest_snapshot,\n+                                uint32_t* seed);\n \n   Status NewDB();\n \n@@ -135,6 +141,7 @@ class DBImpl : public DB {\n   WritableFile* logfile_;\n   uint64_t logfile_number_;\n   log::Writer* log_;\n+  uint32_t seed_;                // For sampling.\n \n   // Queue of writers.\n   std::deque<Writer*> writers_;"
      },
      {
        "sha": "071a54e3f457a02d726b3c0f53212a92ddf0c8de",
        "filename": "src/leveldb/db/db_iter.cc",
        "status": "modified",
        "additions": 29,
        "deletions": 12,
        "changes": 41,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/db_iter.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/db_iter.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_iter.cc?ref=83efc9104fa664aae67d152f996f828343cae4a1",
        "patch": "@@ -5,12 +5,14 @@\n #include \"db/db_iter.h\"\n \n #include \"db/filename.h\"\n+#include \"db/db_impl.h\"\n #include \"db/dbformat.h\"\n #include \"leveldb/env.h\"\n #include \"leveldb/iterator.h\"\n #include \"port/port.h\"\n #include \"util/logging.h\"\n #include \"util/mutexlock.h\"\n+#include \"util/random.h\"\n \n namespace leveldb {\n \n@@ -46,15 +48,16 @@ class DBIter: public Iterator {\n     kReverse\n   };\n \n-  DBIter(const std::string* dbname, Env* env,\n-         const Comparator* cmp, Iterator* iter, SequenceNumber s)\n-      : dbname_(dbname),\n-        env_(env),\n+  DBIter(DBImpl* db, const Comparator* cmp, Iterator* iter, SequenceNumber s,\n+         uint32_t seed)\n+      : db_(db),\n         user_comparator_(cmp),\n         iter_(iter),\n         sequence_(s),\n         direction_(kForward),\n-        valid_(false) {\n+        valid_(false),\n+        rnd_(seed),\n+        bytes_counter_(RandomPeriod()) {\n   }\n   virtual ~DBIter() {\n     delete iter_;\n@@ -100,8 +103,12 @@ class DBIter: public Iterator {\n     }\n   }\n \n-  const std::string* const dbname_;\n-  Env* const env_;\n+  // Pick next gap with average value of config::kReadBytesPeriod.\n+  ssize_t RandomPeriod() {\n+    return rnd_.Uniform(2*config::kReadBytesPeriod);\n+  }\n+\n+  DBImpl* db_;\n   const Comparator* const user_comparator_;\n   Iterator* const iter_;\n   SequenceNumber const sequence_;\n@@ -112,13 +119,23 @@ class DBIter: public Iterator {\n   Direction direction_;\n   bool valid_;\n \n+  Random rnd_;\n+  ssize_t bytes_counter_;\n+\n   // No copying allowed\n   DBIter(const DBIter&);\n   void operator=(const DBIter&);\n };\n \n inline bool DBIter::ParseKey(ParsedInternalKey* ikey) {\n-  if (!ParseInternalKey(iter_->key(), ikey)) {\n+  Slice k = iter_->key();\n+  ssize_t n = k.size() + iter_->value().size();\n+  bytes_counter_ -= n;\n+  while (bytes_counter_ < 0) {\n+    bytes_counter_ += RandomPeriod();\n+    db_->RecordReadSample(k);\n+  }\n+  if (!ParseInternalKey(k, ikey)) {\n     status_ = Status::Corruption(\"corrupted internal key in DBIter\");\n     return false;\n   } else {\n@@ -288,12 +305,12 @@ void DBIter::SeekToLast() {\n }  // anonymous namespace\n \n Iterator* NewDBIterator(\n-    const std::string* dbname,\n-    Env* env,\n+    DBImpl* db,\n     const Comparator* user_key_comparator,\n     Iterator* internal_iter,\n-    const SequenceNumber& sequence) {\n-  return new DBIter(dbname, env, user_key_comparator, internal_iter, sequence);\n+    SequenceNumber sequence,\n+    uint32_t seed) {\n+  return new DBIter(db, user_key_comparator, internal_iter, sequence, seed);\n }\n \n }  // namespace leveldb"
      },
      {
        "sha": "04927e937badff3b29592ec9266d305c44b85b4c",
        "filename": "src/leveldb/db/db_iter.h",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/db_iter.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/db_iter.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/db_iter.h?ref=83efc9104fa664aae67d152f996f828343cae4a1",
        "patch": "@@ -11,15 +11,17 @@\n \n namespace leveldb {\n \n+class DBImpl;\n+\n // Return a new iterator that converts internal keys (yielded by\n // \"*internal_iter\") that were live at the specified \"sequence\" number\n // into appropriate user keys.\n extern Iterator* NewDBIterator(\n-    const std::string* dbname,\n-    Env* env,\n+    DBImpl* db,\n     const Comparator* user_key_comparator,\n     Iterator* internal_iter,\n-    const SequenceNumber& sequence);\n+    SequenceNumber sequence,\n+    uint32_t seed);\n \n }  // namespace leveldb\n "
      },
      {
        "sha": "5d8a032bd3166d2449cedad45305a123b2c6b180",
        "filename": "src/leveldb/db/dbformat.h",
        "status": "modified",
        "additions": 3,
        "deletions": 0,
        "changes": 3,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/dbformat.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/dbformat.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/dbformat.h?ref=83efc9104fa664aae67d152f996f828343cae4a1",
        "patch": "@@ -38,6 +38,9 @@ static const int kL0_StopWritesTrigger = 12;\n // space if the same key space is being repeatedly overwritten.\n static const int kMaxMemCompactLevel = 2;\n \n+// Approximate gap in bytes between samples of data read during iteration.\n+static const int kReadBytesPeriod = 1048576;\n+\n }  // namespace config\n \n class InternalKey;"
      },
      {
        "sha": "66d73be71fbbe1b2af4ba4b1460ee7dcb62f1b96",
        "filename": "src/leveldb/db/version_set.cc",
        "status": "modified",
        "additions": 92,
        "deletions": 4,
        "changes": 96,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/version_set.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/version_set.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_set.cc?ref=83efc9104fa664aae67d152f996f828343cae4a1",
        "patch": "@@ -289,6 +289,51 @@ static bool NewestFirst(FileMetaData* a, FileMetaData* b) {\n   return a->number > b->number;\n }\n \n+void Version::ForEachOverlapping(Slice user_key, Slice internal_key,\n+                                 void* arg,\n+                                 bool (*func)(void*, int, FileMetaData*)) {\n+  // TODO(sanjay): Change Version::Get() to use this function.\n+  const Comparator* ucmp = vset_->icmp_.user_comparator();\n+\n+  // Search level-0 in order from newest to oldest.\n+  std::vector<FileMetaData*> tmp;\n+  tmp.reserve(files_[0].size());\n+  for (uint32_t i = 0; i < files_[0].size(); i++) {\n+    FileMetaData* f = files_[0][i];\n+    if (ucmp->Compare(user_key, f->smallest.user_key()) >= 0 &&\n+        ucmp->Compare(user_key, f->largest.user_key()) <= 0) {\n+      tmp.push_back(f);\n+    }\n+  }\n+  if (!tmp.empty()) {\n+    std::sort(tmp.begin(), tmp.end(), NewestFirst);\n+    for (uint32_t i = 0; i < tmp.size(); i++) {\n+      if (!(*func)(arg, 0, tmp[i])) {\n+        return;\n+      }\n+    }\n+  }\n+\n+  // Search other levels.\n+  for (int level = 1; level < config::kNumLevels; level++) {\n+    size_t num_files = files_[level].size();\n+    if (num_files == 0) continue;\n+\n+    // Binary search to find earliest index whose largest key >= internal_key.\n+    uint32_t index = FindFile(vset_->icmp_, files_[level], internal_key);\n+    if (index < num_files) {\n+      FileMetaData* f = files_[level][index];\n+      if (ucmp->Compare(user_key, f->smallest.user_key()) < 0) {\n+        // All of \"f\" is past any data for user_key\n+      } else {\n+        if (!(*func)(arg, level, f)) {\n+          return;\n+        }\n+      }\n+    }\n+  }\n+}\n+\n Status Version::Get(const ReadOptions& options,\n                     const LookupKey& k,\n                     std::string* value,\n@@ -401,6 +446,44 @@ bool Version::UpdateStats(const GetStats& stats) {\n   return false;\n }\n \n+bool Version::RecordReadSample(Slice internal_key) {\n+  ParsedInternalKey ikey;\n+  if (!ParseInternalKey(internal_key, &ikey)) {\n+    return false;\n+  }\n+\n+  struct State {\n+    GetStats stats;  // Holds first matching file\n+    int matches;\n+\n+    static bool Match(void* arg, int level, FileMetaData* f) {\n+      State* state = reinterpret_cast<State*>(arg);\n+      state->matches++;\n+      if (state->matches == 1) {\n+        // Remember first match.\n+        state->stats.seek_file = f;\n+        state->stats.seek_file_level = level;\n+      }\n+      // We can stop iterating once we have a second match.\n+      return state->matches < 2;\n+    }\n+  };\n+\n+  State state;\n+  state.matches = 0;\n+  ForEachOverlapping(ikey.user_key, internal_key, &state, &State::Match);\n+\n+  // Must have at least two matches since we want to merge across\n+  // files. But what if we have a single file that contains many\n+  // overwrites and deletions?  Should we have another mechanism for\n+  // finding such files?\n+  if (state.matches >= 2) {\n+    // 1MB cost is about 1 seek (see comment in Builder::Apply).\n+    return UpdateStats(state.stats);\n+  }\n+  return false;\n+}\n+\n void Version::Ref() {\n   ++refs_;\n }\n@@ -435,10 +518,13 @@ int Version::PickLevelForMemTableOutput(\n       if (OverlapInLevel(level + 1, &smallest_user_key, &largest_user_key)) {\n         break;\n       }\n-      GetOverlappingInputs(level + 2, &start, &limit, &overlaps);\n-      const int64_t sum = TotalFileSize(overlaps);\n-      if (sum > kMaxGrandParentOverlapBytes) {\n-        break;\n+      if (level + 2 < config::kNumLevels) {\n+        // Check that file does not overlap too many grandparent bytes.\n+        GetOverlappingInputs(level + 2, &start, &limit, &overlaps);\n+        const int64_t sum = TotalFileSize(overlaps);\n+        if (sum > kMaxGrandParentOverlapBytes) {\n+          break;\n+        }\n       }\n       level++;\n     }\n@@ -452,6 +538,8 @@ void Version::GetOverlappingInputs(\n     const InternalKey* begin,\n     const InternalKey* end,\n     std::vector<FileMetaData*>* inputs) {\n+  assert(level >= 0);\n+  assert(level < config::kNumLevels);\n   inputs->clear();\n   Slice user_begin, user_end;\n   if (begin != NULL) {"
      },
      {
        "sha": "20de0e262914cbd538879d55101c5639eb28e1c3",
        "filename": "src/leveldb/db/version_set.h",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/version_set.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/db/version_set.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/db/version_set.h?ref=83efc9104fa664aae67d152f996f828343cae4a1",
        "patch": "@@ -78,6 +78,12 @@ class Version {\n   // REQUIRES: lock is held\n   bool UpdateStats(const GetStats& stats);\n \n+  // Record a sample of bytes read at the specified internal key.\n+  // Samples are taken approximately once every config::kReadBytesPeriod\n+  // bytes.  Returns true if a new compaction may need to be triggered.\n+  // REQUIRES: lock is held\n+  bool RecordReadSample(Slice key);\n+\n   // Reference count management (so Versions do not disappear out from\n   // under live iterators)\n   void Ref();\n@@ -114,6 +120,15 @@ class Version {\n   class LevelFileNumIterator;\n   Iterator* NewConcatenatingIterator(const ReadOptions&, int level) const;\n \n+  // Call func(arg, level, f) for every file that overlaps user_key in\n+  // order from newest to oldest.  If an invocation of func returns\n+  // false, makes no more calls.\n+  //\n+  // REQUIRES: user portion of internal_key == user_key.\n+  void ForEachOverlapping(Slice user_key, Slice internal_key,\n+                          void* arg,\n+                          bool (*func)(void*, int, FileMetaData*));\n+\n   VersionSet* vset_;            // VersionSet to which this Version belongs\n   Version* next_;               // Next version in linked list\n   Version* prev_;               // Previous version in linked list"
      },
      {
        "sha": "57c00a5da005881fa2ad0763b0787b20c6b411c1",
        "filename": "src/leveldb/include/leveldb/db.h",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/include/leveldb/db.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/include/leveldb/db.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/include/leveldb/db.h?ref=83efc9104fa664aae67d152f996f828343cae4a1",
        "patch": "@@ -14,7 +14,7 @@ namespace leveldb {\n \n // Update Makefile if you change these\n static const int kMajorVersion = 1;\n-static const int kMinorVersion = 12;\n+static const int kMinorVersion = 13;\n \n struct Options;\n struct ReadOptions;"
      },
      {
        "sha": "0f5dcfac5a4baf51831534b74224c33585b35326",
        "filename": "src/leveldb/util/env_posix.cc",
        "status": "modified",
        "additions": 32,
        "deletions": 1,
        "changes": 33,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/util/env_posix.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/util/env_posix.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/env_posix.cc?ref=83efc9104fa664aae67d152f996f828343cae4a1",
        "patch": "@@ -320,8 +320,39 @@ class PosixMmapFile : public WritableFile {\n     return Status::OK();\n   }\n \n-  virtual Status Sync() {\n+  Status SyncDirIfManifest() {\n+    const char* f = filename_.c_str();\n+    const char* sep = strrchr(f, '/');\n+    Slice basename;\n+    std::string dir;\n+    if (sep == NULL) {\n+      dir = \".\";\n+      basename = f;\n+    } else {\n+      dir = std::string(f, sep - f);\n+      basename = sep + 1;\n+    }\n     Status s;\n+    if (basename.starts_with(\"MANIFEST\")) {\n+      int fd = open(dir.c_str(), O_RDONLY);\n+      if (fd < 0) {\n+        s = IOError(dir, errno);\n+      } else {\n+        if (fsync(fd) < 0) {\n+          s = IOError(dir, errno);\n+        }\n+        close(fd);\n+      }\n+    }\n+    return s;\n+  }\n+\n+  virtual Status Sync() {\n+    // Ensure new files referred to by the manifest are in the filesystem.\n+    Status s = SyncDirIfManifest();\n+    if (!s.ok()) {\n+      return s;\n+    }\n \n     if (pending_sync_) {\n       // Some unmapped data was not synced"
      },
      {
        "sha": "ddd51b1c7b51743ea34fcfa14a9dec61b38ff577",
        "filename": "src/leveldb/util/random.h",
        "status": "modified",
        "additions": 6,
        "deletions": 1,
        "changes": 7,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/util/random.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/83efc9104fa664aae67d152f996f828343cae4a1/src/leveldb/util/random.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/random.h?ref=83efc9104fa664aae67d152f996f828343cae4a1",
        "patch": "@@ -16,7 +16,12 @@ class Random {\n  private:\n   uint32_t seed_;\n  public:\n-  explicit Random(uint32_t s) : seed_(s & 0x7fffffffu) { }\n+  explicit Random(uint32_t s) : seed_(s & 0x7fffffffu) {\n+    // Avoid bad seeds.\n+    if (seed_ == 0 || seed_ == 2147483647L) {\n+      seed_ = 1;\n+    }\n+  }\n   uint32_t Next() {\n     static const uint32_t M = 2147483647L;   // 2^31-1\n     static const uint64_t A = 16807;  // bits 14, 8, 7, 5, 2, 1, 0"
      }
    ]
  },
  {
    "sha": "e564297156b9103c5af85f6b444df0bdc2476397",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzplNTY0Mjk3MTU2YjkxMDNjNWFmODVmNmI0NDRkZjBiZGMyNDc2Mzk3",
    "commit": {
      "author": {
        "name": "Warren Togami",
        "email": "wtogami@gmail.com",
        "date": "2013-09-08T07:17:27Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-11-28T01:03:33Z"
      },
      "message": "Silence useless warning in src/json/json_spirit_writer_template.h to make important warnings easier to see.\n\nwarning: typedef \u2018Char_type\u2019 locally defined but not used [-Wunused-local-typedefs]",
      "tree": {
        "sha": "1eebee341dd751c41c89b123c12358d5620e40bb",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/1eebee341dd751c41c89b123c12358d5620e40bb"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/e564297156b9103c5af85f6b444df0bdc2476397",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/e564297156b9103c5af85f6b444df0bdc2476397",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/e564297156b9103c5af85f6b444df0bdc2476397",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/e564297156b9103c5af85f6b444df0bdc2476397/comments",
    "author": {
      "login": "wtogami",
      "id": 93665,
      "node_id": "MDQ6VXNlcjkzNjY1",
      "avatar_url": "https://avatars.githubusercontent.com/u/93665?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/wtogami",
      "html_url": "https://github.com/wtogami",
      "followers_url": "https://api.github.com/users/wtogami/followers",
      "following_url": "https://api.github.com/users/wtogami/following{/other_user}",
      "gists_url": "https://api.github.com/users/wtogami/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/wtogami/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/wtogami/subscriptions",
      "organizations_url": "https://api.github.com/users/wtogami/orgs",
      "repos_url": "https://api.github.com/users/wtogami/repos",
      "events_url": "https://api.github.com/users/wtogami/events{/privacy}",
      "received_events_url": "https://api.github.com/users/wtogami/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "83efc9104fa664aae67d152f996f828343cae4a1",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/83efc9104fa664aae67d152f996f828343cae4a1",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/83efc9104fa664aae67d152f996f828343cae4a1"
      }
    ],
    "stats": {
      "total": 3,
      "additions": 2,
      "deletions": 1
    },
    "files": [
      {
        "sha": "6b4978a1ff2480e9274a62b364d9926e577c3fb9",
        "filename": "src/json/json_spirit_writer_template.h",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/e564297156b9103c5af85f6b444df0bdc2476397/src/json/json_spirit_writer_template.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/e564297156b9103c5af85f6b444df0bdc2476397/src/json/json_spirit_writer_template.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/json/json_spirit_writer_template.h?ref=e564297156b9103c5af85f6b444df0bdc2476397",
        "patch": "@@ -28,7 +28,8 @@ namespace json_spirit\n     template< class String_type >\n     String_type non_printable_to_string( unsigned int c )\n     {\n-        typedef typename String_type::value_type Char_type;\n+        // Silence the warning: typedef \u2018Char_type\u2019 locally defined but not used [-Wunused-local-typedefs]\n+        // typedef typename String_type::value_type Char_type;\n \n         String_type result( 6, '\\\\' );\n "
      }
    ]
  },
  {
    "sha": "56ce843314f542bcfdc10fa8367aeeb08cdb5c4c",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo1NmNlODQzMzE0ZjU0MmJjZmRjMTBmYTgzNjdhZWViMDhjZGI1YzRj",
    "commit": {
      "author": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-11-01T19:27:42Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-11-28T01:03:33Z"
      },
      "message": "Refactor: pull alert string sanitization into util\n\nRebased-from: 17faf562629cd27f00fc138e218ebcc1ce071765",
      "tree": {
        "sha": "97880b88babcd72a3df88cd9b94451e9dd501495",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/97880b88babcd72a3df88cd9b94451e9dd501495"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/56ce843314f542bcfdc10fa8367aeeb08cdb5c4c",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/56ce843314f542bcfdc10fa8367aeeb08cdb5c4c",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/56ce843314f542bcfdc10fa8367aeeb08cdb5c4c",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/56ce843314f542bcfdc10fa8367aeeb08cdb5c4c/comments",
    "author": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "e564297156b9103c5af85f6b444df0bdc2476397",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/e564297156b9103c5af85f6b444df0bdc2476397",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/e564297156b9103c5af85f6b444df0bdc2476397"
      }
    ],
    "stats": {
      "total": 24,
      "additions": 15,
      "deletions": 9
    },
    "files": [
      {
        "sha": "8eb9451a7d72f114368ab82c939ee8720b1ca23a",
        "filename": "src/alert.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 9,
        "changes": 10,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/56ce843314f542bcfdc10fa8367aeeb08cdb5c4c/src/alert.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/56ce843314f542bcfdc10fa8367aeeb08cdb5c4c/src/alert.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/alert.cpp?ref=56ce843314f542bcfdc10fa8367aeeb08cdb5c4c",
        "patch": "@@ -243,15 +243,7 @@ bool CAlert::ProcessAlert(bool fThread)\n                 // be safe we first strip anything not in safeChars, then add single quotes around\n                 // the whole string before passing it to the shell:\n                 std::string singleQuote(\"'\");\n-                // safeChars chosen to allow simple messages/URLs/email addresses, but avoid anything\n-                // even possibly remotely dangerous like & or >\n-                std::string safeChars(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ01234567890 .,;_/:?@\");\n-                std::string safeStatus;\n-                for (std::string::size_type i = 0; i < strStatusBar.size(); i++)\n-                {\n-                    if (safeChars.find(strStatusBar[i]) != std::string::npos)\n-                        safeStatus.push_back(strStatusBar[i]);\n-                }\n+                std::string safeStatus = SanitizeString(strStatusBar);\n                 safeStatus = singleQuote+safeStatus+singleQuote;\n                 boost::replace_all(strCmd, \"%s\", safeStatus);\n "
      },
      {
        "sha": "56d558d7af67c2be9d640c05e2e84c8d129209ad",
        "filename": "src/util.cpp",
        "status": "modified",
        "additions": 13,
        "deletions": 0,
        "changes": 13,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/56ce843314f542bcfdc10fa8367aeeb08cdb5c4c/src/util.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/56ce843314f542bcfdc10fa8367aeeb08cdb5c4c/src/util.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/util.cpp?ref=56ce843314f542bcfdc10fa8367aeeb08cdb5c4c",
        "patch": "@@ -454,6 +454,19 @@ bool ParseMoney(const char* pszIn, int64& nRet)\n     return true;\n }\n \n+// safeChars chosen to allow simple messages/URLs/email addresses, but avoid anything\n+// even possibly remotely dangerous like & or >\n+static string safeChars(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ01234567890 .,;_/:?@\");\n+string SanitizeString(const string& str)\n+{\n+    string strResult;\n+    for (std::string::size_type i = 0; i < str.size(); i++)\n+    {\n+        if (safeChars.find(str[i]) != std::string::npos)\n+            strResult.push_back(str[i]);\n+    }\n+    return strResult;\n+}\n \n static const signed char phexdigit[256] =\n { -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,"
      },
      {
        "sha": "a190a3b5da85e2d212e6cda063f5eb05a4e89f22",
        "filename": "src/util.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/56ce843314f542bcfdc10fa8367aeeb08cdb5c4c/src/util.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/56ce843314f542bcfdc10fa8367aeeb08cdb5c4c/src/util.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/util.h?ref=56ce843314f542bcfdc10fa8367aeeb08cdb5c4c",
        "patch": "@@ -187,6 +187,7 @@ void ParseString(const std::string& str, char c, std::vector<std::string>& v);\n std::string FormatMoney(int64 n, bool fPlus=false);\n bool ParseMoney(const std::string& str, int64& nRet);\n bool ParseMoney(const char* pszIn, int64& nRet);\n+std::string SanitizeString(const std::string& str);\n std::vector<unsigned char> ParseHex(const char* psz);\n std::vector<unsigned char> ParseHex(const std::string& str);\n bool IsHex(const std::string& str);"
      }
    ]
  },
  {
    "sha": "3779de9d8916644f9b235da68cb0d6f63a173d13",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzozNzc5ZGU5ZDg5MTY2NDRmOWIyMzVkYTY4Y2IwZDZmNjNhMTczZDEz",
    "commit": {
      "author": {
        "name": "Philip Kaufmann",
        "email": "phil.kaufmann@t-online.de",
        "date": "2013-10-01T14:26:42Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-11-28T01:03:33Z"
      },
      "message": "special case DoS value == 0 in ProcessMessage()\n\n- prevents unneeded log messages, which could make users think something\n  bad was happening\n\nSquashed: style-police code cleanup",
      "tree": {
        "sha": "7919a0a139d8aa63b1c809678a2266140f971e4a",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/7919a0a139d8aa63b1c809678a2266140f971e4a"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/3779de9d8916644f9b235da68cb0d6f63a173d13",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/3779de9d8916644f9b235da68cb0d6f63a173d13",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/3779de9d8916644f9b235da68cb0d6f63a173d13",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/3779de9d8916644f9b235da68cb0d6f63a173d13/comments",
    "author": null,
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "56ce843314f542bcfdc10fa8367aeeb08cdb5c4c",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/56ce843314f542bcfdc10fa8367aeeb08cdb5c4c",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/56ce843314f542bcfdc10fa8367aeeb08cdb5c4c"
      }
    ],
    "stats": {
      "total": 10,
      "additions": 6,
      "deletions": 4
    },
    "files": [
      {
        "sha": "fa7372d69fcba333d7baac49586b8216e0af3470",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 6,
        "deletions": 4,
        "changes": 10,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/3779de9d8916644f9b235da68cb0d6f63a173d13/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/3779de9d8916644f9b235da68cb0d6f63a173d13/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=3779de9d8916644f9b235da68cb0d6f63a173d13",
        "patch": "@@ -3581,9 +3581,10 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n             if (nEvicted > 0)\n                 printf(\"mapOrphan overflow, removed %u tx\\n\", nEvicted);\n         }\n-        int nDoS;\n+        int nDoS = 0;\n         if (state.IsInvalid(nDoS))\n-            pfrom->Misbehaving(nDoS);\n+            if (nDoS > 0)\n+                pfrom->Misbehaving(nDoS);\n     }\n \n \n@@ -3601,9 +3602,10 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n         CValidationState state;\n         if (ProcessBlock(state, pfrom, &block) || state.CorruptionPossible())\n             mapAlreadyAskedFor.erase(inv);\n-        int nDoS;\n+        int nDoS = 0;\n         if (state.IsInvalid(nDoS))\n-            pfrom->Misbehaving(nDoS);\n+            if (nDoS > 0)\n+                pfrom->Misbehaving(nDoS);\n     }\n \n "
      }
    ]
  },
  {
    "sha": "1c53d0a4fbdc1efd9ed316556a12fb52082db32e",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzoxYzUzZDBhNGZiZGMxZWZkOWVkMzE2NTU2YTEyZmI1MjA4MmRiMzJl",
    "commit": {
      "author": {
        "name": "Mike Hearn",
        "email": "hearn@google.com",
        "date": "2013-11-21T13:38:29Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-11-28T01:03:33Z"
      },
      "message": "Add some additional logging to give extra network insight.",
      "tree": {
        "sha": "4e46f29dfe0b5b579d9f73409ab958c69e74c39a",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/4e46f29dfe0b5b579d9f73409ab958c69e74c39a"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/1c53d0a4fbdc1efd9ed316556a12fb52082db32e",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1c53d0a4fbdc1efd9ed316556a12fb52082db32e",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/1c53d0a4fbdc1efd9ed316556a12fb52082db32e",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1c53d0a4fbdc1efd9ed316556a12fb52082db32e/comments",
    "author": null,
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "3779de9d8916644f9b235da68cb0d6f63a173d13",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/3779de9d8916644f9b235da68cb0d6f63a173d13",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/3779de9d8916644f9b235da68cb0d6f63a173d13"
      }
    ],
    "stats": {
      "total": 14,
      "additions": 10,
      "deletions": 4
    },
    "files": [
      {
        "sha": "b0a6e36236743e2002b6d5ff93e03a5db6c3829e",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 10,
        "deletions": 4,
        "changes": 14,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/1c53d0a4fbdc1efd9ed316556a12fb52082db32e/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/1c53d0a4fbdc1efd9ed316556a12fb52082db32e/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=1c53d0a4fbdc1efd9ed316556a12fb52082db32e",
        "patch": "@@ -818,9 +818,6 @@ bool CTxMemPool::accept(CValidationState &state, CTransaction &tx, bool fCheckIn\n         EraseFromWallets(ptxOld->GetHash());\n     SyncWithWallets(hash, tx, NULL, true);\n \n-    printf(\"CTxMemPool::accept() : accepted %s (poolsz %\"PRIszu\")\\n\",\n-           hash.ToString().c_str(),\n-           mapTx.size());\n     return true;\n }\n \n@@ -3288,7 +3285,7 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n \n         pfrom->fSuccessfullyConnected = true;\n \n-        printf(\"receive version message: version %d, blocks=%d, us=%s, them=%s, peer=%s\\n\", pfrom->nVersion, pfrom->nStartingHeight, addrMe.ToString().c_str(), addrFrom.ToString().c_str(), pfrom->addr.ToString().c_str());\n+        printf(\"receive version message: %s: version %d, blocks=%d, us=%s, them=%s, peer=%s\\n\", pfrom->strSubVer.c_str(), pfrom->nVersion, pfrom->nStartingHeight, addrMe.ToString().c_str(), addrFrom.ToString().c_str(), pfrom->addr.ToString().c_str());\n \n         cPeerBlockCounts.input(pfrom->nStartingHeight);\n     }\n@@ -3536,6 +3533,11 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n             vWorkQueue.push_back(inv.hash);\n             vEraseQueue.push_back(inv.hash);\n \n+            printf(\"AcceptToMemoryPool: %s %s : accepted %s (poolsz %\"PRIszu\")\\n\",\n+                pfrom->addr.ToString().c_str(), pfrom->strSubVer.c_str(),\n+                tx.GetHash().ToString().c_str(),\n+                mempool.mapTx.size());\n+\n             // Recursively process any orphan transactions that depended on this one\n             for (unsigned int i = 0; i < vWorkQueue.size(); i++)\n             {\n@@ -3583,8 +3585,12 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n         }\n         int nDoS = 0;\n         if (state.IsInvalid(nDoS))\n+        {\n+            printf(\"%s from %s %s was not accepted into the memory pool\\n\", tx.GetHash().ToString().c_str(),\n+                pfrom->addr.ToString().c_str(), pfrom->strSubVer.c_str());\n             if (nDoS > 0)\n                 pfrom->Misbehaving(nDoS);\n+        }\n     }\n \n "
      }
    ]
  },
  {
    "sha": "7120d76dc6fc9d86aac5d3a5454f248de5a5853f",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo3MTIwZDc2ZGM2ZmM5ZDg2YWFjNWQzYTU0NTRmMjQ4ZGU1YTU4NTNm",
    "commit": {
      "author": {
        "name": "Mike Hearn",
        "email": "hearn@google.com",
        "date": "2013-11-26T11:52:21Z"
      },
      "committer": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-11-28T01:03:33Z"
      },
      "message": "Store and use a sanitized subVer\n\nRebased-from: a946aa8d3ec7009ac670eeb65a525efe5eeb6e84",
      "tree": {
        "sha": "a163b0f198951ca7b6e3e8c71b70e3c6b8f7ec5c",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/a163b0f198951ca7b6e3e8c71b70e3c6b8f7ec5c"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/7120d76dc6fc9d86aac5d3a5454f248de5a5853f",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7120d76dc6fc9d86aac5d3a5454f248de5a5853f",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/7120d76dc6fc9d86aac5d3a5454f248de5a5853f",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7120d76dc6fc9d86aac5d3a5454f248de5a5853f/comments",
    "author": null,
    "committer": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "1c53d0a4fbdc1efd9ed316556a12fb52082db32e",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1c53d0a4fbdc1efd9ed316556a12fb52082db32e",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/1c53d0a4fbdc1efd9ed316556a12fb52082db32e"
      }
    ],
    "stats": {
      "total": 25,
      "additions": 17,
      "deletions": 8
    },
    "files": [
      {
        "sha": "f8f194afc5b46959eeab5c4d29bacbf237f43eaf",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 6,
        "deletions": 4,
        "changes": 10,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/7120d76dc6fc9d86aac5d3a5454f248de5a5853f/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/7120d76dc6fc9d86aac5d3a5454f248de5a5853f/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=7120d76dc6fc9d86aac5d3a5454f248de5a5853f",
        "patch": "@@ -3216,8 +3216,10 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n             pfrom->nVersion = 300;\n         if (!vRecv.empty())\n             vRecv >> addrFrom >> nNonce;\n-        if (!vRecv.empty())\n+        if (!vRecv.empty()) {\n             vRecv >> pfrom->strSubVer;\n+            pfrom->cleanSubVer = SanitizeString(pfrom->strSubVer);\n+        }\n         if (!vRecv.empty())\n             vRecv >> pfrom->nStartingHeight;\n         if (!vRecv.empty())\n@@ -3285,7 +3287,7 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n \n         pfrom->fSuccessfullyConnected = true;\n \n-        printf(\"receive version message: %s: version %d, blocks=%d, us=%s, them=%s, peer=%s\\n\", pfrom->strSubVer.c_str(), pfrom->nVersion, pfrom->nStartingHeight, addrMe.ToString().c_str(), addrFrom.ToString().c_str(), pfrom->addr.ToString().c_str());\n+        printf(\"receive version message: %s: version %d, blocks=%d, us=%s, them=%s, peer=%s\\n\", pfrom->cleanSubVer.c_str(), pfrom->nVersion, pfrom->nStartingHeight, addrMe.ToString().c_str(), addrFrom.ToString().c_str(), pfrom->addr.ToString().c_str());\n \n         cPeerBlockCounts.input(pfrom->nStartingHeight);\n     }\n@@ -3534,7 +3536,7 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n             vEraseQueue.push_back(inv.hash);\n \n             printf(\"AcceptToMemoryPool: %s %s : accepted %s (poolsz %\"PRIszu\")\\n\",\n-                pfrom->addr.ToString().c_str(), pfrom->strSubVer.c_str(),\n+                pfrom->addr.ToString().c_str(), pfrom->cleanSubVer.c_str(),\n                 tx.GetHash().ToString().c_str(),\n                 mempool.mapTx.size());\n \n@@ -3587,7 +3589,7 @@ bool static ProcessMessage(CNode* pfrom, string strCommand, CDataStream& vRecv)\n         if (state.IsInvalid(nDoS))\n         {\n             printf(\"%s from %s %s was not accepted into the memory pool\\n\", tx.GetHash().ToString().c_str(),\n-                pfrom->addr.ToString().c_str(), pfrom->strSubVer.c_str());\n+                pfrom->addr.ToString().c_str(), pfrom->cleanSubVer.c_str());\n             if (nDoS > 0)\n                 pfrom->Misbehaving(nDoS);\n         }"
      },
      {
        "sha": "f40afde20f09c9afe17f9f0c4e479088f15d4bfe",
        "filename": "src/net.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/7120d76dc6fc9d86aac5d3a5454f248de5a5853f/src/net.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/7120d76dc6fc9d86aac5d3a5454f248de5a5853f/src/net.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/net.cpp?ref=7120d76dc6fc9d86aac5d3a5454f248de5a5853f",
        "patch": "@@ -612,7 +612,7 @@ void CNode::copyStats(CNodeStats &stats)\n     X(nTimeConnected);\n     X(addrName);\n     X(nVersion);\n-    X(strSubVer);\n+    X(cleanSubVer);\n     X(fInbound);\n     X(nStartingHeight);\n     X(nMisbehavior);"
      },
      {
        "sha": "dfdd3c8137b95195c64c41d30e880d93b6904d04",
        "filename": "src/net.h",
        "status": "modified",
        "additions": 6,
        "deletions": 2,
        "changes": 8,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/7120d76dc6fc9d86aac5d3a5454f248de5a5853f/src/net.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/7120d76dc6fc9d86aac5d3a5454f248de5a5853f/src/net.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/net.h?ref=7120d76dc6fc9d86aac5d3a5454f248de5a5853f",
        "patch": "@@ -98,7 +98,7 @@ class CNodeStats\n     int64 nTimeConnected;\n     std::string addrName;\n     int nVersion;\n-    std::string strSubVer;\n+    std::string cleanSubVer;\n     bool fInbound;\n     int nStartingHeight;\n     int nMisbehavior;\n@@ -177,7 +177,11 @@ class CNode\n     std::string addrName;\n     CService addrLocal;\n     int nVersion;\n-    std::string strSubVer;\n+    // strSubVer is whatever byte array we read from the wire. However, this field is intended \n+    // to be printed out, displayed to humans in various forms and so on. So we sanitize it and\n+    // store the sanitized version in cleanSubVer. The original should be used when dealing with\n+    // the network or wire types and the cleaned string used when displayed or logged.\n+    std::string strSubVer, cleanSubVer;\n     bool fOneShot;\n     bool fClient;\n     bool fInbound;"
      },
      {
        "sha": "99b94ffa3aee0903cf6a02d49464e6729e4fa271",
        "filename": "src/rpcnet.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/7120d76dc6fc9d86aac5d3a5454f248de5a5853f/src/rpcnet.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/7120d76dc6fc9d86aac5d3a5454f248de5a5853f/src/rpcnet.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/rpcnet.cpp?ref=7120d76dc6fc9d86aac5d3a5454f248de5a5853f",
        "patch": "@@ -55,7 +55,10 @@ Value getpeerinfo(const Array& params, bool fHelp)\n         obj.push_back(Pair(\"bytesrecv\", (boost::int64_t)stats.nRecvBytes));\n         obj.push_back(Pair(\"conntime\", (boost::int64_t)stats.nTimeConnected));\n         obj.push_back(Pair(\"version\", stats.nVersion));\n-        obj.push_back(Pair(\"subver\", stats.strSubVer));\n+        // Use the sanitized form of subver here, to avoid tricksy remote peers from\n+        // corrupting or modifiying the JSON output by putting special characters in\n+        // their ver message.\n+        obj.push_back(Pair(\"subver\", stats.cleanSubVer));\n         obj.push_back(Pair(\"inbound\", stats.fInbound));\n         obj.push_back(Pair(\"startingheight\", stats.nStartingHeight));\n         obj.push_back(Pair(\"banscore\", stats.nMisbehavior));"
      }
    ]
  },
  {
    "sha": "5c029630f91c4cad7b8f781a0324ff54c36163d3",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo1YzAyOTYzMGY5MWM0Y2FkN2I4Zjc4MWEwMzI0ZmY1NGMzNjE2M2Qz",
    "commit": {
      "author": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-11-28T03:03:41Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-28T06:59:53Z"
      },
      "message": "Increase default -blockmaxsize/prioritysize to 300K/30K",
      "tree": {
        "sha": "ba94c34f2eb3653bfc282bf3ee5e1bd74a3aa206",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/ba94c34f2eb3653bfc282bf3ee5e1bd74a3aa206"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/5c029630f91c4cad7b8f781a0324ff54c36163d3",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5c029630f91c4cad7b8f781a0324ff54c36163d3",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/5c029630f91c4cad7b8f781a0324ff54c36163d3",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5c029630f91c4cad7b8f781a0324ff54c36163d3/comments",
    "author": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "7120d76dc6fc9d86aac5d3a5454f248de5a5853f",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7120d76dc6fc9d86aac5d3a5454f248de5a5853f",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/7120d76dc6fc9d86aac5d3a5454f248de5a5853f"
      }
    ],
    "stats": {
      "total": 14,
      "additions": 9,
      "deletions": 5
    },
    "files": [
      {
        "sha": "5dcd378a77e788591be6c50e2de71596f1fbd2f3",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/5c029630f91c4cad7b8f781a0324ff54c36163d3/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/5c029630f91c4cad7b8f781a0324ff54c36163d3/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=5c029630f91c4cad7b8f781a0324ff54c36163d3",
        "patch": "@@ -618,7 +618,7 @@ int64 CTransaction::GetMinFee(unsigned int nBlockSize, bool fAllowFree,\n         else\n         {\n             // Free transaction area\n-            if (nNewBlockSize < 27000)\n+            if (nNewBlockSize < DEFAULT_BLOCK_PRIORITY_SIZE)\n                 nMinFee = 0;\n         }\n     }\n@@ -4219,7 +4219,7 @@ CBlockTemplate* CreateNewBlock(CReserveKey& reservekey)\n     pblocktemplate->vTxSigOps.push_back(-1); // updated at end\n \n     // Largest block you're willing to create:\n-    unsigned int nBlockMaxSize = GetArg(\"-blockmaxsize\", MAX_BLOCK_SIZE_GEN/2);\n+    unsigned int nBlockMaxSize = GetArg(\"-blockmaxsize\", DEFAULT_BLOCK_MAX_SIZE);\n     // Limit to betweeen 1K and MAX_BLOCK_SIZE-1K for sanity:\n     nBlockMaxSize = std::max((unsigned int)1000, std::min((unsigned int)(MAX_BLOCK_SIZE-1000), nBlockMaxSize));\n \n@@ -4229,7 +4229,7 @@ CBlockTemplate* CreateNewBlock(CReserveKey& reservekey)\n \n     // How much of the block should be dedicated to high-priority transactions,\n     // included regardless of the fees they pay\n-    unsigned int nBlockPrioritySize = GetArg(\"-blockprioritysize\", 27000);\n+    unsigned int nBlockPrioritySize = GetArg(\"-blockprioritysize\", DEFAULT_BLOCK_PRIORITY_SIZE);\n     nBlockPrioritySize = std::min(nBlockMaxSize, nBlockPrioritySize);\n \n     // Minimum block size you want to create; block will be filled with free transactions"
      },
      {
        "sha": "2fc683cd6d81f63230427aca8a3a38695c5731ff",
        "filename": "src/main.h",
        "status": "modified",
        "additions": 6,
        "deletions": 2,
        "changes": 8,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/5c029630f91c4cad7b8f781a0324ff54c36163d3/src/main.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/5c029630f91c4cad7b8f781a0324ff54c36163d3/src/main.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.h?ref=5c029630f91c4cad7b8f781a0324ff54c36163d3",
        "patch": "@@ -26,10 +26,14 @@ struct CBlockIndexWorkComparator;\n \n /** The maximum allowed size for a serialized block, in bytes (network rule) */\n static const unsigned int MAX_BLOCK_SIZE = 1000000;\n-/** The maximum size for mined blocks */\n+/** Obsolete: maximum size for mined blocks */\n static const unsigned int MAX_BLOCK_SIZE_GEN = MAX_BLOCK_SIZE/2;\n+/** Default for -blockmaxsize, maximum size for mined blocks **/\n+static const unsigned int DEFAULT_BLOCK_MAX_SIZE = 300000;\n+/** Default for -blockprioritysize, maximum space for zero/low-fee transactions **/\n+static const unsigned int DEFAULT_BLOCK_PRIORITY_SIZE = 30000;\n /** The maximum size for transactions we're willing to relay/mine */\n-static const unsigned int MAX_STANDARD_TX_SIZE = MAX_BLOCK_SIZE_GEN/5;\n+static const unsigned int MAX_STANDARD_TX_SIZE = 100000;\n /** The maximum allowed number of signature check operations in a block (network rule) */\n static const unsigned int MAX_BLOCK_SIGOPS = MAX_BLOCK_SIZE/50;\n /** The maximum number of orphan transactions kept in memory */"
      }
    ]
  },
  {
    "sha": "1ca8a75cb4d916bf225f2879f2722649f509c92d",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzoxY2E4YTc1Y2I0ZDkxNmJmMjI1ZjI4NzlmMjcyMjY0OWY1MDljOTJk",
    "commit": {
      "author": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-09-18T01:48:41Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-28T06:59:54Z"
      },
      "message": "Remove CENT-output free transaction rule when relaying\n\nRemove the (relay/mempool) rule that all outputs of free transactions\nmust be greater than 0.01 XBT. Dust spam is now taken care of by making\ndusty outputs non-standard.",
      "tree": {
        "sha": "d757ae857ba97ad328f217a4a79498de88311cb9",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/d757ae857ba97ad328f217a4a79498de88311cb9"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/1ca8a75cb4d916bf225f2879f2722649f509c92d",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1ca8a75cb4d916bf225f2879f2722649f509c92d",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/1ca8a75cb4d916bf225f2879f2722649f509c92d",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1ca8a75cb4d916bf225f2879f2722649f509c92d/comments",
    "author": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "5c029630f91c4cad7b8f781a0324ff54c36163d3",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5c029630f91c4cad7b8f781a0324ff54c36163d3",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/5c029630f91c4cad7b8f781a0324ff54c36163d3"
      }
    ],
    "stats": {
      "total": 13,
      "additions": 8,
      "deletions": 5
    },
    "files": [
      {
        "sha": "52e89acc26c85a9f784f641145df3330b15cb71c",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 2,
        "changes": 6,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/1ca8a75cb4d916bf225f2879f2722649f509c92d/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/1ca8a75cb4d916bf225f2879f2722649f509c92d/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=1ca8a75cb4d916bf225f2879f2722649f509c92d",
        "patch": "@@ -623,8 +623,10 @@ int64 CTransaction::GetMinFee(unsigned int nBlockSize, bool fAllowFree,\n         }\n     }\n \n-    // To limit dust spam, require base fee if any output is less than 0.01\n-    if (nMinFee < nBaseFee)\n+    // This code can be removed after enough miners have upgraded to version 0.9.\n+    // Until then, be safe when sending and require a fee if any output\n+    // is less than CENT:\n+    if (nMinFee < nBaseFee && mode == GMF_SEND)\n     {\n         BOOST_FOREACH(const CTxOut& txout, vout)\n             if (txout.nValue < CENT)"
      },
      {
        "sha": "73de759b5e51742c487d7d240376899deea07a1b",
        "filename": "src/wallet.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/1ca8a75cb4d916bf225f2879f2722649f509c92d/src/wallet.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/1ca8a75cb4d916bf225f2879f2722649f509c92d/src/wallet.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/wallet.cpp?ref=1ca8a75cb4d916bf225f2879f2722649f509c92d",
        "patch": "@@ -1204,9 +1204,10 @@ bool CWallet::CreateTransaction(const vector<pair<CScript, int64> >& vecSend,\n                 }\n \n                 int64 nChange = nValueIn - nValue - nFeeRet;\n-                // if sub-cent change is required, the fee must be raised to at least nMinTxFee\n-                // or until nChange becomes zero\n-                // NOTE: this depends on the exact behaviour of GetMinFee\n+                // The following if statement should be removed once enough miners\n+                // have upgraded to the 0.9 GetMinFee() rules. Until then, this avoids\n+                // creating free transactions that have change outputs less than\n+                // CENT bitcoins.\n                 if (nFeeRet < CTransaction::nMinTxFee && nChange > 0 && nChange < CENT)\n                 {\n                     int64 nMoveToFee = min(nChange, CTransaction::nMinTxFee - nFeeRet);"
      }
    ]
  },
  {
    "sha": "9612e4c0d9730dbdb9971e53c72df17dd97daa2a",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo5NjEyZTRjMGQ5NzMwZGJkYjk5NzFlNTNjNzJkZjE3ZGQ5N2RhYTJh",
    "commit": {
      "author": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-09-18T02:04:29Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-28T06:59:54Z"
      },
      "message": "Lower maximum size for free transaction creation\n\nChanges the maximum size of a free transaction that will be created\nfrom 10,000 bytes to 1,000 bytes.\n\nThe idea behind this change is to make the free transaction area\navailable to a greater number of people; with the default 27K-per-block,\njust three very-large very-high-priority transactions could fill the space.",
      "tree": {
        "sha": "114cc7ea2bceb56633e4ce6342f3f3df2495af70",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/114cc7ea2bceb56633e4ce6342f3f3df2495af70"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/9612e4c0d9730dbdb9971e53c72df17dd97daa2a",
      "comment_count": 1,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/9612e4c0d9730dbdb9971e53c72df17dd97daa2a",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/9612e4c0d9730dbdb9971e53c72df17dd97daa2a",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/9612e4c0d9730dbdb9971e53c72df17dd97daa2a/comments",
    "author": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "1ca8a75cb4d916bf225f2879f2722649f509c92d",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1ca8a75cb4d916bf225f2879f2722649f509c92d",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/1ca8a75cb4d916bf225f2879f2722649f509c92d"
      }
    ],
    "stats": {
      "total": 21,
      "additions": 8,
      "deletions": 13
    },
    "files": [
      {
        "sha": "9c7eece56d22154a742b2de603ffc7857960988e",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 8,
        "deletions": 13,
        "changes": 21,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/9612e4c0d9730dbdb9971e53c72df17dd97daa2a/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/9612e4c0d9730dbdb9971e53c72df17dd97daa2a/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=9612e4c0d9730dbdb9971e53c72df17dd97daa2a",
        "patch": "@@ -608,19 +608,14 @@ int64 CTransaction::GetMinFee(unsigned int nBlockSize, bool fAllowFree,\n \n     if (fAllowFree)\n     {\n-        if (nBlockSize == 1)\n-        {\n-            // Transactions under 10K are free\n-            // (about 4500 BTC if made of 50 BTC inputs)\n-            if (nBytes < 10000)\n-                nMinFee = 0;\n-        }\n-        else\n-        {\n-            // Free transaction area\n-            if (nNewBlockSize < DEFAULT_BLOCK_PRIORITY_SIZE)\n-                nMinFee = 0;\n-        }\n+        // There is a free transaction area in blocks created by most miners,\n+        // * If we are relaying we allow transactions up to DEFAULT_BLOCK_PRIORITY_SIZE - 1000\n+        //   to be considered to fall into this category. We don't want to encourage sending\n+        //   multiple transactions instead of one big transaction to avoid fees.\n+        // * If we are creating a transaction we allow transactions up to 1,000 bytes\n+        //   to be considered safe and assume they can likely make it into this section.\n+        if (nBytes < (mode == GMF_SEND ? 1000 : (DEFAULT_BLOCK_PRIORITY_SIZE - 1000)))\n+            nMinFee = 0;\n     }\n \n     // This code can be removed after enough miners have upgraded to version 0.9."
      }
    ]
  },
  {
    "sha": "187f8a56750bf5f1815cefa439611c3000f70c14",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzoxODdmOGE1Njc1MGJmNWYxODE1Y2VmYTQzOTYxMWMzMDAwZjcwYzE0",
    "commit": {
      "author": {
        "name": "Robert Backhaus",
        "email": "robbak@robbak.com",
        "date": "2013-05-24T13:40:51Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:39Z"
      },
      "message": "Explictly cast calculation to int, to allow std::max to work.",
      "tree": {
        "sha": "9d643ac50fecb5472acb828c7ab5f52149adb3a7",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/9d643ac50fecb5472acb828c7ab5f52149adb3a7"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/187f8a56750bf5f1815cefa439611c3000f70c14",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/187f8a56750bf5f1815cefa439611c3000f70c14",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/187f8a56750bf5f1815cefa439611c3000f70c14",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/187f8a56750bf5f1815cefa439611c3000f70c14/comments",
    "author": {
      "login": "robbak",
      "id": 1581858,
      "node_id": "MDQ6VXNlcjE1ODE4NTg=",
      "avatar_url": "https://avatars.githubusercontent.com/u/1581858?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/robbak",
      "html_url": "https://github.com/robbak",
      "followers_url": "https://api.github.com/users/robbak/followers",
      "following_url": "https://api.github.com/users/robbak/following{/other_user}",
      "gists_url": "https://api.github.com/users/robbak/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/robbak/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/robbak/subscriptions",
      "organizations_url": "https://api.github.com/users/robbak/orgs",
      "repos_url": "https://api.github.com/users/robbak/repos",
      "events_url": "https://api.github.com/users/robbak/events{/privacy}",
      "received_events_url": "https://api.github.com/users/robbak/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "9612e4c0d9730dbdb9971e53c72df17dd97daa2a",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/9612e4c0d9730dbdb9971e53c72df17dd97daa2a",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/9612e4c0d9730dbdb9971e53c72df17dd97daa2a"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 1,
      "deletions": 1
    },
    "files": [
      {
        "sha": "68ca62288db8d31344f8cbdce42c96028bb7d3ec",
        "filename": "src/init.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/187f8a56750bf5f1815cefa439611c3000f70c14/src/init.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/187f8a56750bf5f1815cefa439611c3000f70c14/src/init.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/init.cpp?ref=187f8a56750bf5f1815cefa439611c3000f70c14",
        "patch": "@@ -533,7 +533,7 @@ bool AppInit2(boost::thread_group& threadGroup)\n     // Make sure enough file descriptors are available\n     int nBind = std::max((int)mapArgs.count(\"-bind\"), 1);\n     nMaxConnections = GetArg(\"-maxconnections\", 125);\n-    nMaxConnections = std::max(std::min(nMaxConnections, FD_SETSIZE - nBind - MIN_CORE_FILEDESCRIPTORS), 0);\n+    nMaxConnections = std::max(std::min(nMaxConnections, (int)(FD_SETSIZE - nBind - MIN_CORE_FILEDESCRIPTORS)), 0);\n     int nFD = RaiseFileDescriptorLimit(nMaxConnections + MIN_CORE_FILEDESCRIPTORS);\n     if (nFD < MIN_CORE_FILEDESCRIPTORS)\n         return InitError(_(\"Not enough file descriptors available.\"));"
      }
    ]
  },
  {
    "sha": "df238b19755c65a6e5f7db660befff2ca8da09a6",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpkZjIzOGIxOTc1NWM2NWE2ZTVmN2RiNjYwYmVmZmYyY2E4ZGEwOWE2",
    "commit": {
      "author": {
        "name": "Roman Mindalev",
        "email": "r000n@r000n.net",
        "date": "2013-08-12T14:28:23Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:39Z"
      },
      "message": "Add missing 0x prefix in chainparams.cpp\n\nConflicts:\n\tsrc/chainparams.cpp",
      "tree": {
        "sha": "b983425372ba030eee19d3e275a925a569fce101",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/b983425372ba030eee19d3e275a925a569fce101"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/df238b19755c65a6e5f7db660befff2ca8da09a6",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/df238b19755c65a6e5f7db660befff2ca8da09a6",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/df238b19755c65a6e5f7db660befff2ca8da09a6",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/df238b19755c65a6e5f7db660befff2ca8da09a6/comments",
    "author": {
      "login": "r000n",
      "id": 2118167,
      "node_id": "MDQ6VXNlcjIxMTgxNjc=",
      "avatar_url": "https://avatars.githubusercontent.com/u/2118167?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/r000n",
      "html_url": "https://github.com/r000n",
      "followers_url": "https://api.github.com/users/r000n/followers",
      "following_url": "https://api.github.com/users/r000n/following{/other_user}",
      "gists_url": "https://api.github.com/users/r000n/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/r000n/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/r000n/subscriptions",
      "organizations_url": "https://api.github.com/users/r000n/orgs",
      "repos_url": "https://api.github.com/users/r000n/repos",
      "events_url": "https://api.github.com/users/r000n/events{/privacy}",
      "received_events_url": "https://api.github.com/users/r000n/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "187f8a56750bf5f1815cefa439611c3000f70c14",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/187f8a56750bf5f1815cefa439611c3000f70c14",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/187f8a56750bf5f1815cefa439611c3000f70c14"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 1,
      "deletions": 1
    },
    "files": [
      {
        "sha": "0f8c4208e4714e5a23b97a5055f4b00f3e19e8a2",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/df238b19755c65a6e5f7db660befff2ca8da09a6/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/df238b19755c65a6e5f7db660befff2ca8da09a6/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=df238b19755c65a6e5f7db660befff2ca8da09a6",
        "patch": "@@ -2724,7 +2724,7 @@ bool LoadBlockIndex()\n         pchMessageStart[1] = 0x11;\n         pchMessageStart[2] = 0x09;\n         pchMessageStart[3] = 0x07;\n-        hashGenesisBlock = uint256(\"000000000933ea01ad0ee984209779baaec3ced90fa3f408719526f8d77f4943\");\n+        hashGenesisBlock = uint256(\"0x000000000933ea01ad0ee984209779baaec3ced90fa3f408719526f8d77f4943\");\n     }\n \n     //"
      }
    ]
  },
  {
    "sha": "89c2ea0891a9b0364982cbfdb65e64275906a49c",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo4OWMyZWEwODkxYTliMDM2NDk4MmNiZmRiNjVlNjQyNzU5MDZhNDlj",
    "commit": {
      "author": {
        "name": "Luke Dashjr",
        "email": "luke-jr+git@utopios.org",
        "date": "2013-07-15T22:13:42Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:39Z"
      },
      "message": "Bugfix: Move boost/version include from db.cpp to walletdb.cpp where BOOST_VERSION is used",
      "tree": {
        "sha": "122ec345c1a53ae159ba63b194c58141def8b581",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/122ec345c1a53ae159ba63b194c58141def8b581"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/89c2ea0891a9b0364982cbfdb65e64275906a49c",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/89c2ea0891a9b0364982cbfdb65e64275906a49c",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/89c2ea0891a9b0364982cbfdb65e64275906a49c",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/89c2ea0891a9b0364982cbfdb65e64275906a49c/comments",
    "author": {
      "login": "luke-jr",
      "id": 1095675,
      "node_id": "MDQ6VXNlcjEwOTU2NzU=",
      "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/luke-jr",
      "html_url": "https://github.com/luke-jr",
      "followers_url": "https://api.github.com/users/luke-jr/followers",
      "following_url": "https://api.github.com/users/luke-jr/following{/other_user}",
      "gists_url": "https://api.github.com/users/luke-jr/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/luke-jr/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
      "organizations_url": "https://api.github.com/users/luke-jr/orgs",
      "repos_url": "https://api.github.com/users/luke-jr/repos",
      "events_url": "https://api.github.com/users/luke-jr/events{/privacy}",
      "received_events_url": "https://api.github.com/users/luke-jr/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "df238b19755c65a6e5f7db660befff2ca8da09a6",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/df238b19755c65a6e5f7db660befff2ca8da09a6",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/df238b19755c65a6e5f7db660befff2ca8da09a6"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 1,
      "deletions": 1
    },
    "files": [
      {
        "sha": "da20d362b653a4fbb20e2e2a226eec719e991607",
        "filename": "src/db.cpp",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/89c2ea0891a9b0364982cbfdb65e64275906a49c/src/db.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/89c2ea0891a9b0364982cbfdb65e64275906a49c/src/db.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/db.cpp?ref=89c2ea0891a9b0364982cbfdb65e64275906a49c",
        "patch": "@@ -6,7 +6,6 @@\n #include \"db.h\"\n #include \"util.h\"\n #include \"main.h\"\n-#include <boost/version.hpp>\n #include <boost/filesystem.hpp>\n #include <boost/filesystem/fstream.hpp>\n "
      },
      {
        "sha": "42f6bbde1e448edb158cf2b0f8127aceb37f8162",
        "filename": "src/walletdb.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/89c2ea0891a9b0364982cbfdb65e64275906a49c/src/walletdb.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/89c2ea0891a9b0364982cbfdb65e64275906a49c/src/walletdb.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/walletdb.cpp?ref=89c2ea0891a9b0364982cbfdb65e64275906a49c",
        "patch": "@@ -5,6 +5,7 @@\n \n #include \"walletdb.h\"\n #include \"wallet.h\"\n+#include <boost/version.hpp>\n #include <boost/filesystem.hpp>\n \n using namespace std;"
      }
    ]
  },
  {
    "sha": "1b350f25a66c6fa4a16ef156a1336bf6458d41a8",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzoxYjM1MGYyNWE2NmM2ZmE0YTE2ZWYxNTZhMTMzNmJmNjQ1OGQ0MWE4",
    "commit": {
      "author": {
        "name": "Philip Kaufmann",
        "email": "phil.kaufmann@t-online.de",
        "date": "2013-07-13T11:05:04Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:39Z"
      },
      "message": "fix invalid conversion error with MinGW 4.8.1 in net.cpp\n\n- fixes src\\net.cpp:1601: Error:invalid conversion from 'void*' to\n  'const char*' [-fpermissive] in a setsockopt() call on Win32 that was\n  found by using MinGW 4.8.1 compiler suite",
      "tree": {
        "sha": "53cc8c45d8529543122130f9506e4e6b54d8fc51",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/53cc8c45d8529543122130f9506e4e6b54d8fc51"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/1b350f25a66c6fa4a16ef156a1336bf6458d41a8",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1b350f25a66c6fa4a16ef156a1336bf6458d41a8",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/1b350f25a66c6fa4a16ef156a1336bf6458d41a8",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1b350f25a66c6fa4a16ef156a1336bf6458d41a8/comments",
    "author": null,
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "89c2ea0891a9b0364982cbfdb65e64275906a49c",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/89c2ea0891a9b0364982cbfdb65e64275906a49c",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/89c2ea0891a9b0364982cbfdb65e64275906a49c"
      }
    ],
    "stats": {
      "total": 4,
      "additions": 4,
      "deletions": 0
    },
    "files": [
      {
        "sha": "156ab6349c3ac4c8bc8660865d84178f9434447b",
        "filename": "src/net.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/1b350f25a66c6fa4a16ef156a1336bf6458d41a8/src/net.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/1b350f25a66c6fa4a16ef156a1336bf6458d41a8/src/net.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/net.cpp?ref=1b350f25a66c6fa4a16ef156a1336bf6458d41a8",
        "patch": "@@ -1737,8 +1737,12 @@ bool BindListenPort(const CService &addrBind, string& strError)\n     // and enable it by default or not. Try to enable it, if possible.\n     if (addrBind.IsIPv6()) {\n #ifdef IPV6_V6ONLY\n+#ifdef WIN32\n+        setsockopt(hListenSocket, IPPROTO_IPV6, IPV6_V6ONLY, (const char*)&nOne, sizeof(int));\n+#else\n         setsockopt(hListenSocket, IPPROTO_IPV6, IPV6_V6ONLY, (void*)&nOne, sizeof(int));\n #endif\n+#endif\n #ifdef WIN32\n         int nProtLevel = 10 /* PROTECTION_LEVEL_UNRESTRICTED */;\n         int nParameterId = 23 /* IPV6_PROTECTION_LEVEl */;"
      }
    ]
  },
  {
    "sha": "9f7fc8b332e25f8d11ff424cddfd2f09e60581a5",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo5ZjdmYzhiMzMyZTI1ZjhkMTFmZjQyNGNkZGZkMmYwOWU2MDU4MWE1",
    "commit": {
      "author": {
        "name": "Han Lin Yap",
        "email": "codler@gmail.com",
        "date": "2013-08-06T11:31:42Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:40Z"
      },
      "message": "fix typo in README.md",
      "tree": {
        "sha": "b086204db88e6e05f4088b84e1c387817427c427",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/b086204db88e6e05f4088b84e1c387817427c427"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/9f7fc8b332e25f8d11ff424cddfd2f09e60581a5",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/9f7fc8b332e25f8d11ff424cddfd2f09e60581a5",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/9f7fc8b332e25f8d11ff424cddfd2f09e60581a5",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/9f7fc8b332e25f8d11ff424cddfd2f09e60581a5/comments",
    "author": {
      "login": "codler",
      "id": 102052,
      "node_id": "MDQ6VXNlcjEwMjA1Mg==",
      "avatar_url": "https://avatars.githubusercontent.com/u/102052?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/codler",
      "html_url": "https://github.com/codler",
      "followers_url": "https://api.github.com/users/codler/followers",
      "following_url": "https://api.github.com/users/codler/following{/other_user}",
      "gists_url": "https://api.github.com/users/codler/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/codler/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/codler/subscriptions",
      "organizations_url": "https://api.github.com/users/codler/orgs",
      "repos_url": "https://api.github.com/users/codler/repos",
      "events_url": "https://api.github.com/users/codler/events{/privacy}",
      "received_events_url": "https://api.github.com/users/codler/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "1b350f25a66c6fa4a16ef156a1336bf6458d41a8",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1b350f25a66c6fa4a16ef156a1336bf6458d41a8",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/1b350f25a66c6fa4a16ef156a1336bf6458d41a8"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 1,
      "deletions": 1
    },
    "files": [
      {
        "sha": "4f69333e59b789a754dd4fd1f770f8438dd55cd1",
        "filename": "README.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/9f7fc8b332e25f8d11ff424cddfd2f09e60581a5/README.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/9f7fc8b332e25f8d11ff424cddfd2f09e60581a5/README.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/README.md?ref=9f7fc8b332e25f8d11ff424cddfd2f09e60581a5",
        "patch": "@@ -38,7 +38,7 @@ submitter will be asked to start a discussion (if they haven't already) on the\n \n The patch will be accepted if there is broad consensus that it is a good thing.\n Developers should expect to rework and resubmit patches if the code doesn't\n-match the project's coding conventions (see `doc/coding.txt`) or are\n+match the project's coding conventions (see `doc/coding.md`) or are\n controversial.\n \n The `master` branch is regularly built and tested, but is not guaranteed to be"
      }
    ]
  },
  {
    "sha": "98289bbe25ba22523d08b67485a61220c2b0b767",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo5ODI4OWJiZTI1YmEyMjUyM2QwOGI2NzQ4NWE2MTIyMGMyYjBiNzY3",
    "commit": {
      "author": {
        "name": "Mark Friedenbach",
        "email": "mark@friedenbach.org",
        "date": "2013-07-27T21:22:42Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:40Z"
      },
      "message": "Fix typo in README.md",
      "tree": {
        "sha": "a56cc2ed6268c85e63026e2b197d92ad5e54c844",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/a56cc2ed6268c85e63026e2b197d92ad5e54c844"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/98289bbe25ba22523d08b67485a61220c2b0b767",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/98289bbe25ba22523d08b67485a61220c2b0b767",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/98289bbe25ba22523d08b67485a61220c2b0b767",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/98289bbe25ba22523d08b67485a61220c2b0b767/comments",
    "author": {
      "login": "maaku",
      "id": 69154,
      "node_id": "MDQ6VXNlcjY5MTU0",
      "avatar_url": "https://avatars.githubusercontent.com/u/69154?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/maaku",
      "html_url": "https://github.com/maaku",
      "followers_url": "https://api.github.com/users/maaku/followers",
      "following_url": "https://api.github.com/users/maaku/following{/other_user}",
      "gists_url": "https://api.github.com/users/maaku/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/maaku/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/maaku/subscriptions",
      "organizations_url": "https://api.github.com/users/maaku/orgs",
      "repos_url": "https://api.github.com/users/maaku/repos",
      "events_url": "https://api.github.com/users/maaku/events{/privacy}",
      "received_events_url": "https://api.github.com/users/maaku/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "9f7fc8b332e25f8d11ff424cddfd2f09e60581a5",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/9f7fc8b332e25f8d11ff424cddfd2f09e60581a5",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/9f7fc8b332e25f8d11ff424cddfd2f09e60581a5"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 1,
      "deletions": 1
    },
    "files": [
      {
        "sha": "5453f998dc23b797145244a57584e3f1c3fa20ef",
        "filename": "README.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/98289bbe25ba22523d08b67485a61220c2b0b767/README.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/98289bbe25ba22523d08b67485a61220c2b0b767/README.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/README.md?ref=98289bbe25ba22523d08b67485a61220c2b0b767",
        "patch": "@@ -15,7 +15,7 @@ out collectively by the network. Bitcoin is also the name of the open source\n software which enables the use of this currency.\n \n For more information, as well as an immediately useable, binary version of\n-the Bitcoin client sofware, see http://www.bitcoin.org.\n+the Bitcoin client software, see http://www.bitcoin.org.\n \n License\n -------"
      }
    ]
  },
  {
    "sha": "d3fe7c44f7ab7ed7c21336c7a43bcca6ecd1b004",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpkM2ZlN2M0NGY3YWI3ZWQ3YzIxMzM2YzdhNDNiY2NhNmVjZDFiMDA0",
    "commit": {
      "author": {
        "name": "Gregory Maxwell",
        "email": "greg@xiph.org",
        "date": "2013-06-25T19:06:43Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:40Z"
      },
      "message": "Make the rand tests determinstic. (fixes #2714)\n\nThis avoids spurious errors with the old tests but still tests\n enough that if the rng is replaced with a totally broken one\n it should still fail.",
      "tree": {
        "sha": "1f57d6e60d30f223aaf01023b3adeb002ba9916d",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/1f57d6e60d30f223aaf01023b3adeb002ba9916d"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/d3fe7c44f7ab7ed7c21336c7a43bcca6ecd1b004",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/d3fe7c44f7ab7ed7c21336c7a43bcca6ecd1b004",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/d3fe7c44f7ab7ed7c21336c7a43bcca6ecd1b004",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/d3fe7c44f7ab7ed7c21336c7a43bcca6ecd1b004/comments",
    "author": {
      "login": "gmaxwell",
      "id": 858454,
      "node_id": "MDQ6VXNlcjg1ODQ1NA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/858454?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gmaxwell",
      "html_url": "https://github.com/gmaxwell",
      "followers_url": "https://api.github.com/users/gmaxwell/followers",
      "following_url": "https://api.github.com/users/gmaxwell/following{/other_user}",
      "gists_url": "https://api.github.com/users/gmaxwell/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gmaxwell/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gmaxwell/subscriptions",
      "organizations_url": "https://api.github.com/users/gmaxwell/orgs",
      "repos_url": "https://api.github.com/users/gmaxwell/repos",
      "events_url": "https://api.github.com/users/gmaxwell/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gmaxwell/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "98289bbe25ba22523d08b67485a61220c2b0b767",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/98289bbe25ba22523d08b67485a61220c2b0b767",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/98289bbe25ba22523d08b67485a61220c2b0b767"
      }
    ],
    "stats": {
      "total": 34,
      "additions": 1,
      "deletions": 33
    },
    "files": [
      {
        "sha": "e2247767db1d198c4e27cdeb4bdc312c261271ab",
        "filename": "src/test/util_tests.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 33,
        "changes": 34,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/d3fe7c44f7ab7ed7c21336c7a43bcca6ecd1b004/src/test/util_tests.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/d3fe7c44f7ab7ed7c21336c7a43bcca6ecd1b004/src/test/util_tests.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/test/util_tests.cpp?ref=d3fe7c44f7ab7ed7c21336c7a43bcca6ecd1b004",
        "patch": "@@ -263,28 +263,10 @@ BOOST_AUTO_TEST_CASE(util_IsHex)\n \n BOOST_AUTO_TEST_CASE(util_seed_insecure_rand)\n {\n-    // Expected results for the determinstic seed.\n-    const uint32_t exp_vals[11] = {  91632771U,1889679809U,3842137544U,3256031132U,\n-                                   1761911779U, 489223532U,2692793790U,2737472863U,\n-                                   2796262275U,1309899767U,840571781U};\n-    // Expected 0s in rand()%(idx+2) for the determinstic seed.\n-    const int exp_count[9] = {5013,3346,2415,1972,1644,1386,1176,1096,1009};\n     int i;\n     int count=0;\n \n-    seed_insecure_rand();\n-\n-    //Does the non-determistic rand give us results that look too like the determinstic one?\n-    for (i=0;i<10;i++)\n-    {\n-        int match = 0;\n-        uint32_t rval = insecure_rand();\n-        for (int j=0;j<11;j++)match |= rval==exp_vals[j];\n-        count += match;\n-    }\n-    // sum(binomial(10,i)*(11/(2^32))^i*(1-(11/(2^32)))^(10-i),i,0,4) ~= 1-1/2^134.73\n-    // So _very_ unlikely to throw a false failure here.\n-    BOOST_CHECK(count<=4);\n+    seed_insecure_rand(true);\n \n     for (int mod=2;mod<11;mod++)\n     {\n@@ -307,20 +289,6 @@ BOOST_AUTO_TEST_CASE(util_seed_insecure_rand)\n         BOOST_CHECK(count<=10000/mod+err);\n         BOOST_CHECK(count>=10000/mod-err);\n     }\n-\n-    seed_insecure_rand(true);\n-\n-    for (i=0;i<11;i++)\n-    {\n-        BOOST_CHECK_EQUAL(insecure_rand(),exp_vals[i]);\n-    }\n-\n-    for (int mod=2;mod<11;mod++)\n-    {\n-        count = 0;\n-        for (i=0;i<10000;i++) count += insecure_rand()%mod==0;\n-        BOOST_CHECK_EQUAL(count,exp_count[mod-2]);\n-    }\n }\n \n BOOST_AUTO_TEST_CASE(util_TimingResistantEqual)"
      }
    ]
  },
  {
    "sha": "c9c67b00dfe15c86adbecdb98d1759e836681aee",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpjOWM2N2IwMGRmZTE1Yzg2YWRiZWNkYjk4ZDE3NTllODM2NjgxYWVl",
    "commit": {
      "author": {
        "name": "Michael Ford",
        "email": "fanquake@gmail.com",
        "date": "2013-06-14T09:45:38Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:40Z"
      },
      "message": "Update HomeBrew osx makefile patch to account for recent changes to makefile",
      "tree": {
        "sha": "03250e09e0339787d77bca9988f770c8d54e4806",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/03250e09e0339787d77bca9988f770c8d54e4806"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/c9c67b00dfe15c86adbecdb98d1759e836681aee",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/c9c67b00dfe15c86adbecdb98d1759e836681aee",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/c9c67b00dfe15c86adbecdb98d1759e836681aee",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/c9c67b00dfe15c86adbecdb98d1759e836681aee/comments",
    "author": {
      "login": "fanquake",
      "id": 863730,
      "node_id": "MDQ6VXNlcjg2MzczMA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/863730?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/fanquake",
      "html_url": "https://github.com/fanquake",
      "followers_url": "https://api.github.com/users/fanquake/followers",
      "following_url": "https://api.github.com/users/fanquake/following{/other_user}",
      "gists_url": "https://api.github.com/users/fanquake/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/fanquake/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/fanquake/subscriptions",
      "organizations_url": "https://api.github.com/users/fanquake/orgs",
      "repos_url": "https://api.github.com/users/fanquake/repos",
      "events_url": "https://api.github.com/users/fanquake/events{/privacy}",
      "received_events_url": "https://api.github.com/users/fanquake/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "d3fe7c44f7ab7ed7c21336c7a43bcca6ecd1b004",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/d3fe7c44f7ab7ed7c21336c7a43bcca6ecd1b004",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/d3fe7c44f7ab7ed7c21336c7a43bcca6ecd1b004"
      }
    ],
    "stats": {
      "total": 5,
      "additions": 3,
      "deletions": 2
    },
    "files": [
      {
        "sha": "287db2fdf22a8430bac24df20c3a6094a4de4e22",
        "filename": "contrib/homebrew/makefile.osx.patch",
        "status": "modified",
        "additions": 3,
        "deletions": 2,
        "changes": 5,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/c9c67b00dfe15c86adbecdb98d1759e836681aee/contrib/homebrew/makefile.osx.patch",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/c9c67b00dfe15c86adbecdb98d1759e836681aee/contrib/homebrew/makefile.osx.patch",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/contrib/homebrew/makefile.osx.patch?ref=c9c67b00dfe15c86adbecdb98d1759e836681aee",
        "patch": "@@ -1,5 +1,5 @@\n diff --git a/src/makefile.osx b/src/makefile.osx\n-index 8b7c559..8a0560c 100644\n+index bef0ef3..07ef8d3 100644\n --- a/src/makefile.osx\n +++ b/src/makefile.osx\n @@ -7,17 +7,21 @@\n@@ -28,7 +28,7 @@ index 8b7c559..8a0560c 100644\n  \n  USE_UPNP:=1\n  USE_IPV6:=1\n-@@ -31,13 +35,13 @@ ifdef STATIC\n+@@ -31,14 +35,14 @@ ifdef STATIC\n  TESTLIBS += \\\n   $(DEPSDIR)/lib/libboost_unit_test_framework-mt.a\n  LIBS += \\\n@@ -38,6 +38,7 @@ index 8b7c559..8a0560c 100644\n   $(DEPSDIR)/lib/libboost_filesystem-mt.a \\\n   $(DEPSDIR)/lib/libboost_program_options-mt.a \\\n   $(DEPSDIR)/lib/libboost_thread-mt.a \\\n+  $(DEPSDIR)/lib/libboost_chrono-mt.a \\\n - $(DEPSDIR)/lib/libssl.a \\\n - $(DEPSDIR)/lib/libcrypto.a \\\n + $(OPENSSLDIR)/lib/libssl.a \\"
      }
    ]
  },
  {
    "sha": "901ae4bffd4c634a24fe689094247828d16a8016",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo5MDFhZTRiZmZkNGM2MzRhMjRmZTY4OTA5NDI0NzgyOGQxNmE4MDE2",
    "commit": {
      "author": {
        "name": "Cozz Lovan",
        "email": "cozzlovan@yahoo.com",
        "date": "2013-06-11T03:01:52Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:40Z"
      },
      "message": "fix comment about dust logic\n\nConflicts:\n\tsrc/core.h",
      "tree": {
        "sha": "e37b2bd5968246cfff2ecf192f4b1fc700ac498f",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/e37b2bd5968246cfff2ecf192f4b1fc700ac498f"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/901ae4bffd4c634a24fe689094247828d16a8016",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/901ae4bffd4c634a24fe689094247828d16a8016",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/901ae4bffd4c634a24fe689094247828d16a8016",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/901ae4bffd4c634a24fe689094247828d16a8016/comments",
    "author": {
      "login": "cozz",
      "id": 2814559,
      "node_id": "MDQ6VXNlcjI4MTQ1NTk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/2814559?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/cozz",
      "html_url": "https://github.com/cozz",
      "followers_url": "https://api.github.com/users/cozz/followers",
      "following_url": "https://api.github.com/users/cozz/following{/other_user}",
      "gists_url": "https://api.github.com/users/cozz/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/cozz/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/cozz/subscriptions",
      "organizations_url": "https://api.github.com/users/cozz/orgs",
      "repos_url": "https://api.github.com/users/cozz/repos",
      "events_url": "https://api.github.com/users/cozz/events{/privacy}",
      "received_events_url": "https://api.github.com/users/cozz/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "c9c67b00dfe15c86adbecdb98d1759e836681aee",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/c9c67b00dfe15c86adbecdb98d1759e836681aee",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/c9c67b00dfe15c86adbecdb98d1759e836681aee"
      }
    ],
    "stats": {
      "total": 4,
      "additions": 2,
      "deletions": 2
    },
    "files": [
      {
        "sha": "e7a7140a916aa9463a49a83b2dc5d809f988a474",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/901ae4bffd4c634a24fe689094247828d16a8016/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/901ae4bffd4c634a24fe689094247828d16a8016/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=901ae4bffd4c634a24fe689094247828d16a8016",
        "patch": "@@ -359,10 +359,10 @@ bool CTxOut::IsDust() const\n     // which has units satoshis-per-kilobyte.\n     // If you'd pay more than 1/3 in fees\n     // to spend something, then we consider it dust.\n-    // A typical txout is 33 bytes big, and will\n+    // A typical txout is 34 bytes big, and will\n     // need a CTxIn of at least 148 bytes to spend,\n     // so dust is a txout less than 54 uBTC\n-    // (5430 satoshis) with default nMinRelayTxFee\n+    // (5460 satoshis) with default nMinRelayTxFee\n     return ((nValue*1000)/(3*((int)GetSerializeSize(SER_DISK,0)+148)) < CTransaction::nMinRelayTxFee);\n }\n "
      }
    ]
  },
  {
    "sha": "96896a0ecea9289278f5c07d7b63155af94ce765",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo5Njg5NmEwZWNlYTkyODkyNzhmNWMwN2Q3YjYzMTU1YWY5NGNlNzY1",
    "commit": {
      "author": {
        "name": "Warren Togami",
        "email": "wtogami@gmail.com",
        "date": "2013-07-07T13:25:22Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:40Z"
      },
      "message": "Add wtogami to gitian download scripts.",
      "tree": {
        "sha": "034200d41d7ad2ad62db70efde7cb75a599bdcdc",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/034200d41d7ad2ad62db70efde7cb75a599bdcdc"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/96896a0ecea9289278f5c07d7b63155af94ce765",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/96896a0ecea9289278f5c07d7b63155af94ce765",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/96896a0ecea9289278f5c07d7b63155af94ce765",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/96896a0ecea9289278f5c07d7b63155af94ce765/comments",
    "author": {
      "login": "wtogami",
      "id": 93665,
      "node_id": "MDQ6VXNlcjkzNjY1",
      "avatar_url": "https://avatars.githubusercontent.com/u/93665?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/wtogami",
      "html_url": "https://github.com/wtogami",
      "followers_url": "https://api.github.com/users/wtogami/followers",
      "following_url": "https://api.github.com/users/wtogami/following{/other_user}",
      "gists_url": "https://api.github.com/users/wtogami/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/wtogami/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/wtogami/subscriptions",
      "organizations_url": "https://api.github.com/users/wtogami/orgs",
      "repos_url": "https://api.github.com/users/wtogami/repos",
      "events_url": "https://api.github.com/users/wtogami/events{/privacy}",
      "received_events_url": "https://api.github.com/users/wtogami/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "901ae4bffd4c634a24fe689094247828d16a8016",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/901ae4bffd4c634a24fe689094247828d16a8016",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/901ae4bffd4c634a24fe689094247828d16a8016"
      }
    ],
    "stats": {
      "total": 139,
      "additions": 139,
      "deletions": 0
    },
    "files": [
      {
        "sha": "8340a5dd24754b0e15767d9c7d0b58053f9b591d",
        "filename": "contrib/gitian-downloader/linux-download-config",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/96896a0ecea9289278f5c07d7b63155af94ce765/contrib/gitian-downloader/linux-download-config",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/96896a0ecea9289278f5c07d7b63155af94ce765/contrib/gitian-downloader/linux-download-config",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/contrib/gitian-downloader/linux-download-config?ref=96896a0ecea9289278f5c07d7b63155af94ce765",
        "patch": "@@ -35,4 +35,8 @@ signers:\n     weight: 40\n     name: \"Wladimir J. van der Laan\"\n     key: laanwj\n+  AEC1884398647C47413C1C3FB1179EB7347DC10D:\n+    weight: 40\n+    name: \"Warren Togami\"\n+    key: wtogami\n minimum_weight: 120"
      },
      {
        "sha": "49d52851b6b8884479b5c4de9137eef1ecd40670",
        "filename": "contrib/gitian-downloader/win32-download-config",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/96896a0ecea9289278f5c07d7b63155af94ce765/contrib/gitian-downloader/win32-download-config",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/96896a0ecea9289278f5c07d7b63155af94ce765/contrib/gitian-downloader/win32-download-config",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/contrib/gitian-downloader/win32-download-config?ref=96896a0ecea9289278f5c07d7b63155af94ce765",
        "patch": "@@ -35,4 +35,8 @@ signers:\n     weight: 40\n     name: \"Wladimir J. van der Laan\"\n     key: laanwj\n+  AEC1884398647C47413C1C3FB1179EB7347DC10D:\n+    weight: 40\n+    name: \"Warren Togami\"\n+    key: wtogami\n minimum_weight: 120"
      },
      {
        "sha": "e0f6c4c5fdf56ffbd6308d4e0a6624290feadff6",
        "filename": "contrib/gitian-downloader/wtogami-key.pgp",
        "status": "added",
        "additions": 131,
        "deletions": 0,
        "changes": 131,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/96896a0ecea9289278f5c07d7b63155af94ce765/contrib/gitian-downloader/wtogami-key.pgp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/96896a0ecea9289278f5c07d7b63155af94ce765/contrib/gitian-downloader/wtogami-key.pgp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/contrib/gitian-downloader/wtogami-key.pgp?ref=96896a0ecea9289278f5c07d7b63155af94ce765",
        "patch": "@@ -0,0 +1,131 @@\n+-----BEGIN PGP PUBLIC KEY BLOCK-----\n+Version: GnuPG v1.4.13 (GNU/Linux)\n+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\n+Jk1/tIJqcyc1BfY6uFHFXWWnqQnjl0XpZo+/bMDxTVy8yND2\n+=icdI\n+-----END PGP PUBLIC KEY BLOCK-----"
      }
    ]
  },
  {
    "sha": "138705a173276d5e59e4a0608ed3eadc9d048984",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzoxMzg3MDVhMTczMjc2ZDVlNTllNGEwNjA4ZWQzZWFkYzlkMDQ4OTg0",
    "commit": {
      "author": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-06-08T08:03:23Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:40Z"
      },
      "message": "Create parent directories if needed in GetDataDir\n\nOne-line change. Fixes #2752.",
      "tree": {
        "sha": "cf9808eac69449c4972fd1ff9bd3be83c96f3fe7",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/cf9808eac69449c4972fd1ff9bd3be83c96f3fe7"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/138705a173276d5e59e4a0608ed3eadc9d048984",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/138705a173276d5e59e4a0608ed3eadc9d048984",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/138705a173276d5e59e4a0608ed3eadc9d048984",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/138705a173276d5e59e4a0608ed3eadc9d048984/comments",
    "author": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "96896a0ecea9289278f5c07d7b63155af94ce765",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/96896a0ecea9289278f5c07d7b63155af94ce765",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/96896a0ecea9289278f5c07d7b63155af94ce765"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 1,
      "deletions": 1
    },
    "files": [
      {
        "sha": "00cd1c92f407855e5832ad08e00e1bbee527739c",
        "filename": "src/util.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/138705a173276d5e59e4a0608ed3eadc9d048984/src/util.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/138705a173276d5e59e4a0608ed3eadc9d048984/src/util.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/util.cpp?ref=138705a173276d5e59e4a0608ed3eadc9d048984",
        "patch": "@@ -1084,7 +1084,7 @@ const boost::filesystem::path &GetDataDir(bool fNetSpecific)\n     if (fNetSpecific && GetBoolArg(\"-testnet\", false))\n         path /= \"testnet3\";\n \n-    fs::create_directory(path);\n+    fs::create_directories(path);\n \n     fCachedPath[fNetSpecific] = true;\n     return path;"
      }
    ]
  },
  {
    "sha": "daf9e4627f5dda2b7afec25bd17f6b75d803269a",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpkYWY5ZTQ2MjdmNWRkYTJiN2FmZWMyNWJkMTdmNmI3NWQ4MDMyNjlh",
    "commit": {
      "author": {
        "name": "Micha",
        "email": "michagogo@server.fake",
        "date": "2013-07-07T16:24:15Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:40Z"
      },
      "message": "Made the build/release process completable verbatim as listed in\nrelease-process.md\n\nConflicts:\n\tdoc/release-process.md",
      "tree": {
        "sha": "e205a4a85dd8333bea5d6869d53764a994cebde5",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/e205a4a85dd8333bea5d6869d53764a994cebde5"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/daf9e4627f5dda2b7afec25bd17f6b75d803269a",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/daf9e4627f5dda2b7afec25bd17f6b75d803269a",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/daf9e4627f5dda2b7afec25bd17f6b75d803269a",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/daf9e4627f5dda2b7afec25bd17f6b75d803269a/comments",
    "author": {
      "login": "Michagogo",
      "id": 2559390,
      "node_id": "MDQ6VXNlcjI1NTkzOTA=",
      "avatar_url": "https://avatars.githubusercontent.com/u/2559390?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Michagogo",
      "html_url": "https://github.com/Michagogo",
      "followers_url": "https://api.github.com/users/Michagogo/followers",
      "following_url": "https://api.github.com/users/Michagogo/following{/other_user}",
      "gists_url": "https://api.github.com/users/Michagogo/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Michagogo/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Michagogo/subscriptions",
      "organizations_url": "https://api.github.com/users/Michagogo/orgs",
      "repos_url": "https://api.github.com/users/Michagogo/repos",
      "events_url": "https://api.github.com/users/Michagogo/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Michagogo/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "138705a173276d5e59e4a0608ed3eadc9d048984",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/138705a173276d5e59e4a0608ed3eadc9d048984",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/138705a173276d5e59e4a0608ed3eadc9d048984"
      }
    ],
    "stats": {
      "total": 9,
      "additions": 5,
      "deletions": 4
    },
    "files": [
      {
        "sha": "b8aa7d9a915723127fce830bc8d977a8822e519a",
        "filename": "doc/release-process.md",
        "status": "modified",
        "additions": 5,
        "deletions": 4,
        "changes": 9,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/daf9e4627f5dda2b7afec25bd17f6b75d803269a/doc/release-process.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/daf9e4627f5dda2b7afec25bd17f6b75d803269a/doc/release-process.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/release-process.md?ref=daf9e4627f5dda2b7afec25bd17f6b75d803269a",
        "patch": "@@ -31,7 +31,7 @@ Release Process\n   \n \texport SIGNER=(your gitian key, ie bluematt, sipa, etc)\n \texport VERSION=0.8.0\n-\tcd ./gitian-builder\n+\tpushd ./gitian-builder\n \n  Fetch and build inputs: (first time, or when dependency versions change)\n \n@@ -58,13 +58,14 @@ Release Process\n \t./bin/gsign --signer $SIGNER --release ${VERSION} --destination ../gitian.sigs/ ../bitcoin/contrib/gitian-descriptors/gitian.yml\n \tpushd build/out\n \tzip -r bitcoin-${VERSION}-linux-gitian.zip *\n-\tmv bitcoin-${VERSION}-linux-gitian.zip ../../\n+\tmv bitcoin-${VERSION}-linux-gitian.zip ../../../\n \tpopd\n \t./bin/gbuild --commit bitcoin=v${VERSION} ../bitcoin/contrib/gitian-descriptors/gitian-win32.yml\n \t./bin/gsign --signer $SIGNER --release ${VERSION}-win32 --destination ../gitian.sigs/ ../bitcoin/contrib/gitian-descriptors/gitian-win32.yml\n \tpushd build/out\n \tzip -r bitcoin-${VERSION}-win32-gitian.zip *\n-\tmv bitcoin-${VERSION}-win32-gitian.zip ../../\n+\tmv bitcoin-${VERSION}-win32-gitian.zip ../../../\n+\tpopd\n \tpopd\n \n   Build output expected:\n@@ -161,4 +162,4 @@ From a directory containing bitcoin source, gitian.sigs and gitian zips\n \tpopd\n \n - Upload gitian zips to SourceForge\n-- Celebrate \n\\ No newline at end of file\n+- Celebrate "
      }
    ]
  },
  {
    "sha": "6f21e7317e86cac4f26234116513fa153111032a",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo2ZjIxZTczMTdlODZjYWM0ZjI2MjM0MTE2NTEzZmExNTMxMTEwMzJh",
    "commit": {
      "author": {
        "name": "Daniel Larimer",
        "email": "dlarimer@gmail.com",
        "date": "2013-05-19T19:46:11Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:41Z"
      },
      "message": "Fix memory leak on exception in Key::SignCompact",
      "tree": {
        "sha": "547a648ece895bb6d5342e2c01e8a8b474e8ae5b",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/547a648ece895bb6d5342e2c01e8a8b474e8ae5b"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/6f21e7317e86cac4f26234116513fa153111032a",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6f21e7317e86cac4f26234116513fa153111032a",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/6f21e7317e86cac4f26234116513fa153111032a",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6f21e7317e86cac4f26234116513fa153111032a/comments",
    "author": {
      "login": "bytemaster",
      "id": 347290,
      "node_id": "MDQ6VXNlcjM0NzI5MA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/347290?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/bytemaster",
      "html_url": "https://github.com/bytemaster",
      "followers_url": "https://api.github.com/users/bytemaster/followers",
      "following_url": "https://api.github.com/users/bytemaster/following{/other_user}",
      "gists_url": "https://api.github.com/users/bytemaster/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/bytemaster/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/bytemaster/subscriptions",
      "organizations_url": "https://api.github.com/users/bytemaster/orgs",
      "repos_url": "https://api.github.com/users/bytemaster/repos",
      "events_url": "https://api.github.com/users/bytemaster/events{/privacy}",
      "received_events_url": "https://api.github.com/users/bytemaster/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "daf9e4627f5dda2b7afec25bd17f6b75d803269a",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/daf9e4627f5dda2b7afec25bd17f6b75d803269a",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/daf9e4627f5dda2b7afec25bd17f6b75d803269a"
      }
    ],
    "stats": {
      "total": 3,
      "additions": 3,
      "deletions": 0
    },
    "files": [
      {
        "sha": "af9ffaf792f08dd88a022f0542a55f3f758a66ad",
        "filename": "src/key.cpp",
        "status": "modified",
        "additions": 3,
        "deletions": 0,
        "changes": 3,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/6f21e7317e86cac4f26234116513fa153111032a/src/key.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/6f21e7317e86cac4f26234116513fa153111032a/src/key.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/key.cpp?ref=6f21e7317e86cac4f26234116513fa153111032a",
        "patch": "@@ -328,7 +328,10 @@ bool CKey::SignCompact(uint256 hash, std::vector<unsigned char>& vchSig)\n         }\n \n         if (nRecId == -1)\n+        {\n+            ECDSA_SIG_free(sig);\n             throw key_error(\"CKey::SignCompact() : unable to construct recoverable key\");\n+        }\n \n         vchSig[0] = nRecId+27+(fCompressedPubKey ? 4 : 0);\n         BN_bn2bin(sig->r,&vchSig[33-(nBitsR+7)/8]);"
      }
    ]
  },
  {
    "sha": "27e35bf84079ebb37ea6b756ad01ea9ed2bdad5f",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzoyN2UzNWJmODQwNzllYmIzN2VhNmI3NTZhZDAxZWE5ZWQyYmRhZDVm",
    "commit": {
      "author": {
        "name": "Daniel Larimer",
        "email": "dlarimer@gmail.com",
        "date": "2013-05-19T16:45:52Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:41Z"
      },
      "message": "fix memory leak in CKey::SetCompactSignature()",
      "tree": {
        "sha": "7f2d3bbbcdb9564f9b545cb30cfcd65102ed2c5b",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/7f2d3bbbcdb9564f9b545cb30cfcd65102ed2c5b"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/27e35bf84079ebb37ea6b756ad01ea9ed2bdad5f",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/27e35bf84079ebb37ea6b756ad01ea9ed2bdad5f",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/27e35bf84079ebb37ea6b756ad01ea9ed2bdad5f",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/27e35bf84079ebb37ea6b756ad01ea9ed2bdad5f/comments",
    "author": {
      "login": "bytemaster",
      "id": 347290,
      "node_id": "MDQ6VXNlcjM0NzI5MA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/347290?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/bytemaster",
      "html_url": "https://github.com/bytemaster",
      "followers_url": "https://api.github.com/users/bytemaster/followers",
      "following_url": "https://api.github.com/users/bytemaster/following{/other_user}",
      "gists_url": "https://api.github.com/users/bytemaster/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/bytemaster/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/bytemaster/subscriptions",
      "organizations_url": "https://api.github.com/users/bytemaster/orgs",
      "repos_url": "https://api.github.com/users/bytemaster/repos",
      "events_url": "https://api.github.com/users/bytemaster/events{/privacy}",
      "received_events_url": "https://api.github.com/users/bytemaster/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "6f21e7317e86cac4f26234116513fa153111032a",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6f21e7317e86cac4f26234116513fa153111032a",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/6f21e7317e86cac4f26234116513fa153111032a"
      }
    ],
    "stats": {
      "total": 1,
      "additions": 1,
      "deletions": 0
    },
    "files": [
      {
        "sha": "75114c6afe14943616403f7b5ad84359f83d4bd7",
        "filename": "src/key.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/27e35bf84079ebb37ea6b756ad01ea9ed2bdad5f/src/key.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/27e35bf84079ebb37ea6b756ad01ea9ed2bdad5f/src/key.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/key.cpp?ref=27e35bf84079ebb37ea6b756ad01ea9ed2bdad5f",
        "patch": "@@ -370,6 +370,7 @@ bool CKey::SetCompactSignature(uint256 hash, const std::vector<unsigned char>& v\n         ECDSA_SIG_free(sig);\n         return true;\n     }\n+    ECDSA_SIG_free(sig);\n     return false;\n }\n "
      }
    ]
  },
  {
    "sha": "df0f6d020a97181d97b5f232e1310b1e982edf36",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpkZjBmNmQwMjBhOTcxODFkOTdiNWYyMzJlMTMxMGIxZTk4MmVkZjM2",
    "commit": {
      "author": {
        "name": "Robert Backhaus",
        "email": "robbak@robbak.com",
        "date": "2013-05-29T00:33:36Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:41Z"
      },
      "message": "Don't attempt to resize vector to negative size.",
      "tree": {
        "sha": "79fdd1c31dfb984021b5e9b950773187dd73e54b",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/79fdd1c31dfb984021b5e9b950773187dd73e54b"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/df0f6d020a97181d97b5f232e1310b1e982edf36",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/df0f6d020a97181d97b5f232e1310b1e982edf36",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/df0f6d020a97181d97b5f232e1310b1e982edf36",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/df0f6d020a97181d97b5f232e1310b1e982edf36/comments",
    "author": {
      "login": "robbak",
      "id": 1581858,
      "node_id": "MDQ6VXNlcjE1ODE4NTg=",
      "avatar_url": "https://avatars.githubusercontent.com/u/1581858?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/robbak",
      "html_url": "https://github.com/robbak",
      "followers_url": "https://api.github.com/users/robbak/followers",
      "following_url": "https://api.github.com/users/robbak/following{/other_user}",
      "gists_url": "https://api.github.com/users/robbak/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/robbak/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/robbak/subscriptions",
      "organizations_url": "https://api.github.com/users/robbak/orgs",
      "repos_url": "https://api.github.com/users/robbak/repos",
      "events_url": "https://api.github.com/users/robbak/events{/privacy}",
      "received_events_url": "https://api.github.com/users/robbak/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "27e35bf84079ebb37ea6b756ad01ea9ed2bdad5f",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/27e35bf84079ebb37ea6b756ad01ea9ed2bdad5f",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/27e35bf84079ebb37ea6b756ad01ea9ed2bdad5f"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 2,
      "deletions": 0
    },
    "files": [
      {
        "sha": "2bda0659ba61e8c8da269025e97c9f604185d3b7",
        "filename": "src/db.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/df0f6d020a97181d97b5f232e1310b1e982edf36/src/db.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/df0f6d020a97181d97b5f232e1310b1e982edf36/src/db.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/db.cpp?ref=df0f6d020a97181d97b5f232e1310b1e982edf36",
        "patch": "@@ -540,6 +540,8 @@ bool CAddrDB::Read(CAddrMan& addr)\n     // use file size to size memory buffer\n     int fileSize = GetFilesize(filein);\n     int dataSize = fileSize - sizeof(uint256);\n+    //Don't try to resize to a negative number if file is small\n+    if ( dataSize < 0 ) dataSize = 0;\n     vector<unsigned char> vchData;\n     vchData.resize(dataSize);\n     uint256 hashIn;"
      }
    ]
  },
  {
    "sha": "3b8868dca49ba4e317d562279d2128ed8fff9a81",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzozYjg4NjhkY2E0OWJhNGUzMTdkNTYyMjc5ZDIxMjhlZDhmZmY5YTgx",
    "commit": {
      "author": {
        "name": "Timothy Stranex",
        "email": "timothy@Timothys-MacBook-Pro.local",
        "date": "2013-09-02T15:27:27Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:41Z"
      },
      "message": "Fix typo in a comment: it's base58, not base48.",
      "tree": {
        "sha": "f933b309ad647acee91145f9aa56ac63f93e36d1",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/f933b309ad647acee91145f9aa56ac63f93e36d1"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/3b8868dca49ba4e317d562279d2128ed8fff9a81",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/3b8868dca49ba4e317d562279d2128ed8fff9a81",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/3b8868dca49ba4e317d562279d2128ed8fff9a81",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/3b8868dca49ba4e317d562279d2128ed8fff9a81/comments",
    "author": null,
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "df0f6d020a97181d97b5f232e1310b1e982edf36",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/df0f6d020a97181d97b5f232e1310b1e982edf36",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/df0f6d020a97181d97b5f232e1310b1e982edf36"
      }
    ],
    "stats": {
      "total": 4,
      "additions": 2,
      "deletions": 2
    },
    "files": [
      {
        "sha": "b7f4dfee96a98de266197533613d89431df69bc4",
        "filename": "src/qt/bitcoinaddressvalidator.h",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/3b8868dca49ba4e317d562279d2128ed8fff9a81/src/qt/bitcoinaddressvalidator.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/3b8868dca49ba4e317d562279d2128ed8fff9a81/src/qt/bitcoinaddressvalidator.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/qt/bitcoinaddressvalidator.h?ref=3b8868dca49ba4e317d562279d2128ed8fff9a81",
        "patch": "@@ -3,8 +3,8 @@\n \n #include <QValidator>\n \n-/** Base48 entry widget validator.\n-   Corrects near-miss characters and refuses characters that are no part of base48.\n+/** Base58 entry widget validator.\n+   Corrects near-miss characters and refuses characters that are not part of base58.\n  */\n class BitcoinAddressValidator : public QValidator\n {"
      }
    ]
  },
  {
    "sha": "bfb08569862b91a75ef8d099ce79ed0797f645cd",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpiZmIwODU2OTg2MmI5MWE3NWVmOGQwOTljZTc5ZWQwNzk3ZjY0NWNk",
    "commit": {
      "author": {
        "name": "Philip Kaufmann",
        "email": "phil.kaufmann@t-online.de",
        "date": "2013-08-02T11:53:03Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:15:41Z"
      },
      "message": "Bitcoin-Qt: Fix display of window when bitcoin: URI is opened\n\nWalletView:\n- add new signal showNormalIfMinimized()\n- emit the new signal in handleURI() to fix a bug, preventing the main\n  window to show up when using bitcoin: URIs\n\nUpstream: dbc0a6aba2cf94aa1b167145a18e0b9c671aef5b",
      "tree": {
        "sha": "6b55692d0490b07dd7bb8beecbfcd72ce7c7fe60",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/6b55692d0490b07dd7bb8beecbfcd72ce7c7fe60"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/bfb08569862b91a75ef8d099ce79ed0797f645cd",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/bfb08569862b91a75ef8d099ce79ed0797f645cd",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/bfb08569862b91a75ef8d099ce79ed0797f645cd",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/bfb08569862b91a75ef8d099ce79ed0797f645cd/comments",
    "author": null,
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "3b8868dca49ba4e317d562279d2128ed8fff9a81",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/3b8868dca49ba4e317d562279d2128ed8fff9a81",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/3b8868dca49ba4e317d562279d2128ed8fff9a81"
      }
    ],
    "stats": {
      "total": 10,
      "additions": 10,
      "deletions": 0
    },
    "files": [
      {
        "sha": "3576d55cea56f2d94f29bc6372f9700319ebef1f",
        "filename": "src/qt/walletstack.cpp",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/bfb08569862b91a75ef8d099ce79ed0797f645cd/src/qt/walletstack.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/bfb08569862b91a75ef8d099ce79ed0797f645cd/src/qt/walletstack.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/qt/walletstack.cpp?ref=bfb08569862b91a75ef8d099ce79ed0797f645cd",
        "patch": "@@ -13,6 +13,7 @@\n \n WalletStack::WalletStack(QWidget *parent) :\n     QStackedWidget(parent),\n+    gui(0),\n     clientModel(0),\n     bOutOfSync(true)\n {\n@@ -35,6 +36,10 @@ bool WalletStack::addWallet(const QString& name, WalletModel *walletModel)\n     walletView->showOutOfSyncWarning(bOutOfSync);\n     addWidget(walletView);\n     mapWalletViews[name] = walletView;\n+\n+    // Ensure a walletView is able to show the main window\n+\tconnect(walletView, SIGNAL(showNormalIfMinimized()), gui, SLOT(showNormalIfMinimized()));\n+\n     return true;\n }\n "
      },
      {
        "sha": "1de145c32df33f9219082fc5f3d15fe825138bac",
        "filename": "src/qt/walletview.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/bfb08569862b91a75ef8d099ce79ed0797f645cd/src/qt/walletview.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/bfb08569862b91a75ef8d099ce79ed0797f645cd/src/qt/walletview.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/qt/walletview.cpp?ref=bfb08569862b91a75ef8d099ce79ed0797f645cd",
        "patch": "@@ -203,6 +203,7 @@ bool WalletView::handleURI(const QString& strURI)\n     if (sendCoinsPage->handleURI(strURI))\n     {\n         gotoSendCoinsPage();\n+        emit showNormalIfMinimized();\n         return true;\n     }\n     else"
      },
      {
        "sha": "6ad5180d567c300410ac3fc916ffa5059fd9bce2",
        "filename": "src/qt/walletview.h",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/bfb08569862b91a75ef8d099ce79ed0797f645cd/src/qt/walletview.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/bfb08569862b91a75ef8d099ce79ed0797f645cd/src/qt/walletview.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/qt/walletview.h?ref=bfb08569862b91a75ef8d099ce79ed0797f645cd",
        "patch": "@@ -99,6 +99,10 @@ public slots:\n     void unlockWallet();\n \n     void setEncryptionStatus();\n+\n+signals:\n+    /** Signal that we want to show the main window */\n+    void showNormalIfMinimized();\n };\n \n #endif // WALLETVIEW_H"
      }
    ]
  },
  {
    "sha": "22db46cf11cb3cc0d2e73946366bdaeee48a3ff7",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzoyMmRiNDZjZjExY2IzY2MwZDJlNzM5NDYzNjZiZGFlZWU0OGEzZmY3",
    "commit": {
      "author": {
        "name": "Gavin Andresen",
        "email": "gavinandresen@gmail.com",
        "date": "2013-11-30T04:54:36Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:16:48Z"
      },
      "message": "Fix typo: make default maxblocksize 350k (not 300)",
      "tree": {
        "sha": "731a32e9b9b832990dfffcc1a28fa136e25afacd",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/731a32e9b9b832990dfffcc1a28fa136e25afacd"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/22db46cf11cb3cc0d2e73946366bdaeee48a3ff7",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/22db46cf11cb3cc0d2e73946366bdaeee48a3ff7",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/22db46cf11cb3cc0d2e73946366bdaeee48a3ff7",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/22db46cf11cb3cc0d2e73946366bdaeee48a3ff7/comments",
    "author": {
      "login": "gavinandresen",
      "id": 331997,
      "node_id": "MDQ6VXNlcjMzMTk5Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/331997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gavinandresen",
      "html_url": "https://github.com/gavinandresen",
      "followers_url": "https://api.github.com/users/gavinandresen/followers",
      "following_url": "https://api.github.com/users/gavinandresen/following{/other_user}",
      "gists_url": "https://api.github.com/users/gavinandresen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gavinandresen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gavinandresen/subscriptions",
      "organizations_url": "https://api.github.com/users/gavinandresen/orgs",
      "repos_url": "https://api.github.com/users/gavinandresen/repos",
      "events_url": "https://api.github.com/users/gavinandresen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gavinandresen/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "bfb08569862b91a75ef8d099ce79ed0797f645cd",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/bfb08569862b91a75ef8d099ce79ed0797f645cd",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/bfb08569862b91a75ef8d099ce79ed0797f645cd"
      }
    ],
    "stats": {
      "total": 2,
      "additions": 1,
      "deletions": 1
    },
    "files": [
      {
        "sha": "6665e5af021f7a677c8be8b5b018caf61f9505f4",
        "filename": "src/main.h",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/22db46cf11cb3cc0d2e73946366bdaeee48a3ff7/src/main.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/22db46cf11cb3cc0d2e73946366bdaeee48a3ff7/src/main.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.h?ref=22db46cf11cb3cc0d2e73946366bdaeee48a3ff7",
        "patch": "@@ -29,7 +29,7 @@ static const unsigned int MAX_BLOCK_SIZE = 1000000;\n /** Obsolete: maximum size for mined blocks */\n static const unsigned int MAX_BLOCK_SIZE_GEN = MAX_BLOCK_SIZE/2;\n /** Default for -blockmaxsize, maximum size for mined blocks **/\n-static const unsigned int DEFAULT_BLOCK_MAX_SIZE = 300000;\n+static const unsigned int DEFAULT_BLOCK_MAX_SIZE = 350000;\n /** Default for -blockprioritysize, maximum space for zero/low-fee transactions **/\n static const unsigned int DEFAULT_BLOCK_PRIORITY_SIZE = 30000;\n /** The maximum size for transactions we're willing to relay/mine */"
      }
    ]
  },
  {
    "sha": "a0a5b20156b7f0c7037bc7e11819ee86e579c47e",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzphMGE1YjIwMTU2YjdmMGM3MDM3YmM3ZTExODE5ZWU4NmU1NzljNDdl",
    "commit": {
      "author": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-30T06:51:55Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:16:48Z"
      },
      "message": "release notes: add warning about 32-bit systems",
      "tree": {
        "sha": "d0c95d8cc4fb8d5497d9a1edf514eafcc8a8fba6",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/d0c95d8cc4fb8d5497d9a1edf514eafcc8a8fba6"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/a0a5b20156b7f0c7037bc7e11819ee86e579c47e",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/a0a5b20156b7f0c7037bc7e11819ee86e579c47e",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/a0a5b20156b7f0c7037bc7e11819ee86e579c47e",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/a0a5b20156b7f0c7037bc7e11819ee86e579c47e/comments",
    "author": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "22db46cf11cb3cc0d2e73946366bdaeee48a3ff7",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/22db46cf11cb3cc0d2e73946366bdaeee48a3ff7",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/22db46cf11cb3cc0d2e73946366bdaeee48a3ff7"
      }
    ],
    "stats": {
      "total": 8,
      "additions": 8,
      "deletions": 0
    },
    "files": [
      {
        "sha": "58271610625c1ca48f226ad3779acabc0c555a1c",
        "filename": "doc/release-notes.md",
        "status": "modified",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/a0a5b20156b7f0c7037bc7e11819ee86e579c47e/doc/release-notes.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/a0a5b20156b7f0c7037bc7e11819ee86e579c47e/doc/release-notes.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/release-notes.md?ref=a0a5b20156b7f0c7037bc7e11819ee86e579c47e",
        "patch": "@@ -6,3 +6,11 @@ release time)\n \n TODO\n \n+Warning\n+--------\n+\n+- There have been frequent reports of users running out of virtual memory on 32-bit systems\n+  during the initial sync.\n+  Hence it is recommended to use a 64-bit executable if possible.\n+  A 64-bit executable for Windows is planned for 0.9.\n+"
      }
    ]
  },
  {
    "sha": "8c3ba8be017791799dcfbc2126c24b0580fe48d5",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo4YzNiYThiZTAxNzc5MTc5OWRjZmJjMjEyNmMyNGIwNTgwZmU0OGQ1",
    "commit": {
      "author": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-04T07:17:57Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T08:16:48Z"
      },
      "message": "qt: use deleteLater to remove send entries\n\nUse deleteLater() instead of delete, as it is not allowed\nto delete widgets directly in an event handler.\nShould solve the MacOSX random crashes on send with coincontrol.\n\nRebased-From: 6c98cca9e47ddb9c786cd3f0445175c378515e0d",
      "tree": {
        "sha": "9ae303afc2fffe74834828f58f5769877db45060",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/9ae303afc2fffe74834828f58f5769877db45060"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/8c3ba8be017791799dcfbc2126c24b0580fe48d5",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/8c3ba8be017791799dcfbc2126c24b0580fe48d5",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/8c3ba8be017791799dcfbc2126c24b0580fe48d5",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/8c3ba8be017791799dcfbc2126c24b0580fe48d5/comments",
    "author": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "a0a5b20156b7f0c7037bc7e11819ee86e579c47e",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/a0a5b20156b7f0c7037bc7e11819ee86e579c47e",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/a0a5b20156b7f0c7037bc7e11819ee86e579c47e"
      }
    ],
    "stats": {
      "total": 4,
      "additions": 2,
      "deletions": 2
    },
    "files": [
      {
        "sha": "d64d70baa50aa4cf22ed35e37f53fc2f8884a202",
        "filename": "src/qt/sendcoinsdialog.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/8c3ba8be017791799dcfbc2126c24b0580fe48d5/src/qt/sendcoinsdialog.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/8c3ba8be017791799dcfbc2126c24b0580fe48d5/src/qt/sendcoinsdialog.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/qt/sendcoinsdialog.cpp?ref=8c3ba8be017791799dcfbc2126c24b0580fe48d5",
        "patch": "@@ -170,7 +170,7 @@ void SendCoinsDialog::clear()\n     // Remove entries until only one left\n     while(ui->entries->count())\n     {\n-        delete ui->entries->takeAt(0)->widget();\n+        ui->entries->takeAt(0)->widget()->deleteLater();\n     }\n     addEntry();\n \n@@ -226,7 +226,7 @@ void SendCoinsDialog::updateRemoveEnabled()\n \n void SendCoinsDialog::removeEntry(SendCoinsEntry* entry)\n {\n-    delete entry;\n+    entry->deleteLater();\n     updateRemoveEnabled();\n }\n "
      }
    ]
  },
  {
    "sha": "ec93d0aa4389b41d36f90678901322d9f135f743",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzplYzkzZDBhYTQzODliNDFkMzZmOTA2Nzg5MDEzMjJkOWYxMzVmNzQz",
    "commit": {
      "author": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-04T14:40:51Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T10:05:14Z"
      },
      "message": "Refuse to retransmit transactions without vins\n\nVersions of bitcoin before 0.8.6 have a bug that inserted\nempty transactions into the vtxPrev in the wallet, which will cause the node to be\nbanned when retransmitted, hence add a check for !tx.vin.empty()\nbefore RelayTransaction.",
      "tree": {
        "sha": "244aae51899043f0b32bce63d16b726613498112",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/244aae51899043f0b32bce63d16b726613498112"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/ec93d0aa4389b41d36f90678901322d9f135f743",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/ec93d0aa4389b41d36f90678901322d9f135f743",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/ec93d0aa4389b41d36f90678901322d9f135f743",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/ec93d0aa4389b41d36f90678901322d9f135f743/comments",
    "author": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "8c3ba8be017791799dcfbc2126c24b0580fe48d5",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/8c3ba8be017791799dcfbc2126c24b0580fe48d5",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/8c3ba8be017791799dcfbc2126c24b0580fe48d5"
      }
    ],
    "stats": {
      "total": 5,
      "additions": 4,
      "deletions": 1
    },
    "files": [
      {
        "sha": "a519c7a17f3c98e3f496998bb930703eaeaa12ca",
        "filename": "src/wallet.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/ec93d0aa4389b41d36f90678901322d9f135f743/src/wallet.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/ec93d0aa4389b41d36f90678901322d9f135f743/src/wallet.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/wallet.cpp?ref=ec93d0aa4389b41d36f90678901322d9f135f743",
        "patch": "@@ -844,7 +844,10 @@ void CWalletTx::RelayWalletTransaction()\n {\n     BOOST_FOREACH(const CMerkleTx& tx, vtxPrev)\n     {\n-        if (!tx.IsCoinBase())\n+        // Important: versions of bitcoin before 0.8.6 had a bug that inserted\n+        // empty transactions into the vtxPrev, which will cause the node to be\n+        // banned when retransmitted, hence the check for !tx.vin.empty()\n+        if (!tx.IsCoinBase() && !tx.vin.empty())\n             if (tx.GetDepthInMainChain() == 0)\n                 RelayTransaction((CTransaction)tx, tx.GetHash());\n     }"
      }
    ]
  },
  {
    "sha": "1eb11e32e82deec9a85b77b3969684739f4a5d4e",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzoxZWIxMWUzMmU4MmRlZWM5YTg1Yjc3YjM5Njk2ODQ3MzlmNGE1ZDRl",
    "commit": {
      "author": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-04T14:30:23Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T10:05:14Z"
      },
      "message": "Prevent empty transactions from being added to vtxPrev\n\nCWalletTx::AddSupportingTransactions() was adding empty transaction\nto vtxPrev in some cases. Skip over these.\n\nPart one of the solution to #3190. This prevents invalid vtxPrev from\nentering the wallet, but not current ones being transmitted.",
      "tree": {
        "sha": "3522f4e59d0237edf87e3525d47d61f443e40839",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/3522f4e59d0237edf87e3525d47d61f443e40839"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/1eb11e32e82deec9a85b77b3969684739f4a5d4e",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1eb11e32e82deec9a85b77b3969684739f4a5d4e",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/1eb11e32e82deec9a85b77b3969684739f4a5d4e",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1eb11e32e82deec9a85b77b3969684739f4a5d4e/comments",
    "author": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "ec93d0aa4389b41d36f90678901322d9f135f743",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/ec93d0aa4389b41d36f90678901322d9f135f743",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/ec93d0aa4389b41d36f90678901322d9f135f743"
      }
    ],
    "stats": {
      "total": 4,
      "additions": 4,
      "deletions": 0
    },
    "files": [
      {
        "sha": "1c5fb528ba107db3de3a5b809eb244ca33772f07",
        "filename": "src/wallet.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/1eb11e32e82deec9a85b77b3969684739f4a5d4e/src/wallet.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/1eb11e32e82deec9a85b77b3969684739f4a5d4e/src/wallet.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/wallet.cpp?ref=1eb11e32e82deec9a85b77b3969684739f4a5d4e",
        "patch": "@@ -740,6 +740,10 @@ void CWalletTx::AddSupportingTransactions()\n                 {\n                     tx = *mapWalletPrev[hash];\n                 }\n+                else\n+                {\n+                    continue;\n+                }\n \n                 int nDepth = tx.SetMerkleBranch();\n                 vtxPrev.push_back(tx);"
      }
    ]
  },
  {
    "sha": "5f553f8422667f956c6a750a0febcd262f37c506",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo1ZjU1M2Y4NDIyNjY3Zjk1NmM2YTc1MGEwZmViY2QyNjJmMzdjNTA2",
    "commit": {
      "author": {
        "name": "Patrick Strateman",
        "email": "patrick.strateman@gmail.com",
        "date": "2013-12-02T05:01:16Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T12:10:24Z"
      },
      "message": "dont use mmap in leveldb, this is a marginal performance hit\n\nfail on short writes\n\nEnsure new files referred to by the manifest are in the filesystem.",
      "tree": {
        "sha": "e62e5ded8b2fbef3c35dde1922853975a7c6ed78",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/e62e5ded8b2fbef3c35dde1922853975a7c6ed78"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/5f553f8422667f956c6a750a0febcd262f37c506",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5f553f8422667f956c6a750a0febcd262f37c506",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/5f553f8422667f956c6a750a0febcd262f37c506",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5f553f8422667f956c6a750a0febcd262f37c506/comments",
    "author": {
      "login": "pstratem",
      "id": 620611,
      "node_id": "MDQ6VXNlcjYyMDYxMQ==",
      "avatar_url": "https://avatars.githubusercontent.com/u/620611?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/pstratem",
      "html_url": "https://github.com/pstratem",
      "followers_url": "https://api.github.com/users/pstratem/followers",
      "following_url": "https://api.github.com/users/pstratem/following{/other_user}",
      "gists_url": "https://api.github.com/users/pstratem/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/pstratem/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/pstratem/subscriptions",
      "organizations_url": "https://api.github.com/users/pstratem/orgs",
      "repos_url": "https://api.github.com/users/pstratem/repos",
      "events_url": "https://api.github.com/users/pstratem/events{/privacy}",
      "received_events_url": "https://api.github.com/users/pstratem/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "1eb11e32e82deec9a85b77b3969684739f4a5d4e",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1eb11e32e82deec9a85b77b3969684739f4a5d4e",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/1eb11e32e82deec9a85b77b3969684739f4a5d4e"
      }
    ],
    "stats": {
      "total": 102,
      "additions": 86,
      "deletions": 16
    },
    "files": [
      {
        "sha": "9859e25c4b0166cb63e8819ac06d3fecd7d99862",
        "filename": "src/leveldb/util/env_posix.cc",
        "status": "modified",
        "additions": 86,
        "deletions": 16,
        "changes": 102,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/5f553f8422667f956c6a750a0febcd262f37c506/src/leveldb/util/env_posix.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/5f553f8422667f956c6a750a0febcd262f37c506/src/leveldb/util/env_posix.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/env_posix.cc?ref=5f553f8422667f956c6a750a0febcd262f37c506",
        "patch": "@@ -377,6 +377,91 @@ class PosixMmapFile : public WritableFile {\n   }\n };\n \n+class PosixWriteableFile : public WritableFile {\n+ private:\n+  std::string filename_;\n+  int fd_;\n+ public:\n+  PosixWriteableFile(const std::string& fname, int fd)\n+  : filename_(fname),\n+  fd_(fd)\n+  { }\n+\n+\n+  ~PosixWriteableFile() {\n+    if (fd_ >= 0) {\n+      PosixWriteableFile::Close();\n+    }\n+  }\n+\n+  virtual Status Append(const Slice& data) {\n+    Status s;\n+    int ret;\n+    ret = write(fd_, data.data(), data.size());\n+    if (ret < 0) {\n+      s = IOError(filename_, errno);\n+    } else if (ret < data.size()) {\n+      s = Status::IOError(filename_, \"short write\");\n+    }\n+    \n+    return s;\n+  }\n+\n+  virtual Status Close() {\n+    Status s;\n+    if (close(fd_) < 0) {\n+      s = IOError(filename_, errno);\n+    }\n+    fd_ = -1;\n+    return s;\n+  }\n+\n+  virtual Status Flush() {\n+    return Status::OK();\n+  }\n+  \n+  Status SyncDirIfManifest() {\n+    const char* f = filename_.c_str();\n+    const char* sep = strrchr(f, '/');\n+    Slice basename;\n+    std::string dir;\n+    if (sep == NULL) {\n+      dir = \".\";\n+      basename = f;\n+    } else {\n+      dir = std::string(f, sep - f);\n+      basename = sep + 1;\n+    }\n+    Status s;\n+    if (basename.starts_with(\"MANIFEST\")) {\n+      int fd = open(dir.c_str(), O_RDONLY);\n+      if (fd < 0) {\n+        s = IOError(dir, errno);\n+      } else {\n+        if (fsync(fd) < 0) {\n+          s = IOError(dir, errno);\n+        }\n+        close(fd);\n+      }\n+    }\n+    return s;\n+  }\n+  \n+  virtual Status Sync() {\n+    // Ensure new files referred to by the manifest are in the filesystem.\n+    Status s = SyncDirIfManifest();\n+    if (!s.ok()) {\n+      return s;\n+    }\n+    \n+    if (fdatasync(fd_) < 0) {\n+      s = IOError(filename_, errno);\n+    }\n+    \n+    return s;\n+  }\n+};\n+\n static int LockOrUnlock(int fd, bool lock) {\n   errno = 0;\n   struct flock f;\n@@ -439,21 +524,6 @@ class PosixEnv : public Env {\n     int fd = open(fname.c_str(), O_RDONLY);\n     if (fd < 0) {\n       s = IOError(fname, errno);\n-    } else if (mmap_limit_.Acquire()) {\n-      uint64_t size;\n-      s = GetFileSize(fname, &size);\n-      if (s.ok()) {\n-        void* base = mmap(NULL, size, PROT_READ, MAP_SHARED, fd, 0);\n-        if (base != MAP_FAILED) {\n-          *result = new PosixMmapReadableFile(fname, base, size, &mmap_limit_);\n-        } else {\n-          s = IOError(fname, errno);\n-        }\n-      }\n-      close(fd);\n-      if (!s.ok()) {\n-        mmap_limit_.Release();\n-      }\n     } else {\n       *result = new PosixRandomAccessFile(fname, fd);\n     }\n@@ -468,7 +538,7 @@ class PosixEnv : public Env {\n       *result = NULL;\n       s = IOError(fname, errno);\n     } else {\n-      *result = new PosixMmapFile(fname, fd, page_size_);\n+      *result = new PosixWriteableFile(fname, fd);\n     }\n     return s;\n   }"
      }
    ]
  },
  {
    "sha": "77f7bcb352cfaac2549bf7f25dcaeb808204c185",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo3N2Y3YmNiMzUyY2ZhYWMyNTQ5YmY3ZjI1ZGNhZWI4MDgyMDRjMTg1",
    "commit": {
      "author": {
        "name": "Warren Togami",
        "email": "wtogami@gmail.com",
        "date": "2013-12-03T05:21:46Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T12:10:24Z"
      },
      "message": "LevelDB: use PosixWriteableFile only on MacOS X\n\nmmap is proven on the other platforms, we are not changing it at\nthe last moment before release.",
      "tree": {
        "sha": "2f78503b04cd8c0fa4ae9f7b81c795c8270d8ebc",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/2f78503b04cd8c0fa4ae9f7b81c795c8270d8ebc"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/77f7bcb352cfaac2549bf7f25dcaeb808204c185",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/77f7bcb352cfaac2549bf7f25dcaeb808204c185",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/77f7bcb352cfaac2549bf7f25dcaeb808204c185",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/77f7bcb352cfaac2549bf7f25dcaeb808204c185/comments",
    "author": {
      "login": "wtogami",
      "id": 93665,
      "node_id": "MDQ6VXNlcjkzNjY1",
      "avatar_url": "https://avatars.githubusercontent.com/u/93665?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/wtogami",
      "html_url": "https://github.com/wtogami",
      "followers_url": "https://api.github.com/users/wtogami/followers",
      "following_url": "https://api.github.com/users/wtogami/following{/other_user}",
      "gists_url": "https://api.github.com/users/wtogami/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/wtogami/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/wtogami/subscriptions",
      "organizations_url": "https://api.github.com/users/wtogami/orgs",
      "repos_url": "https://api.github.com/users/wtogami/repos",
      "events_url": "https://api.github.com/users/wtogami/events{/privacy}",
      "received_events_url": "https://api.github.com/users/wtogami/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "5f553f8422667f956c6a750a0febcd262f37c506",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/5f553f8422667f956c6a750a0febcd262f37c506",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/5f553f8422667f956c6a750a0febcd262f37c506"
      }
    ],
    "stats": {
      "total": 23,
      "additions": 23,
      "deletions": 0
    },
    "files": [
      {
        "sha": "aff5df9db810e694cb81e61cef797258e6a7de6e",
        "filename": "src/leveldb/util/env_posix.cc",
        "status": "modified",
        "additions": 23,
        "deletions": 0,
        "changes": 23,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/77f7bcb352cfaac2549bf7f25dcaeb808204c185/src/leveldb/util/env_posix.cc",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/77f7bcb352cfaac2549bf7f25dcaeb808204c185/src/leveldb/util/env_posix.cc",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/util/env_posix.cc?ref=77f7bcb352cfaac2549bf7f25dcaeb808204c185",
        "patch": "@@ -377,6 +377,7 @@ class PosixMmapFile : public WritableFile {\n   }\n };\n \n+#if defined(OS_MACOSX)\n class PosixWriteableFile : public WritableFile {\n  private:\n   std::string filename_;\n@@ -461,6 +462,7 @@ class PosixWriteableFile : public WritableFile {\n     return s;\n   }\n };\n+#endif\n \n static int LockOrUnlock(int fd, bool lock) {\n   errno = 0;\n@@ -524,6 +526,23 @@ class PosixEnv : public Env {\n     int fd = open(fname.c_str(), O_RDONLY);\n     if (fd < 0) {\n       s = IOError(fname, errno);\n+#if !defined(OS_MACOSX)\n+    } else if (mmap_limit_.Acquire()) {\n+      uint64_t size;\n+      s = GetFileSize(fname, &size);\n+      if (s.ok()) {\n+        void* base = mmap(NULL, size, PROT_READ, MAP_SHARED, fd, 0);\n+        if (base != MAP_FAILED) {\n+          *result = new PosixMmapReadableFile(fname, base, size, &mmap_limit_);\n+        } else {\n+          s = IOError(fname, errno);\n+        }\n+      }\n+      close(fd);\n+      if (!s.ok()) {\n+        mmap_limit_.Release();\n+      }\n+#endif\n     } else {\n       *result = new PosixRandomAccessFile(fname, fd);\n     }\n@@ -538,7 +557,11 @@ class PosixEnv : public Env {\n       *result = NULL;\n       s = IOError(fname, errno);\n     } else {\n+#if defined(OS_MACOSX)\n       *result = new PosixWriteableFile(fname, fd);\n+#else\n+      *result = new PosixMmapFile(fname, fd, page_size_);\n+#endif\n     }\n     return s;\n   }"
      }
    ]
  },
  {
    "sha": "d3381e71a1b30a804b5b0fc993b189af53205bb2",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpkMzM4MWU3MWExYjMwYTgwNGI1YjBmYzk5M2IxODlhZjUzMjA1YmIy",
    "commit": {
      "author": {
        "name": "theuni",
        "email": "theuni-nospam@xbmc.org",
        "date": "2013-11-26T00:24:23Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T12:10:24Z"
      },
      "message": "leveldb: Use native Darwin memory barriers",
      "tree": {
        "sha": "ee085990b48a2bb9fd659b9b7293f9ea43afebdf",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/ee085990b48a2bb9fd659b9b7293f9ea43afebdf"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/d3381e71a1b30a804b5b0fc993b189af53205bb2",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/d3381e71a1b30a804b5b0fc993b189af53205bb2",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/d3381e71a1b30a804b5b0fc993b189af53205bb2",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/d3381e71a1b30a804b5b0fc993b189af53205bb2/comments",
    "author": null,
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "77f7bcb352cfaac2549bf7f25dcaeb808204c185",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/77f7bcb352cfaac2549bf7f25dcaeb808204c185",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/77f7bcb352cfaac2549bf7f25dcaeb808204c185"
      }
    ],
    "stats": {
      "total": 16,
      "additions": 8,
      "deletions": 8
    },
    "files": [
      {
        "sha": "844fba25fedf3f5adb9c0aea0626f10e5a2f0eff",
        "filename": "src/leveldb/port/atomic_pointer.h",
        "status": "modified",
        "additions": 8,
        "deletions": 8,
        "changes": 16,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/d3381e71a1b30a804b5b0fc993b189af53205bb2/src/leveldb/port/atomic_pointer.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/d3381e71a1b30a804b5b0fc993b189af53205bb2/src/leveldb/port/atomic_pointer.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/leveldb/port/atomic_pointer.h?ref=d3381e71a1b30a804b5b0fc993b189af53205bb2",
        "patch": "@@ -50,6 +50,13 @@ namespace port {\n // http://msdn.microsoft.com/en-us/library/ms684208(v=vs.85).aspx\n #define LEVELDB_HAVE_MEMORY_BARRIER\n \n+// Mac OS\n+#elif defined(OS_MACOSX)\n+inline void MemoryBarrier() {\n+  OSMemoryBarrier();\n+}\n+#define LEVELDB_HAVE_MEMORY_BARRIER\n+\n // Gcc on x86\n #elif defined(ARCH_CPU_X86_FAMILY) && defined(__GNUC__)\n inline void MemoryBarrier() {\n@@ -62,19 +69,12 @@ inline void MemoryBarrier() {\n // Sun Studio\n #elif defined(ARCH_CPU_X86_FAMILY) && defined(__SUNPRO_CC)\n inline void MemoryBarrier() {\n-  // See http://gcc.gnu.org/ml/gcc/2003-04/msg01180.html for a discussion on\n+     \t// See http://gcc.gnu.org/ml/gcc/2003-04/msg01180.html for a discussion on\n   // this idiom. Also see http://en.wikipedia.org/wiki/Memory_ordering.\n   asm volatile(\"\" : : : \"memory\");\n }\n #define LEVELDB_HAVE_MEMORY_BARRIER\n \n-// Mac OS\n-#elif defined(OS_MACOSX)\n-inline void MemoryBarrier() {\n-  OSMemoryBarrier();\n-}\n-#define LEVELDB_HAVE_MEMORY_BARRIER\n-\n // ARM Linux\n #elif defined(ARCH_CPU_ARM_FAMILY) && defined(__linux__)\n typedef void (*LinuxKernelMemoryBarrierFunc)(void);"
      }
    ]
  },
  {
    "sha": "03a7d673876dc8fbae876290b455c02b0cac80bd",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzowM2E3ZDY3Mzg3NmRjOGZiYWU4NzYyOTBiNDU1YzAyYjBjYWM4MGJk",
    "commit": {
      "author": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-11-27T12:17:13Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2013-12-05T12:11:26Z"
      },
      "message": "Release notes for 0.8.6",
      "tree": {
        "sha": "ddba3d06fd5dd17263bac8bf16fcab52a8a7e046",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/ddba3d06fd5dd17263bac8bf16fcab52a8a7e046"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/03a7d673876dc8fbae876290b455c02b0cac80bd",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/03a7d673876dc8fbae876290b455c02b0cac80bd",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/03a7d673876dc8fbae876290b455c02b0cac80bd",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/03a7d673876dc8fbae876290b455c02b0cac80bd/comments",
    "author": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "d3381e71a1b30a804b5b0fc993b189af53205bb2",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/d3381e71a1b30a804b5b0fc993b189af53205bb2",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/d3381e71a1b30a804b5b0fc993b189af53205bb2"
      }
    ],
    "stats": {
      "total": 29,
      "additions": 28,
      "deletions": 1
    },
    "files": [
      {
        "sha": "2a4874b7b2ac5168fa280974439a45d7e0e296e2",
        "filename": "doc/release-notes.md",
        "status": "modified",
        "additions": 28,
        "deletions": 1,
        "changes": 29,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/03a7d673876dc8fbae876290b455c02b0cac80bd/doc/release-notes.md",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/03a7d673876dc8fbae876290b455c02b0cac80bd/doc/release-notes.md",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/doc/release-notes.md?ref=03a7d673876dc8fbae876290b455c02b0cac80bd",
        "patch": "@@ -4,7 +4,34 @@ release time)\n 0.8.6 changes\n =============\n \n-TODO\n+- Default block size increase for miners\n+  (see https://gist.github.com/gavinandresen/7670433#086-accept-into-block)\n+\n+- Remove the all-outputs-must-be-greater-than-CENT-to-qualify-as-free rule for relaying\n+  (see https://gist.github.com/gavinandresen/7670433#086-relaying)\n+\n+- Lower maximum size for free transaction creation\n+  (see https://gist.github.com/gavinandresen/7670433#086-wallet)\n+\n+- OSX block chain database corruption fixes\n+  - Update leveldb to 1.13\n+  - Use fcntl with `F_FULLSYNC` instead of fsync on OSX\n+  - Use native Darwin memory barriers\n+  - Replace use of mmap in leveldb for improved reliability (only on OSX)\n+\n+- Fix nodes forwarding transactions with empty vins and getting banned\n+\n+- Network code performance and robustness improvements\n+\n+- Additional debug.log logging for diagnosis of network problems, log timestamps by default\n+\n+- Fix Bitcoin-Qt startup crash when clicking dock icon on OSX \n+\n+- Fix memory leaks in CKey::SetCompactSignature() and Key::SignCompact()\n+\n+- Fix rare GUI crash on send\n+\n+- Various small GUI, documentation and build fixes\n \n Warning\n --------"
      }
    ]
  },
  {
    "sha": "f4134ee30100a3ba8a25e3f7ac6c4e0b39fb05a3",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpmNDEzNGVlMzAxMDBhM2JhOGEyNWUzZjdhYzZjNGUwYjM5ZmIwNWEz",
    "commit": {
      "author": {
        "name": "Cory Fields",
        "email": "cory-nospam-@coryfields.com",
        "date": "2015-01-09T11:27:06Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2015-01-10T08:13:53Z"
      },
      "message": "consensus: guard against openssl's new strict DER checks\n\nNew versions of OpenSSL will reject non-canonical DER signatures. However,\nit'll happily decode them. Decode then re-encode before verification in order\nto ensure that it is properly consumed.\n\nGithub-Pull: #5634\nRebased-From: 488ed32f2ada1d1dd108fc245d025c4d5f252783",
      "tree": {
        "sha": "dde43d575a215169199c3798857f5e46e68ecb2e",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/dde43d575a215169199c3798857f5e46e68ecb2e"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/f4134ee30100a3ba8a25e3f7ac6c4e0b39fb05a3",
      "comment_count": 0,
      "verification": {
        "verified": true,
        "reason": "valid",
        "signature": "-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v1\n\niQEcBAABCgAGBQJUsN9CAAoJEHSBCwEjRsmmVocIALElKp7uE/tlyWExr9ivw3Wo\nE/fPz/0fcQrI1nZ0n41XqnlJdXkU/3vXHoUcMVEUmo5cTnL5LCcg8X5ZEHn/ji/t\nz2b+0BmUpyLzK84hdM1RVbm3DOqBjqBjK8v8AnXodH6uU7IzvsC+eCDVCogZ5sLB\nYhiX5y39NXpgOXUqL367suhfBDM7We8enhwVm+6EXB3/crfEqV4pQvuTm+3to3cS\ndXZbRZQssCEALK09g8UlFh1ffEEccpx4TrR4MDltJUVAI6NAkJkeWZ/S1rkK8+ZD\nWCAHtQ/f9sOKXIRmKqLEha3kg7Zqbb+roBElCDWpzILqjqCuCDDuGjoXtdmKoeQ=\n=ln+/\n-----END PGP SIGNATURE-----",
        "payload": "tree dde43d575a215169199c3798857f5e46e68ecb2e\nparent 03a7d673876dc8fbae876290b455c02b0cac80bd\nauthor Cory Fields <cory-nospam-@coryfields.com> 1420802826 +0100\ncommitter Wladimir J. van der Laan <laanwj@gmail.com> 1420877633 +0100\n\nconsensus: guard against openssl's new strict DER checks\n\nNew versions of OpenSSL will reject non-canonical DER signatures. However,\nit'll happily decode them. Decode then re-encode before verification in order\nto ensure that it is properly consumed.\n\nGithub-Pull: #5634\nRebased-From: 488ed32f2ada1d1dd108fc245d025c4d5f252783\n"
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f4134ee30100a3ba8a25e3f7ac6c4e0b39fb05a3",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/f4134ee30100a3ba8a25e3f7ac6c4e0b39fb05a3",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f4134ee30100a3ba8a25e3f7ac6c4e0b39fb05a3/comments",
    "author": {
      "login": "theuni",
      "id": 417043,
      "node_id": "MDQ6VXNlcjQxNzA0Mw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/417043?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/theuni",
      "html_url": "https://github.com/theuni",
      "followers_url": "https://api.github.com/users/theuni/followers",
      "following_url": "https://api.github.com/users/theuni/following{/other_user}",
      "gists_url": "https://api.github.com/users/theuni/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/theuni/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/theuni/subscriptions",
      "organizations_url": "https://api.github.com/users/theuni/orgs",
      "repos_url": "https://api.github.com/users/theuni/repos",
      "events_url": "https://api.github.com/users/theuni/events{/privacy}",
      "received_events_url": "https://api.github.com/users/theuni/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "03a7d673876dc8fbae876290b455c02b0cac80bd",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/03a7d673876dc8fbae876290b455c02b0cac80bd",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/03a7d673876dc8fbae876290b455c02b0cac80bd"
      }
    ],
    "stats": {
      "total": 15,
      "additions": 12,
      "deletions": 3
    },
    "files": [
      {
        "sha": "8a1bfef771a69f679d74a2aa233e605a789b7a1e",
        "filename": "src/key.cpp",
        "status": "modified",
        "additions": 12,
        "deletions": 3,
        "changes": 15,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/f4134ee30100a3ba8a25e3f7ac6c4e0b39fb05a3/src/key.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/f4134ee30100a3ba8a25e3f7ac6c4e0b39fb05a3/src/key.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/key.cpp?ref=f4134ee30100a3ba8a25e3f7ac6c4e0b39fb05a3",
        "patch": "@@ -376,11 +376,20 @@ bool CKey::SetCompactSignature(uint256 hash, const std::vector<unsigned char>& v\n \n bool CKey::Verify(uint256 hash, const std::vector<unsigned char>& vchSig)\n {\n-    // -1 = error, 0 = bad sig, 1 = good\n-    if (ECDSA_verify(0, (unsigned char*)&hash, sizeof(hash), &vchSig[0], vchSig.size(), pkey) != 1)\n+    // New versions of OpenSSL will reject non-canonical DER signatures. de/re-serialize first.\n+    unsigned char *norm_der = NULL;\n+    ECDSA_SIG *norm_sig = ECDSA_SIG_new();\n+    const unsigned char* sigptr = &vchSig[0];\n+    d2i_ECDSA_SIG(&norm_sig, &sigptr, vchSig.size());\n+    int derlen = i2d_ECDSA_SIG(norm_sig, &norm_der);\n+    ECDSA_SIG_free(norm_sig);\n+    if (derlen <= 0)\n         return false;\n \n-    return true;\n+    // -1 = error, 0 = bad sig, 1 = good\n+    bool ret = ECDSA_verify(0, (unsigned char*)&hash, sizeof(hash), norm_der, derlen, pkey) == 1;\n+    OPENSSL_free(norm_der);\n+    return ret;\n }\n \n bool CKey::VerifyCompact(uint256 hash, const std::vector<unsigned char>& vchSig)"
      }
    ]
  },
  {
    "sha": "91e1332011ca647362f95f34ae6c530640bfef98",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo5MWUxMzMyMDExY2E2NDczNjJmOTVmMzRhZTZjNTMwNjQwYmZlZjk4",
    "commit": {
      "author": {
        "name": "Cory Fields",
        "email": "cory-nospam-@coryfields.com",
        "date": "2015-01-10T08:14:17Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2015-01-10T08:14:17Z"
      },
      "message": "fail immediately on an empty signature\n\nGithub-Pull: #5634\nRebased-From: 8dccba6a45db0466370726ed462b9da2eae43bce",
      "tree": {
        "sha": "417ada4d7eb58da3e6ac5a146e38336d3fb78315",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/417ada4d7eb58da3e6ac5a146e38336d3fb78315"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/91e1332011ca647362f95f34ae6c530640bfef98",
      "comment_count": 0,
      "verification": {
        "verified": true,
        "reason": "valid",
        "signature": "-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v1\n\niQEcBAABCgAGBQJUsN9wAAoJEHSBCwEjRsmmBfoIAISznK8wkQI/JUx52gSVsaAH\nGkQvnu4G4Kzk7x5SRVBbmHQZBhtt1ZjH9XKV02pdi+ySFFwM391Aslu8zwuXqo0N\nQX93trH5wUVHegN681u5Iufk7/jrXmZDGS7XcS0GVQYub0eJ846Pi3GcS9iBJb5S\ncoHgDeWv1nbxShATv/vmnsMl0Yh9aiIYR+V3+CYkJICgzgky9vQqg9FSz4yfVoNm\nzL5UDCJPWt3eXMOdOHx5tEqW7oNvBBYBnLZNyGNwUSLg3uEXjGkZweOydKMmcrhf\nA3KBXsg0PPfyzDz+Pf0v8MKAuUATY8SRE/RKFfEVNxHHFHW17xL3g3I+V72J5Rk=\n=tylY\n-----END PGP SIGNATURE-----",
        "payload": "tree 417ada4d7eb58da3e6ac5a146e38336d3fb78315\nparent f4134ee30100a3ba8a25e3f7ac6c4e0b39fb05a3\nauthor Cory Fields <cory-nospam-@coryfields.com> 1420877657 +0100\ncommitter Wladimir J. van der Laan <laanwj@gmail.com> 1420877657 +0100\n\nfail immediately on an empty signature\n\nGithub-Pull: #5634\nRebased-From: 8dccba6a45db0466370726ed462b9da2eae43bce\n"
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/91e1332011ca647362f95f34ae6c530640bfef98",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/91e1332011ca647362f95f34ae6c530640bfef98",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/91e1332011ca647362f95f34ae6c530640bfef98/comments",
    "author": {
      "login": "theuni",
      "id": 417043,
      "node_id": "MDQ6VXNlcjQxNzA0Mw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/417043?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/theuni",
      "html_url": "https://github.com/theuni",
      "followers_url": "https://api.github.com/users/theuni/followers",
      "following_url": "https://api.github.com/users/theuni/following{/other_user}",
      "gists_url": "https://api.github.com/users/theuni/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/theuni/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/theuni/subscriptions",
      "organizations_url": "https://api.github.com/users/theuni/orgs",
      "repos_url": "https://api.github.com/users/theuni/repos",
      "events_url": "https://api.github.com/users/theuni/events{/privacy}",
      "received_events_url": "https://api.github.com/users/theuni/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "f4134ee30100a3ba8a25e3f7ac6c4e0b39fb05a3",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f4134ee30100a3ba8a25e3f7ac6c4e0b39fb05a3",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/f4134ee30100a3ba8a25e3f7ac6c4e0b39fb05a3"
      }
    ],
    "stats": {
      "total": 3,
      "additions": 3,
      "deletions": 0
    },
    "files": [
      {
        "sha": "7fcb17d574f9ea70b840d2c8d9826d8354aca8ec",
        "filename": "src/key.cpp",
        "status": "modified",
        "additions": 3,
        "deletions": 0,
        "changes": 3,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/91e1332011ca647362f95f34ae6c530640bfef98/src/key.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/91e1332011ca647362f95f34ae6c530640bfef98/src/key.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/key.cpp?ref=91e1332011ca647362f95f34ae6c530640bfef98",
        "patch": "@@ -376,6 +376,9 @@ bool CKey::SetCompactSignature(uint256 hash, const std::vector<unsigned char>& v\n \n bool CKey::Verify(uint256 hash, const std::vector<unsigned char>& vchSig)\n {\n+    if (vchSig.empty())\n+        return false;\n+\n     // New versions of OpenSSL will reject non-canonical DER signatures. de/re-serialize first.\n     unsigned char *norm_der = NULL;\n     ECDSA_SIG *norm_sig = ECDSA_SIG_new();"
      }
    ]
  },
  {
    "sha": "f19dded6e4bf6b5a345de899863310281846eb61",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpmMTlkZGVkNmU0YmY2YjVhMzQ1ZGU4OTk4NjMzMTAyODE4NDZlYjYx",
    "commit": {
      "author": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2015-01-12T10:05:04Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2015-01-12T10:05:04Z"
      },
      "message": "Improve robustness of DER recoding code\n\nAdd some defensive programming on top of #5634.\n\nThis copies the respective OpenSSL code in ECDSA_verify in\nOpenSSL pre-1.0.1k (e.g. https://github.com/openssl/openssl/blob/OpenSSL_1_0_1j/crypto/ecdsa/ecs_vrf.c#L89)\nmore closely.\n\nAs reported by @sergiodemianlerner.\n\nGithub-Pull: #5640\nRebased-From: c6b7b29f232c651f898eeffb93f36c8f537c56d2",
      "tree": {
        "sha": "a4d634f2d1b44633baa39edd1a0b0d0f683d8264",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/a4d634f2d1b44633baa39edd1a0b0d0f683d8264"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/f19dded6e4bf6b5a345de899863310281846eb61",
      "comment_count": 0,
      "verification": {
        "verified": true,
        "reason": "valid",
        "signature": "-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v1\n\niQEcBAABCgAGBQJUs5x1AAoJEHSBCwEjRsmmuckH/RClDbOkeaN5dAlfUHZ1hyxw\nqbpd772OfqbN91Ft0ZXPQhUA6YISkbZY6cgtGrnZTPnSAHe8qyGY+GGCMMMHRIHR\nKETwVOiAFADOL39O8BahN4LJAB+jsbyZPqChYYoJ336W1Vcc3xBKmj2M7vepEsN/\n+0X3sIIyn/oZ14A+Ez5eZEaSVcOI8etagSToT/q9JQm7PBIE5lF/Uf93RLZI1scQ\nLgJ9wHvQGvEj6On1PPC72yZW0UjpzMZLx2fCyOPQRuk1YulLdFVWjNNS8++S+M8n\nw+eC6V3VHBZPdIjPCXVij64bqWuvBaCpU5ame2QUvm2Tn4az+8Db3Ar5RL3Mbuw=\n=cpfD\n-----END PGP SIGNATURE-----",
        "payload": "tree a4d634f2d1b44633baa39edd1a0b0d0f683d8264\nparent 91e1332011ca647362f95f34ae6c530640bfef98\nauthor Wladimir J. van der Laan <laanwj@gmail.com> 1421057104 +0100\ncommitter Wladimir J. van der Laan <laanwj@gmail.com> 1421057104 +0100\n\nImprove robustness of DER recoding code\n\nAdd some defensive programming on top of #5634.\n\nThis copies the respective OpenSSL code in ECDSA_verify in\nOpenSSL pre-1.0.1k (e.g. https://github.com/openssl/openssl/blob/OpenSSL_1_0_1j/crypto/ecdsa/ecs_vrf.c#L89)\nmore closely.\n\nAs reported by @sergiodemianlerner.\n\nGithub-Pull: #5640\nRebased-From: c6b7b29f232c651f898eeffb93f36c8f537c56d2\n"
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f19dded6e4bf6b5a345de899863310281846eb61",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/f19dded6e4bf6b5a345de899863310281846eb61",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f19dded6e4bf6b5a345de899863310281846eb61/comments",
    "author": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "91e1332011ca647362f95f34ae6c530640bfef98",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/91e1332011ca647362f95f34ae6c530640bfef98",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/91e1332011ca647362f95f34ae6c530640bfef98"
      }
    ],
    "stats": {
      "total": 13,
      "additions": 12,
      "deletions": 1
    },
    "files": [
      {
        "sha": "2b25308fe550ae5390aaba0c90e19dc599b73098",
        "filename": "src/key.cpp",
        "status": "modified",
        "additions": 12,
        "deletions": 1,
        "changes": 13,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/f19dded6e4bf6b5a345de899863310281846eb61/src/key.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/f19dded6e4bf6b5a345de899863310281846eb61/src/key.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/key.cpp?ref=f19dded6e4bf6b5a345de899863310281846eb61",
        "patch": "@@ -383,7 +383,18 @@ bool CKey::Verify(uint256 hash, const std::vector<unsigned char>& vchSig)\n     unsigned char *norm_der = NULL;\n     ECDSA_SIG *norm_sig = ECDSA_SIG_new();\n     const unsigned char* sigptr = &vchSig[0];\n-    d2i_ECDSA_SIG(&norm_sig, &sigptr, vchSig.size());\n+    assert(norm_sig);\n+    if (d2i_ECDSA_SIG(&norm_sig, &sigptr, vchSig.size()) == NULL)\n+    {\n+        /* As of OpenSSL 1.0.0p d2i_ECDSA_SIG frees and nulls the pointer on\n+         * error. But OpenSSL's own use of this function redundantly frees the\n+         * result. As ECDSA_SIG_free(NULL) is a no-op, and in the absence of a\n+         * clear contract for the function behaving the same way is more\n+         * conservative.\n+         */\n+        ECDSA_SIG_free(norm_sig);\n+        return false;\n+    }\n     int derlen = i2d_ECDSA_SIG(norm_sig, &norm_der);\n     ECDSA_SIG_free(norm_sig);\n     if (derlen <= 0)"
      }
    ]
  },
  {
    "sha": "da7ba593d74e7a09599d4b7df5ecccd79a3a1c64",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzpkYTdiYTU5M2Q3NGU3YTA5NTk5ZDRiN2RmNWVjY2NkNzlhM2ExYzY0",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2015-02-06T04:56:10Z"
      },
      "committer": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2015-02-06T19:17:29Z"
      },
      "message": "Implement BIP 66 validation rules and switchover logic",
      "tree": {
        "sha": "069a299e0a2aed8c67220705a4224b19090a468f",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/069a299e0a2aed8c67220705a4224b19090a468f"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/da7ba593d74e7a09599d4b7df5ecccd79a3a1c64",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/da7ba593d74e7a09599d4b7df5ecccd79a3a1c64",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/da7ba593d74e7a09599d4b7df5ecccd79a3a1c64",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/da7ba593d74e7a09599d4b7df5ecccd79a3a1c64/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "f19dded6e4bf6b5a345de899863310281846eb61",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/f19dded6e4bf6b5a345de899863310281846eb61",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/f19dded6e4bf6b5a345de899863310281846eb61"
      }
    ],
    "stats": {
      "total": 104,
      "additions": 104,
      "deletions": 0
    },
    "files": [
      {
        "sha": "b4f69f8c969a8233a2e37433a6b52fe45c84aaf0",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/da7ba593d74e7a09599d4b7df5ecccd79a3a1c64/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/da7ba593d74e7a09599d4b7df5ecccd79a3a1c64/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=da7ba593d74e7a09599d4b7df5ecccd79a3a1c64",
        "patch": "@@ -1645,6 +1645,12 @@ bool CBlock::ConnectBlock(CValidationState &state, CBlockIndex* pindex, CCoinsVi\n     unsigned int flags = SCRIPT_VERIFY_NOCACHE |\n                          (fStrictPayToScriptHash ? SCRIPT_VERIFY_P2SH : SCRIPT_VERIFY_NONE);\n \n+    if (nVersion >= 3 &&\n+        ((!fTestNet && CBlockIndex::IsSuperMajority(3, pindex->pprev, 750, 1000)) ||\n+            (fTestNet && CBlockIndex::IsSuperMajority(3, pindex->pprev, 51, 100)))) {\n+        flags |= SCRIPT_VERIFY_DERSIG;\n+    }\n+\n     CBlockUndo blockundo;\n \n     CCheckQueueControl<CScriptCheck> control(fScriptChecks && nScriptCheckThreads ? &scriptcheckqueue : NULL);\n@@ -2193,6 +2199,15 @@ bool CBlock::AcceptBlock(CValidationState &state, CDiskBlockPos *dbp)\n                 return state.Invalid(error(\"AcceptBlock() : rejected nVersion=1 block\"));\n             }\n         }\n+        // Reject block.nVersion=2 blocks when 95% (75% on testnet) of the network has upgraded:\n+        if (nVersion < 3)\n+        {\n+            if ((!fTestNet && CBlockIndex::IsSuperMajority(3, pindexPrev, 950, 1000)) ||\n+                (fTestNet && CBlockIndex::IsSuperMajority(3, pindexPrev, 75, 100)))\n+            {\n+                return state.Invalid(error(\"AcceptBlock() : rejected nVersion=2 block\"));\n+            }\n+        }\n         // Enforce block.nVersion=2 rule that the coinbase starts with serialized block height\n         if (nVersion >= 2)\n         {"
      },
      {
        "sha": "8a61716e9353ab02cca8742938334b97bc25b525",
        "filename": "src/script.cpp",
        "status": "modified",
        "additions": 88,
        "deletions": 0,
        "changes": 88,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/da7ba593d74e7a09599d4b7df5ecccd79a3a1c64/src/script.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/da7ba593d74e7a09599d4b7df5ecccd79a3a1c64/src/script.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/script.cpp?ref=da7ba593d74e7a09599d4b7df5ecccd79a3a1c64",
        "patch": "@@ -289,6 +289,86 @@ bool IsCanonicalSignature(const valtype &vchSig) {\n     return true;\n }\n \n+// BIP 66 defined signature encoding check. This largely overlaps with\n+// IsCanonicalSignature above, but lacks hashtype constraints, and uses the\n+// exact implementation code from BIP 66.\n+bool static IsValidSignatureEncoding(const std::vector<unsigned char> &sig) {\n+    // Format: 0x30 [total-length] 0x02 [R-length] [R] 0x02 [S-length] [S] [sighash]\n+    // * total-length: 1-byte length descriptor of everything that follows,\n+    //   excluding the sighash byte.\n+    // * R-length: 1-byte length descriptor of the R value that follows.\n+    // * R: arbitrary-length big-endian encoded R value. It must use the shortest\n+    //   possible encoding for a positive integers (which means no null bytes at\n+    //   the start, except a single one when the next byte has its highest bit set).\n+    // * S-length: 1-byte length descriptor of the S value that follows.\n+    // * S: arbitrary-length big-endian encoded S value. The same rules apply.\n+    // * sighash: 1-byte value indicating what data is hashed (not part of the DER\n+    //   signature)\n+\n+    // Minimum and maximum size constraints.\n+    if (sig.size() < 9) return false;\n+    if (sig.size() > 73) return false;\n+\n+    // A signature is of type 0x30 (compound).\n+    if (sig[0] != 0x30) return false;\n+\n+    // Make sure the length covers the entire signature.\n+    if (sig[1] != sig.size() - 3) return false;\n+\n+    // Extract the length of the R element.\n+    unsigned int lenR = sig[3];\n+\n+    // Make sure the length of the S element is still inside the signature.\n+    if (5 + lenR >= sig.size()) return false;\n+\n+    // Extract the length of the S element.\n+    unsigned int lenS = sig[5 + lenR];\n+\n+    // Verify that the length of the signature matches the sum of the length\n+    // of the elements.\n+    if ((size_t)(lenR + lenS + 7) != sig.size()) return false;\n+ \n+    // Check whether the R element is an integer.\n+    if (sig[2] != 0x02) return false;\n+\n+    // Zero-length integers are not allowed for R.\n+    if (lenR == 0) return false;\n+\n+    // Negative numbers are not allowed for R.\n+    if (sig[4] & 0x80) return false;\n+\n+    // Null bytes at the start of R are not allowed, unless R would\n+    // otherwise be interpreted as a negative number.\n+    if (lenR > 1 && (sig[4] == 0x00) && !(sig[5] & 0x80)) return false;\n+\n+    // Check whether the S element is an integer.\n+    if (sig[lenR + 4] != 0x02) return false;\n+\n+    // Zero-length integers are not allowed for S.\n+    if (lenS == 0) return false;\n+\n+    // Negative numbers are not allowed for S.\n+    if (sig[lenR + 6] & 0x80) return false;\n+\n+    // Null bytes at the start of S are not allowed, unless S would otherwise be\n+    // interpreted as a negative number.\n+    if (lenS > 1 && (sig[lenR + 6] == 0x00) && !(sig[lenR + 7] & 0x80)) return false;\n+\n+    return true;\n+}\n+\n+bool static CheckSignatureEncoding(const valtype &vchSig, unsigned int flags) {\n+    // Empty signature. Not strictly DER encoded, but allowed to provide a\n+    // compact way to provide an invalid signature for use with CHECK(MULTI)SIG\n+    if (vchSig.size() == 0) {\n+        return true;\n+    }\n+    if ((flags & SCRIPT_VERIFY_DERSIG) != 0 && !IsValidSignatureEncoding(vchSig)) {\n+        return false;\n+    }\n+    return true;\n+}\n+\n bool EvalScript(vector<vector<unsigned char> >& stack, const CScript& script, const CTransaction& txTo, unsigned int nIn, unsigned int flags, int nHashType)\n {\n     CAutoBN_CTX pctx;\n@@ -841,6 +921,10 @@ bool EvalScript(vector<vector<unsigned char> >& stack, const CScript& script, co\n                     // Drop the signature, since there's no way for a signature to sign itself\n                     scriptCode.FindAndDelete(CScript(vchSig));\n \n+                    if (!CheckSignatureEncoding(vchSig, flags)) {\n+                        return false;\n+                    }\n+\n                     bool fSuccess = (!fStrictEncodings || (IsCanonicalSignature(vchSig) && IsCanonicalPubKey(vchPubKey)));\n                     if (fSuccess)\n                         fSuccess = CheckSig(vchSig, vchPubKey, scriptCode, txTo, nIn, nHashType, flags);\n@@ -902,6 +986,10 @@ bool EvalScript(vector<vector<unsigned char> >& stack, const CScript& script, co\n                         valtype& vchSig    = stacktop(-isig);\n                         valtype& vchPubKey = stacktop(-ikey);\n \n+                        if (!CheckSignatureEncoding(vchSig, flags)) {\n+                            return false;\n+                        }\n+\n                         // Check signature\n                         bool fOk = (!fStrictEncodings || (IsCanonicalSignature(vchSig) && IsCanonicalPubKey(vchPubKey)));\n                         if (fOk)"
      },
      {
        "sha": "6a843437521fc4c87c7ece64250dd530b4c5b971",
        "filename": "src/script.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/da7ba593d74e7a09599d4b7df5ecccd79a3a1c64/src/script.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/da7ba593d74e7a09599d4b7df5ecccd79a3a1c64/src/script.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/script.h?ref=da7ba593d74e7a09599d4b7df5ecccd79a3a1c64",
        "patch": "@@ -35,6 +35,7 @@ enum\n     SCRIPT_VERIFY_P2SH      = (1U << 0),\n     SCRIPT_VERIFY_STRICTENC = (1U << 1),\n     SCRIPT_VERIFY_NOCACHE   = (1U << 2),\n+    SCRIPT_VERIFY_DERSIG    = (1U << 3), // enforce signature encodings as defined by BIP 66 (which is a softfork, while STRICTENC is not)\n };\n \n enum txnouttype"
      }
    ]
  },
  {
    "sha": "534e6dac47c87d078a8870c810cd6d1dbce91a99",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo1MzRlNmRhYzQ3Yzg3ZDA3OGE4ODcwYzgxMGNkNmQxZGJjZTkxYTk5",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2015-02-06T18:42:01Z"
      },
      "committer": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2015-02-06T19:17:29Z"
      },
      "message": "Raise version of created blocks, and enforce DERSIG in mempool",
      "tree": {
        "sha": "ae273def7790386f475c63498abd7c908eb863e2",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/ae273def7790386f475c63498abd7c908eb863e2"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/534e6dac47c87d078a8870c810cd6d1dbce91a99",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/534e6dac47c87d078a8870c810cd6d1dbce91a99",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/534e6dac47c87d078a8870c810cd6d1dbce91a99",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/534e6dac47c87d078a8870c810cd6d1dbce91a99/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "6690ef7fea5ae7785809e04fc6de3995b0dfae17",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6690ef7fea5ae7785809e04fc6de3995b0dfae17",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/6690ef7fea5ae7785809e04fc6de3995b0dfae17"
      }
    ],
    "stats": {
      "total": 4,
      "additions": 2,
      "deletions": 2
    },
    "files": [
      {
        "sha": "41322d6a8012922523126881eb63d83f782daf26",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/534e6dac47c87d078a8870c810cd6d1dbce91a99/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/534e6dac47c87d078a8870c810cd6d1dbce91a99/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=534e6dac47c87d078a8870c810cd6d1dbce91a99",
        "patch": "@@ -792,7 +792,7 @@ bool CTxMemPool::accept(CValidationState &state, CTransaction &tx, bool fCheckIn\n \n         // Check against previous transactions\n         // This is done last to help prevent CPU exhaustion denial-of-service attacks.\n-        if (!tx.CheckInputs(state, view, true, SCRIPT_VERIFY_P2SH | SCRIPT_VERIFY_STRICTENC))\n+        if (!tx.CheckInputs(state, view, true, SCRIPT_VERIFY_P2SH | SCRIPT_VERIFY_STRICTENC | SCRIPT_VERIFY_DERSIG))\n         {\n             return error(\"CTxMemPool::accept() : ConnectInputs failed %s\", hash.ToString().c_str());\n         }"
      },
      {
        "sha": "82d40787a31788509b9103b837f8ee600fbfbf25",
        "filename": "src/main.h",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/534e6dac47c87d078a8870c810cd6d1dbce91a99/src/main.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/534e6dac47c87d078a8870c810cd6d1dbce91a99/src/main.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.h?ref=534e6dac47c87d078a8870c810cd6d1dbce91a99",
        "patch": "@@ -1268,7 +1268,7 @@ class CBlockHeader\n {\n public:\n     // header\n-    static const int CURRENT_VERSION=2;\n+    static const int CURRENT_VERSION=3;\n     int nVersion;\n     uint256 hashPrevBlock;\n     uint256 hashMerkleRoot;"
      }
    ]
  },
  {
    "sha": "6690ef7fea5ae7785809e04fc6de3995b0dfae17",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo2NjkwZWY3ZmVhNWFlNzc4NTgwOWUwNGZjNmRlMzk5NWIwZGZhZTE3",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2015-02-06T05:26:15Z"
      },
      "committer": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2015-02-06T19:17:29Z"
      },
      "message": "Backport of some of BIP66's tests",
      "tree": {
        "sha": "a3fc6fde09c3d286648ef607e08d446c8d4b7914",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/a3fc6fde09c3d286648ef607e08d446c8d4b7914"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/6690ef7fea5ae7785809e04fc6de3995b0dfae17",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6690ef7fea5ae7785809e04fc6de3995b0dfae17",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/6690ef7fea5ae7785809e04fc6de3995b0dfae17",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6690ef7fea5ae7785809e04fc6de3995b0dfae17/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "da7ba593d74e7a09599d4b7df5ecccd79a3a1c64",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/da7ba593d74e7a09599d4b7df5ecccd79a3a1c64",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/da7ba593d74e7a09599d4b7df5ecccd79a3a1c64"
      }
    ],
    "stats": {
      "total": 118,
      "additions": 114,
      "deletions": 4
    },
    "files": [
      {
        "sha": "f1c68da17830e90a7b1d2aed71235fcd460f0b68",
        "filename": "src/test/data/script_invalid.json",
        "status": "modified",
        "additions": 41,
        "deletions": 1,
        "changes": 42,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/6690ef7fea5ae7785809e04fc6de3995b0dfae17/src/test/data/script_invalid.json",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/6690ef7fea5ae7785809e04fc6de3995b0dfae17/src/test/data/script_invalid.json",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/test/data/script_invalid.json?ref=6690ef7fea5ae7785809e04fc6de3995b0dfae17",
        "patch": "@@ -310,5 +310,45 @@\n [\"NOP1 0x01 1\", \"HASH160 0x14 0xda1745e9b549bd0bfa1a569971c77eba30cd5a4b EQUAL\"],\n \n [\"0 0x01 0x50\", \"HASH160 0x14 0xece424a6bb6ddf4db592c0faed60685047a361b1 EQUAL\", \"OP_RESERVED in P2SH should fail\"],\n-[\"0 0x01 VER\", \"HASH160 0x14 0x0f4d7845db968f2a81b530b6f3c1d6246d4c7e01 EQUAL\", \"OP_VER in P2SH should fail\"]\n+[\"0 0x01 VER\", \"HASH160 0x14 0x0f4d7845db968f2a81b530b6f3c1d6246d4c7e01 EQUAL\", \"OP_VER in P2SH should fail\"],\n+\n+[\"0x4a 0x0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", \"0 CHECKSIG NOT\", \"DERSIG\", \"Overly long signature is incorrectly encoded for DERSIG\"],\n+[\"0x25 0x30220220000000000000000000000000000000000000000000000000000000000000000000\", \"0 CHECKSIG NOT\", \"DERSIG\", \"Missing S is incorrectly encoded for DERSIG\"],\n+[\"0x27 0x3024021077777777777777777777777777777777020a7777777777777777777777777777777701\", \"0 CHECKSIG NOT\", \"DERSIG\", \"S with invalid S length is incorrectly encoded for DERSIG\"],\n+[\"0x27 0x302403107777777777777777777777777777777702107777777777777777777777777777777701\", \"0 CHECKSIG NOT\", \"DERSIG\", \"Non-integer R is incorrectly encoded for DERSIG\"],\n+[\"0x27 0x302402107777777777777777777777777777777703107777777777777777777777777777777701\", \"0 CHECKSIG NOT\", \"DERSIG\", \"Non-integer S is incorrectly encoded for DERSIG\"],\n+[\"0x17 0x3014020002107777777777777777777777777777777701\", \"0 CHECKSIG NOT\", \"DERSIG\", \"Zero-length R is incorrectly encoded for DERSIG\"],\n+[\"0x17 0x3014021077777777777777777777777777777777020001\", \"0 CHECKSIG NOT\", \"DERSIG\", \"Zero-length S is incorrectly encoded for DERSIG\"],\n+[\"0x27 0x302402107777777777777777777777777777777702108777777777777777777777777777777701\", \"0 CHECKSIG NOT\", \"DERSIG\", \"Negative S is incorrectly encoded for DERSIG\"],\n+\n+[\n+    \"0x47 0x30440220003040725f724b0e2142fc44ac71f6e13161f6410aeb6dee477952ede3b6a6ca022041ff4940ee3d88116ad281d7cc556e1f2c9427d82290bd7974a25addbcd5bede01\",\n+    \"0x21 0x03363d90d447b00c9c99ceac05b6262ee053441c7e55552ffe526bad8f83ff4640 CHECKSIG NOT\",\n+    \"DERSIG\",\n+    \"P2PK NOT with bad sig with too much R padding\"\n+],\n+[\n+    \"0x47 0x30440220003040725f724a0e2142fc44ac71f6e13161f6410aeb6dee477952ede3b6a6ca022041ff4940ee3d88116ad281d7cc556e1f2c9427d82290bd7974a25addbcd5bede01\",\n+    \"0x21 0x03363d90d447b00c9c99ceac05b6262ee053441c7e55552ffe526bad8f83ff4640 CHECKSIG NOT\",\n+    \"DERSIG\",\n+    \"P2PK NOT with too much R padding\"\n+],\n+[\n+    \"0x47 0x304402208e43c0b91f7c1e5bc58e41c8185f8a6086e111b0090187968a86f2822462d3c902200a58f4076b1133b18ff1dc83ee51676e44c60cc608d9534e0df5ace0424fc0be01\",\n+    \"0x21 0x038282263212c609d9ea2a6e3e172de238d8c39cabd5ac1ca10646e23fd5f51508 CHECKSIG NOT\",\n+    \"DERSIG\",\n+    \"BIP66 example 2, with DERSIG\"\n+],\n+[\n+    \"1\",\n+    \"0x21 0x038282263212c609d9ea2a6e3e172de238d8c39cabd5ac1ca10646e23fd5f51508 CHECKSIG NOT\",\n+    \"DERSIG\",\n+    \"BIP66 example 6, with DERSIG\"\n+],\n+[\n+    \"0 0 0x47 0x30440220afa76a8f60622f813b05711f051c6c3407e32d1b1b70b0576c1f01b54e4c05c702200d58e9df044fd1845cabfbeef6e624ba0401daf7d7e084736f9ff601c3783bf501\",\n+    \"2 0x21 0x038282263212c609d9ea2a6e3e172de238d8c39cabd5ac1ca10646e23fd5f51508 0x21 0x03363d90d447b00c9c99ceac05b6262ee053441c7e55552ffe526bad8f83ff4640 2 CHECKMULTISIG NOT\",\n+    \"DERSIG\",\n+    \"BIP66 example 10, with DERSIG\"\n+]\n ]"
      },
      {
        "sha": "1af06e1287b34838949effc1992ae827a8cc8b32",
        "filename": "src/test/data/script_valid.json",
        "status": "modified",
        "additions": 61,
        "deletions": 1,
        "changes": 62,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/6690ef7fea5ae7785809e04fc6de3995b0dfae17/src/test/data/script_valid.json",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/6690ef7fea5ae7785809e04fc6de3995b0dfae17/src/test/data/script_valid.json",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/test/data/script_valid.json?ref=6690ef7fea5ae7785809e04fc6de3995b0dfae17",
        "patch": "@@ -375,5 +375,65 @@\n [\"NOP\", \"NOP10 1\"],\n \n [\"0 0x01 1\", \"HASH160 0x14 0xda1745e9b549bd0bfa1a569971c77eba30cd5a4b EQUAL\", \"Very basic P2SH\"],\n-[\"0x4c 0 0x01 1\", \"HASH160 0x14 0xda1745e9b549bd0bfa1a569971c77eba30cd5a4b EQUAL\"]\n+[\"0x4c 0 0x01 1\", \"HASH160 0x14 0xda1745e9b549bd0bfa1a569971c77eba30cd5a4b EQUAL\"],\n+\n+[\"0x40 0x42424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242\",\n+\"0x4d 0x4000 0x42424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242 EQUAL\",\n+\"Basic PUSH signedness check\"],\n+\n+[\"0x4c 0x40 0x42424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242\",\n+\"0x4d 0x4000 0x42424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242 EQUAL\",\n+\"Basic PUSHDATA1 signedness check\"],\n+\n+[\"0x4a 0x0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", \"0 CHECKSIG NOT\", \"\", \"Overly long signature is correctly encoded\"],\n+[\"0x25 0x30220220000000000000000000000000000000000000000000000000000000000000000000\", \"0 CHECKSIG NOT\", \"\", \"Missing S is correctly encoded\"],\n+[\"0x27 0x3024021077777777777777777777777777777777020a7777777777777777777777777777777701\", \"0 CHECKSIG NOT\", \"\", \"S with invalid S length is correctly encoded\"],\n+[\"0x27 0x302403107777777777777777777777777777777702107777777777777777777777777777777701\", \"0 CHECKSIG NOT\", \"\", \"Non-integer R is correctly encoded\"],\n+[\"0x27 0x302402107777777777777777777777777777777703107777777777777777777777777777777701\", \"0 CHECKSIG NOT\", \"\", \"Non-integer S is correctly encoded\"],\n+[\"0x17 0x3014020002107777777777777777777777777777777701\", \"0 CHECKSIG NOT\", \"\", \"Zero-length R is correctly encoded\"],\n+[\"0x17 0x3014021077777777777777777777777777777777020001\", \"0 CHECKSIG NOT\", \"\", \"Zero-length S is correctly encoded\"],\n+[\"0x27 0x302402107777777777777777777777777777777702108777777777777777777777777777777701\", \"0 CHECKSIG NOT\", \"\", \"Negative S is correctly encoded\"],\n+\n+[\n+    \"0x47 0x30440220003040725f724b0e2142fc44ac71f6e13161f6410aeb6dee477952ede3b6a6ca022041ff4940ee3d88116ad281d7cc556e1f2c9427d82290bd7974a25addbcd5bede01\",\n+    \"0x21 0x03363d90d447b00c9c99ceac05b6262ee053441c7e55552ffe526bad8f83ff4640 CHECKSIG NOT\",\n+    \"\",\n+    \"P2PK NOT with bad sig with too much R padding but no DERSIG\"\n+],\n+[\n+    \"0\",\n+    \"0x21 0x038282263212c609d9ea2a6e3e172de238d8c39cabd5ac1ca10646e23fd5f51508 CHECKSIG NOT\",\n+    \"\",\n+    \"BIP66 example 4, without DERSIG\"\n+],\n+[\n+    \"0\",\n+    \"0x21 0x038282263212c609d9ea2a6e3e172de238d8c39cabd5ac1ca10646e23fd5f51508 CHECKSIG NOT\",\n+    \"DERSIG\",\n+    \"BIP66 example 4, with DERSIG\"\n+],\n+[\n+    \"1\",\n+    \"0x21 0x038282263212c609d9ea2a6e3e172de238d8c39cabd5ac1ca10646e23fd5f51508 CHECKSIG NOT\",\n+    \"\",\n+    \"BIP66 example 6, without DERSIG\"\n+],\n+[\n+    \"0 0 0x47 0x30440220afa76a8f60622f813b05711f051c6c3407e32d1b1b70b0576c1f01b54e4c05c702200d58e9df044fd1845cabfbeef6e624ba0401daf7d7e084736f9ff601c3783bf501\",\n+    \"2 0x21 0x038282263212c609d9ea2a6e3e172de238d8c39cabd5ac1ca10646e23fd5f51508 0x21 0x03363d90d447b00c9c99ceac05b6262ee053441c7e55552ffe526bad8f83ff4640 2 CHECKMULTISIG NOT\",\n+    \"\",\n+    \"BIP66 example 10, without DERSIG\"\n+],\n+[\n+    \"0 0x47 0x30440220f00a77260d34ec2f0c59621dc710f58169d0ca06df1a88cd4b1f1b97bd46991b02201ee220c7e04f26aed03f94aa97fb09ca5627163bf4ba07e6979972ec737db22601 0\",\n+    \"2 0x21 0x038282263212c609d9ea2a6e3e172de238d8c39cabd5ac1ca10646e23fd5f51508 0x21 0x03363d90d447b00c9c99ceac05b6262ee053441c7e55552ffe526bad8f83ff4640 2 CHECKMULTISIG NOT\",\n+    \"\",\n+    \"BIP66 example 12, without DERSIG\"\n+],\n+[\n+    \"0 0x47 0x30440220f00a77260d34ec2f0c59621dc710f58169d0ca06df1a88cd4b1f1b97bd46991b02201ee220c7e04f26aed03f94aa97fb09ca5627163bf4ba07e6979972ec737db22601 0\",\n+    \"2 0x21 0x038282263212c609d9ea2a6e3e172de238d8c39cabd5ac1ca10646e23fd5f51508 0x21 0x03363d90d447b00c9c99ceac05b6262ee053441c7e55552ffe526bad8f83ff4640 2 CHECKMULTISIG NOT\",\n+    \"DERSIG\",\n+    \"BIP66 example 12, with DERSIG\"\n+]\n ]"
      },
      {
        "sha": "ee81dbcdc276b8d7a1e946a737f7754475009c5a",
        "filename": "src/test/script_tests.cpp",
        "status": "modified",
        "additions": 12,
        "deletions": 2,
        "changes": 14,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/6690ef7fea5ae7785809e04fc6de3995b0dfae17/src/test/script_tests.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/6690ef7fea5ae7785809e04fc6de3995b0dfae17/src/test/script_tests.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/test/script_tests.cpp?ref=6690ef7fea5ae7785809e04fc6de3995b0dfae17",
        "patch": "@@ -142,8 +142,13 @@ BOOST_AUTO_TEST_CASE(script_valid)\n         string scriptPubKeyString = test[1].get_str();\n         CScript scriptPubKey = ParseScript(scriptPubKeyString);\n \n+        int flagsNow = flags;\n+        if (test.size() > 3 && (\",\" + test[2].get_str() + \",\").find(\",DERSIG,\") != string::npos) {\n+            flagsNow |= SCRIPT_VERIFY_DERSIG;\n+        }\n+\n         CTransaction tx;\n-        BOOST_CHECK_MESSAGE(VerifyScript(scriptSig, scriptPubKey, tx, 0, flags, SIGHASH_NONE), strTest);\n+        BOOST_CHECK_MESSAGE(VerifyScript(scriptSig, scriptPubKey, tx, 0, flagsNow, SIGHASH_NONE), strTest);\n     }\n }\n \n@@ -166,8 +171,13 @@ BOOST_AUTO_TEST_CASE(script_invalid)\n         string scriptPubKeyString = test[1].get_str();\n         CScript scriptPubKey = ParseScript(scriptPubKeyString);\n \n+        int flagsNow = flags;\n+        if (test.size() > 3 && (\",\" + test[2].get_str() + \",\").find(\",DERSIG,\") != string::npos) {\n+            flagsNow |= SCRIPT_VERIFY_DERSIG;\n+        }\n+\n         CTransaction tx;\n-        BOOST_CHECK_MESSAGE(!VerifyScript(scriptSig, scriptPubKey, tx, 0, flags, SIGHASH_NONE), strTest);\n+        BOOST_CHECK_MESSAGE(!VerifyScript(scriptSig, scriptPubKey, tx, 0, flagsNow, SIGHASH_NONE), strTest);\n     }\n }\n "
      }
    ]
  },
  {
    "sha": "9d11aba4c4770fdd7fa1840aff2c4faa2873d5ca",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo5ZDExYWJhNGM0NzcwZmRkN2ZhMTg0MGFmZjJjNGZhYTI4NzNkNWNh",
    "commit": {
      "author": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2015-02-17T09:03:33Z"
      },
      "committer": {
        "name": "Wladimir J. van der Laan",
        "email": "laanwj@gmail.com",
        "date": "2015-02-17T09:04:23Z"
      },
      "message": "Merge #5765: Implement BIP66 (0.8)",
      "tree": {
        "sha": "ae273def7790386f475c63498abd7c908eb863e2",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/ae273def7790386f475c63498abd7c908eb863e2"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/9d11aba4c4770fdd7fa1840aff2c4faa2873d5ca",
      "comment_count": 0,
      "verification": {
        "verified": true,
        "reason": "valid",
        "signature": "-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v1\n\niQEcBAABCgAGBQJU4wQYAAoJEHSBCwEjRsmmw0kH/1XgEOugJ+3dMmJK0j4qyn+q\nAzlZLuFX4uq12lN1KqVZm3JqI8QPRjq+ZJE6PW18GZwjbG0rJilWD4Dh6xjHNeHk\nKisEbE+stR2TC/srG1dlhr3sAz4XmczXn1kayvdSKJEpVN0ur9K4DymPOQUGxwGp\nNzfxigYMOyYVengywtpZtAoCfBRD4ZBUBBFqDkqIlCK1fk8AVk3/3ZTWbvO/hPE6\nkh+aEBEi25OwOhoZR1coXXDIgQoXnhGtNhsjls9/nIS0pEvo7cfN4nl0JufqOWHX\nBStKCHCu4xtiWzBzE+DcgfFzsjbp1XoiPueySUkZgyavpE3hFzQYagLVbBITH+A=\n=UEBD\n-----END PGP SIGNATURE-----",
        "payload": "tree ae273def7790386f475c63498abd7c908eb863e2\nparent 6690ef7fea5ae7785809e04fc6de3995b0dfae17\nparent 534e6dac47c87d078a8870c810cd6d1dbce91a99\nauthor Wladimir J. van der Laan <laanwj@gmail.com> 1424163813 +0100\ncommitter Wladimir J. van der Laan <laanwj@gmail.com> 1424163863 +0100\n\nMerge #5765: Implement BIP66 (0.8)\n"
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/9d11aba4c4770fdd7fa1840aff2c4faa2873d5ca",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/9d11aba4c4770fdd7fa1840aff2c4faa2873d5ca",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/9d11aba4c4770fdd7fa1840aff2c4faa2873d5ca/comments",
    "author": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "laanwj",
      "id": 126646,
      "node_id": "MDQ6VXNlcjEyNjY0Ng==",
      "avatar_url": "https://avatars.githubusercontent.com/u/126646?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/laanwj",
      "html_url": "https://github.com/laanwj",
      "followers_url": "https://api.github.com/users/laanwj/followers",
      "following_url": "https://api.github.com/users/laanwj/following{/other_user}",
      "gists_url": "https://api.github.com/users/laanwj/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/laanwj/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/laanwj/subscriptions",
      "organizations_url": "https://api.github.com/users/laanwj/orgs",
      "repos_url": "https://api.github.com/users/laanwj/repos",
      "events_url": "https://api.github.com/users/laanwj/events{/privacy}",
      "received_events_url": "https://api.github.com/users/laanwj/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "6690ef7fea5ae7785809e04fc6de3995b0dfae17",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/6690ef7fea5ae7785809e04fc6de3995b0dfae17",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/6690ef7fea5ae7785809e04fc6de3995b0dfae17"
      },
      {
        "sha": "534e6dac47c87d078a8870c810cd6d1dbce91a99",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/534e6dac47c87d078a8870c810cd6d1dbce91a99",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/534e6dac47c87d078a8870c810cd6d1dbce91a99"
      }
    ],
    "stats": {
      "total": 4,
      "additions": 2,
      "deletions": 2
    },
    "files": [
      {
        "sha": "41322d6a8012922523126881eb63d83f782daf26",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/9d11aba4c4770fdd7fa1840aff2c4faa2873d5ca/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/9d11aba4c4770fdd7fa1840aff2c4faa2873d5ca/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=9d11aba4c4770fdd7fa1840aff2c4faa2873d5ca",
        "patch": "@@ -792,7 +792,7 @@ bool CTxMemPool::accept(CValidationState &state, CTransaction &tx, bool fCheckIn\n \n         // Check against previous transactions\n         // This is done last to help prevent CPU exhaustion denial-of-service attacks.\n-        if (!tx.CheckInputs(state, view, true, SCRIPT_VERIFY_P2SH | SCRIPT_VERIFY_STRICTENC))\n+        if (!tx.CheckInputs(state, view, true, SCRIPT_VERIFY_P2SH | SCRIPT_VERIFY_STRICTENC | SCRIPT_VERIFY_DERSIG))\n         {\n             return error(\"CTxMemPool::accept() : ConnectInputs failed %s\", hash.ToString().c_str());\n         }"
      },
      {
        "sha": "82d40787a31788509b9103b837f8ee600fbfbf25",
        "filename": "src/main.h",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/9d11aba4c4770fdd7fa1840aff2c4faa2873d5ca/src/main.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/9d11aba4c4770fdd7fa1840aff2c4faa2873d5ca/src/main.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.h?ref=9d11aba4c4770fdd7fa1840aff2c4faa2873d5ca",
        "patch": "@@ -1268,7 +1268,7 @@ class CBlockHeader\n {\n public:\n     // header\n-    static const int CURRENT_VERSION=2;\n+    static const int CURRENT_VERSION=3;\n     int nVersion;\n     uint256 hashPrevBlock;\n     uint256 hashMerkleRoot;"
      }
    ]
  }
]