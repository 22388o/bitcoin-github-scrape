[
  {
    "sha": "1c29ac40fb1853df20572a389db70ce75d80ef93",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzoxYzI5YWM0MGZiMTg1M2RmMjA1NzJhMzg5ZGI3MGNlNzVkODBlZjkz",
    "commit": {
      "author": {
        "name": "John Newbery",
        "email": "john@johnnewbery.com",
        "date": "2019-03-29T14:07:56Z"
      },
      "committer": {
        "name": "John Newbery",
        "email": "john@johnnewbery.com",
        "date": "2019-03-29T15:43:26Z"
      },
      "message": "[tests] style fixes in feature_pruning.py\n\nMinor style fixups. No functional change.",
      "tree": {
        "sha": "3fc71e551edbc132eea32441aac1a699ce805c55",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/3fc71e551edbc132eea32441aac1a699ce805c55"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/1c29ac40fb1853df20572a389db70ce75d80ef93",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1c29ac40fb1853df20572a389db70ce75d80ef93",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/1c29ac40fb1853df20572a389db70ce75d80ef93",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1c29ac40fb1853df20572a389db70ce75d80ef93/comments",
    "author": {
      "login": "jnewbery",
      "id": 1063656,
      "node_id": "MDQ6VXNlcjEwNjM2NTY=",
      "avatar_url": "https://avatars.githubusercontent.com/u/1063656?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jnewbery",
      "html_url": "https://github.com/jnewbery",
      "followers_url": "https://api.github.com/users/jnewbery/followers",
      "following_url": "https://api.github.com/users/jnewbery/following{/other_user}",
      "gists_url": "https://api.github.com/users/jnewbery/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jnewbery/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jnewbery/subscriptions",
      "organizations_url": "https://api.github.com/users/jnewbery/orgs",
      "repos_url": "https://api.github.com/users/jnewbery/repos",
      "events_url": "https://api.github.com/users/jnewbery/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jnewbery/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "jnewbery",
      "id": 1063656,
      "node_id": "MDQ6VXNlcjEwNjM2NTY=",
      "avatar_url": "https://avatars.githubusercontent.com/u/1063656?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jnewbery",
      "html_url": "https://github.com/jnewbery",
      "followers_url": "https://api.github.com/users/jnewbery/followers",
      "following_url": "https://api.github.com/users/jnewbery/following{/other_user}",
      "gists_url": "https://api.github.com/users/jnewbery/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jnewbery/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jnewbery/subscriptions",
      "organizations_url": "https://api.github.com/users/jnewbery/orgs",
      "repos_url": "https://api.github.com/users/jnewbery/repos",
      "events_url": "https://api.github.com/users/jnewbery/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jnewbery/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "3702e1c17b6ccb01de2c0cde66dac26bd05e3183",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/3702e1c17b6ccb01de2c0cde66dac26bd05e3183",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/3702e1c17b6ccb01de2c0cde66dac26bd05e3183"
      }
    ],
    "stats": {
      "total": 81,
      "additions": 34,
      "deletions": 47
    },
    "files": [
      {
        "sha": "c69d3855803d9f6fac7c2fed4d6173db2e9ae4a8",
        "filename": "test/functional/feature_pruning.py",
        "status": "modified",
        "additions": 34,
        "deletions": 47,
        "changes": 81,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/1c29ac40fb1853df20572a389db70ce75d80ef93/test/functional/feature_pruning.py",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/1c29ac40fb1853df20572a389db70ce75d80ef93/test/functional/feature_pruning.py",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/test/functional/feature_pruning.py?ref=1c29ac40fb1853df20572a389db70ce75d80ef93",
        "patch": "@@ -23,7 +23,7 @@\n \n \n def calc_usage(blockdir):\n-    return sum(os.path.getsize(blockdir+f) for f in os.listdir(blockdir) if os.path.isfile(os.path.join(blockdir, f))) / (1024. * 1024.)\n+    return sum(os.path.getsize(blockdir + f) for f in os.listdir(blockdir) if os.path.isfile(os.path.join(blockdir, f))) / (1024. * 1024.)\n \n class PruneTest(BitcoinTestFramework):\n     def set_test_params(self):\n@@ -55,7 +55,7 @@ def setup_network(self):\n \n         connect_nodes(self.nodes[0], 1)\n         connect_nodes(self.nodes[1], 2)\n-        connect_nodes(self.nodes[2], 0)\n+        connect_nodes(self.nodes[0], 2)\n         connect_nodes(self.nodes[0], 3)\n         connect_nodes(self.nodes[0], 4)\n         sync_blocks(self.nodes[0:5])\n@@ -71,15 +71,15 @@ def create_big_chain(self):\n         self.nodes[1].generate(200)\n         sync_blocks(self.nodes[0:2])\n         self.nodes[0].generate(150)\n+\n         # Then mine enough full blocks to create more than 550MiB of data\n         for i in range(645):\n             mine_large_block(self.nodes[0], self.utxo_cache_0)\n \n         sync_blocks(self.nodes[0:5])\n \n     def test_height_min(self):\n-        if not os.path.isfile(os.path.join(self.prunedir, \"blk00000.dat\")):\n-            raise AssertionError(\"blk00000.dat is missing, pruning too early\")\n+        assert os.path.isfile(os.path.join(self.prunedir, \"blk00000.dat\")), \"blk00000.dat is missing, pruning too early\"\n         self.log.info(\"Success\")\n         self.log.info(\"Though we're already using more than 550MiB, current usage: %d\" % calc_usage(self.prunedir))\n         self.log.info(\"Mining 25 more blocks should cause the first block file to be pruned\")\n@@ -93,8 +93,7 @@ def test_height_min(self):\n         self.log.info(\"Success\")\n         usage = calc_usage(self.prunedir)\n         self.log.info(\"Usage should be below target: %d\" % usage)\n-        if (usage > 550):\n-            raise AssertionError(\"Pruning target not being met\")\n+        assert_greater_than(550, usage)\n \n     def create_chain_with_staleblocks(self):\n         # Create stale blocks in manageable sized chunks\n@@ -121,8 +120,8 @@ def create_chain_with_staleblocks(self):\n                 mine_large_block(self.nodes[0], self.utxo_cache_0)\n \n             # Create connections in the order so both nodes can see the reorg at the same time\n-            connect_nodes(self.nodes[1], 0)\n-            connect_nodes(self.nodes[2], 0)\n+            connect_nodes(self.nodes[0], 1)\n+            connect_nodes(self.nodes[0], 2)\n             sync_blocks(self.nodes[0:3])\n \n         self.log.info(\"Usage can be over target because of high stale rate: %d\" % calc_usage(self.prunedir))\n@@ -138,20 +137,20 @@ def reorg_test(self):\n         height = self.nodes[1].getblockcount()\n         self.log.info(\"Current block height: %d\" % height)\n \n-        invalidheight = height-287\n-        badhash = self.nodes[1].getblockhash(invalidheight)\n-        self.log.info(\"Invalidating block %s at height %d\" % (badhash,invalidheight))\n-        self.nodes[1].invalidateblock(badhash)\n+        self.forkheight = height - 287\n+        self.forkhash = self.nodes[1].getblockhash(self.forkheight)\n+        self.log.info(\"Invalidating block %s at height %d\" % (self.forkhash, self.forkheight))\n+        self.nodes[1].invalidateblock(self.forkhash)\n \n         # We've now switched to our previously mined-24 block fork on node 1, but that's not what we want\n         # So invalidate that fork as well, until we're on the same chain as node 0/2 (but at an ancestor 288 blocks ago)\n-        mainchainhash = self.nodes[0].getblockhash(invalidheight - 1)\n-        curhash = self.nodes[1].getblockhash(invalidheight - 1)\n+        mainchainhash = self.nodes[0].getblockhash(self.forkheight - 1)\n+        curhash = self.nodes[1].getblockhash(self.forkheight - 1)\n         while curhash != mainchainhash:\n             self.nodes[1].invalidateblock(curhash)\n-            curhash = self.nodes[1].getblockhash(invalidheight - 1)\n+            curhash = self.nodes[1].getblockhash(self.forkheight - 1)\n \n-        assert self.nodes[1].getblockcount() == invalidheight - 1\n+        assert self.nodes[1].getblockcount() == self.forkheight - 1\n         self.log.info(\"New best height: %d\" % self.nodes[1].getblockcount())\n \n         # Reboot node1 to clear those giant tx's from mempool\n@@ -163,13 +162,12 @@ def reorg_test(self):\n \n         self.log.info(\"Reconnect nodes\")\n         connect_nodes(self.nodes[0], 1)\n-        connect_nodes(self.nodes[2], 1)\n+        connect_nodes(self.nodes[1], 2)\n         sync_blocks(self.nodes[0:3], timeout=120)\n \n         self.log.info(\"Verify height on node 2: %d\" % self.nodes[2].getblockcount())\n-        self.log.info(\"Usage possibly still high bc of stale blocks in block files: %d\" % calc_usage(self.prunedir))\n-\n-        self.log.info(\"Mine 220 more blocks so we have requisite history (some blocks will be big and cause pruning of previous chain)\")\n+        self.log.info(\"Usage possibly still high because of stale blocks in block files: %d\" % calc_usage(self.prunedir))\n+        self.log.info(\"Mine 220 more large blocks so we have requisite history\")\n \n         # Get node0's wallet transactions back in its mempool, to avoid the\n         # mined blocks from being too small.\n@@ -183,10 +181,7 @@ def reorg_test(self):\n \n         usage = calc_usage(self.prunedir)\n         self.log.info(\"Usage should be below target: %d\" % usage)\n-        if (usage > 550):\n-            raise AssertionError(\"Pruning target not being met\")\n-\n-        return invalidheight,badhash\n+        assert_greater_than(550, usage)\n \n     def reorg_back(self):\n         # Verify that a block on the old main chain fork has been pruned away\n@@ -219,17 +214,17 @@ def reorg_back(self):\n             blocks_to_mine = first_reorg_height + 1 - self.mainchainheight\n             self.log.info(\"Rewind node 0 to prev main chain to mine longer chain to trigger redownload. Blocks needed: %d\" % blocks_to_mine)\n             self.nodes[0].invalidateblock(curchainhash)\n-            assert self.nodes[0].getblockcount() == self.mainchainheight\n-            assert self.nodes[0].getbestblockhash() == self.mainchainhash2\n+            assert_equal(self.nodes[0].getblockcount(), self.mainchainheight)\n+            assert_equal(self.nodes[0].getbestblockhash(), self.mainchainhash2)\n             goalbesthash = self.nodes[0].generate(blocks_to_mine)[-1]\n             goalbestheight = first_reorg_height + 1\n \n         self.log.info(\"Verify node 2 reorged back to the main chain, some blocks of which it had to redownload\")\n         # Wait for Node 2 to reorg to proper height\n         wait_until(lambda: self.nodes[2].getblockcount() >= goalbestheight, timeout=900)\n-        assert self.nodes[2].getbestblockhash() == goalbesthash\n+        assert_equal(self.nodes[2].getbestblockhash(), goalbesthash)\n         # Verify we can now have the data for a block previously pruned\n-        assert self.nodes[2].getblock(self.forkhash)[\"height\"] == self.forkheight\n+        assert_equal(self.nodes[2].getblock(self.forkhash)[\"height\"], self.forkheight)\n \n     def manual_test(self, node_number, use_timestamp):\n         # at this point, node has 995 blocks and has not yet run in prune mode\n@@ -287,38 +282,30 @@ def has_block(index):\n \n         # height=100 too low to prune first block file so this is a no-op\n         prune(100)\n-        if not has_block(0):\n-            raise AssertionError(\"blk00000.dat is missing when should still be there\")\n+        assert has_block(0), \"blk00000.dat is missing when should still be there\"\n \n         # Does nothing\n         node.pruneblockchain(height(0))\n-        if not has_block(0):\n-            raise AssertionError(\"blk00000.dat is missing when should still be there\")\n+        assert has_block(0), \"blk00000.dat is missing when should still be there\"\n \n         # height=500 should prune first file\n         prune(500)\n-        if has_block(0):\n-            raise AssertionError(\"blk00000.dat is still there, should be pruned by now\")\n-        if not has_block(1):\n-            raise AssertionError(\"blk00001.dat is missing when should still be there\")\n+        assert not has_block(0), \"blk00000.dat is still there, should be pruned by now\"\n+        assert has_block(1), \"blk00001.dat is missing when should still be there\"\n \n         # height=650 should prune second file\n         prune(650)\n-        if has_block(1):\n-            raise AssertionError(\"blk00001.dat is still there, should be pruned by now\")\n+        assert not has_block(1), \"blk00001.dat is still there, should be pruned by now\"\n \n         # height=1000 should not prune anything more, because tip-288 is in blk00002.dat.\n         prune(1000, 1001 - MIN_BLOCKS_TO_KEEP)\n-        if not has_block(2):\n-            raise AssertionError(\"blk00002.dat is still there, should be pruned by now\")\n+        assert has_block(2), \"blk00002.dat is still there, should be pruned by now\"\n \n         # advance the tip so blk00002.dat and blk00003.dat can be pruned (the last 288 blocks should now be in blk00004.dat)\n         node.generate(288)\n         prune(1000)\n-        if has_block(2):\n-            raise AssertionError(\"blk00002.dat is still there, should be pruned by now\")\n-        if has_block(3):\n-            raise AssertionError(\"blk00003.dat is still there, should be pruned by now\")\n+        assert not has_block(2), \"blk00002.dat is still there, should be pruned by now\"\n+        assert not has_block(3), \"blk00003.dat is still there, should be pruned by now\"\n \n         # stop node, start back up with auto-prune at 550 MiB, make sure still runs\n         self.stop_node(node_number)\n@@ -339,7 +326,7 @@ def wallet_test(self):\n         connect_nodes(self.nodes[0], 5)\n         nds = [self.nodes[0], self.nodes[5]]\n         sync_blocks(nds, wait=5, timeout=300)\n-        self.stop_node(5) #stop and start to trigger rescan\n+        self.stop_node(5)  # stop and start to trigger rescan\n         self.start_node(5, extra_args=[\"-prune=550\"])\n         self.log.info(\"Success\")\n \n@@ -394,11 +381,11 @@ def run_test(self):\n         #                    +...+(1044)  &..                    $...$(1319)\n \n         # Save some current chain state for later use\n-        self.mainchainheight = self.nodes[2].getblockcount()   #1320\n+        self.mainchainheight = self.nodes[2].getblockcount()  # 1320\n         self.mainchainhash2 = self.nodes[2].getblockhash(self.mainchainheight)\n \n         self.log.info(\"Check that we can survive a 288 block reorg still\")\n-        (self.forkheight,self.forkhash) = self.reorg_test() #(1033, )\n+        self.reorg_test()  # (1033, )\n         # Now create a 288 block reorg by mining a longer chain on N1\n         # First disconnect N1\n         # Then invalidate 1033 on main chain and 1032 on fork so height is 1032 on main chain"
      }
    ]
  },
  {
    "sha": "03d6d238104d228acfae9f3e122879bddef8d27d",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzowM2Q2ZDIzODEwNGQyMjhhY2ZhZTlmM2UxMjI4NzliZGRlZjhkMjdk",
    "commit": {
      "author": {
        "name": "John Newbery",
        "email": "john@johnnewbery.com",
        "date": "2017-06-13T18:36:44Z"
      },
      "committer": {
        "name": "John Newbery",
        "email": "john@johnnewbery.com",
        "date": "2019-03-29T15:43:41Z"
      },
      "message": "[tests] make pruning test faster\n\nThis commit makes the pruning.py much faster.\n\nKey insights to do this:\n\n- pruning.py doesn't care what kind of transactions make up the big\nblocks that are pruned in the test. Instead of making blocks with\nseveral large, expensive to construct and validate transactions,\ninstead make the large blocks contain a single coinbase transaction with\na huge OP_RETURN txout.\n- avoid stop-starting nodes where possible.\n\nThis test could probably be made even faster by using the P2P interface\nfor submitting blocks instead of the submitblock RPC.",
      "tree": {
        "sha": "a3ebc80476eb9e9c7d143ffeb30bee9998d5d425",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/a3ebc80476eb9e9c7d143ffeb30bee9998d5d425"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/03d6d238104d228acfae9f3e122879bddef8d27d",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/03d6d238104d228acfae9f3e122879bddef8d27d",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/03d6d238104d228acfae9f3e122879bddef8d27d",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/03d6d238104d228acfae9f3e122879bddef8d27d/comments",
    "author": {
      "login": "jnewbery",
      "id": 1063656,
      "node_id": "MDQ6VXNlcjEwNjM2NTY=",
      "avatar_url": "https://avatars.githubusercontent.com/u/1063656?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jnewbery",
      "html_url": "https://github.com/jnewbery",
      "followers_url": "https://api.github.com/users/jnewbery/followers",
      "following_url": "https://api.github.com/users/jnewbery/following{/other_user}",
      "gists_url": "https://api.github.com/users/jnewbery/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jnewbery/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jnewbery/subscriptions",
      "organizations_url": "https://api.github.com/users/jnewbery/orgs",
      "repos_url": "https://api.github.com/users/jnewbery/repos",
      "events_url": "https://api.github.com/users/jnewbery/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jnewbery/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "jnewbery",
      "id": 1063656,
      "node_id": "MDQ6VXNlcjEwNjM2NTY=",
      "avatar_url": "https://avatars.githubusercontent.com/u/1063656?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jnewbery",
      "html_url": "https://github.com/jnewbery",
      "followers_url": "https://api.github.com/users/jnewbery/followers",
      "following_url": "https://api.github.com/users/jnewbery/following{/other_user}",
      "gists_url": "https://api.github.com/users/jnewbery/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jnewbery/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jnewbery/subscriptions",
      "organizations_url": "https://api.github.com/users/jnewbery/orgs",
      "repos_url": "https://api.github.com/users/jnewbery/repos",
      "events_url": "https://api.github.com/users/jnewbery/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jnewbery/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "1c29ac40fb1853df20572a389db70ce75d80ef93",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1c29ac40fb1853df20572a389db70ce75d80ef93",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/1c29ac40fb1853df20572a389db70ce75d80ef93"
      }
    ],
    "stats": {
      "total": 107,
      "additions": 60,
      "deletions": 47
    },
    "files": [
      {
        "sha": "49951e24f3d6497d91b6feb1ef25262b9f81031a",
        "filename": "test/functional/feature_pruning.py",
        "status": "modified",
        "additions": 60,
        "deletions": 47,
        "changes": 107,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/03d6d238104d228acfae9f3e122879bddef8d27d/test/functional/feature_pruning.py",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/03d6d238104d228acfae9f3e122879bddef8d27d/test/functional/feature_pruning.py",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/test/functional/feature_pruning.py?ref=03d6d238104d228acfae9f3e122879bddef8d27d",
        "patch": "@@ -8,11 +8,13 @@\n This test uses 4GB of disk space.\n This test takes 30 mins or more (up to 2 hours)\n \"\"\"\n+import os\n \n+from test_framework.blocktools import create_coinbase\n+from test_framework.messages import CBlock, ToHex\n+from test_framework.script import CScript, OP_RETURN, OP_NOP\n from test_framework.test_framework import BitcoinTestFramework\n-from test_framework.util import assert_equal, assert_greater_than, assert_raises_rpc_error, connect_nodes, mine_large_block, sync_blocks, wait_until\n-\n-import os\n+from test_framework.util import assert_equal, assert_greater_than, assert_raises_rpc_error, connect_nodes, disconnect_nodes, sync_blocks, wait_until\n \n MIN_BLOCKS_TO_KEEP = 288\n \n@@ -21,6 +23,47 @@\n # compatible with pruning based on key creation time.\n TIMESTAMP_WINDOW = 2 * 60 * 60\n \n+def mine_large_blocks(node, n):\n+    # Make a large scriptPubKey for the coinbase transaction. This is OP_RETURN\n+    # followed by 950k of OP_NOP. This would be non-standard in a non-coinbase\n+    # transaction but is consensus valid.\n+\n+    # Get the block parameters for the first block\n+    big_script = CScript([OP_RETURN] + [OP_NOP] * 950000)\n+    best_block = node.getblock(node.getbestblockhash())\n+    height = int(best_block[\"height\"]) + 1\n+    try:\n+        # Static variable ensures that time is monotonicly increasing and is therefore\n+        # different for each block created => blockhash is unique.\n+        mine_large_blocks.nTime = min(mine_large_blocks.nTime, int(best_block[\"time\"])) + 1\n+    except AttributeError:\n+        mine_large_blocks.nTime = int(best_block[\"time\"]) + 1\n+    previousblockhash = int(best_block[\"hash\"], 16)\n+\n+    for _ in range(n):\n+        # Build the coinbase transaction (with large scriptPubKey)\n+        coinbase_tx = create_coinbase(height)\n+        coinbase_tx.vin[0].nSequence = 2 ** 32 - 1\n+        coinbase_tx.vout[0].scriptPubKey = big_script\n+        coinbase_tx.rehash()\n+\n+        # Build the block\n+        block = CBlock()\n+        block.nVersion = best_block[\"version\"]\n+        block.hashPrevBlock = previousblockhash\n+        block.nTime = mine_large_blocks.nTime\n+        block.nBits = int('207fffff', 16)\n+        block.nNonce = 0\n+        block.vtx = [coinbase_tx]\n+        block.hashMerkleRoot = block.calc_merkle_root()\n+        block.solve()\n+\n+        # Submit to the node\n+        node.submitblock(ToHex(block))\n+\n+        previousblockhash = block.sha256\n+        height += 1\n+        mine_large_blocks.nTime += 1\n \n def calc_usage(blockdir):\n     return sum(os.path.getsize(blockdir + f) for f in os.listdir(blockdir) if os.path.isfile(os.path.join(blockdir, f))) / (1024. * 1024.)\n@@ -29,11 +72,10 @@ class PruneTest(BitcoinTestFramework):\n     def set_test_params(self):\n         self.setup_clean_chain = True\n         self.num_nodes = 6\n-        self.rpc_timeout = 900\n \n         # Create nodes 0 and 1 to mine.\n         # Create node 2 to test pruning.\n-        self.full_node_default_args = [\"-maxreceivebuffer=20000\", \"-checkblocks=5\", \"-limitdescendantcount=100\", \"-limitdescendantsize=5000\", \"-limitancestorcount=100\", \"-limitancestorsize=5000\"]\n+        self.full_node_default_args = [\"-maxreceivebuffer=20000\", \"-checkblocks=5\"]\n         # Create nodes 3 and 4 to test manual pruning (they will be re-started with manual pruning later)\n         # Create nodes 5 to test wallet in prune mode, but do not connect\n         self.extra_args = [\n@@ -73,8 +115,7 @@ def create_big_chain(self):\n         self.nodes[0].generate(150)\n \n         # Then mine enough full blocks to create more than 550MiB of data\n-        for i in range(645):\n-            mine_large_block(self.nodes[0], self.utxo_cache_0)\n+        mine_large_blocks(self.nodes[0], 645)\n \n         sync_blocks(self.nodes[0:5])\n \n@@ -84,8 +125,7 @@ def test_height_min(self):\n         self.log.info(\"Though we're already using more than 550MiB, current usage: %d\" % calc_usage(self.prunedir))\n         self.log.info(\"Mining 25 more blocks should cause the first block file to be pruned\")\n         # Pruning doesn't run until we're allocating another chunk, 20 full blocks past the height cutoff will ensure this\n-        for i in range(25):\n-            mine_large_block(self.nodes[0], self.utxo_cache_0)\n+        mine_large_blocks(self.nodes[0], 25)\n \n         # Wait for blk00000.dat to be pruned\n         wait_until(lambda: not os.path.isfile(os.path.join(self.prunedir, \"blk00000.dat\")), timeout=30)\n@@ -102,22 +142,13 @@ def create_chain_with_staleblocks(self):\n         for j in range(12):\n             # Disconnect node 0 so it can mine a longer reorg chain without knowing about node 1's soon-to-be-stale chain\n             # Node 2 stays connected, so it hears about the stale blocks and then reorg's when node0 reconnects\n-            # Stopping node 0 also clears its mempool, so it doesn't have node1's transactions to accidentally mine\n-            self.stop_node(0)\n-            self.start_node(0, extra_args=self.full_node_default_args)\n+            disconnect_nodes(self.nodes[0], 1)\n+            disconnect_nodes(self.nodes[0], 2)\n             # Mine 24 blocks in node 1\n-            for i in range(24):\n-                if j == 0:\n-                    mine_large_block(self.nodes[1], self.utxo_cache_1)\n-                else:\n-                    # Add node1's wallet transactions back to the mempool, to\n-                    # avoid the mined blocks from being too small.\n-                    self.nodes[1].resendwallettransactions()\n-                    self.nodes[1].generate(1) #tx's already in mempool from previous disconnects\n+            mine_large_blocks(self.nodes[1], 24)\n \n             # Reorg back with 25 block chain from node 0\n-            for i in range(25):\n-                mine_large_block(self.nodes[0], self.utxo_cache_0)\n+            mine_large_blocks(self.nodes[0], 25)\n \n             # Create connections in the order so both nodes can see the reorg at the same time\n             connect_nodes(self.nodes[0], 1)\n@@ -129,10 +160,6 @@ def create_chain_with_staleblocks(self):\n     def reorg_test(self):\n         # Node 1 will mine a 300 block chain starting 287 blocks back from Node 0 and Node 2's tip\n         # This will cause Node 2 to do a reorg requiring 288 blocks of undo data to the reorg_test chain\n-        # Reboot node 1 to clear its mempool (hopefully make the invalidate faster)\n-        # Lower the block max size so we don't keep mining all our big mempool transactions (from disconnected blocks)\n-        self.stop_node(1)\n-        self.start_node(1, extra_args=[\"-maxreceivebuffer=20000\",\"-blockmaxweight=20000\", \"-checkblocks=5\"])\n \n         height = self.nodes[1].getblockcount()\n         self.log.info(\"Current block height: %d\" % height)\n@@ -153,9 +180,9 @@ def reorg_test(self):\n         assert self.nodes[1].getblockcount() == self.forkheight - 1\n         self.log.info(\"New best height: %d\" % self.nodes[1].getblockcount())\n \n-        # Reboot node1 to clear those giant tx's from mempool\n-        self.stop_node(1)\n-        self.start_node(1, extra_args=[\"-maxreceivebuffer=20000\",\"-blockmaxweight=20000\", \"-checkblocks=5\"])\n+        # Disconnect node1 and generate the new chain\n+        disconnect_nodes(self.nodes[0], 1)\n+        disconnect_nodes(self.nodes[1], 2)\n \n         self.log.info(\"Generating new longer chain of 300 more blocks\")\n         self.nodes[1].generate(300)\n@@ -167,17 +194,10 @@ def reorg_test(self):\n \n         self.log.info(\"Verify height on node 2: %d\" % self.nodes[2].getblockcount())\n         self.log.info(\"Usage possibly still high because of stale blocks in block files: %d\" % calc_usage(self.prunedir))\n-        self.log.info(\"Mine 220 more large blocks so we have requisite history\")\n \n-        # Get node0's wallet transactions back in its mempool, to avoid the\n-        # mined blocks from being too small.\n-        self.nodes[0].resendwallettransactions()\n+        self.log.info(\"Mine 220 more large blocks so we have requisite history\")\n \n-        for i in range(22):\n-            # This can be slow, so do this in multiple RPC calls to avoid\n-            # RPC timeouts.\n-            self.nodes[0].generate(10) #node 0 has many large tx's in its mempool from the disconnects\n-        sync_blocks(self.nodes[0:3], timeout=300)\n+        mine_large_blocks(self.nodes[0], 220)\n \n         usage = calc_usage(self.prunedir)\n         self.log.info(\"Usage should be below target: %d\" % usage)\n@@ -331,16 +351,9 @@ def wallet_test(self):\n         self.log.info(\"Success\")\n \n     def run_test(self):\n-        self.log.info(\"Warning! This test requires 4GB of disk space and takes over 30 mins (up to 2 hours)\")\n-        self.log.info(\"Mining a big blockchain of 995 blocks\")\n-\n-        # Determine default relay fee\n-        self.relayfee = self.nodes[0].getnetworkinfo()[\"relayfee\"]\n-\n-        # Cache for utxos, as the listunspent may take a long time later in the test\n-        self.utxo_cache_0 = []\n-        self.utxo_cache_1 = []\n+        self.log.info(\"Warning! This test requires 4GB of disk space\")\n \n+        self.log.info(\"Mining a big blockchain of 995 blocks\")\n         self.create_big_chain()\n         # Chain diagram key:\n         # *   blocks on main chain"
      }
    ]
  }
]