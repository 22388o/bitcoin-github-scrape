instagibbs,2018-10-04T07:29:16Z,"Can you post ""typical"" case benchmark comparisons?",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-426913811,426913811,
DrahtBot,2018-10-04T08:38:43Z,"<!--e57a25ab6845829454e8d69fc972939a-->\n\nThe following sections might be updated with supplementary metadata relevant to reviewers and maintainers.\n\n<!--174a7506f384e20aa4161008e828411d-->\n### Conflicts\nReviewers, this pull request conflicts with the following ones:\n\n* [#14400](https://drahtbot.github.io/bitcoin_core_issue_redirect/r/14400.html) (Add Benchmark to test input de-duplication ",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-426933620,426933620,
practicalswift,2018-10-04T08:58:00Z,"@JeremyRubin Impressive speedup! What is the risk-reward trade-off we're facing with this change? More specifically: what risks do you see associated with this change to consensus critical code?\n\nDoes the change in which DoS error gets reported for transactions which have both duplicates and null inputs have any consequences or impose any risks?",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-426939786,426939786,
promag,2018-10-04T23:33:33Z,Anyone measured `-reindex` with and without this change? ,https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-427202088,427202088,
jnewbery,2018-10-05T07:00:01Z,"My immediate reaction is that this seems very complex compared to a naive std::set comparison! This also pulls our SipHash implementation into consensus-critical code, which seems like a big price to pay for a performance win. I lean pretty strongly towards concept NACK.\n\n@JeremyRubin - your PR description talks about what this PR does, but not why. This makes block propagation faster, but do ",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-427265702,427265702,
JeremyRubin,2018-10-05T07:10:31Z,"Goal was to minimize the performance regression caused by the CVE fix. Understand this is sensitive code for that reason. This code is also generally theoretically useful for several other contexts because it is O(N). An adapted version (different parameters) could be used to check for duplicate inputs across a large number of txns (e.g., mempool syncing context).\n\n\nIt's actually not thaaat ",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-427267896,427267896,
practicalswift,2018-10-05T07:40:07Z,"@JeremyRubin It is sufficiently complicated to introduce undefined behaviour in consensus critical code without any of the reviewers noticing .-)\n\nI'm afraid the code as it is currently formulated will trigger undefined behaviour due to shift exponents being too large.\n",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-427274569,427274569,
JeremyRubin,2018-10-05T07:57:16Z,@practicalswift i think I fixed that -- can you confirm? (and also a copy-paste error on which bit was being set :(  ),https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-427278833,427278833,
practicalswift,2018-10-05T09:24:49Z,"@practicalswift No, I'm afraid the undefined behaviour is still present.\n\nCheck this code:\n\nhttps://github.com/bitcoin/bitcoin/blob/4ec7597add35cfc458680fcba2b8fb64931711ef/src/consensus/tx_verify.cpp#L281-L288",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-427301352,427301352,
gmaxwell,2018-10-05T20:32:27Z,I feel like this is too much review work vs the gain.,https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-427490408,427490408,
JeremyRubin,2018-10-06T02:13:13Z,"@gmaxwell @sipa please re-review the updated version (should you have time). @practicalswift I think I have eliminated the UB, if not please let me know.\n\nIn this version I have kept complexity limited in scope to validation.cpp.\n\nPerformance wise this version is actually a bit better in the worst case compared to using the filter per-transaction (DuplicateInputs) and better in an average ",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-427538381,427538381,
gmaxwell,2018-10-07T16:39:50Z,"> The major benefit of this approach (as amended) is that we not only detect duplicate inputs per transaction, but across the entire block at the sam\n\nOne can't do that without losing the ability to cache the check as part of the wtxid validity caching, as the simpler check could be.\n\nI am failing to see the argument for a gain here that even justifies reviewing the change at all. ",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-427667282,427667282,
JeremyRubin,2018-10-09T02:18:28Z,"@gmaxwell That's not accurate -- by fixing the salt for the hash (which should be safe -- I will consider this more closely), you could store three uint64_t's per input in the wtxid validity cache and then re-insert those on the forward pass of the algorithm. To be clear, this just saves sip-hashing at the expense of memory, but you can just keep a table of precomputed sip-hashes for the inputs in",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-428037255,428037255,
DrahtBot,2018-11-06T15:31:36Z,<!--cf906140f33d8803c4a75a2196329ecb-->Needs rebase,https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-436294355,436294355,
gmaxwell,2018-11-29T08:11:20Z,"@jnewbery  nothing in this PR is in the block propagation typical case anymore (not since we finally implemented the full BIP152 HB mode forwarding), so the belief that this speeds up block propagation is largely mistaken.\n\n""Goal was to minimize the performance regression caused by the CVE fix."" -- there wasn't one, or at least not an interesting one.  The speedup to duplicate checking was lat",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-442743132,442743132,
JeremyRubin,2018-11-29T08:34:00Z,"@gmaxwell see #14837 which supersedes this PR.\n\n In my opinion, you are incorrect that CheckBlock is not latency sensitive. Certainly there are a large class of users for whom CheckBlock performance is critical (e.g., miners performing full validation before mining a new block, and miners calling testblockvalidity to get a new template).\n\nThis also has a non negligible impact on benchmarks",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-442748969,442748969,
fanquake,2018-11-29T08:34:52Z,Closing in favour of #14837.,https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-442749214,442749214,
gmaxwell,2018-11-29T08:50:25Z,"> In my opinion, you are incorrect that CheckBlock is not latency sensitive. Certainly there are a large class of users for whom CheckBlock performance is critical (e.g., miners performing full validation before mining a new block, and miners calling testblockvalidity to get a new template).\n\nWhen it was on the critical path of propagation the small delays involved were cumulative across the w",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-442753508,442753508,
practicalswift,2018-11-29T08:58:48Z,"@JeremyRand Regarding my comment regarding UB above. The problem was the following:\n\n```\nuint64_t bit1 = 1<<((std::get<0>(h)) & 63);\n```\n\n... is problematic due to ...\n\n```\n$ cling\n[cling]$ #include <cstdint>\n[cling]$ 1 << 63\nwarning: shift count >= width of type [-Wshift-count-overflow]\n 1 << 63\n   ^  ~~\n(int) 0\n```\n\n... which can be contrasted to ...\n\n```",https://github.com/bitcoin/bitcoin/pull/14387#issuecomment-442755968,442755968,
MarcoFalke,2018-10-04T07:24:17Z,"nit: some trailing white space here?\n\nAlso, I removed all comments about the linter failure, because they are just distracting from the actual pull request.",https://github.com/bitcoin/bitcoin/pull/14387#discussion_r222560971,222560971,src/bench/duplicate_inputs.cpp
leishman,2018-10-04T13:08:53Z,Why 8?,https://github.com/bitcoin/bitcoin/pull/14387#discussion_r222660847,222660847,src/consensus/tx_verify.cpp
JeremyRubin,2018-10-04T23:59:31Z,"It's my favorite number!\n\nIt's to get the expected work below one for the worst case as shown in the analysis.\n\nWe could add a 9th hash (we have more bits in the hash computed) but every hash adds more memory accesses in our table.\n\nWe could also remove a hash or two and the EV would be less than 10, which is probably acceptable 2. At about 4 or 5 hashes is when it blows up a bit more ",https://github.com/bitcoin/bitcoin/pull/14387#discussion_r222861377,222861377,src/consensus/tx_verify.cpp
JeremyRubin,2018-10-05T03:54:19Z,"More precisely, it's one of two choices (9 hashes or 8) with 21 bits with expected work less than 1.\n\nExtracting 9 hashes from 3 64 bit integers is a bit more complex code wise, but doable.\n\n>>> sorted((sum(i*(float(i)*x/ 2.0**y)**x for i in xrange(24930)) if (x*y <= 192) else  ('Inf', 0, 0),y,x) for y in xrange(1,22) for x in xrange(1,20))[:10]\n\n[(0.10374566662377155, 21, 9), (0.41573",https://github.com/bitcoin/bitcoin/pull/14387#discussion_r222886217,222886217,src/consensus/tx_verify.cpp
leishman,2018-10-05T06:27:00Z,How does your implementation compare to using `std::unordered_set`?,https://github.com/bitcoin/bitcoin/pull/14387#discussion_r222902655,222902655,src/consensus/tx_verify.cpp
leishman,2018-10-05T06:30:01Z,"Ok cool. Thanks for the analysis. 8 seems like a pretty reasonable choice. I left a comment above, but was wondering how this bloom filter compares to native unordered set implementations in the stdlib.\n\n",https://github.com/bitcoin/bitcoin/pull/14387#discussion_r222903474,222903474,src/consensus/tx_verify.cpp
JeremyRubin,2018-10-05T06:56:00Z,"See https://github.com/bitcoin/bitcoin/pull/14397 for a more obvious version that is just the obvious thing.\n\nFor std::unordered_set, I'm clocking much worse performance for DeserializeAndCheckBlockTest and 2x worse performance for DuplicateInputs.\n",https://github.com/bitcoin/bitcoin/pull/14387#discussion_r222907779,222907779,src/consensus/tx_verify.cpp
sipa,2018-10-05T06:58:09Z,"@leishman I tried a few alternatives:\n* Master: 13.6 ms\n* #14397: 6.3 ms\n* Using a sorted vector with SipHash'ed prevouts: 3.7 ms\n* This PR: 2.7 ms\n",https://github.com/bitcoin/bitcoin/pull/14387#discussion_r222908232,222908232,src/consensus/tx_verify.cpp
JeremyRubin,2018-10-05T07:05:38Z,I like sorting the siphash'd prevouts -- I'm guessing you then do the expensive check if that collides? Or are you tracking the pointers when you sort too?,https://github.com/bitcoin/bitcoin/pull/14387#discussion_r222909637,222909637,src/consensus/tx_verify.cpp
sipa,2018-10-05T07:09:53Z,"@JeremyRubin Yeah, just delay the expensive check until after the cheap check fails. I haven't PR'ed that because I only have PoC, and I don't want to overload reviewers with a series of PRs without even knowing if we want to increase complexity here at all.",https://github.com/bitcoin/bitcoin/pull/14387#discussion_r222910439,222910439,src/consensus/tx_verify.cpp
JeremyRubin,2018-10-05T07:37:23Z,"For the expensive check, you can still do it in O(n) per colliding entry FYI, which is less expensive than doing the full O(n log n) expensive check given that we don't expect colliding entries without a duplicate.\n",https://github.com/bitcoin/bitcoin/pull/14387#discussion_r222916084,222916084,src/consensus/tx_verify.cpp
