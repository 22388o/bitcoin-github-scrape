[
  {
    "sha": "797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo3OTdmNmMxZWZlOGZiZTE4ZjljMWIzMGZjMTRmYTE2NTZjNWE2ZDQ4",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2018-06-10T23:37:59Z"
      },
      "committer": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2018-10-12T23:40:08Z"
      },
      "message": "Add 1-way SSE4 SHA256 implementation using intrinsics",
      "tree": {
        "sha": "bbbd57433ca7edcc50691678ea694ecbe901f1c6",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/bbbd57433ca7edcc50691678ea694ecbe901f1c6"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "be992701b018f256db6d64786624be4cb60d8975",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/be992701b018f256db6d64786624be4cb60d8975",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/be992701b018f256db6d64786624be4cb60d8975"
      }
    ],
    "stats": {
      "total": 214,
      "additions": 214,
      "deletions": 0
    },
    "files": [
      {
        "sha": "2455b4b49840c0602369b849b04f983b508303f1",
        "filename": "src/crypto/sha256.cpp",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48/src/crypto/sha256.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48/src/crypto/sha256.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/crypto/sha256.cpp?ref=797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48",
        "patch": "@@ -16,6 +16,11 @@ namespace sha256_sse4\n {\n void Transform(uint32_t* s, const unsigned char* chunk, size_t blocks);\n }\n+\n+namespace sha256_sse41\n+{\n+void Transform(uint32_t* s, const unsigned char* chunk, size_t blocks);\n+}\n #endif\n #endif\n "
      },
      {
        "sha": "f98912bec04b74cbd4bb6482b15568921dbedcd9",
        "filename": "src/crypto/sha256_sse41.cpp",
        "status": "modified",
        "additions": 209,
        "deletions": 0,
        "changes": 209,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48/src/crypto/sha256_sse41.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48/src/crypto/sha256_sse41.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/crypto/sha256_sse41.cpp?ref=797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48",
        "patch": "@@ -1,3 +1,7 @@\n+// Copyright (c) 2017-2018 The Bitcoin Core developers\n+// Distributed under the MIT software license, see the accompanying\n+// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n+\n #ifdef ENABLE_SSE41\n \n #include <stdint.h>\n@@ -6,6 +10,211 @@\n #include <crypto/sha256.h>\n #include <crypto/common.h>\n \n+namespace sha256_sse41 {\n+\n+/*\n+* The implementation in this namespace is a conversion to intrinsics from a\n+* NASM implementation by Intel, found at\n+* https://github.com/intel/intel-ipsec-mb/blob/master/sse/sha256_one_block_sse.asm\n+*\n+* Its original copyright text:\n+\n+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n+; Copyright (c) 2012, Intel Corporation\n+;\n+; All rights reserved.\n+;\n+; Redistribution and use in source and binary forms, with or without\n+; modification, are permitted provided that the following conditions are\n+; met:\n+;\n+; * Redistributions of source code must retain the above copyright\n+;   notice, this list of conditions and the following disclaimer.\n+;\n+; * Redistributions in binary form must reproduce the above copyright\n+;   notice, this list of conditions and the following disclaimer in the\n+;   documentation and/or other materials provided with the\n+;   distribution.\n+;\n+; * Neither the name of the Intel Corporation nor the names of its\n+;   contributors may be used to endorse or promote products derived from\n+;   this software without specific prior written permission.\n+;\n+; THIS SOFTWARE IS PROVIDED BY INTEL CORPORATION \"AS IS\" AND ANY\n+; EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n+; IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n+; PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL INTEL CORPORATION OR\n+; CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n+; EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n+; PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n+; PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n+; LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n+; NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n+; SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+*/\n+\n+namespace {\n+\n+uint32_t inline __attribute__((always_inline)) Ror(uint32_t x, int val) { return ((x >> val) | (x << (32 - val))); }\n+\n+/** Compute one round of SHA256. */\n+void inline __attribute__((always_inline)) Round(uint32_t& a, uint32_t& b, uint32_t& c, uint32_t& d, uint32_t& e, uint32_t& f, uint32_t& g, uint32_t& h, uint32_t w)\n+{\n+    uint32_t t0, t1, t2;\n+    t0 = Ror(e, 25 - 11) ^ e;\n+    t1 = Ror(a, 22 - 13) ^ a;\n+    t2 = (f ^ g) & e;\n+    t0 = Ror(t0, 11 - 6) ^ e;\n+    t1 = Ror(t1, 13 - 2) ^ a;\n+    t0 = Ror(t0, 6);\n+    t2 = (t2 ^ g) + t0 + w;\n+    t1 = Ror(t1, 2);\n+    h += t2;\n+    t0 = (a | c) & b;\n+    d += h;\n+    h += t1;\n+    t0 |= (a & c);\n+    h += t0;\n+}\n+\n+/** Compute 4 rounds of SHA256, while simultaneously computing the expansion for\n+ *  16 rounds later.\n+ *\n+ *  Input: a,b,c,d,e,f,g,h: The state variables to update with 4 rounds\n+ *         x0,x1,x2,x3:     4 128-bit variables containing expansions.\n+ *                          If the current round is r, x0,x1,x2,x3 contain the\n+ *                          expansions for rounds r..r+15. x0 will be updated\n+ *                          to have the expansions for round r+16..r+19.\n+ *         W:               The round constants for r..r+3.\n+ */\n+void inline __attribute__((always_inline)) QuadRoundSched(uint32_t& a, uint32_t& b, uint32_t& c, uint32_t& d, uint32_t& e, uint32_t& f, uint32_t& g, uint32_t& h, __m128i& x0, __m128i x1, __m128i x2, __m128i x3, __m128i w)\n+{\n+    alignas(__m128i) uint32_t w32[4];\n+    __m128i t0, t1, t2, t3, t4;\n+\n+    w = _mm_add_epi32(w, x0);\n+    _mm_store_si128((__m128i*)w32, w);\n+\n+    Round(a, b, c, d, e, f, g, h, w32[0]);\n+    t0 = _mm_add_epi32(_mm_alignr_epi8(x3, x2, 4), x0);\n+    t3 = t2 = t1 = _mm_alignr_epi8(x1, x0, 4);\n+    t2 = _mm_srli_epi32(t2, 7);\n+    t1 = _mm_or_si128(_mm_slli_epi32(t1, 32 - 7), t2);\n+\n+    Round(h, a, b, c, d, e, f, g, w32[1]);\n+    t4 = t2 = t3;\n+    t3 = _mm_slli_epi32(t3, 32 - 18);\n+    t2 = _mm_srli_epi32(t2, 18);\n+    t1 = _mm_xor_si128(t1, t3);\n+    t4 = _mm_srli_epi32(t4, 3);\n+    t1 = _mm_xor_si128(_mm_xor_si128(t1, t2), t4);\n+    t2 = _mm_shuffle_epi32(x3, 0xFA);\n+    t0 = _mm_add_epi32(t0, t1);\n+\n+    Round(g, h, a, b, c, d, e, f, w32[2]);\n+    t4 = t3 = t2;\n+    t2 = _mm_srli_epi64(t2, 17);\n+    t3 = _mm_srli_epi64(t3, 19);\n+    t4 = _mm_srli_epi32(t4, 10);\n+    t2 = _mm_xor_si128(t2, t3);\n+    t4 = _mm_shuffle_epi8(_mm_xor_si128(t4, t2), _mm_set_epi64x(0xFFFFFFFFFFFFFFFFULL, 0x0b0a090803020100ULL));\n+    t0 = _mm_add_epi32(t0, t4);\n+    t2 = _mm_shuffle_epi32(t0, 0x50);\n+\n+    Round(f, g, h, a, b, c, d, e, w32[3]);\n+    x0 = t3 = t2;\n+    t2 = _mm_srli_epi64(t2, 17);\n+    t3 = _mm_srli_epi64(t3, 19);\n+    x0 = _mm_srli_epi32(x0, 10);\n+    t2 = _mm_xor_si128(t2, t3);\n+    x0 = _mm_add_epi32(_mm_shuffle_epi8(_mm_xor_si128(x0, t2), _mm_set_epi64x(0x0b0a090803020100ULL, 0xFFFFFFFFFFFFFFFFULL)), t0);\n+}\n+\n+void inline __attribute__((always_inline)) QuadRound(uint32_t& a, uint32_t& b, uint32_t& c, uint32_t& d, uint32_t& e, uint32_t& f, uint32_t& g, uint32_t& h, __m128i x0, __m128i w)\n+{\n+    alignas(__m128i) uint32_t w32[32];\n+\n+    x0 = _mm_add_epi32(x0, w);\n+    _mm_store_si128((__m128i*)w32, x0);\n+\n+    Round(a, b, c, d, e, f, g, h, w32[0]);\n+    Round(h, a, b, c, d, e, f, g, w32[1]);\n+    Round(g, h, a, b, c, d, e, f, w32[2]);\n+    Round(f, g, h, a, b, c, d, e, w32[3]);\n+}\n+\n+__m128i inline __attribute__((always_inline)) KK(uint32_t a, uint32_t b, uint32_t c, uint32_t d)\n+{\n+    return _mm_set_epi32(d, c, b, a);\n+}\n+\n+}\n+\n+void Transform(uint32_t* s, const unsigned char* chunk, size_t blocks)\n+{\n+    const unsigned char* end = chunk + blocks * 64;\n+    static const __m128i BYTE_FLIP_MASK = _mm_set_epi64x(0x0c0d0e0f08090a0b, 0x0405060700010203);\n+\n+    static const __m128i TBL[16] = {\n+        KK(0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5),\n+        KK(0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5),\n+        KK(0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3),\n+        KK(0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174),\n+        KK(0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc),\n+        KK(0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da),\n+        KK(0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7),\n+        KK(0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967),\n+        KK(0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13),\n+        KK(0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85),\n+        KK(0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3),\n+        KK(0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070),\n+        KK(0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5),\n+        KK(0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3),\n+        KK(0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208),\n+        KK(0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2),\n+    };\n+\n+    uint32_t a = s[0], b = s[1], c = s[2], d = s[3], e = s[4], f = s[5], g = s[6], h = s[7];\n+\n+    __m128i x0, x1, x2, x3;\n+\n+    while (chunk != end) {\n+        x0 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i*)chunk), BYTE_FLIP_MASK);\n+        x1 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i*)(chunk + 16)), BYTE_FLIP_MASK);\n+        x2 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i*)(chunk + 32)), BYTE_FLIP_MASK);\n+        x3 = _mm_shuffle_epi8(_mm_loadu_si128((const __m128i*)(chunk + 48)), BYTE_FLIP_MASK);\n+\n+        QuadRoundSched(a, b, c, d, e, f, g, h, x0, x1, x2, x3, TBL[0]);\n+        QuadRoundSched(e, f, g, h, a, b, c, d, x1, x2, x3, x0, TBL[1]);\n+        QuadRoundSched(a, b, c, d, e, f, g, h, x2, x3, x0, x1, TBL[2]);\n+        QuadRoundSched(e, f, g, h, a, b, c, d, x3, x0, x1, x2, TBL[3]);\n+        QuadRoundSched(a, b, c, d, e, f, g, h, x0, x1, x2, x3, TBL[4]);\n+        QuadRoundSched(e, f, g, h, a, b, c, d, x1, x2, x3, x0, TBL[5]);\n+        QuadRoundSched(a, b, c, d, e, f, g, h, x2, x3, x0, x1, TBL[6]);\n+        QuadRoundSched(e, f, g, h, a, b, c, d, x3, x0, x1, x2, TBL[7]);\n+        QuadRoundSched(a, b, c, d, e, f, g, h, x0, x1, x2, x3, TBL[8]);\n+        QuadRoundSched(e, f, g, h, a, b, c, d, x1, x2, x3, x0, TBL[9]);\n+        QuadRoundSched(a, b, c, d, e, f, g, h, x2, x3, x0, x1, TBL[10]);\n+        QuadRoundSched(e, f, g, h, a, b, c, d, x3, x0, x1, x2, TBL[11]);\n+        QuadRound(a, b, c, d, e, f, g, h, x0, TBL[12]);\n+        QuadRound(e, f, g, h, a, b, c, d, x1, TBL[13]);\n+        QuadRound(a, b, c, d, e, f, g, h, x2, TBL[14]);\n+        QuadRound(e, f, g, h, a, b, c, d, x3, TBL[15]);\n+\n+        a += s[0]; s[0] = a;\n+        b += s[1]; s[1] = b;\n+        c += s[2]; s[2] = c;\n+        d += s[3]; s[3] = d;\n+        e += s[4]; s[4] = e;\n+        f += s[5]; s[5] = f;\n+        g += s[6]; s[6] = g;\n+        h += s[7]; s[7] = h;\n+\n+        chunk += 64;\n+    }\n+}\n+}\n+\n namespace sha256d64_sse41 {\n namespace {\n "
      }
    ]
  },
  {
    "sha": "7ebae4d40af7cd60da65704cc8be566e9af07ade",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo3ZWJhZTRkNDBhZjdjZDYwZGE2NTcwNGNjOGJlNTY2ZTlhZjA3YWRl",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2018-06-11T23:19:04Z"
      },
      "committer": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2018-10-12T23:40:08Z"
      },
      "message": "Switch 1-way SSE4 SHA256 to intrinsics based implementation",
      "tree": {
        "sha": "f214b6ff2f1f63e2cc93650afb730c8b43533703",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/f214b6ff2f1f63e2cc93650afb730c8b43533703"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/7ebae4d40af7cd60da65704cc8be566e9af07ade",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7ebae4d40af7cd60da65704cc8be566e9af07ade",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/7ebae4d40af7cd60da65704cc8be566e9af07ade",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7ebae4d40af7cd60da65704cc8be566e9af07ade/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/797f6c1efe8fbe18f9c1b30fc14fa1656c5a6d48"
      }
    ],
    "stats": {
      "total": 7,
      "additions": 2,
      "deletions": 5
    },
    "files": [
      {
        "sha": "819a5165187c3868402fed10031d14f275948185",
        "filename": "src/crypto/sha256.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 5,
        "changes": 7,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/7ebae4d40af7cd60da65704cc8be566e9af07ade/src/crypto/sha256.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/7ebae4d40af7cd60da65704cc8be566e9af07ade/src/crypto/sha256.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/crypto/sha256.cpp?ref=7ebae4d40af7cd60da65704cc8be566e9af07ade",
        "patch": "@@ -620,12 +620,9 @@ std::string SHA256AutoDetect()\n #endif\n \n     if (have_sse4) {\n-#if defined(__x86_64__) || defined(__amd64__)\n-        Transform = sha256_sse4::Transform;\n-        TransformD64 = TransformD64Wrapper<sha256_sse4::Transform>;\n-        ret = \"sse4(1way)\";\n-#endif\n #if defined(ENABLE_SSE41) && !defined(BUILD_BITCOIN_INTERNAL)\n+        Transform = sha256_sse41::Transform;\n+        TransformD64 = TransformD64Wrapper<sha256_sse41::Transform>;\n         TransformD64_4way = sha256d64_sse41::Transform_4way;\n         ret += \",sse41(4way)\";\n #endif"
      }
    ]
  },
  {
    "sha": "4a221cef1bc5ce093616f58d2295bad9dc6ad5b1",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo0YTIyMWNlZjFiYzVjZTA5MzYxNmY1OGQyMjk1YmFkOWRjNmFkNWIx",
    "commit": {
      "author": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2018-06-12T03:12:23Z"
      },
      "committer": {
        "name": "Pieter Wuille",
        "email": "pieter.wuille@gmail.com",
        "date": "2018-10-12T23:40:08Z"
      },
      "message": "Remove SSE4 assembly implementation",
      "tree": {
        "sha": "dc0aa0bd43b0d88bbab971e82db3f0659cf6734b",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/dc0aa0bd43b0d88bbab971e82db3f0659cf6734b"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/4a221cef1bc5ce093616f58d2295bad9dc6ad5b1",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/4a221cef1bc5ce093616f58d2295bad9dc6ad5b1",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/4a221cef1bc5ce093616f58d2295bad9dc6ad5b1",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/4a221cef1bc5ce093616f58d2295bad9dc6ad5b1/comments",
    "author": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "sipa",
      "id": 548488,
      "node_id": "MDQ6VXNlcjU0ODQ4OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/548488?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sipa",
      "html_url": "https://github.com/sipa",
      "followers_url": "https://api.github.com/users/sipa/followers",
      "following_url": "https://api.github.com/users/sipa/following{/other_user}",
      "gists_url": "https://api.github.com/users/sipa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sipa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sipa/subscriptions",
      "organizations_url": "https://api.github.com/users/sipa/orgs",
      "repos_url": "https://api.github.com/users/sipa/repos",
      "events_url": "https://api.github.com/users/sipa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sipa/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "7ebae4d40af7cd60da65704cc8be566e9af07ade",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7ebae4d40af7cd60da65704cc8be566e9af07ade",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/7ebae4d40af7cd60da65704cc8be566e9af07ade"
      }
    ],
    "stats": {
      "total": 1519,
      "additions": 2,
      "deletions": 1517
    },
    "files": [
      {
        "sha": "7697547c82307168b58645aaf9dfac41efc68712",
        "filename": "src/Makefile.am",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/4a221cef1bc5ce093616f58d2295bad9dc6ad5b1/src/Makefile.am",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/4a221cef1bc5ce093616f58d2295bad9dc6ad5b1/src/Makefile.am",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/Makefile.am?ref=4a221cef1bc5ce093616f58d2295bad9dc6ad5b1",
        "patch": "@@ -323,10 +323,6 @@ crypto_libbitcoin_crypto_base_a_SOURCES = \\\n   crypto/sha512.cpp \\\n   crypto/sha512.h\n \n-if USE_ASM\n-crypto_libbitcoin_crypto_base_a_SOURCES += crypto/sha256_sse4.cpp\n-endif\n-\n crypto_libbitcoin_crypto_sse41_a_CXXFLAGS = $(AM_CXXFLAGS) $(PIE_FLAGS)\n crypto_libbitcoin_crypto_sse41_a_CPPFLAGS = $(AM_CPPFLAGS)\n crypto_libbitcoin_crypto_sse41_a_CXXFLAGS += $(SSE41_CXXFLAGS)"
      },
      {
        "sha": "2178de5aafb9b713319154d4bc1cfa54ca0a83a7",
        "filename": "src/crypto/sha256.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 7,
        "changes": 9,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/4a221cef1bc5ce093616f58d2295bad9dc6ad5b1/src/crypto/sha256.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/4a221cef1bc5ce093616f58d2295bad9dc6ad5b1/src/crypto/sha256.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/crypto/sha256.cpp?ref=4a221cef1bc5ce093616f58d2295bad9dc6ad5b1",
        "patch": "@@ -10,19 +10,12 @@\n #include <atomic>\n \n #if defined(__x86_64__) || defined(__amd64__) || defined(__i386__)\n-#if defined(USE_ASM)\n #include <cpuid.h>\n-namespace sha256_sse4\n-{\n-void Transform(uint32_t* s, const unsigned char* chunk, size_t blocks);\n-}\n \n namespace sha256_sse41\n {\n void Transform(uint32_t* s, const unsigned char* chunk, size_t blocks);\n }\n-#endif\n-#endif\n \n namespace sha256d64_sse41\n {\n@@ -44,6 +37,8 @@ namespace sha256_shani\n void Transform(uint32_t* s, const unsigned char* chunk, size_t blocks);\n }\n \n+#endif\n+\n // Internal implementation code.\n namespace\n {"
      },
      {
        "sha": "89f529a3abcff52044b0db662aa857518c523e80",
        "filename": "src/crypto/sha256_sse4.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 1506,
        "changes": 1506,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/7ebae4d40af7cd60da65704cc8be566e9af07ade/src/crypto/sha256_sse4.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/7ebae4d40af7cd60da65704cc8be566e9af07ade/src/crypto/sha256_sse4.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/crypto/sha256_sse4.cpp?ref=7ebae4d40af7cd60da65704cc8be566e9af07ade",
        "patch": "@@ -1,1506 +0,0 @@\n-// Copyright (c) 2017 The Bitcoin Core developers\n-// Distributed under the MIT software license, see the accompanying\n-// file COPYING or http://www.opensource.org/licenses/mit-license.php.\n-//\n-// This is a translation to GCC extended asm syntax from YASM code by Intel\n-// (available at the bottom of this file).\n-\n-#include <stdint.h>\n-#include <stdlib.h>\n-\n-#if defined(__x86_64__) || defined(__amd64__)\n-\n-namespace sha256_sse4\n-{\n-void Transform(uint32_t* s, const unsigned char* chunk, size_t blocks)\n-{\n-    static const uint32_t K256 alignas(16) [] = {\n-        0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5,\n-        0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,\n-        0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,\n-        0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,\n-        0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc,\n-        0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,\n-        0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7,\n-        0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,\n-        0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,\n-        0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,\n-        0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3,\n-        0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,\n-        0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5,\n-        0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,\n-        0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,\n-        0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2,\n-    };\n-    static const uint32_t FLIP_MASK alignas(16) [] = {0x00010203, 0x04050607, 0x08090a0b, 0x0c0d0e0f};\n-    static const uint32_t SHUF_00BA alignas(16) [] = {0x03020100, 0x0b0a0908, 0xffffffff, 0xffffffff};\n-    static const uint32_t SHUF_DC00 alignas(16) [] = {0xffffffff, 0xffffffff, 0x03020100, 0x0b0a0908};\n-    uint32_t a, b, c, d, f, g, h, y0, y1, y2;\n-    uint64_t tbl;\n-    uint64_t inp_end, inp;\n-    uint32_t xfer alignas(16) [4];\n-\n-    __asm__ __volatile__(\n-        \"shl    $0x6,%2;\"\n-        \"je     Ldone_hash_%=;\"\n-        \"add    %1,%2;\"\n-        \"mov    %2,%14;\"\n-        \"mov    (%0),%3;\"\n-        \"mov    0x4(%0),%4;\"\n-        \"mov    0x8(%0),%5;\"\n-        \"mov    0xc(%0),%6;\"\n-        \"mov    0x10(%0),%k2;\"\n-        \"mov    0x14(%0),%7;\"\n-        \"mov    0x18(%0),%8;\"\n-        \"mov    0x1c(%0),%9;\"\n-        \"movdqa %18,%%xmm12;\"\n-        \"movdqa %19,%%xmm10;\"\n-        \"movdqa %20,%%xmm11;\"\n-\n-        \"Lloop0_%=:\"\n-        \"lea    %17,%13;\"\n-        \"movdqu (%1),%%xmm4;\"\n-        \"pshufb %%xmm12,%%xmm4;\"\n-        \"movdqu 0x10(%1),%%xmm5;\"\n-        \"pshufb %%xmm12,%%xmm5;\"\n-        \"movdqu 0x20(%1),%%xmm6;\"\n-        \"pshufb %%xmm12,%%xmm6;\"\n-        \"movdqu 0x30(%1),%%xmm7;\"\n-        \"pshufb %%xmm12,%%xmm7;\"\n-        \"mov    %1,%15;\"\n-        \"mov    $3,%1;\"\n-\n-        \"Lloop1_%=:\"\n-        \"movdqa 0x0(%13),%%xmm9;\"\n-        \"paddd  %%xmm4,%%xmm9;\"\n-        \"movdqa %%xmm9,%16;\"\n-        \"movdqa %%xmm7,%%xmm0;\"\n-        \"mov    %k2,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %3,%11;\"\n-        \"palignr $0x4,%%xmm6,%%xmm0;\"\n-        \"ror    $0x9,%11;\"\n-        \"xor    %k2,%10;\"\n-        \"mov    %7,%12;\"\n-        \"ror    $0x5,%10;\"\n-        \"movdqa %%xmm5,%%xmm1;\"\n-        \"xor    %3,%11;\"\n-        \"xor    %8,%12;\"\n-        \"paddd  %%xmm4,%%xmm0;\"\n-        \"xor    %k2,%10;\"\n-        \"and    %k2,%12;\"\n-        \"ror    $0xb,%11;\"\n-        \"palignr $0x4,%%xmm4,%%xmm1;\"\n-        \"xor    %3,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %8,%12;\"\n-        \"movdqa %%xmm1,%%xmm2;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    %10,%12;\"\n-        \"add    %16,%12;\"\n-        \"movdqa %%xmm1,%%xmm3;\"\n-        \"mov    %3,%10;\"\n-        \"add    %12,%9;\"\n-        \"mov    %3,%12;\"\n-        \"pslld  $0x19,%%xmm1;\"\n-        \"or     %5,%10;\"\n-        \"add    %9,%6;\"\n-        \"and    %5,%12;\"\n-        \"psrld  $0x7,%%xmm2;\"\n-        \"and    %4,%10;\"\n-        \"add    %11,%9;\"\n-        \"por    %%xmm2,%%xmm1;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%9;\"\n-        \"movdqa %%xmm3,%%xmm2;\"\n-        \"mov    %6,%10;\"\n-        \"mov    %9,%11;\"\n-        \"movdqa %%xmm3,%%xmm8;\"\n-        \"ror    $0xe,%10;\"\n-        \"xor    %6,%10;\"\n-        \"mov    %k2,%12;\"\n-        \"ror    $0x9,%11;\"\n-        \"pslld  $0xe,%%xmm3;\"\n-        \"xor    %9,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"xor    %7,%12;\"\n-        \"psrld  $0x12,%%xmm2;\"\n-        \"ror    $0xb,%11;\"\n-        \"xor    %6,%10;\"\n-        \"and    %6,%12;\"\n-        \"ror    $0x6,%10;\"\n-        \"pxor   %%xmm3,%%xmm1;\"\n-        \"xor    %9,%11;\"\n-        \"xor    %7,%12;\"\n-        \"psrld  $0x3,%%xmm8;\"\n-        \"add    %10,%12;\"\n-        \"add    4+%16,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"pxor   %%xmm2,%%xmm1;\"\n-        \"mov    %9,%10;\"\n-        \"add    %12,%8;\"\n-        \"mov    %9,%12;\"\n-        \"pxor   %%xmm8,%%xmm1;\"\n-        \"or     %4,%10;\"\n-        \"add    %8,%5;\"\n-        \"and    %4,%12;\"\n-        \"pshufd $0xfa,%%xmm7,%%xmm2;\"\n-        \"and    %3,%10;\"\n-        \"add    %11,%8;\"\n-        \"paddd  %%xmm1,%%xmm0;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%8;\"\n-        \"movdqa %%xmm2,%%xmm3;\"\n-        \"mov    %5,%10;\"\n-        \"mov    %8,%11;\"\n-        \"ror    $0xe,%10;\"\n-        \"movdqa %%xmm2,%%xmm8;\"\n-        \"xor    %5,%10;\"\n-        \"ror    $0x9,%11;\"\n-        \"mov    %6,%12;\"\n-        \"xor    %8,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"psrlq  $0x11,%%xmm2;\"\n-        \"xor    %k2,%12;\"\n-        \"psrlq  $0x13,%%xmm3;\"\n-        \"xor    %5,%10;\"\n-        \"and    %5,%12;\"\n-        \"psrld  $0xa,%%xmm8;\"\n-        \"ror    $0xb,%11;\"\n-        \"xor    %8,%11;\"\n-        \"xor    %k2,%12;\"\n-        \"ror    $0x6,%10;\"\n-        \"pxor   %%xmm3,%%xmm2;\"\n-        \"add    %10,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    8+%16,%12;\"\n-        \"pxor   %%xmm2,%%xmm8;\"\n-        \"mov    %8,%10;\"\n-        \"add    %12,%7;\"\n-        \"mov    %8,%12;\"\n-        \"pshufb %%xmm10,%%xmm8;\"\n-        \"or     %3,%10;\"\n-        \"add    %7,%4;\"\n-        \"and    %3,%12;\"\n-        \"paddd  %%xmm8,%%xmm0;\"\n-        \"and    %9,%10;\"\n-        \"add    %11,%7;\"\n-        \"pshufd $0x50,%%xmm0,%%xmm2;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%7;\"\n-        \"movdqa %%xmm2,%%xmm3;\"\n-        \"mov    %4,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %7,%11;\"\n-        \"movdqa %%xmm2,%%xmm4;\"\n-        \"ror    $0x9,%11;\"\n-        \"xor    %4,%10;\"\n-        \"mov    %5,%12;\"\n-        \"ror    $0x5,%10;\"\n-        \"psrlq  $0x11,%%xmm2;\"\n-        \"xor    %7,%11;\"\n-        \"xor    %6,%12;\"\n-        \"psrlq  $0x13,%%xmm3;\"\n-        \"xor    %4,%10;\"\n-        \"and    %4,%12;\"\n-        \"ror    $0xb,%11;\"\n-        \"psrld  $0xa,%%xmm4;\"\n-        \"xor    %7,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %6,%12;\"\n-        \"pxor   %%xmm3,%%xmm2;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    %10,%12;\"\n-        \"add    12+%16,%12;\"\n-        \"pxor   %%xmm2,%%xmm4;\"\n-        \"mov    %7,%10;\"\n-        \"add    %12,%k2;\"\n-        \"mov    %7,%12;\"\n-        \"pshufb %%xmm11,%%xmm4;\"\n-        \"or     %9,%10;\"\n-        \"add    %k2,%3;\"\n-        \"and    %9,%12;\"\n-        \"paddd  %%xmm0,%%xmm4;\"\n-        \"and    %8,%10;\"\n-        \"add    %11,%k2;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%k2;\"\n-        \"movdqa 0x10(%13),%%xmm9;\"\n-        \"paddd  %%xmm5,%%xmm9;\"\n-        \"movdqa %%xmm9,%16;\"\n-        \"movdqa %%xmm4,%%xmm0;\"\n-        \"mov    %3,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %k2,%11;\"\n-        \"palignr $0x4,%%xmm7,%%xmm0;\"\n-        \"ror    $0x9,%11;\"\n-        \"xor    %3,%10;\"\n-        \"mov    %4,%12;\"\n-        \"ror    $0x5,%10;\"\n-        \"movdqa %%xmm6,%%xmm1;\"\n-        \"xor    %k2,%11;\"\n-        \"xor    %5,%12;\"\n-        \"paddd  %%xmm5,%%xmm0;\"\n-        \"xor    %3,%10;\"\n-        \"and    %3,%12;\"\n-        \"ror    $0xb,%11;\"\n-        \"palignr $0x4,%%xmm5,%%xmm1;\"\n-        \"xor    %k2,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %5,%12;\"\n-        \"movdqa %%xmm1,%%xmm2;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    %10,%12;\"\n-        \"add    %16,%12;\"\n-        \"movdqa %%xmm1,%%xmm3;\"\n-        \"mov    %k2,%10;\"\n-        \"add    %12,%6;\"\n-        \"mov    %k2,%12;\"\n-        \"pslld  $0x19,%%xmm1;\"\n-        \"or     %8,%10;\"\n-        \"add    %6,%9;\"\n-        \"and    %8,%12;\"\n-        \"psrld  $0x7,%%xmm2;\"\n-        \"and    %7,%10;\"\n-        \"add    %11,%6;\"\n-        \"por    %%xmm2,%%xmm1;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%6;\"\n-        \"movdqa %%xmm3,%%xmm2;\"\n-        \"mov    %9,%10;\"\n-        \"mov    %6,%11;\"\n-        \"movdqa %%xmm3,%%xmm8;\"\n-        \"ror    $0xe,%10;\"\n-        \"xor    %9,%10;\"\n-        \"mov    %3,%12;\"\n-        \"ror    $0x9,%11;\"\n-        \"pslld  $0xe,%%xmm3;\"\n-        \"xor    %6,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"xor    %4,%12;\"\n-        \"psrld  $0x12,%%xmm2;\"\n-        \"ror    $0xb,%11;\"\n-        \"xor    %9,%10;\"\n-        \"and    %9,%12;\"\n-        \"ror    $0x6,%10;\"\n-        \"pxor   %%xmm3,%%xmm1;\"\n-        \"xor    %6,%11;\"\n-        \"xor    %4,%12;\"\n-        \"psrld  $0x3,%%xmm8;\"\n-        \"add    %10,%12;\"\n-        \"add    4+%16,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"pxor   %%xmm2,%%xmm1;\"\n-        \"mov    %6,%10;\"\n-        \"add    %12,%5;\"\n-        \"mov    %6,%12;\"\n-        \"pxor   %%xmm8,%%xmm1;\"\n-        \"or     %7,%10;\"\n-        \"add    %5,%8;\"\n-        \"and    %7,%12;\"\n-        \"pshufd $0xfa,%%xmm4,%%xmm2;\"\n-        \"and    %k2,%10;\"\n-        \"add    %11,%5;\"\n-        \"paddd  %%xmm1,%%xmm0;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%5;\"\n-        \"movdqa %%xmm2,%%xmm3;\"\n-        \"mov    %8,%10;\"\n-        \"mov    %5,%11;\"\n-        \"ror    $0xe,%10;\"\n-        \"movdqa %%xmm2,%%xmm8;\"\n-        \"xor    %8,%10;\"\n-        \"ror    $0x9,%11;\"\n-        \"mov    %9,%12;\"\n-        \"xor    %5,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"psrlq  $0x11,%%xmm2;\"\n-        \"xor    %3,%12;\"\n-        \"psrlq  $0x13,%%xmm3;\"\n-        \"xor    %8,%10;\"\n-        \"and    %8,%12;\"\n-        \"psrld  $0xa,%%xmm8;\"\n-        \"ror    $0xb,%11;\"\n-        \"xor    %5,%11;\"\n-        \"xor    %3,%12;\"\n-        \"ror    $0x6,%10;\"\n-        \"pxor   %%xmm3,%%xmm2;\"\n-        \"add    %10,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    8+%16,%12;\"\n-        \"pxor   %%xmm2,%%xmm8;\"\n-        \"mov    %5,%10;\"\n-        \"add    %12,%4;\"\n-        \"mov    %5,%12;\"\n-        \"pshufb %%xmm10,%%xmm8;\"\n-        \"or     %k2,%10;\"\n-        \"add    %4,%7;\"\n-        \"and    %k2,%12;\"\n-        \"paddd  %%xmm8,%%xmm0;\"\n-        \"and    %6,%10;\"\n-        \"add    %11,%4;\"\n-        \"pshufd $0x50,%%xmm0,%%xmm2;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%4;\"\n-        \"movdqa %%xmm2,%%xmm3;\"\n-        \"mov    %7,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %4,%11;\"\n-        \"movdqa %%xmm2,%%xmm5;\"\n-        \"ror    $0x9,%11;\"\n-        \"xor    %7,%10;\"\n-        \"mov    %8,%12;\"\n-        \"ror    $0x5,%10;\"\n-        \"psrlq  $0x11,%%xmm2;\"\n-        \"xor    %4,%11;\"\n-        \"xor    %9,%12;\"\n-        \"psrlq  $0x13,%%xmm3;\"\n-        \"xor    %7,%10;\"\n-        \"and    %7,%12;\"\n-        \"ror    $0xb,%11;\"\n-        \"psrld  $0xa,%%xmm5;\"\n-        \"xor    %4,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %9,%12;\"\n-        \"pxor   %%xmm3,%%xmm2;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    %10,%12;\"\n-        \"add    12+%16,%12;\"\n-        \"pxor   %%xmm2,%%xmm5;\"\n-        \"mov    %4,%10;\"\n-        \"add    %12,%3;\"\n-        \"mov    %4,%12;\"\n-        \"pshufb %%xmm11,%%xmm5;\"\n-        \"or     %6,%10;\"\n-        \"add    %3,%k2;\"\n-        \"and    %6,%12;\"\n-        \"paddd  %%xmm0,%%xmm5;\"\n-        \"and    %5,%10;\"\n-        \"add    %11,%3;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%3;\"\n-        \"movdqa 0x20(%13),%%xmm9;\"\n-        \"paddd  %%xmm6,%%xmm9;\"\n-        \"movdqa %%xmm9,%16;\"\n-        \"movdqa %%xmm5,%%xmm0;\"\n-        \"mov    %k2,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %3,%11;\"\n-        \"palignr $0x4,%%xmm4,%%xmm0;\"\n-        \"ror    $0x9,%11;\"\n-        \"xor    %k2,%10;\"\n-        \"mov    %7,%12;\"\n-        \"ror    $0x5,%10;\"\n-        \"movdqa %%xmm7,%%xmm1;\"\n-        \"xor    %3,%11;\"\n-        \"xor    %8,%12;\"\n-        \"paddd  %%xmm6,%%xmm0;\"\n-        \"xor    %k2,%10;\"\n-        \"and    %k2,%12;\"\n-        \"ror    $0xb,%11;\"\n-        \"palignr $0x4,%%xmm6,%%xmm1;\"\n-        \"xor    %3,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %8,%12;\"\n-        \"movdqa %%xmm1,%%xmm2;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    %10,%12;\"\n-        \"add    %16,%12;\"\n-        \"movdqa %%xmm1,%%xmm3;\"\n-        \"mov    %3,%10;\"\n-        \"add    %12,%9;\"\n-        \"mov    %3,%12;\"\n-        \"pslld  $0x19,%%xmm1;\"\n-        \"or     %5,%10;\"\n-        \"add    %9,%6;\"\n-        \"and    %5,%12;\"\n-        \"psrld  $0x7,%%xmm2;\"\n-        \"and    %4,%10;\"\n-        \"add    %11,%9;\"\n-        \"por    %%xmm2,%%xmm1;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%9;\"\n-        \"movdqa %%xmm3,%%xmm2;\"\n-        \"mov    %6,%10;\"\n-        \"mov    %9,%11;\"\n-        \"movdqa %%xmm3,%%xmm8;\"\n-        \"ror    $0xe,%10;\"\n-        \"xor    %6,%10;\"\n-        \"mov    %k2,%12;\"\n-        \"ror    $0x9,%11;\"\n-        \"pslld  $0xe,%%xmm3;\"\n-        \"xor    %9,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"xor    %7,%12;\"\n-        \"psrld  $0x12,%%xmm2;\"\n-        \"ror    $0xb,%11;\"\n-        \"xor    %6,%10;\"\n-        \"and    %6,%12;\"\n-        \"ror    $0x6,%10;\"\n-        \"pxor   %%xmm3,%%xmm1;\"\n-        \"xor    %9,%11;\"\n-        \"xor    %7,%12;\"\n-        \"psrld  $0x3,%%xmm8;\"\n-        \"add    %10,%12;\"\n-        \"add    4+%16,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"pxor   %%xmm2,%%xmm1;\"\n-        \"mov    %9,%10;\"\n-        \"add    %12,%8;\"\n-        \"mov    %9,%12;\"\n-        \"pxor   %%xmm8,%%xmm1;\"\n-        \"or     %4,%10;\"\n-        \"add    %8,%5;\"\n-        \"and    %4,%12;\"\n-        \"pshufd $0xfa,%%xmm5,%%xmm2;\"\n-        \"and    %3,%10;\"\n-        \"add    %11,%8;\"\n-        \"paddd  %%xmm1,%%xmm0;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%8;\"\n-        \"movdqa %%xmm2,%%xmm3;\"\n-        \"mov    %5,%10;\"\n-        \"mov    %8,%11;\"\n-        \"ror    $0xe,%10;\"\n-        \"movdqa %%xmm2,%%xmm8;\"\n-        \"xor    %5,%10;\"\n-        \"ror    $0x9,%11;\"\n-        \"mov    %6,%12;\"\n-        \"xor    %8,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"psrlq  $0x11,%%xmm2;\"\n-        \"xor    %k2,%12;\"\n-        \"psrlq  $0x13,%%xmm3;\"\n-        \"xor    %5,%10;\"\n-        \"and    %5,%12;\"\n-        \"psrld  $0xa,%%xmm8;\"\n-        \"ror    $0xb,%11;\"\n-        \"xor    %8,%11;\"\n-        \"xor    %k2,%12;\"\n-        \"ror    $0x6,%10;\"\n-        \"pxor   %%xmm3,%%xmm2;\"\n-        \"add    %10,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    8+%16,%12;\"\n-        \"pxor   %%xmm2,%%xmm8;\"\n-        \"mov    %8,%10;\"\n-        \"add    %12,%7;\"\n-        \"mov    %8,%12;\"\n-        \"pshufb %%xmm10,%%xmm8;\"\n-        \"or     %3,%10;\"\n-        \"add    %7,%4;\"\n-        \"and    %3,%12;\"\n-        \"paddd  %%xmm8,%%xmm0;\"\n-        \"and    %9,%10;\"\n-        \"add    %11,%7;\"\n-        \"pshufd $0x50,%%xmm0,%%xmm2;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%7;\"\n-        \"movdqa %%xmm2,%%xmm3;\"\n-        \"mov    %4,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %7,%11;\"\n-        \"movdqa %%xmm2,%%xmm6;\"\n-        \"ror    $0x9,%11;\"\n-        \"xor    %4,%10;\"\n-        \"mov    %5,%12;\"\n-        \"ror    $0x5,%10;\"\n-        \"psrlq  $0x11,%%xmm2;\"\n-        \"xor    %7,%11;\"\n-        \"xor    %6,%12;\"\n-        \"psrlq  $0x13,%%xmm3;\"\n-        \"xor    %4,%10;\"\n-        \"and    %4,%12;\"\n-        \"ror    $0xb,%11;\"\n-        \"psrld  $0xa,%%xmm6;\"\n-        \"xor    %7,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %6,%12;\"\n-        \"pxor   %%xmm3,%%xmm2;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    %10,%12;\"\n-        \"add    12+%16,%12;\"\n-        \"pxor   %%xmm2,%%xmm6;\"\n-        \"mov    %7,%10;\"\n-        \"add    %12,%k2;\"\n-        \"mov    %7,%12;\"\n-        \"pshufb %%xmm11,%%xmm6;\"\n-        \"or     %9,%10;\"\n-        \"add    %k2,%3;\"\n-        \"and    %9,%12;\"\n-        \"paddd  %%xmm0,%%xmm6;\"\n-        \"and    %8,%10;\"\n-        \"add    %11,%k2;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%k2;\"\n-        \"movdqa 0x30(%13),%%xmm9;\"\n-        \"paddd  %%xmm7,%%xmm9;\"\n-        \"movdqa %%xmm9,%16;\"\n-        \"add    $0x40,%13;\"\n-        \"movdqa %%xmm6,%%xmm0;\"\n-        \"mov    %3,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %k2,%11;\"\n-        \"palignr $0x4,%%xmm5,%%xmm0;\"\n-        \"ror    $0x9,%11;\"\n-        \"xor    %3,%10;\"\n-        \"mov    %4,%12;\"\n-        \"ror    $0x5,%10;\"\n-        \"movdqa %%xmm4,%%xmm1;\"\n-        \"xor    %k2,%11;\"\n-        \"xor    %5,%12;\"\n-        \"paddd  %%xmm7,%%xmm0;\"\n-        \"xor    %3,%10;\"\n-        \"and    %3,%12;\"\n-        \"ror    $0xb,%11;\"\n-        \"palignr $0x4,%%xmm7,%%xmm1;\"\n-        \"xor    %k2,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %5,%12;\"\n-        \"movdqa %%xmm1,%%xmm2;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    %10,%12;\"\n-        \"add    %16,%12;\"\n-        \"movdqa %%xmm1,%%xmm3;\"\n-        \"mov    %k2,%10;\"\n-        \"add    %12,%6;\"\n-        \"mov    %k2,%12;\"\n-        \"pslld  $0x19,%%xmm1;\"\n-        \"or     %8,%10;\"\n-        \"add    %6,%9;\"\n-        \"and    %8,%12;\"\n-        \"psrld  $0x7,%%xmm2;\"\n-        \"and    %7,%10;\"\n-        \"add    %11,%6;\"\n-        \"por    %%xmm2,%%xmm1;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%6;\"\n-        \"movdqa %%xmm3,%%xmm2;\"\n-        \"mov    %9,%10;\"\n-        \"mov    %6,%11;\"\n-        \"movdqa %%xmm3,%%xmm8;\"\n-        \"ror    $0xe,%10;\"\n-        \"xor    %9,%10;\"\n-        \"mov    %3,%12;\"\n-        \"ror    $0x9,%11;\"\n-        \"pslld  $0xe,%%xmm3;\"\n-        \"xor    %6,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"xor    %4,%12;\"\n-        \"psrld  $0x12,%%xmm2;\"\n-        \"ror    $0xb,%11;\"\n-        \"xor    %9,%10;\"\n-        \"and    %9,%12;\"\n-        \"ror    $0x6,%10;\"\n-        \"pxor   %%xmm3,%%xmm1;\"\n-        \"xor    %6,%11;\"\n-        \"xor    %4,%12;\"\n-        \"psrld  $0x3,%%xmm8;\"\n-        \"add    %10,%12;\"\n-        \"add    4+%16,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"pxor   %%xmm2,%%xmm1;\"\n-        \"mov    %6,%10;\"\n-        \"add    %12,%5;\"\n-        \"mov    %6,%12;\"\n-        \"pxor   %%xmm8,%%xmm1;\"\n-        \"or     %7,%10;\"\n-        \"add    %5,%8;\"\n-        \"and    %7,%12;\"\n-        \"pshufd $0xfa,%%xmm6,%%xmm2;\"\n-        \"and    %k2,%10;\"\n-        \"add    %11,%5;\"\n-        \"paddd  %%xmm1,%%xmm0;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%5;\"\n-        \"movdqa %%xmm2,%%xmm3;\"\n-        \"mov    %8,%10;\"\n-        \"mov    %5,%11;\"\n-        \"ror    $0xe,%10;\"\n-        \"movdqa %%xmm2,%%xmm8;\"\n-        \"xor    %8,%10;\"\n-        \"ror    $0x9,%11;\"\n-        \"mov    %9,%12;\"\n-        \"xor    %5,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"psrlq  $0x11,%%xmm2;\"\n-        \"xor    %3,%12;\"\n-        \"psrlq  $0x13,%%xmm3;\"\n-        \"xor    %8,%10;\"\n-        \"and    %8,%12;\"\n-        \"psrld  $0xa,%%xmm8;\"\n-        \"ror    $0xb,%11;\"\n-        \"xor    %5,%11;\"\n-        \"xor    %3,%12;\"\n-        \"ror    $0x6,%10;\"\n-        \"pxor   %%xmm3,%%xmm2;\"\n-        \"add    %10,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    8+%16,%12;\"\n-        \"pxor   %%xmm2,%%xmm8;\"\n-        \"mov    %5,%10;\"\n-        \"add    %12,%4;\"\n-        \"mov    %5,%12;\"\n-        \"pshufb %%xmm10,%%xmm8;\"\n-        \"or     %k2,%10;\"\n-        \"add    %4,%7;\"\n-        \"and    %k2,%12;\"\n-        \"paddd  %%xmm8,%%xmm0;\"\n-        \"and    %6,%10;\"\n-        \"add    %11,%4;\"\n-        \"pshufd $0x50,%%xmm0,%%xmm2;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%4;\"\n-        \"movdqa %%xmm2,%%xmm3;\"\n-        \"mov    %7,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %4,%11;\"\n-        \"movdqa %%xmm2,%%xmm7;\"\n-        \"ror    $0x9,%11;\"\n-        \"xor    %7,%10;\"\n-        \"mov    %8,%12;\"\n-        \"ror    $0x5,%10;\"\n-        \"psrlq  $0x11,%%xmm2;\"\n-        \"xor    %4,%11;\"\n-        \"xor    %9,%12;\"\n-        \"psrlq  $0x13,%%xmm3;\"\n-        \"xor    %7,%10;\"\n-        \"and    %7,%12;\"\n-        \"ror    $0xb,%11;\"\n-        \"psrld  $0xa,%%xmm7;\"\n-        \"xor    %4,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %9,%12;\"\n-        \"pxor   %%xmm3,%%xmm2;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    %10,%12;\"\n-        \"add    12+%16,%12;\"\n-        \"pxor   %%xmm2,%%xmm7;\"\n-        \"mov    %4,%10;\"\n-        \"add    %12,%3;\"\n-        \"mov    %4,%12;\"\n-        \"pshufb %%xmm11,%%xmm7;\"\n-        \"or     %6,%10;\"\n-        \"add    %3,%k2;\"\n-        \"and    %6,%12;\"\n-        \"paddd  %%xmm0,%%xmm7;\"\n-        \"and    %5,%10;\"\n-        \"add    %11,%3;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%3;\"\n-        \"sub    $0x1,%1;\"\n-        \"jne    Lloop1_%=;\"\n-        \"mov    $0x2,%1;\"\n-\n-        \"Lloop2_%=:\"\n-        \"paddd  0x0(%13),%%xmm4;\"\n-        \"movdqa %%xmm4,%16;\"\n-        \"mov    %k2,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %3,%11;\"\n-        \"xor    %k2,%10;\"\n-        \"ror    $0x9,%11;\"\n-        \"mov    %7,%12;\"\n-        \"xor    %3,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"xor    %8,%12;\"\n-        \"xor    %k2,%10;\"\n-        \"ror    $0xb,%11;\"\n-        \"and    %k2,%12;\"\n-        \"xor    %3,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %8,%12;\"\n-        \"add    %10,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    %16,%12;\"\n-        \"mov    %3,%10;\"\n-        \"add    %12,%9;\"\n-        \"mov    %3,%12;\"\n-        \"or     %5,%10;\"\n-        \"add    %9,%6;\"\n-        \"and    %5,%12;\"\n-        \"and    %4,%10;\"\n-        \"add    %11,%9;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%9;\"\n-        \"mov    %6,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %9,%11;\"\n-        \"xor    %6,%10;\"\n-        \"ror    $0x9,%11;\"\n-        \"mov    %k2,%12;\"\n-        \"xor    %9,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"xor    %7,%12;\"\n-        \"xor    %6,%10;\"\n-        \"ror    $0xb,%11;\"\n-        \"and    %6,%12;\"\n-        \"xor    %9,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %7,%12;\"\n-        \"add    %10,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    4+%16,%12;\"\n-        \"mov    %9,%10;\"\n-        \"add    %12,%8;\"\n-        \"mov    %9,%12;\"\n-        \"or     %4,%10;\"\n-        \"add    %8,%5;\"\n-        \"and    %4,%12;\"\n-        \"and    %3,%10;\"\n-        \"add    %11,%8;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%8;\"\n-        \"mov    %5,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %8,%11;\"\n-        \"xor    %5,%10;\"\n-        \"ror    $0x9,%11;\"\n-        \"mov    %6,%12;\"\n-        \"xor    %8,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"xor    %k2,%12;\"\n-        \"xor    %5,%10;\"\n-        \"ror    $0xb,%11;\"\n-        \"and    %5,%12;\"\n-        \"xor    %8,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %k2,%12;\"\n-        \"add    %10,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    8+%16,%12;\"\n-        \"mov    %8,%10;\"\n-        \"add    %12,%7;\"\n-        \"mov    %8,%12;\"\n-        \"or     %3,%10;\"\n-        \"add    %7,%4;\"\n-        \"and    %3,%12;\"\n-        \"and    %9,%10;\"\n-        \"add    %11,%7;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%7;\"\n-        \"mov    %4,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %7,%11;\"\n-        \"xor    %4,%10;\"\n-        \"ror    $0x9,%11;\"\n-        \"mov    %5,%12;\"\n-        \"xor    %7,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"xor    %6,%12;\"\n-        \"xor    %4,%10;\"\n-        \"ror    $0xb,%11;\"\n-        \"and    %4,%12;\"\n-        \"xor    %7,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %6,%12;\"\n-        \"add    %10,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    12+%16,%12;\"\n-        \"mov    %7,%10;\"\n-        \"add    %12,%k2;\"\n-        \"mov    %7,%12;\"\n-        \"or     %9,%10;\"\n-        \"add    %k2,%3;\"\n-        \"and    %9,%12;\"\n-        \"and    %8,%10;\"\n-        \"add    %11,%k2;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%k2;\"\n-        \"paddd  0x10(%13),%%xmm5;\"\n-        \"movdqa %%xmm5,%16;\"\n-        \"add    $0x20,%13;\"\n-        \"mov    %3,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %k2,%11;\"\n-        \"xor    %3,%10;\"\n-        \"ror    $0x9,%11;\"\n-        \"mov    %4,%12;\"\n-        \"xor    %k2,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"xor    %5,%12;\"\n-        \"xor    %3,%10;\"\n-        \"ror    $0xb,%11;\"\n-        \"and    %3,%12;\"\n-        \"xor    %k2,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %5,%12;\"\n-        \"add    %10,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    %16,%12;\"\n-        \"mov    %k2,%10;\"\n-        \"add    %12,%6;\"\n-        \"mov    %k2,%12;\"\n-        \"or     %8,%10;\"\n-        \"add    %6,%9;\"\n-        \"and    %8,%12;\"\n-        \"and    %7,%10;\"\n-        \"add    %11,%6;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%6;\"\n-        \"mov    %9,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %6,%11;\"\n-        \"xor    %9,%10;\"\n-        \"ror    $0x9,%11;\"\n-        \"mov    %3,%12;\"\n-        \"xor    %6,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"xor    %4,%12;\"\n-        \"xor    %9,%10;\"\n-        \"ror    $0xb,%11;\"\n-        \"and    %9,%12;\"\n-        \"xor    %6,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %4,%12;\"\n-        \"add    %10,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    4+%16,%12;\"\n-        \"mov    %6,%10;\"\n-        \"add    %12,%5;\"\n-        \"mov    %6,%12;\"\n-        \"or     %7,%10;\"\n-        \"add    %5,%8;\"\n-        \"and    %7,%12;\"\n-        \"and    %k2,%10;\"\n-        \"add    %11,%5;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%5;\"\n-        \"mov    %8,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %5,%11;\"\n-        \"xor    %8,%10;\"\n-        \"ror    $0x9,%11;\"\n-        \"mov    %9,%12;\"\n-        \"xor    %5,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"xor    %3,%12;\"\n-        \"xor    %8,%10;\"\n-        \"ror    $0xb,%11;\"\n-        \"and    %8,%12;\"\n-        \"xor    %5,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %3,%12;\"\n-        \"add    %10,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    8+%16,%12;\"\n-        \"mov    %5,%10;\"\n-        \"add    %12,%4;\"\n-        \"mov    %5,%12;\"\n-        \"or     %k2,%10;\"\n-        \"add    %4,%7;\"\n-        \"and    %k2,%12;\"\n-        \"and    %6,%10;\"\n-        \"add    %11,%4;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%4;\"\n-        \"mov    %7,%10;\"\n-        \"ror    $0xe,%10;\"\n-        \"mov    %4,%11;\"\n-        \"xor    %7,%10;\"\n-        \"ror    $0x9,%11;\"\n-        \"mov    %8,%12;\"\n-        \"xor    %4,%11;\"\n-        \"ror    $0x5,%10;\"\n-        \"xor    %9,%12;\"\n-        \"xor    %7,%10;\"\n-        \"ror    $0xb,%11;\"\n-        \"and    %7,%12;\"\n-        \"xor    %4,%11;\"\n-        \"ror    $0x6,%10;\"\n-        \"xor    %9,%12;\"\n-        \"add    %10,%12;\"\n-        \"ror    $0x2,%11;\"\n-        \"add    12+%16,%12;\"\n-        \"mov    %4,%10;\"\n-        \"add    %12,%3;\"\n-        \"mov    %4,%12;\"\n-        \"or     %6,%10;\"\n-        \"add    %3,%k2;\"\n-        \"and    %6,%12;\"\n-        \"and    %5,%10;\"\n-        \"add    %11,%3;\"\n-        \"or     %12,%10;\"\n-        \"add    %10,%3;\"\n-        \"movdqa %%xmm6,%%xmm4;\"\n-        \"movdqa %%xmm7,%%xmm5;\"\n-        \"sub    $0x1,%1;\"\n-        \"jne    Lloop2_%=;\"\n-        \"add    (%0),%3;\"\n-        \"mov    %3,(%0);\"\n-        \"add    0x4(%0),%4;\"\n-        \"mov    %4,0x4(%0);\"\n-        \"add    0x8(%0),%5;\"\n-        \"mov    %5,0x8(%0);\"\n-        \"add    0xc(%0),%6;\"\n-        \"mov    %6,0xc(%0);\"\n-        \"add    0x10(%0),%k2;\"\n-        \"mov    %k2,0x10(%0);\"\n-        \"add    0x14(%0),%7;\"\n-        \"mov    %7,0x14(%0);\"\n-        \"add    0x18(%0),%8;\"\n-        \"mov    %8,0x18(%0);\"\n-        \"add    0x1c(%0),%9;\"\n-        \"mov    %9,0x1c(%0);\"\n-        \"mov    %15,%1;\"\n-        \"add    $0x40,%1;\"\n-        \"cmp    %14,%1;\"\n-        \"jne    Lloop0_%=;\"\n-\n-        \"Ldone_hash_%=:\"\n-\n-        : \"+r\"(s), \"+r\"(chunk), \"+r\"(blocks), \"=r\"(a), \"=r\"(b), \"=r\"(c), \"=r\"(d), /* e = chunk */ \"=r\"(f), \"=r\"(g), \"=r\"(h), \"=r\"(y0), \"=r\"(y1), \"=r\"(y2), \"=r\"(tbl), \"+m\"(inp_end), \"+m\"(inp), \"+m\"(xfer)\n-        : \"m\"(K256), \"m\"(FLIP_MASK), \"m\"(SHUF_00BA), \"m\"(SHUF_DC00)\n-        : \"cc\", \"memory\", \"xmm0\", \"xmm1\", \"xmm2\", \"xmm3\", \"xmm4\", \"xmm5\", \"xmm6\", \"xmm7\", \"xmm8\", \"xmm9\", \"xmm10\", \"xmm11\", \"xmm12\"\n-   );\n-}\n-}\n-\n-/*\n-;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n-; Copyright (c) 2012, Intel Corporation \n-; \n-; All rights reserved. \n-; \n-; Redistribution and use in source and binary forms, with or without\n-; modification, are permitted provided that the following conditions are\n-; met: \n-; \n-; * Redistributions of source code must retain the above copyright\n-;   notice, this list of conditions and the following disclaimer.  \n-; \n-; * Redistributions in binary form must reproduce the above copyright\n-;   notice, this list of conditions and the following disclaimer in the\n-;   documentation and/or other materials provided with the\n-;   distribution. \n-; \n-; * Neither the name of the Intel Corporation nor the names of its\n-;   contributors may be used to endorse or promote products derived from\n-;   this software without specific prior written permission. \n-; \n-; \n-; THIS SOFTWARE IS PROVIDED BY INTEL CORPORATION \"AS IS\" AND ANY\n-; EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n-; IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n-; PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL INTEL CORPORATION OR\n-; CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n-; EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n-; PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n-; PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n-; LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n-; NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n-; SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n-;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n-;\n-; Example YASM command lines:\n-; Windows:  yasm -Xvc -f x64 -rnasm -pnasm -o sha256_sse4.obj -g cv8 sha256_sse4.asm\n-; Linux:    yasm -f x64 -f elf64 -X gnu -g dwarf2 -D LINUX -o sha256_sse4.o sha256_sse4.asm\n-;\n-;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n-;\n-; This code is described in an Intel White-Paper:\n-; \"Fast SHA-256 Implementations on Intel Architecture Processors\"\n-;\n-; To find it, surf to http://www.intel.com/p/en_US/embedded \n-; and search for that title.\n-; The paper is expected to be released roughly at the end of April, 2012\n-;\n-;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n-; This code schedules 1 blocks at a time, with 4 lanes per block\n-;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n-\n-%define\tMOVDQ movdqu ;; assume buffers not aligned \n-\n-;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; Define Macros\n-\n-; addm [mem], reg\n-; Add reg to mem using reg-mem add and store\n-%macro addm 2\n-    add\t%2, %1\n-    mov\t%1, %2\n-%endm\n-\n-;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n-\n-; COPY_XMM_AND_BSWAP xmm, [mem], byte_flip_mask\n-; Load xmm with mem and byte swap each dword\n-%macro COPY_XMM_AND_BSWAP 3\n-    MOVDQ %1, %2\n-    pshufb %1, %3\n-%endmacro\n-\n-;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n-\n-%define X0 xmm4\n-%define X1 xmm5\n-%define X2 xmm6\n-%define X3 xmm7\n-\n-%define XTMP0 xmm0\n-%define XTMP1 xmm1\n-%define XTMP2 xmm2\n-%define XTMP3 xmm3\n-%define XTMP4 xmm8\n-%define XFER  xmm9\n-\n-%define SHUF_00BA\txmm10 ; shuffle xBxA -> 00BA\n-%define SHUF_DC00\txmm11 ; shuffle xDxC -> DC00\n-%define BYTE_FLIP_MASK\txmm12\n-    \n-%ifdef LINUX\n-%define NUM_BLKS rdx\t; 3rd arg\n-%define CTX\trsi\t; 2nd arg\n-%define INP\trdi\t; 1st arg\n-\n-%define SRND\trdi\t; clobbers INP\n-%define c\tecx\n-%define d \tr8d\n-%define e \tedx\n-%else\n-%define NUM_BLKS r8\t; 3rd arg\n-%define CTX\trdx \t; 2nd arg\n-%define INP\trcx \t; 1st arg\n-\n-%define SRND\trcx\t; clobbers INP\n-%define c \tedi \n-%define d\tesi \n-%define e \tr8d\n-    \n-%endif\n-%define TBL\trbp\n-%define a eax\n-%define b ebx\n-\n-%define f r9d\n-%define g r10d\n-%define h r11d\n-\n-%define y0 r13d\n-%define y1 r14d\n-%define y2 r15d\n-\n-\n-\n-_INP_END_SIZE\tequ 8\n-_INP_SIZE\tequ 8\n-_XFER_SIZE\tequ 8\n-%ifdef LINUX\n-_XMM_SAVE_SIZE\tequ 0\n-%else\n-_XMM_SAVE_SIZE\tequ 7*16\n-%endif\n-; STACK_SIZE plus pushes must be an odd multiple of 8\n-_ALIGN_SIZE\tequ 8\n-\n-_INP_END\tequ 0\n-_INP\t\tequ _INP_END  + _INP_END_SIZE\n-_XFER\t\tequ _INP      + _INP_SIZE\n-_XMM_SAVE\tequ _XFER     + _XFER_SIZE + _ALIGN_SIZE\n-STACK_SIZE\tequ _XMM_SAVE + _XMM_SAVE_SIZE\n-\n-; rotate_Xs\n-; Rotate values of symbols X0...X3\n-%macro rotate_Xs 0\n-%xdefine X_ X0\n-%xdefine X0 X1\n-%xdefine X1 X2\n-%xdefine X2 X3\n-%xdefine X3 X_\n-%endm\n-\n-; ROTATE_ARGS\n-; Rotate values of symbols a...h\n-%macro ROTATE_ARGS 0\n-%xdefine TMP_ h\n-%xdefine h g\n-%xdefine g f\n-%xdefine f e\n-%xdefine e d\n-%xdefine d c\n-%xdefine c b\n-%xdefine b a\n-%xdefine a TMP_\n-%endm\n-\n-%macro FOUR_ROUNDS_AND_SCHED 0\n-\t;; compute s0 four at a time and s1 two at a time\n-\t;; compute W[-16] + W[-7] 4 at a time\n-\tmovdqa\tXTMP0, X3\n-    mov\ty0, e\t\t; y0 = e\n-    ror\ty0, (25-11)\t; y0 = e >> (25-11)\n-    mov\ty1, a\t\t; y1 = a\n-\tpalignr\tXTMP0, X2, 4\t; XTMP0 = W[-7]\n-    ror\ty1, (22-13)\t; y1 = a >> (22-13)\n-    xor\ty0, e\t\t; y0 = e ^ (e >> (25-11))\n-    mov\ty2, f\t\t; y2 = f\n-    ror\ty0, (11-6)\t; y0 = (e >> (11-6)) ^ (e >> (25-6))\n-\tmovdqa\tXTMP1, X1\n-    xor\ty1, a\t\t; y1 = a ^ (a >> (22-13)\n-    xor\ty2, g\t\t; y2 = f^g\n-\tpaddd\tXTMP0, X0\t; XTMP0 = W[-7] + W[-16]\n-    xor\ty0, e\t\t; y0 = e ^ (e >> (11-6)) ^ (e >> (25-6))\n-    and\ty2, e\t\t; y2 = (f^g)&e\n-    ror\ty1, (13-2)\t; y1 = (a >> (13-2)) ^ (a >> (22-2))\n-\t;; compute s0\n-\tpalignr\tXTMP1, X0, 4\t; XTMP1 = W[-15]\n-    xor\ty1, a\t\t; y1 = a ^ (a >> (13-2)) ^ (a >> (22-2))\n-    ror\ty0, 6\t\t; y0 = S1 = (e>>6) & (e>>11) ^ (e>>25)\n-    xor\ty2, g\t\t; y2 = CH = ((f^g)&e)^g\n-\tmovdqa\tXTMP2, XTMP1\t; XTMP2 = W[-15]\n-    ror\ty1, 2\t\t; y1 = S0 = (a>>2) ^ (a>>13) ^ (a>>22)\n-    add\ty2, y0\t\t; y2 = S1 + CH\n-    add\ty2, [rsp + _XFER + 0*4]\t; y2 = k + w + S1 + CH\n-\tmovdqa\tXTMP3, XTMP1\t; XTMP3 = W[-15]\n-    mov\ty0, a\t\t; y0 = a\n-    add\th, y2\t\t; h = h + S1 + CH + k + w\n-    mov\ty2, a\t\t; y2 = a\n-\tpslld\tXTMP1, (32-7)\n-    or\ty0, c\t\t; y0 = a|c\n-    add\td, h\t\t; d = d + h + S1 + CH + k + w\n-    and\ty2, c\t\t; y2 = a&c\n-\tpsrld\tXTMP2, 7\n-    and\ty0, b\t\t; y0 = (a|c)&b\n-    add\th, y1\t\t; h = h + S1 + CH + k + w + S0\n-\tpor\tXTMP1, XTMP2\t; XTMP1 = W[-15] ror 7\n-    or\ty0, y2\t\t; y0 = MAJ = (a|c)&b)|(a&c)\n-    add\th, y0\t\t; h = h + S1 + CH + k + w + S0 + MAJ\n-\n-ROTATE_ARGS\n-\tmovdqa\tXTMP2, XTMP3\t; XTMP2 = W[-15]\n-    mov\ty0, e\t\t; y0 = e\n-    mov\ty1, a\t\t; y1 = a\n-\tmovdqa\tXTMP4, XTMP3\t; XTMP4 = W[-15]\n-    ror\ty0, (25-11)\t; y0 = e >> (25-11)\n-    xor\ty0, e\t\t; y0 = e ^ (e >> (25-11))\n-    mov\ty2, f\t\t; y2 = f\n-    ror\ty1, (22-13)\t; y1 = a >> (22-13)\n-\tpslld\tXTMP3, (32-18)\n-    xor\ty1, a\t\t; y1 = a ^ (a >> (22-13)\n-    ror\ty0, (11-6)\t; y0 = (e >> (11-6)) ^ (e >> (25-6))\n-    xor\ty2, g\t\t; y2 = f^g\n-\tpsrld\tXTMP2, 18\n-    ror\ty1, (13-2)\t; y1 = (a >> (13-2)) ^ (a >> (22-2))\n-    xor\ty0, e\t\t; y0 = e ^ (e >> (11-6)) ^ (e >> (25-6))\n-    and\ty2, e\t\t; y2 = (f^g)&e\n-    ror\ty0, 6\t\t; y0 = S1 = (e>>6) & (e>>11) ^ (e>>25)\n-\tpxor\tXTMP1, XTMP3\n-    xor\ty1, a\t\t; y1 = a ^ (a >> (13-2)) ^ (a >> (22-2))\n-    xor\ty2, g\t\t; y2 = CH = ((f^g)&e)^g\n-\tpsrld\tXTMP4, 3\t; XTMP4 = W[-15] >> 3\n-    add\ty2, y0\t\t; y2 = S1 + CH\n-    add\ty2, [rsp + _XFER + 1*4]\t; y2 = k + w + S1 + CH\n-    ror\ty1, 2\t\t; y1 = S0 = (a>>2) ^ (a>>13) ^ (a>>22)\n-\tpxor\tXTMP1, XTMP2\t; XTMP1 = W[-15] ror 7 ^ W[-15] ror 18\n-    mov\ty0, a\t\t; y0 = a\n-    add\th, y2\t\t; h = h + S1 + CH + k + w\n-    mov\ty2, a\t\t; y2 = a\n-\tpxor\tXTMP1, XTMP4\t; XTMP1 = s0\n-    or\ty0, c\t\t; y0 = a|c\n-    add\td, h\t\t; d = d + h + S1 + CH + k + w\n-    and\ty2, c\t\t; y2 = a&c\n-\t;; compute low s1\n-\tpshufd\tXTMP2, X3, 11111010b\t; XTMP2 = W[-2] {BBAA}\n-    and\ty0, b\t\t; y0 = (a|c)&b\n-    add\th, y1\t\t; h = h + S1 + CH + k + w + S0\n-\tpaddd\tXTMP0, XTMP1\t; XTMP0 = W[-16] + W[-7] + s0\n-    or\ty0, y2\t\t; y0 = MAJ = (a|c)&b)|(a&c)\n-    add\th, y0\t\t; h = h + S1 + CH + k + w + S0 + MAJ\n-\n-ROTATE_ARGS\n-\tmovdqa\tXTMP3, XTMP2\t; XTMP3 = W[-2] {BBAA}\n-    mov\ty0, e\t\t; y0 = e\n-    mov\ty1, a\t\t; y1 = a\n-    ror\ty0, (25-11)\t; y0 = e >> (25-11)\n-\tmovdqa\tXTMP4, XTMP2\t; XTMP4 = W[-2] {BBAA}\n-    xor\ty0, e\t\t; y0 = e ^ (e >> (25-11))\n-    ror\ty1, (22-13)\t; y1 = a >> (22-13)\n-    mov\ty2, f\t\t; y2 = f\n-    xor\ty1, a\t\t; y1 = a ^ (a >> (22-13)\n-    ror\ty0, (11-6)\t; y0 = (e >> (11-6)) ^ (e >> (25-6))\n-\tpsrlq\tXTMP2, 17\t; XTMP2 = W[-2] ror 17 {xBxA}\n-    xor\ty2, g\t\t; y2 = f^g\n-\tpsrlq\tXTMP3, 19\t; XTMP3 = W[-2] ror 19 {xBxA}\n-    xor\ty0, e\t\t; y0 = e ^ (e >> (11-6)) ^ (e >> (25-6))\n-    and\ty2, e\t\t; y2 = (f^g)&e\n-\tpsrld\tXTMP4, 10\t; XTMP4 = W[-2] >> 10 {BBAA}\n-    ror\ty1, (13-2)\t; y1 = (a >> (13-2)) ^ (a >> (22-2))\n-    xor\ty1, a\t\t; y1 = a ^ (a >> (13-2)) ^ (a >> (22-2))\n-    xor\ty2, g\t\t; y2 = CH = ((f^g)&e)^g\n-    ror\ty0, 6\t\t; y0 = S1 = (e>>6) & (e>>11) ^ (e>>25)\n-\tpxor\tXTMP2, XTMP3\n-    add\ty2, y0\t\t; y2 = S1 + CH\n-    ror\ty1, 2\t\t; y1 = S0 = (a>>2) ^ (a>>13) ^ (a>>22)\n-    add\ty2, [rsp + _XFER + 2*4]\t; y2 = k + w + S1 + CH\n-\tpxor\tXTMP4, XTMP2\t; XTMP4 = s1 {xBxA}\n-    mov\ty0, a\t\t; y0 = a\n-    add\th, y2\t\t; h = h + S1 + CH + k + w\n-    mov\ty2, a\t\t; y2 = a\n-\tpshufb\tXTMP4, SHUF_00BA\t; XTMP4 = s1 {00BA}\n-    or\ty0, c\t\t; y0 = a|c\n-    add\td, h\t\t; d = d + h + S1 + CH + k + w\n-    and\ty2, c\t\t; y2 = a&c\n-\tpaddd\tXTMP0, XTMP4\t; XTMP0 = {..., ..., W[1], W[0]}\n-    and\ty0, b\t\t; y0 = (a|c)&b\n-    add\th, y1\t\t; h = h + S1 + CH + k + w + S0\n-\t;; compute high s1\n-\tpshufd\tXTMP2, XTMP0, 01010000b\t; XTMP2 = W[-2] {DDCC}\n-    or\ty0, y2\t\t; y0 = MAJ = (a|c)&b)|(a&c)\n-    add\th, y0\t\t; h = h + S1 + CH + k + w + S0 + MAJ\n-\n-ROTATE_ARGS\n-\tmovdqa\tXTMP3, XTMP2\t; XTMP3 = W[-2] {DDCC}\n-    mov\ty0, e\t\t; y0 = e\n-    ror\ty0, (25-11)\t; y0 = e >> (25-11)\n-    mov\ty1, a\t\t; y1 = a\n-\tmovdqa\tX0,    XTMP2\t; X0    = W[-2] {DDCC}\n-    ror\ty1, (22-13)\t; y1 = a >> (22-13)\n-    xor\ty0, e\t\t; y0 = e ^ (e >> (25-11))\n-    mov\ty2, f\t\t; y2 = f\n-    ror\ty0, (11-6)\t; y0 = (e >> (11-6)) ^ (e >> (25-6))\n-\tpsrlq\tXTMP2, 17\t; XTMP2 = W[-2] ror 17 {xDxC}\n-    xor\ty1, a\t\t; y1 = a ^ (a >> (22-13)\n-    xor\ty2, g\t\t; y2 = f^g\n-\tpsrlq\tXTMP3, 19\t; XTMP3 = W[-2] ror 19 {xDxC}\n-    xor\ty0, e\t\t; y0 = e ^ (e >> (11-6)) ^ (e >> (25-6))\n-    and\ty2, e\t\t; y2 = (f^g)&e\n-    ror\ty1, (13-2)\t; y1 = (a >> (13-2)) ^ (a >> (22-2))\n-\tpsrld\tX0,    10\t; X0 = W[-2] >> 10 {DDCC}\n-    xor\ty1, a\t\t; y1 = a ^ (a >> (13-2)) ^ (a >> (22-2))\n-    ror\ty0, 6\t\t; y0 = S1 = (e>>6) & (e>>11) ^ (e>>25)\n-    xor\ty2, g\t\t; y2 = CH = ((f^g)&e)^g\n-\tpxor\tXTMP2, XTMP3\n-    ror\ty1, 2\t\t; y1 = S0 = (a>>2) ^ (a>>13) ^ (a>>22)\n-    add\ty2, y0\t\t; y2 = S1 + CH\n-    add\ty2, [rsp + _XFER + 3*4]\t; y2 = k + w + S1 + CH\n-\tpxor\tX0, XTMP2\t; X0 = s1 {xDxC}\n-    mov\ty0, a\t\t; y0 = a\n-    add\th, y2\t\t; h = h + S1 + CH + k + w\n-    mov\ty2, a\t\t; y2 = a\n-\tpshufb\tX0, SHUF_DC00\t; X0 = s1 {DC00}\n-    or\ty0, c\t\t; y0 = a|c\n-    add\td, h\t\t; d = d + h + S1 + CH + k + w\n-    and\ty2, c\t\t; y2 = a&c\n-\tpaddd\tX0, XTMP0\t; X0 = {W[3], W[2], W[1], W[0]}\n-    and\ty0, b\t\t; y0 = (a|c)&b\n-    add\th, y1\t\t; h = h + S1 + CH + k + w + S0\n-    or\ty0, y2\t\t; y0 = MAJ = (a|c)&b)|(a&c)\n-    add\th, y0\t\t; h = h + S1 + CH + k + w + S0 + MAJ\n-\n-ROTATE_ARGS\n-rotate_Xs\n-%endm\n-\n-;; input is [rsp + _XFER + %1 * 4]\n-%macro DO_ROUND 1\n-    mov\ty0, e\t\t; y0 = e\n-    ror\ty0, (25-11)\t; y0 = e >> (25-11)\n-    mov\ty1, a\t\t; y1 = a\n-    xor\ty0, e\t\t; y0 = e ^ (e >> (25-11))\n-    ror\ty1, (22-13)\t; y1 = a >> (22-13)\n-    mov\ty2, f\t\t; y2 = f\n-    xor\ty1, a\t\t; y1 = a ^ (a >> (22-13)\n-    ror\ty0, (11-6)\t; y0 = (e >> (11-6)) ^ (e >> (25-6))\n-    xor\ty2, g\t\t; y2 = f^g\n-    xor\ty0, e\t\t; y0 = e ^ (e >> (11-6)) ^ (e >> (25-6))\n-    ror\ty1, (13-2)\t; y1 = (a >> (13-2)) ^ (a >> (22-2))\n-    and\ty2, e\t\t; y2 = (f^g)&e\n-    xor\ty1, a\t\t; y1 = a ^ (a >> (13-2)) ^ (a >> (22-2))\n-    ror\ty0, 6\t\t; y0 = S1 = (e>>6) & (e>>11) ^ (e>>25)\n-    xor\ty2, g\t\t; y2 = CH = ((f^g)&e)^g\n-    add\ty2, y0\t\t; y2 = S1 + CH\n-    ror\ty1, 2\t\t; y1 = S0 = (a>>2) ^ (a>>13) ^ (a>>22)\n-    add\ty2, [rsp + _XFER + %1 * 4]\t; y2 = k + w + S1 + CH\n-    mov\ty0, a\t\t; y0 = a\n-    add\th, y2\t\t; h = h + S1 + CH + k + w\n-    mov\ty2, a\t\t; y2 = a\n-    or\ty0, c\t\t; y0 = a|c\n-    add\td, h\t\t; d = d + h + S1 + CH + k + w\n-    and\ty2, c\t\t; y2 = a&c\n-    and\ty0, b\t\t; y0 = (a|c)&b\n-    add\th, y1\t\t; h = h + S1 + CH + k + w + S0\n-    or\ty0, y2\t\t; y0 = MAJ = (a|c)&b)|(a&c)\n-    add\th, y0\t\t; h = h + S1 + CH + k + w + S0 + MAJ\n-    ROTATE_ARGS\n-%endm\n-\n-;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n-;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n-;; void sha256_sse4(void *input_data, UINT32 digest[8], UINT64 num_blks)\n-;; arg 1 : pointer to input data\n-;; arg 2 : pointer to digest\n-;; arg 3 : Num blocks\n-section .text\n-global sha256_sse4\n-align 32\n-sha256_sse4:\n-    push\trbx\n-%ifndef LINUX\n-    push\trsi\n-    push\trdi\n-%endif\n-    push\trbp\n-    push\tr13\n-    push\tr14\n-    push\tr15\n-\n-    sub\trsp,STACK_SIZE\n-%ifndef LINUX\n-    movdqa\t[rsp + _XMM_SAVE + 0*16],xmm6\t\n-    movdqa\t[rsp + _XMM_SAVE + 1*16],xmm7\n-    movdqa\t[rsp + _XMM_SAVE + 2*16],xmm8\t\n-    movdqa\t[rsp + _XMM_SAVE + 3*16],xmm9\t\n-    movdqa\t[rsp + _XMM_SAVE + 4*16],xmm10\n-    movdqa\t[rsp + _XMM_SAVE + 5*16],xmm11\n-    movdqa\t[rsp + _XMM_SAVE + 6*16],xmm12\n-%endif\n-\n-    shl\tNUM_BLKS, 6\t; convert to bytes\n-    jz\tdone_hash\n-    add\tNUM_BLKS, INP\t; pointer to end of data\n-    mov\t[rsp + _INP_END], NUM_BLKS\n-\n-    ;; load initial digest\n-    mov\ta,[4*0 + CTX]\n-    mov\tb,[4*1 + CTX]\n-    mov\tc,[4*2 + CTX]\n-    mov\td,[4*3 + CTX]\n-    mov\te,[4*4 + CTX]\n-    mov\tf,[4*5 + CTX]\n-    mov\tg,[4*6 + CTX]\n-    mov\th,[4*7 + CTX]\n-\n-    movdqa\tBYTE_FLIP_MASK, [PSHUFFLE_BYTE_FLIP_MASK wrt rip]\n-    movdqa\tSHUF_00BA, [_SHUF_00BA wrt rip]\n-    movdqa\tSHUF_DC00, [_SHUF_DC00 wrt rip]\n-\n-loop0:\n-    lea\tTBL,[K256 wrt rip]\n-\n-    ;; byte swap first 16 dwords\n-    COPY_XMM_AND_BSWAP\tX0, [INP + 0*16], BYTE_FLIP_MASK\n-    COPY_XMM_AND_BSWAP\tX1, [INP + 1*16], BYTE_FLIP_MASK\n-    COPY_XMM_AND_BSWAP\tX2, [INP + 2*16], BYTE_FLIP_MASK\n-    COPY_XMM_AND_BSWAP\tX3, [INP + 3*16], BYTE_FLIP_MASK\n-    \n-    mov\t[rsp + _INP], INP\n-\n-    ;; schedule 48 input dwords, by doing 3 rounds of 16 each\n-    mov\tSRND, 3\n-align 16\n-loop1:\n-    movdqa\tXFER, [TBL + 0*16]\n-    paddd\tXFER, X0\n-    movdqa\t[rsp + _XFER], XFER\n-    FOUR_ROUNDS_AND_SCHED\n-\n-    movdqa\tXFER, [TBL + 1*16]\n-    paddd\tXFER, X0\n-    movdqa\t[rsp + _XFER], XFER\n-    FOUR_ROUNDS_AND_SCHED\n-\n-    movdqa\tXFER, [TBL + 2*16]\n-    paddd\tXFER, X0\n-    movdqa\t[rsp + _XFER], XFER\n-    FOUR_ROUNDS_AND_SCHED\n-\n-    movdqa\tXFER, [TBL + 3*16]\n-    paddd\tXFER, X0\n-    movdqa\t[rsp + _XFER], XFER\n-    add\tTBL, 4*16\n-    FOUR_ROUNDS_AND_SCHED\n-\n-    sub\tSRND, 1\n-    jne\tloop1\n-\n-    mov\tSRND, 2\n-loop2:\n-    paddd\tX0, [TBL + 0*16]\n-    movdqa\t[rsp + _XFER], X0\n-    DO_ROUND\t0\n-    DO_ROUND\t1\n-    DO_ROUND\t2\n-    DO_ROUND\t3\n-    paddd\tX1, [TBL + 1*16]\n-    movdqa\t[rsp + _XFER], X1\n-    add\tTBL, 2*16\n-    DO_ROUND\t0\n-    DO_ROUND\t1\n-    DO_ROUND\t2\n-    DO_ROUND\t3\n-\n-    movdqa\tX0, X2\n-    movdqa\tX1, X3\n-\n-    sub\tSRND, 1\n-    jne\tloop2\n-\n-    addm\t[4*0 + CTX],a\n-    addm\t[4*1 + CTX],b\n-    addm\t[4*2 + CTX],c\n-    addm\t[4*3 + CTX],d\n-    addm\t[4*4 + CTX],e\n-    addm\t[4*5 + CTX],f\n-    addm\t[4*6 + CTX],g\n-    addm\t[4*7 + CTX],h\n-\n-    mov\tINP, [rsp + _INP]\n-    add\tINP, 64\n-    cmp\tINP, [rsp + _INP_END]\n-    jne\tloop0\n-\n-done_hash:\n-%ifndef LINUX\n-    movdqa\txmm6,[rsp + _XMM_SAVE + 0*16]\n-    movdqa\txmm7,[rsp + _XMM_SAVE + 1*16]\n-    movdqa\txmm8,[rsp + _XMM_SAVE + 2*16]\n-    movdqa\txmm9,[rsp + _XMM_SAVE + 3*16]\n-    movdqa\txmm10,[rsp + _XMM_SAVE + 4*16]\n-    movdqa\txmm11,[rsp + _XMM_SAVE + 5*16]\n-    movdqa\txmm12,[rsp + _XMM_SAVE + 6*16]\n-%endif\n-\n-    add\trsp, STACK_SIZE\n-\n-    pop\tr15\n-    pop\tr14\n-    pop\tr13\n-    pop\trbp\n-%ifndef LINUX\n-    pop\trdi\n-    pop\trsi\n-%endif\n-    pop\trbx\n-\n-    ret\t\n-    \n-\n-section .data\n-align 64\n-K256:\n-    dd\t0x428a2f98,0x71374491,0xb5c0fbcf,0xe9b5dba5\n-    dd\t0x3956c25b,0x59f111f1,0x923f82a4,0xab1c5ed5\n-    dd\t0xd807aa98,0x12835b01,0x243185be,0x550c7dc3\n-    dd\t0x72be5d74,0x80deb1fe,0x9bdc06a7,0xc19bf174\n-    dd\t0xe49b69c1,0xefbe4786,0x0fc19dc6,0x240ca1cc\n-    dd\t0x2de92c6f,0x4a7484aa,0x5cb0a9dc,0x76f988da\n-    dd\t0x983e5152,0xa831c66d,0xb00327c8,0xbf597fc7\n-    dd\t0xc6e00bf3,0xd5a79147,0x06ca6351,0x14292967\n-    dd\t0x27b70a85,0x2e1b2138,0x4d2c6dfc,0x53380d13\n-    dd\t0x650a7354,0x766a0abb,0x81c2c92e,0x92722c85\n-    dd\t0xa2bfe8a1,0xa81a664b,0xc24b8b70,0xc76c51a3\n-    dd\t0xd192e819,0xd6990624,0xf40e3585,0x106aa070\n-    dd\t0x19a4c116,0x1e376c08,0x2748774c,0x34b0bcb5\n-    dd\t0x391c0cb3,0x4ed8aa4a,0x5b9cca4f,0x682e6ff3\n-    dd\t0x748f82ee,0x78a5636f,0x84c87814,0x8cc70208\n-    dd\t0x90befffa,0xa4506ceb,0xbef9a3f7,0xc67178f2\n-\n-PSHUFFLE_BYTE_FLIP_MASK: ddq 0x0c0d0e0f08090a0b0405060700010203\n-\n-; shuffle xBxA -> 00BA\n-_SHUF_00BA:              ddq 0xFFFFFFFFFFFFFFFF0b0a090803020100\n-\n-; shuffle xDxC -> DC00\n-_SHUF_DC00:              ddq 0x0b0a090803020100FFFFFFFFFFFFFFFF\n-*/\n-\n-#endif"
      }
    ]
  }
]