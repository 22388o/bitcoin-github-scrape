[
  {
    "sha": "8510c7e289a754c0a4cfc6b7b690815fa313ab89",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo4NTEwYzdlMjg5YTc1NGMwYTRjZmM2YjdiNjkwODE1ZmEzMTNhYjg5",
    "commit": {
      "author": {
        "name": "MarcoFalke",
        "email": "falke.marco@gmail.com",
        "date": "2018-03-29T15:41:45Z"
      },
      "committer": {
        "name": "MarcoFalke",
        "email": "falke.marco@gmail.com",
        "date": "2018-04-10T15:51:57Z"
      },
      "message": "test: Add parallel.py from gtest-parallel\n\nAs of commit 9ae552468cf096cb281d1ab7c87d9baea56e86c9\n\nhttps://github.com/google/gtest-parallel/commit/9ae552468cf096cb281d1ab7c87d9baea56e86c9",
      "tree": {
        "sha": "bbb117d11768cf1a0264ccdc71a91bf97dc7e41d",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/bbb117d11768cf1a0264ccdc71a91bf97dc7e41d"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/8510c7e289a754c0a4cfc6b7b690815fa313ab89",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/8510c7e289a754c0a4cfc6b7b690815fa313ab89",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/8510c7e289a754c0a4cfc6b7b690815fa313ab89",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/8510c7e289a754c0a4cfc6b7b690815fa313ab89/comments",
    "author": {
      "login": "MarcoFalke",
      "id": 6399679,
      "node_id": "MDQ6VXNlcjYzOTk2Nzk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/6399679?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/MarcoFalke",
      "html_url": "https://github.com/MarcoFalke",
      "followers_url": "https://api.github.com/users/MarcoFalke/followers",
      "following_url": "https://api.github.com/users/MarcoFalke/following{/other_user}",
      "gists_url": "https://api.github.com/users/MarcoFalke/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/MarcoFalke/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/MarcoFalke/subscriptions",
      "organizations_url": "https://api.github.com/users/MarcoFalke/orgs",
      "repos_url": "https://api.github.com/users/MarcoFalke/repos",
      "events_url": "https://api.github.com/users/MarcoFalke/events{/privacy}",
      "received_events_url": "https://api.github.com/users/MarcoFalke/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "MarcoFalke",
      "id": 6399679,
      "node_id": "MDQ6VXNlcjYzOTk2Nzk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/6399679?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/MarcoFalke",
      "html_url": "https://github.com/MarcoFalke",
      "followers_url": "https://api.github.com/users/MarcoFalke/followers",
      "following_url": "https://api.github.com/users/MarcoFalke/following{/other_user}",
      "gists_url": "https://api.github.com/users/MarcoFalke/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/MarcoFalke/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/MarcoFalke/subscriptions",
      "organizations_url": "https://api.github.com/users/MarcoFalke/orgs",
      "repos_url": "https://api.github.com/users/MarcoFalke/repos",
      "events_url": "https://api.github.com/users/MarcoFalke/events{/privacy}",
      "received_events_url": "https://api.github.com/users/MarcoFalke/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "8d651ae32013440b2af1267e87a9d93759a9471f",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/8d651ae32013440b2af1267e87a9d93759a9471f",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/8d651ae32013440b2af1267e87a9d93759a9471f"
      }
    ],
    "stats": {
      "total": 822,
      "additions": 822,
      "deletions": 0
    },
    "files": [
      {
        "sha": "bb4b187d197ae50d5a53fa99e7c678604d83b633",
        "filename": "src/test/parallel.py",
        "status": "added",
        "additions": 822,
        "deletions": 0,
        "changes": 822,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/8510c7e289a754c0a4cfc6b7b690815fa313ab89/src/test/parallel.py",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/8510c7e289a754c0a4cfc6b7b690815fa313ab89/src/test/parallel.py",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/test/parallel.py?ref=8510c7e289a754c0a4cfc6b7b690815fa313ab89",
        "patch": "@@ -0,0 +1,822 @@\n+# Copyright 2013 Google Inc. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+try:\n+    import _pickle as cPickle\n+except ImportError:\n+    import cPickle\n+import errno\n+from functools import total_ordering\n+import gzip\n+import json\n+import multiprocessing\n+import optparse\n+import os\n+from pickle import HIGHEST_PROTOCOL as PICKLE_HIGHEST_PROTOCOL\n+import re\n+import shutil\n+import signal\n+import subprocess\n+import sys\n+import tempfile\n+try:\n+    import _thread as thread\n+except ImportError:\n+    import thread\n+import threading\n+import time\n+\n+if sys.version_info.major >= 3:\n+    long = int\n+\n+if sys.platform == 'win32':\n+  import msvcrt\n+else:\n+  import fcntl\n+\n+\n+# An object that catches SIGINT sent to the Python process and notices\n+# if processes passed to wait() die by SIGINT (we need to look for\n+# both of those cases, because pressing Ctrl+C can result in either\n+# the main process or one of the subprocesses getting the signal).\n+#\n+# Before a SIGINT is seen, wait(p) will simply call p.wait() and\n+# return the result. Once a SIGINT has been seen (in the main process\n+# or a subprocess, including the one the current call is waiting for),\n+# wait(p) will call p.terminate() and raise ProcessWasInterrupted.\n+class SigintHandler(object):\n+  class ProcessWasInterrupted(Exception): pass\n+  sigint_returncodes = {-signal.SIGINT,  # Unix\n+                        -1073741510,     # Windows\n+                        }\n+  def __init__(self):\n+    self.__lock = threading.Lock()\n+    self.__processes = set()\n+    self.__got_sigint = False\n+    signal.signal(signal.SIGINT, lambda signal_num, frame: self.interrupt())\n+  def __on_sigint(self):\n+    self.__got_sigint = True\n+    while self.__processes:\n+      try:\n+        self.__processes.pop().terminate()\n+      except OSError:\n+        pass\n+  def interrupt(self):\n+    with self.__lock:\n+      self.__on_sigint()\n+  def got_sigint(self):\n+    with self.__lock:\n+      return self.__got_sigint\n+  def wait(self, p):\n+    with self.__lock:\n+      if self.__got_sigint:\n+        p.terminate()\n+      self.__processes.add(p)\n+    code = p.wait()\n+    with self.__lock:\n+      self.__processes.discard(p)\n+      if code in self.sigint_returncodes:\n+        self.__on_sigint()\n+      if self.__got_sigint:\n+        raise self.ProcessWasInterrupted\n+    return code\n+sigint_handler = SigintHandler()\n+\n+\n+# Return the width of the terminal, or None if it couldn't be\n+# determined (e.g. because we're not being run interactively).\n+def term_width(out):\n+  if not out.isatty():\n+    return None\n+  try:\n+    p = subprocess.Popen([\"stty\", \"size\"],\n+                         stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n+    (out, err) = p.communicate()\n+    if p.returncode != 0 or err:\n+      return None\n+    return int(out.split()[1])\n+  except (IndexError, OSError, ValueError):\n+    return None\n+\n+\n+# Output transient and permanent lines of text. If several transient\n+# lines are written in sequence, the new will overwrite the old. We\n+# use this to ensure that lots of unimportant info (tests passing)\n+# won't drown out important info (tests failing).\n+class Outputter(object):\n+  def __init__(self, out_file):\n+    self.__out_file = out_file\n+    self.__previous_line_was_transient = False\n+    self.__width = term_width(out_file)  # Line width, or None if not a tty.\n+  def transient_line(self, msg):\n+    if self.__width is None:\n+      self.__out_file.write(msg + \"\\n\")\n+    else:\n+      self.__out_file.write(\"\\r\" + msg[:self.__width].ljust(self.__width))\n+      self.__previous_line_was_transient = True\n+  def flush_transient_output(self):\n+    if self.__previous_line_was_transient:\n+      self.__out_file.write(\"\\n\")\n+      self.__previous_line_was_transient = False\n+  def permanent_line(self, msg):\n+    self.flush_transient_output()\n+    self.__out_file.write(msg + \"\\n\")\n+\n+\n+def get_save_file_path():\n+  \"\"\"Return path to file for saving transient data.\"\"\"\n+  if sys.platform == 'win32':\n+    default_cache_path = os.path.join(os.path.expanduser('~'),\n+                                      'AppData', 'Local')\n+    cache_path = os.environ.get('LOCALAPPDATA', default_cache_path)\n+  else:\n+    # We don't use xdg module since it's not a standard.\n+    default_cache_path = os.path.join(os.path.expanduser('~'), '.cache')\n+    cache_path = os.environ.get('XDG_CACHE_HOME', default_cache_path)\n+\n+  if os.path.isdir(cache_path):\n+    return os.path.join(cache_path, 'gtest-parallel')\n+  else:\n+    sys.stderr.write('Directory {} does not exist'.format(cache_path))\n+    return os.path.join(os.path.expanduser('~'), '.gtest-parallel-times')\n+\n+\n+@total_ordering\n+class Task(object):\n+  \"\"\"Stores information about a task (single execution of a test).\n+\n+  This class stores information about the test to be executed (gtest binary and\n+  test name), and its result (log file, exit code and runtime).\n+  Each task is uniquely identified by the gtest binary, the test name and an\n+  execution number that increases each time the test is executed.\n+  Additionaly we store the last execution time, so that next time the test is\n+  executed, the slowest tests are run first.\n+  \"\"\"\n+  def __init__(self, test_binary, test_name, test_command, execution_number,\n+               last_execution_time, output_dir):\n+    self.test_name = test_name\n+    self.output_dir = output_dir\n+    self.test_binary = test_binary\n+    self.test_command = test_command\n+    self.execution_number = execution_number\n+    self.last_execution_time = last_execution_time\n+\n+    self.exit_code = None\n+    self.runtime_ms = None\n+\n+    self.test_id = (test_binary, test_name)\n+    self.task_id = (test_binary, test_name, self.execution_number)\n+\n+    self.log_file = Task._logname(self.output_dir, self.test_binary,\n+                                  test_name, self.execution_number)\n+\n+  def __sorting_key(self):\n+    # Unseen or failing tests (both missing execution time) take precedence over\n+    # execution time. Tests are greater (seen as slower) when missing times so\n+    # that they are executed first.\n+    return (1 if self.last_execution_time is None else 0,\n+            self.last_execution_time)\n+\n+  def __eq__(self, other):\n+      return self.__sorting_key() == other.__sorting_key()\n+\n+  def __ne__(self, other):\n+      return not (self == other)\n+\n+  def __lt__(self, other):\n+      return self.__sorting_key() < other.__sorting_key()\n+\n+  @staticmethod\n+  def _normalize(string):\n+    return re.sub('[^A-Za-z0-9]', '_', string)\n+\n+  @staticmethod\n+  def _logname(output_dir, test_binary, test_name, execution_number):\n+    # Store logs to temporary files if there is no output_dir.\n+    if output_dir is None:\n+      (log_handle, log_name) = tempfile.mkstemp(prefix='gtest_parallel_',\n+                                                suffix=\".log\")\n+      os.close(log_handle)\n+      return log_name\n+\n+    log_name = '%s-%s-%d.log' % (Task._normalize(os.path.basename(test_binary)),\n+                                 Task._normalize(test_name), execution_number)\n+\n+    return os.path.join(output_dir, log_name)\n+\n+  def run(self):\n+    begin = time.time()\n+    with open(self.log_file, 'w') as log:\n+      task = subprocess.Popen(self.test_command, stdout=log, stderr=log)\n+      try:\n+        self.exit_code = sigint_handler.wait(task)\n+      except sigint_handler.ProcessWasInterrupted:\n+        thread.exit()\n+    self.runtime_ms = int(1000 * (time.time() - begin))\n+    self.last_execution_time = None if self.exit_code else self.runtime_ms\n+\n+\n+class TaskManager(object):\n+  \"\"\"Executes the tasks and stores the passed, failed and interrupted tasks.\n+\n+  When a task is run, this class keeps track if it passed, failed or was\n+  interrupted. After a task finishes it calls the relevant functions of the\n+  Logger, TestResults and TestTimes classes, and in case of failure, retries the\n+  test as specified by the --retry_failed flag.\n+  \"\"\"\n+  def __init__(self, times, logger, test_results, task_factory, times_to_retry,\n+               initial_execution_number):\n+    self.times = times\n+    self.logger = logger\n+    self.test_results = test_results\n+    self.task_factory = task_factory\n+    self.times_to_retry = times_to_retry\n+    self.initial_execution_number = initial_execution_number\n+\n+    self.global_exit_code = 0\n+\n+    self.passed = []\n+    self.failed = []\n+    self.started = {}\n+    self.execution_number = {}\n+\n+    self.lock = threading.Lock()\n+\n+  def __get_next_execution_number(self, test_id):\n+    with self.lock:\n+      next_execution_number = self.execution_number.setdefault(\n+          test_id, self.initial_execution_number)\n+      self.execution_number[test_id] += 1\n+    return next_execution_number\n+\n+  def __register_start(self, task):\n+    with self.lock:\n+      self.started[task.task_id] = task\n+\n+  def __register_exit(self, task):\n+    self.logger.log_exit(task)\n+    self.times.record_test_time(task.test_binary, task.test_name,\n+                                task.last_execution_time)\n+    if self.test_results:\n+      self.test_results.log(task.test_name, task.runtime_ms,\n+                            \"PASS\" if task.exit_code == 0 else \"FAIL\")\n+\n+    with self.lock:\n+      self.started.pop(task.task_id)\n+      if task.exit_code == 0:\n+        self.passed.append(task)\n+      else:\n+        self.failed.append(task)\n+\n+  def run_task(self, task):\n+    for try_number in range(self.times_to_retry + 1):\n+      self.__register_start(task)\n+      task.run()\n+      self.__register_exit(task)\n+\n+      if task.exit_code == 0:\n+        break\n+\n+      if try_number < self.times_to_retry:\n+        execution_number = self.__get_next_execution_number(task.test_id)\n+        # We need create a new Task instance. Each task represents a single test\n+        # execution, with its own runtime, exit code and log file.\n+        task = self.task_factory(task.test_binary, task.test_name,\n+                                 task.test_command, execution_number,\n+                                 task.last_execution_time, task.output_dir)\n+\n+    with self.lock:\n+      if task.exit_code != 0:\n+        self.global_exit_code = task.exit_code\n+\n+\n+class FilterFormat(object):\n+  def __init__(self, output_dir):\n+    if sys.stdout.isatty():\n+      # stdout needs to be unbuffered since the output is interactive.\n+      sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)\n+\n+    self.output_dir = output_dir\n+\n+    self.total_tasks = 0\n+    self.finished_tasks = 0\n+    self.out = Outputter(sys.stdout)\n+    self.stdout_lock = threading.Lock()\n+\n+  def move_to(self, destination_dir, tasks):\n+    if self.output_dir is None:\n+      return\n+\n+    destination_dir = os.path.join(self.output_dir, destination_dir)\n+    os.makedirs(destination_dir)\n+    for task in tasks:\n+      shutil.move(task.log_file, destination_dir)\n+\n+  def print_tests(self, message, tasks, print_try_number):\n+    self.out.permanent_line(\"%s (%s/%s):\" %\n+                            (message, len(tasks), self.total_tasks))\n+    for task in sorted(tasks):\n+      runtime_ms = 'Interrupted'\n+      if task.runtime_ms is not None:\n+        runtime_ms = '%d ms' % task.runtime_ms\n+      self.out.permanent_line(\"%11s: %s %s%s\" % (\n+          runtime_ms, task.test_binary, task.test_name,\n+          (\" (try #%d)\" % task.execution_number) if print_try_number else \"\"))\n+\n+  def log_exit(self, task):\n+    with self.stdout_lock:\n+      self.finished_tasks += 1\n+      self.out.transient_line(\"[%d/%d] %s (%d ms)\"\n+                              % (self.finished_tasks, self.total_tasks,\n+                                 task.test_name, task.runtime_ms))\n+      if task.exit_code != 0:\n+        with open(task.log_file) as f:\n+          for line in f.readlines():\n+            self.out.permanent_line(line.rstrip())\n+        self.out.permanent_line(\n+          \"[%d/%d] %s returned/aborted with exit code %d (%d ms)\"\n+          % (self.finished_tasks, self.total_tasks, task.test_name,\n+             task.exit_code, task.runtime_ms))\n+\n+    if self.output_dir is None:\n+      # Try to remove the file 100 times (sleeping for 0.1 second in between).\n+      # This is a workaround for a process handle seemingly holding on to the\n+      # file for too long inside os.subprocess. This workaround is in place\n+      # until we figure out a minimal repro to report upstream (or a better\n+      # suspect) to prevent os.remove exceptions.\n+      num_tries = 100\n+      for i in range(num_tries):\n+        try:\n+          os.remove(task.log_file)\n+        except OSError as e:\n+          if e.errno is not errno.ENOENT:\n+            if i is num_tries - 1:\n+              self.out.permanent_line('Could not remove temporary log file: ' + str(e))\n+            else:\n+              time.sleep(0.1)\n+            continue\n+        break\n+\n+  def log_tasks(self, total_tasks):\n+    self.total_tasks += total_tasks\n+    self.out.transient_line(\"[0/%d] Running tests...\" % self.total_tasks)\n+\n+  def summarize(self, passed_tasks, failed_tasks, interrupted_tasks):\n+    stats = {}\n+    def add_stats(stats, task, idx):\n+      task_key = (task.test_binary, task.test_name)\n+      if not task_key in stats:\n+        # (passed, failed, interrupted) task_key is added as tie breaker to get\n+        # alphabetic sorting on equally-stable tests\n+        stats[task_key] = [0, 0, 0, task_key]\n+      stats[task_key][idx] += 1\n+\n+    for task in passed_tasks:\n+      add_stats(stats, task, 0)\n+    for task in failed_tasks:\n+      add_stats(stats, task, 1)\n+    for task in interrupted_tasks:\n+      add_stats(stats, task, 2)\n+\n+    self.out.permanent_line(\"SUMMARY:\")\n+    for task_key in sorted(stats, key=stats.__getitem__):\n+      (num_passed, num_failed, num_interrupted, _) = stats[task_key]\n+      (test_binary, task_name) = task_key\n+      self.out.permanent_line(\n+          \"  %s %s passed %d / %d times%s.\" %\n+              (test_binary, task_name, num_passed,\n+               num_passed + num_failed + num_interrupted,\n+               \"\" if num_interrupted == 0 else (\" (%d interrupted)\" % num_interrupted)))\n+\n+  def flush(self):\n+    self.out.flush_transient_output()\n+\n+\n+class CollectTestResults(object):\n+  def __init__(self, json_dump_filepath):\n+    self.test_results_lock = threading.Lock()\n+    self.json_dump_file = open(json_dump_filepath, 'w')\n+    self.test_results = {\n+        \"interrupted\": False,\n+        \"path_delimiter\": \".\",\n+        # Third version of the file format. See the link in the flag description\n+        # for details.\n+        \"version\": 3,\n+        \"seconds_since_epoch\": int(time.time()),\n+        \"num_failures_by_type\": {\n+            \"PASS\": 0,\n+            \"FAIL\": 0,\n+        },\n+        \"tests\": {},\n+    }\n+\n+  def log(self, test, runtime_ms, actual_result):\n+    with self.test_results_lock:\n+      self.test_results['num_failures_by_type'][actual_result] += 1\n+      results = self.test_results['tests']\n+      for name in test.split('.'):\n+        results = results.setdefault(name, {})\n+\n+      if results:\n+        results['actual'] += ' ' + actual_result\n+        results['times'].append(runtime_ms)\n+      else:  # This is the first invocation of the test\n+        results['actual'] = actual_result\n+        results['times'] = [runtime_ms]\n+        results['time'] = runtime_ms\n+        results['expected'] = 'PASS'\n+\n+  def dump_to_file_and_close(self):\n+    json.dump(self.test_results, self.json_dump_file)\n+    self.json_dump_file.close()\n+\n+\n+# Record of test runtimes. Has built-in locking.\n+class TestTimes(object):\n+  class LockedFile(object):\n+    def __init__(self, filename, mode):\n+      self._filename = filename\n+      self._mode = mode\n+      self._fo = None\n+\n+    def __enter__(self):\n+      self._fo = open(self._filename, self._mode)\n+\n+      # Regardless of opening mode we always seek to the beginning of file.\n+      # This simplifies code working with LockedFile and also ensures that\n+      # we lock (and unlock below) always the same region in file on win32.\n+      self._fo.seek(0)\n+\n+      try:\n+        if sys.platform == 'win32':\n+          # We are locking here fixed location in file to use it as\n+          # an exclusive lock on entire file.\n+          msvcrt.locking(self._fo.fileno(), msvcrt.LK_LOCK, 1)\n+        else:\n+          fcntl.flock(self._fo.fileno(), fcntl.LOCK_EX)\n+      except IOError:\n+        self._fo.close()\n+        raise\n+\n+      return self._fo\n+\n+    def __exit__(self, exc_type, exc_value, traceback):\n+      # Flush any buffered data to disk. This is needed to prevent race\n+      # condition which happens from the moment of releasing file lock\n+      # till closing the file.\n+      self._fo.flush()\n+\n+      try:\n+        if sys.platform == 'win32':\n+          self._fo.seek(0)\n+          msvcrt.locking(self._fo.fileno(), msvcrt.LK_UNLCK, 1)\n+        else:\n+          fcntl.flock(self._fo.fileno(), fcntl.LOCK_UN)\n+      finally:\n+        self._fo.close()\n+\n+      return exc_value is None\n+\n+  def __init__(self, save_file):\n+    \"Create new object seeded with saved test times from the given file.\"\n+    self.__times = {}  # (test binary, test name) -> runtime in ms\n+\n+    # Protects calls to record_test_time(); other calls are not\n+    # expected to be made concurrently.\n+    self.__lock = threading.Lock()\n+\n+    try:\n+      with TestTimes.LockedFile(save_file, 'rb') as fd:\n+        times = TestTimes.__read_test_times_file(fd)\n+    except IOError:\n+      # We couldn't obtain the lock.\n+      return\n+\n+    # Discard saved times if the format isn't right.\n+    if type(times) is not dict:\n+      return\n+    for ((test_binary, test_name), runtime) in list(times.items()):\n+      if (type(test_binary) is not str or type(test_name) is not str\n+          or type(runtime) not in {int, long, type(None)}):\n+        return\n+\n+    self.__times = times\n+\n+  def get_test_time(self, binary, testname):\n+    \"\"\"Return the last duration for the given test as an integer number of\n+    milliseconds, or None if the test failed or if there's no record for it.\"\"\"\n+    return self.__times.get((binary, testname), None)\n+\n+  def record_test_time(self, binary, testname, runtime_ms):\n+    \"\"\"Record that the given test ran in the specified number of\n+    milliseconds. If the test failed, runtime_ms should be None.\"\"\"\n+    with self.__lock:\n+      self.__times[(binary, testname)] = runtime_ms\n+\n+  def write_to_file(self, save_file):\n+    \"Write all the times to file.\"\n+    try:\n+      with TestTimes.LockedFile(save_file, 'a+b') as fd:\n+        times = TestTimes.__read_test_times_file(fd)\n+\n+        if times is None:\n+          times = self.__times\n+        else:\n+          times.update(self.__times)\n+\n+        # We erase data from file while still holding a lock to it. This\n+        # way reading old test times and appending new ones are atomic\n+        # for external viewer.\n+        fd.seek(0)\n+        fd.truncate()\n+        with gzip.GzipFile(fileobj=fd, mode='wb') as gzf:\n+          cPickle.dump(times, gzf, PICKLE_HIGHEST_PROTOCOL)\n+    except IOError:\n+      pass  # ignore errors---saving the times isn't that important\n+\n+  @staticmethod\n+  def __read_test_times_file(fd):\n+    try:\n+      with gzip.GzipFile(fileobj=fd, mode='rb') as gzf:\n+        times = cPickle.load(gzf)\n+    except Exception:\n+      # File doesn't exist, isn't readable, is malformed---whatever.\n+      # Just ignore it.\n+      return None\n+    else:\n+      return times\n+\n+\n+def find_tests(binaries, additional_args, options, times):\n+  test_count = 0\n+  tasks = []\n+  for test_binary in binaries:\n+    command = [test_binary]\n+    if options.gtest_also_run_disabled_tests:\n+      command += ['--gtest_also_run_disabled_tests']\n+\n+    list_command = command + ['--gtest_list_tests']\n+    if options.gtest_filter != '':\n+      list_command += ['--gtest_filter=' + options.gtest_filter]\n+\n+    try:\n+      test_list = subprocess.check_output(list_command,\n+                                          stderr=subprocess.STDOUT)\n+    except subprocess.CalledProcessError as e:\n+      sys.exit(\"%s: %s\" % (test_binary, str(e)))\n+\n+    command += additional_args + ['--gtest_color=' + options.gtest_color]\n+\n+    test_group = ''\n+    for line in test_list.split('\\n'):\n+      if not line.strip():\n+        continue\n+      if line[0] != \" \":\n+        # Remove comments for typed tests and strip whitespace.\n+        test_group = line.split('#')[0].strip()\n+        continue\n+      # Remove comments for parameterized tests and strip whitespace.\n+      line = line.split('#')[0].strip()\n+      if not line:\n+        continue\n+\n+      test_name = test_group + line\n+      if not options.gtest_also_run_disabled_tests and 'DISABLED_' in test_name:\n+        continue\n+\n+      last_execution_time = times.get_test_time(test_binary, test_name)\n+      if options.failed and last_execution_time is not None:\n+        continue\n+\n+      test_command = command + ['--gtest_filter=' + test_name]\n+      if (test_count - options.shard_index) % options.shard_count == 0:\n+        for execution_number in range(options.repeat):\n+          tasks.append(Task(test_binary, test_name, test_command,\n+                            execution_number + 1, last_execution_time,\n+                            options.output_dir))\n+\n+      test_count += 1\n+\n+  # Sort the tasks to run the slowest tests first, so that faster ones can be\n+  # finished in parallel.\n+  return sorted(tasks, reverse=True)\n+\n+\n+def execute_tasks(tasks, pool_size, task_manager,\n+                  timeout, serialize_test_cases):\n+  class WorkerFn(object):\n+    def __init__(self, tasks, running_groups):\n+      self.tasks = tasks\n+      self.running_groups = running_groups\n+      self.task_lock = threading.Lock()\n+\n+    def __call__(self):\n+      while True:\n+        with self.task_lock:\n+          for task_id in range(len(self.tasks)):\n+            task = self.tasks[task_id]\n+\n+            if self.running_groups is not None:\n+              test_group = task.test_name.split('.')[0]\n+              if test_group in self.running_groups:\n+                # Try to find other non-running test group.\n+                continue\n+              else:\n+                self.running_groups.add(test_group)\n+\n+            del self.tasks[task_id]\n+            break\n+          else:\n+            # Either there is no tasks left or number or remaining test\n+            # cases (groups) is less than number or running threads.\n+            return\n+\n+        task_manager.run_task(task)\n+\n+        if self.running_groups is not None:\n+          with self.task_lock:\n+            self.running_groups.remove(test_group)\n+\n+  def start_daemon(func):\n+    t = threading.Thread(target=func)\n+    t.daemon = True\n+    t.start()\n+    return t\n+\n+  try:\n+    if timeout:\n+      timeout.start()\n+    running_groups = set() if serialize_test_cases else None\n+    worker_fn = WorkerFn(tasks, running_groups)\n+    workers = [start_daemon(worker_fn) for _ in range(pool_size)]\n+    for worker in workers:\n+      worker.join()\n+  finally:\n+    if timeout:\n+      timeout.cancel()\n+\n+\n+def default_options_parser():\n+  parser = optparse.OptionParser(\n+      usage = 'usage: %prog [options] binary [binary ...] -- [additional args]')\n+\n+  parser.add_option('-d', '--output_dir', type='string', default=None,\n+                    help='Output directory for test logs. Logs will be '\n+                         'available under gtest-parallel-logs/, so '\n+                         '--output_dir=/tmp will results in all logs being '\n+                         'available under /tmp/gtest-parallel-logs/.')\n+  parser.add_option('-r', '--repeat', type='int', default=1,\n+                    help='Number of times to execute all the tests.')\n+  parser.add_option('--retry_failed', type='int', default=0,\n+                    help='Number of times to repeat failed tests.')\n+  parser.add_option('--failed', action='store_true', default=False,\n+                    help='run only failed and new tests')\n+  parser.add_option('-w', '--workers', type='int',\n+                    default=multiprocessing.cpu_count(),\n+                    help='number of workers to spawn')\n+  parser.add_option('--gtest_color', type='string', default='yes',\n+                    help='color output')\n+  parser.add_option('--gtest_filter', type='string', default='',\n+                    help='test filter')\n+  parser.add_option('--gtest_also_run_disabled_tests', action='store_true',\n+                    default=False, help='run disabled tests too')\n+  parser.add_option('--print_test_times', action='store_true', default=False,\n+                    help='list the run time of each test at the end of execution')\n+  parser.add_option('--shard_count', type='int', default=1,\n+                    help='total number of shards (for sharding test execution '\n+                         'between multiple machines)')\n+  parser.add_option('--shard_index', type='int', default=0,\n+                    help='zero-indexed number identifying this shard (for '\n+                         'sharding test execution between multiple machines)')\n+  parser.add_option('--dump_json_test_results', type='string', default=None,\n+                    help='Saves the results of the tests as a JSON machine-'\n+                         'readable file. The format of the file is specified at '\n+                         'https://www.chromium.org/developers/the-json-test-results-format')\n+  parser.add_option('--timeout', type='int', default=None,\n+                    help='Interrupt all remaining processes after the given '\n+                         'time (in seconds).')\n+  parser.add_option('--serialize_test_cases', action='store_true',\n+                    default=False, help='Do not run tests from the same test '\n+                                        'case in parallel.')\n+  return parser\n+\n+\n+def main():\n+  # Remove additional arguments (anything after --).\n+  additional_args = []\n+\n+  for i in range(len(sys.argv)):\n+    if sys.argv[i] == '--':\n+      additional_args = sys.argv[i+1:]\n+      sys.argv = sys.argv[:i]\n+      break\n+\n+  parser = default_options_parser()\n+  (options, binaries) = parser.parse_args()\n+\n+  if (options.output_dir is not None and\n+      not os.path.isdir(options.output_dir)):\n+    parser.error('--output_dir value must be an existing directory, '\n+                 'current value is \"%s\"' % options.output_dir)\n+\n+  # Append gtest-parallel-logs to log output, this is to avoid deleting user\n+  # data if an user passes a directory where files are already present. If a\n+  # user specifies --output_dir=Docs/, we'll create Docs/gtest-parallel-logs\n+  # and clean that directory out on startup, instead of nuking Docs/.\n+  if options.output_dir:\n+    options.output_dir = os.path.join(options.output_dir,\n+                                      'gtest-parallel-logs')\n+\n+  if binaries == []:\n+    parser.print_usage()\n+    sys.exit(1)\n+\n+  if options.shard_count < 1:\n+    parser.error(\"Invalid number of shards: %d. Must be at least 1.\" %\n+                 options.shard_count)\n+  if not (0 <= options.shard_index < options.shard_count):\n+    parser.error(\"Invalid shard index: %d. Must be between 0 and %d \"\n+                 \"(less than the number of shards).\" %\n+                 (options.shard_index, options.shard_count - 1))\n+\n+  # Check that all test binaries have an unique basename. That way we can ensure\n+  # the logs are saved to unique files even when two different binaries have\n+  # common tests.\n+  unique_binaries = set(os.path.basename(binary) for binary in binaries)\n+  assert len(unique_binaries) == len(binaries), (\n+      \"All test binaries must have an unique basename.\")\n+\n+  if options.output_dir:\n+    # Remove files from old test runs.\n+    if os.path.isdir(options.output_dir):\n+      shutil.rmtree(options.output_dir)\n+    # Create directory for test log output.\n+    try:\n+      os.makedirs(options.output_dir)\n+    except OSError as e:\n+      # Ignore errors if this directory already exists.\n+      if e.errno != errno.EEXIST or not os.path.isdir(options.output_dir):\n+        raise e\n+\n+  timeout = None\n+  if options.timeout is not None:\n+    timeout = threading.Timer(options.timeout, sigint_handler.interrupt)\n+\n+  test_results = None\n+  if options.dump_json_test_results is not None:\n+    test_results = CollectTestResults(options.dump_json_test_results)\n+\n+  save_file = get_save_file_path()\n+\n+  times = TestTimes(save_file)\n+  logger = FilterFormat(options.output_dir)\n+\n+  task_manager = TaskManager(times, logger, test_results, Task,\n+                             options.retry_failed, options.repeat + 1)\n+\n+  tasks = find_tests(binaries, additional_args, options, times)\n+  logger.log_tasks(len(tasks))\n+  execute_tasks(tasks, options.workers, task_manager,\n+                timeout, options.serialize_test_cases)\n+\n+  print_try_number = options.retry_failed > 0 or options.repeat > 1\n+  if task_manager.passed:\n+    logger.move_to('passed', task_manager.passed)\n+    if options.print_test_times:\n+      logger.print_tests('PASSED TESTS', task_manager.passed, print_try_number)\n+\n+  if task_manager.failed:\n+    logger.print_tests('FAILED TESTS', task_manager.failed, print_try_number)\n+    logger.move_to('failed', task_manager.failed)\n+\n+  if task_manager.started:\n+    logger.print_tests(\n+        'INTERRUPTED TESTS', list(task_manager.started.values()), print_try_number)\n+    logger.move_to('interrupted', list(task_manager.started.values()))\n+\n+  if options.repeat > 1 and (task_manager.failed or task_manager.started):\n+    logger.summarize(task_manager.passed, task_manager.failed,\n+                     list(task_manager.started.values()))\n+\n+  logger.flush()\n+  times.write_to_file(save_file)\n+  if test_results:\n+    test_results.dump_to_file_and_close()\n+\n+  if sigint_handler.got_sigint():\n+    return -signal.SIGINT\n+\n+  return task_manager.global_exit_code\n+\n+if __name__ == \"__main__\":\n+  sys.exit(main())"
      }
    ]
  },
  {
    "sha": "7785663d6f9960ffd38bec6c40fad7efbd8141fd",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo3Nzg1NjYzZDZmOTk2MGZmZDM4YmVjNmM0MGZhZDdlZmJkODE0MWZk",
    "commit": {
      "author": {
        "name": "MarcoFalke",
        "email": "falke.marco@gmail.com",
        "date": "2018-03-28T15:13:05Z"
      },
      "committer": {
        "name": "MarcoFalke",
        "email": "falke.marco@gmail.com",
        "date": "2018-04-10T16:05:47Z"
      },
      "message": "test: Adjust parallel.find_tests for our unit tests",
      "tree": {
        "sha": "18293ac408163f91a6a29a4803c3e4b5c1fa36e1",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/18293ac408163f91a6a29a4803c3e4b5c1fa36e1"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/7785663d6f9960ffd38bec6c40fad7efbd8141fd",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7785663d6f9960ffd38bec6c40fad7efbd8141fd",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/7785663d6f9960ffd38bec6c40fad7efbd8141fd",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/7785663d6f9960ffd38bec6c40fad7efbd8141fd/comments",
    "author": {
      "login": "MarcoFalke",
      "id": 6399679,
      "node_id": "MDQ6VXNlcjYzOTk2Nzk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/6399679?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/MarcoFalke",
      "html_url": "https://github.com/MarcoFalke",
      "followers_url": "https://api.github.com/users/MarcoFalke/followers",
      "following_url": "https://api.github.com/users/MarcoFalke/following{/other_user}",
      "gists_url": "https://api.github.com/users/MarcoFalke/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/MarcoFalke/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/MarcoFalke/subscriptions",
      "organizations_url": "https://api.github.com/users/MarcoFalke/orgs",
      "repos_url": "https://api.github.com/users/MarcoFalke/repos",
      "events_url": "https://api.github.com/users/MarcoFalke/events{/privacy}",
      "received_events_url": "https://api.github.com/users/MarcoFalke/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "MarcoFalke",
      "id": 6399679,
      "node_id": "MDQ6VXNlcjYzOTk2Nzk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/6399679?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/MarcoFalke",
      "html_url": "https://github.com/MarcoFalke",
      "followers_url": "https://api.github.com/users/MarcoFalke/followers",
      "following_url": "https://api.github.com/users/MarcoFalke/following{/other_user}",
      "gists_url": "https://api.github.com/users/MarcoFalke/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/MarcoFalke/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/MarcoFalke/subscriptions",
      "organizations_url": "https://api.github.com/users/MarcoFalke/orgs",
      "repos_url": "https://api.github.com/users/MarcoFalke/repos",
      "events_url": "https://api.github.com/users/MarcoFalke/events{/privacy}",
      "received_events_url": "https://api.github.com/users/MarcoFalke/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "8510c7e289a754c0a4cfc6b7b690815fa313ab89",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/8510c7e289a754c0a4cfc6b7b690815fa313ab89",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/8510c7e289a754c0a4cfc6b7b690815fa313ab89"
      }
    ],
    "stats": {
      "total": 79,
      "additions": 37,
      "deletions": 42
    },
    "files": [
      {
        "sha": "1d25007932ef2562e4bdde99a6b5724d4b1ef564",
        "filename": "configure.ac",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/7785663d6f9960ffd38bec6c40fad7efbd8141fd/configure.ac",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/7785663d6f9960ffd38bec6c40fad7efbd8141fd/configure.ac",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/configure.ac?ref=7785663d6f9960ffd38bec6c40fad7efbd8141fd",
        "patch": "@@ -1307,6 +1307,7 @@ AC_CONFIG_FILES([contrib/devtools/split-debug.sh],[chmod +x contrib/devtools/spl\n AM_COND_IF([HAVE_DOXYGEN], [AC_CONFIG_FILES([doc/Doxyfile])])\n AC_CONFIG_LINKS([contrib/filter-lcov.py:contrib/filter-lcov.py])\n AC_CONFIG_LINKS([test/functional/test_runner.py:test/functional/test_runner.py])\n+AC_CONFIG_LINKS([src/test/parallel.py:src/test/parallel.py])\n AC_CONFIG_LINKS([test/util/bitcoin-util-test.py:test/util/bitcoin-util-test.py])\n \n dnl boost's m4 checks do something really nasty: they export these vars. As a"
      },
      {
        "sha": "201c8cbbaa4523d86d554bb33fa85dfd82cbc22a",
        "filename": "src/Makefile.test.include",
        "status": "modified",
        "additions": 3,
        "deletions": 5,
        "changes": 8,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/7785663d6f9960ffd38bec6c40fad7efbd8141fd/src/Makefile.test.include",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/7785663d6f9960ffd38bec6c40fad7efbd8141fd/src/Makefile.test.include",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/Makefile.test.include?ref=7785663d6f9960ffd38bec6c40fad7efbd8141fd",
        "patch": "@@ -153,18 +153,16 @@ bitcoin_test_check: $(TEST_BINARY) FORCE\n bitcoin_test_clean : FORCE\n \trm -f $(CLEAN_BITCOIN_TEST) $(test_test_bitcoin_OBJECTS) $(TEST_BINARY)\n \n-check-local: $(BITCOIN_TESTS:.cpp=.cpp.test)\n+check-local:\n+\t@echo \"Running src/test/parallel.py...\"\n+\t$(PYTHON) $(top_builddir)/src/test/parallel.py\n \t@echo \"Running test/util/bitcoin-util-test.py...\"\n \t$(PYTHON) $(top_builddir)/test/util/bitcoin-util-test.py\n \t$(AM_V_at)$(MAKE) $(AM_MAKEFLAGS) -C secp256k1 check\n if EMBEDDED_UNIVALUE\n \t$(AM_V_at)$(MAKE) $(AM_MAKEFLAGS) -C univalue check\n endif\n \n-%.cpp.test: %.cpp\n-\t@echo Running tests: `cat $< | grep \"BOOST_FIXTURE_TEST_SUITE(\\|BOOST_AUTO_TEST_SUITE(\" | cut -d '(' -f 2 | cut -d ',' -f 1 | cut -d ')' -f 1` from $<\n-\t$(AM_V_at)$(TEST_BINARY) -l test_suite -t \"`cat $< | grep \"BOOST_FIXTURE_TEST_SUITE(\\|BOOST_AUTO_TEST_SUITE(\" | cut -d '(' -f 2 | cut -d ',' -f 1 | cut -d ')' -f 1`\" > $<.log 2>&1 || (cat $<.log && false)\n-\n %.json.h: %.json\n \t@$(MKDIR_P) $(@D)\n \t@{ \\"
      },
      {
        "sha": "13390e379ea64f3e902c2542bfcf65849e063a31",
        "filename": "src/test/parallel.py",
        "status": "modified",
        "additions": 33,
        "deletions": 37,
        "changes": 70,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/7785663d6f9960ffd38bec6c40fad7efbd8141fd/src/test/parallel.py",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/7785663d6f9960ffd38bec6c40fad7efbd8141fd/src/test/parallel.py",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/test/parallel.py?ref=7785663d6f9960ffd38bec6c40fad7efbd8141fd",
        "patch": "@@ -1,3 +1,5 @@\n+#!/usr/bin/env python\n+#\n # Copyright 2013 Google Inc. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -20,6 +22,7 @@\n import gzip\n import json\n import multiprocessing\n+import configparser\n import optparse\n import os\n from pickle import HIGHEST_PROTOCOL as PICKLE_HIGHEST_PROTOCOL\n@@ -557,48 +560,19 @@ def __read_test_times_file(fd):\n       return times\n \n \n-def find_tests(binaries, additional_args, options, times):\n+def find_tests(test_list, binaries, additional_args, options, times):\n   test_count = 0\n   tasks = []\n   for test_binary in binaries:\n     command = [test_binary]\n-    if options.gtest_also_run_disabled_tests:\n-      command += ['--gtest_also_run_disabled_tests']\n-\n-    list_command = command + ['--gtest_list_tests']\n-    if options.gtest_filter != '':\n-      list_command += ['--gtest_filter=' + options.gtest_filter]\n-\n-    try:\n-      test_list = subprocess.check_output(list_command,\n-                                          stderr=subprocess.STDOUT)\n-    except subprocess.CalledProcessError as e:\n-      sys.exit(\"%s: %s\" % (test_binary, str(e)))\n-\n-    command += additional_args + ['--gtest_color=' + options.gtest_color]\n-\n-    test_group = ''\n-    for line in test_list.split('\\n'):\n-      if not line.strip():\n-        continue\n-      if line[0] != \" \":\n-        # Remove comments for typed tests and strip whitespace.\n-        test_group = line.split('#')[0].strip()\n-        continue\n-      # Remove comments for parameterized tests and strip whitespace.\n-      line = line.split('#')[0].strip()\n-      if not line:\n-        continue\n-\n-      test_name = test_group + line\n-      if not options.gtest_also_run_disabled_tests and 'DISABLED_' in test_name:\n-        continue\n+    command += additional_args\n \n+    for test_name in test_list:\n       last_execution_time = times.get_test_time(test_binary, test_name)\n       if options.failed and last_execution_time is not None:\n         continue\n \n-      test_command = command + ['--gtest_filter=' + test_name]\n+      test_command = command + ['--run_test=' + test_name]\n       if (test_count - options.shard_index) % options.shard_count == 0:\n         for execution_number in range(options.repeat):\n           tasks.append(Task(test_binary, test_name, test_command,\n@@ -666,7 +640,7 @@ def start_daemon(func):\n       timeout.cancel()\n \n \n-def default_options_parser():\n+def default_options_parser(default_binary):\n   parser = optparse.OptionParser(\n       usage = 'usage: %prog [options] binary [binary ...] -- [additional args]')\n \n@@ -708,6 +682,10 @@ def default_options_parser():\n   parser.add_option('--serialize_test_cases', action='store_true',\n                     default=False, help='Do not run tests from the same test '\n                                         'case in parallel.')\n+# Bitcoin Core specific options parsing: start\n+  parser.add_option('-b', '--binary', type=str, default=default_binary,\n+                    help='The test binary (default: {})'.format(default_binary))\n+# Bitcoin Core specific options parsing: end\n   return parser\n \n \n@@ -720,9 +698,17 @@ def main():\n       additional_args = sys.argv[i+1:]\n       sys.argv = sys.argv[:i]\n       break\n-\n-  parser = default_options_parser()\n+# Bitcoin Core specific options parsing: start\n+  config = configparser.ConfigParser()\n+  configfile = os.path.abspath(os.path.dirname(__file__)) + \"/../../test/config.ini\"\n+  config.read_file(open(configfile))\n+  env = config['environment']\n+  DEFAULT_BINARY = os.path.join(env['BUILDDIR'],'src', 'test','test_bitcoin'+env['EXEEXT'])\n+  parser = default_options_parser(DEFAULT_BINARY)\n   (options, binaries) = parser.parse_args()\n+  test_list = read_test_list(os.path.join(env['SRCDIR'],'src'))\n+  binaries = [options.binary]\n+# Bitcoin Core specific options parsing: end\n \n   if (options.output_dir is not None and\n       not os.path.isdir(options.output_dir)):\n@@ -784,7 +770,7 @@ def main():\n   task_manager = TaskManager(times, logger, test_results, Task,\n                              options.retry_failed, options.repeat + 1)\n \n-  tasks = find_tests(binaries, additional_args, options, times)\n+  tasks = find_tests(test_list, binaries, additional_args, options, times)\n   logger.log_tasks(len(tasks))\n   execute_tasks(tasks, options.workers, task_manager,\n                 timeout, options.serialize_test_cases)\n@@ -818,5 +804,15 @@ def main():\n \n   return task_manager.global_exit_code\n \n+# Bitcoin Core specific functions: start\n+def read_test_list(src_dir):\n+    output = subprocess.check_output(['git', 'grep', 'BOOST_FIXTURE_TEST_SUITE', '--', src_dir+'/**.cpp', ':(exclude)'+src_dir+'/leveldb/', ':(exclude)'+src_dir+'/secp256k1/', ':(exclude)'+src_dir+'/univalue/'], universal_newlines=True)\n+    test_list = []\n+    import re\n+    for m in re.findall('BOOST_FIXTURE_TEST_SUITE\\([^,]+,', output):\n+        test_list += [m[len('BOOST_FIXTURE_TEST_SUITE('):-1].strip()]\n+    return test_list\n+# Bitcoin Core specific functions: end\n+\n if __name__ == \"__main__\":\n   sys.exit(main())"
      }
    ]
  }
]