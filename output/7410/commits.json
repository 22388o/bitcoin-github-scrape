[
  {
    "sha": "8d3a84c242598ef3cdc733e99dddebfecdad84a6",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzo4ZDNhODRjMjQyNTk4ZWYzY2RjNzMzZTk5ZGRkZWJmZWNkYWQ4NGE2",
    "commit": {
      "author": {
        "name": "Luke Dashjr",
        "email": "luke-jr+git@utopios.org",
        "date": "2016-01-16T03:23:42Z"
      },
      "committer": {
        "name": "Luke Dashjr",
        "email": "luke-jr+git@utopios.org",
        "date": "2016-01-16T03:38:29Z"
      },
      "message": "HARDFORK: Use Keccak for proof-of-work beginning 2016 Apr 14",
      "tree": {
        "sha": "a26af9f6bdcdb86f6951b77abe796acc0647e3f5",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/a26af9f6bdcdb86f6951b77abe796acc0647e3f5"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/8d3a84c242598ef3cdc733e99dddebfecdad84a6",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/8d3a84c242598ef3cdc733e99dddebfecdad84a6",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/8d3a84c242598ef3cdc733e99dddebfecdad84a6",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/8d3a84c242598ef3cdc733e99dddebfecdad84a6/comments",
    "author": {
      "login": "luke-jr",
      "id": 1095675,
      "node_id": "MDQ6VXNlcjEwOTU2NzU=",
      "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/luke-jr",
      "html_url": "https://github.com/luke-jr",
      "followers_url": "https://api.github.com/users/luke-jr/followers",
      "following_url": "https://api.github.com/users/luke-jr/following{/other_user}",
      "gists_url": "https://api.github.com/users/luke-jr/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/luke-jr/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
      "organizations_url": "https://api.github.com/users/luke-jr/orgs",
      "repos_url": "https://api.github.com/users/luke-jr/repos",
      "events_url": "https://api.github.com/users/luke-jr/events{/privacy}",
      "received_events_url": "https://api.github.com/users/luke-jr/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "luke-jr",
      "id": 1095675,
      "node_id": "MDQ6VXNlcjEwOTU2NzU=",
      "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/luke-jr",
      "html_url": "https://github.com/luke-jr",
      "followers_url": "https://api.github.com/users/luke-jr/followers",
      "following_url": "https://api.github.com/users/luke-jr/following{/other_user}",
      "gists_url": "https://api.github.com/users/luke-jr/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/luke-jr/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
      "organizations_url": "https://api.github.com/users/luke-jr/orgs",
      "repos_url": "https://api.github.com/users/luke-jr/repos",
      "events_url": "https://api.github.com/users/luke-jr/events{/privacy}",
      "received_events_url": "https://api.github.com/users/luke-jr/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "3cd836c1d855b92e7c73ab31979f471c4f8dad68",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/3cd836c1d855b92e7c73ab31979f471c4f8dad68",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/3cd836c1d855b92e7c73ab31979f471c4f8dad68"
      }
    ],
    "stats": {
      "total": 4118,
      "additions": 4117,
      "deletions": 1
    },
    "files": [
      {
        "sha": "e1039bf7a5285e1989bdf828fe98081915d458fb",
        "filename": "src/Makefile.am",
        "status": "modified",
        "additions": 3,
        "deletions": 0,
        "changes": 3,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/8d3a84c242598ef3cdc733e99dddebfecdad84a6/src/Makefile.am",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/8d3a84c242598ef3cdc733e99dddebfecdad84a6/src/Makefile.am",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/Makefile.am?ref=8d3a84c242598ef3cdc733e99dddebfecdad84a6",
        "patch": "@@ -142,6 +142,8 @@ BITCOIN_CORE_H = \\\n   script/sign.h \\\n   script/standard.h \\\n   serialize.h \\\n+  sph_types.h \\\n+  sph_keccak.h \\\n   streams.h \\\n   support/allocators/secure.h \\\n   support/allocators/zeroafterfree.h \\\n@@ -274,6 +276,7 @@ libbitcoin_common_a_SOURCES = \\\n   core_read.cpp \\\n   core_write.cpp \\\n   hash.cpp \\\n+  keccak.cpp \\\n   key.cpp \\\n   keystore.cpp \\\n   netbase.cpp \\"
      },
      {
        "sha": "b6e0284ac24b17699df2a83804a128e60ef09d8c",
        "filename": "src/keccak.cpp",
        "status": "added",
        "additions": 1829,
        "deletions": 0,
        "changes": 1829,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/8d3a84c242598ef3cdc733e99dddebfecdad84a6/src/keccak.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/8d3a84c242598ef3cdc733e99dddebfecdad84a6/src/keccak.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/keccak.cpp?ref=8d3a84c242598ef3cdc733e99dddebfecdad84a6",
        "patch": "@@ -0,0 +1,1829 @@\n+/* $Id: keccak.c 259 2011-07-19 22:11:27Z tp $ */\n+/*\n+ * Keccak implementation.\n+ *\n+ * ==========================(LICENSE BEGIN)============================\n+ *\n+ * Copyright (c) 2007-2010  Projet RNRT SAPHIR\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files (the\n+ * \"Software\"), to deal in the Software without restriction, including\n+ * without limitation the rights to use, copy, modify, merge, publish,\n+ * distribute, sublicense, and/or sell copies of the Software, and to\n+ * permit persons to whom the Software is furnished to do so, subject to\n+ * the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ *\n+ * ===========================(LICENSE END)=============================\n+ *\n+ * @author   Thomas Pornin <thomas.pornin@cryptolog.com>\n+ */\n+ \n+#include <stddef.h>\n+#include <string.h>\n+ \n+#include \"sph_keccak.h\"\n+ \n+#ifdef __cplusplus\n+extern \"C\"{\n+#endif\n+ \n+/*\n+ * Parameters:\n+ *\n+ *  SPH_KECCAK_64          use a 64-bit type\n+ *  SPH_KECCAK_UNROLL      number of loops to unroll (0/undef for full unroll)\n+ *  SPH_KECCAK_INTERLEAVE  use bit-interleaving (32-bit type only)\n+ *  SPH_KECCAK_NOCOPY      do not copy the state into local variables\n+ *\n+ * If there is no usable 64-bit type, the code automatically switches\n+ * back to the 32-bit implementation.\n+ *\n+ * Some tests on an Intel Core2 Q6600 (both 64-bit and 32-bit, 32 kB L1\n+ * code cache), a PowerPC (G3, 32 kB L1 code cache), an ARM920T core\n+ * (16 kB L1 code cache), and a small MIPS-compatible CPU (Broadcom BCM3302,\n+ * 8 kB L1 code cache), seem to show that the following are optimal:\n+ *\n+ * -- x86, 64-bit: use the 64-bit implementation, unroll 8 rounds,\n+ * do not copy the state; unrolling 2, 6 or all rounds also provides\n+ * near-optimal performance.\n+ * -- x86, 32-bit: use the 32-bit implementation, unroll 6 rounds,\n+ * interleave, do not copy the state. Unrolling 1, 2, 4 or 8 rounds\n+ * also provides near-optimal performance.\n+ * -- PowerPC: use the 64-bit implementation, unroll 8 rounds,\n+ * copy the state. Unrolling 4 or 6 rounds is near-optimal.\n+ * -- ARM: use the 64-bit implementation, unroll 2 or 4 rounds,\n+ * copy the state.\n+ * -- MIPS: use the 64-bit implementation, unroll 2 rounds, copy\n+ * the state. Unrolling only 1 round is also near-optimal.\n+ *\n+ * Also, interleaving does not always yield actual improvements when\n+ * using a 32-bit implementation; in particular when the architecture\n+ * does not offer a native rotation opcode (interleaving replaces one\n+ * 64-bit rotation with two 32-bit rotations, which is a gain only if\n+ * there is a native 32-bit rotation opcode and not a native 64-bit\n+ * rotation opcode; also, interleaving implies a small overhead when\n+ * processing input words).\n+ *\n+ * To sum up:\n+ * -- when possible, use the 64-bit code\n+ * -- exception: on 32-bit x86, use 32-bit code\n+ * -- when using 32-bit code, use interleaving\n+ * -- copy the state, except on x86\n+ * -- unroll 8 rounds on \"big\" machine, 2 rounds on \"small\" machines\n+ */\n+ \n+#if SPH_SMALL_FOOTPRINT && !defined SPH_SMALL_FOOTPRINT_KECCAK\n+#define SPH_SMALL_FOOTPRINT_KECCAK   1\n+#endif\n+ \n+/*\n+ * By default, we select the 64-bit implementation if a 64-bit type\n+ * is available, unless a 32-bit x86 is detected.\n+ */\n+#if !defined SPH_KECCAK_64 && SPH_64 \\\n+        && !(defined __i386__ || SPH_I386_GCC || SPH_I386_MSVC)\n+#define SPH_KECCAK_64   1\n+#endif\n+ \n+/*\n+ * If using a 32-bit implementation, we prefer to interleave.\n+ */\n+#if !SPH_KECCAK_64 && !defined SPH_KECCAK_INTERLEAVE\n+#define SPH_KECCAK_INTERLEAVE   1\n+#endif\n+ \n+/*\n+ * Unroll 8 rounds on big systems, 2 rounds on small systems.\n+ */\n+#ifndef SPH_KECCAK_UNROLL\n+#if SPH_SMALL_FOOTPRINT_KECCAK\n+#define SPH_KECCAK_UNROLL   2\n+#else\n+#define SPH_KECCAK_UNROLL   8\n+#endif\n+#endif\n+ \n+/*\n+ * We do not want to copy the state to local variables on x86 (32-bit\n+ * and 64-bit alike).\n+ */\n+#ifndef SPH_KECCAK_NOCOPY\n+#if defined __i386__ || defined __x86_64 || SPH_I386_MSVC || SPH_I386_GCC\n+#define SPH_KECCAK_NOCOPY   1\n+#else\n+#define SPH_KECCAK_NOCOPY   0\n+#endif\n+#endif\n+ \n+#ifdef _MSC_VER\n+#pragma warning (disable: 4146)\n+#endif\n+ \n+#if SPH_KECCAK_64\n+ \n+static const sph_u64 RC[] = {\n+        SPH_C64(0x0000000000000001), SPH_C64(0x0000000000008082),\n+        SPH_C64(0x800000000000808A), SPH_C64(0x8000000080008000),\n+        SPH_C64(0x000000000000808B), SPH_C64(0x0000000080000001),\n+        SPH_C64(0x8000000080008081), SPH_C64(0x8000000000008009),\n+        SPH_C64(0x000000000000008A), SPH_C64(0x0000000000000088),\n+        SPH_C64(0x0000000080008009), SPH_C64(0x000000008000000A),\n+        SPH_C64(0x000000008000808B), SPH_C64(0x800000000000008B),\n+        SPH_C64(0x8000000000008089), SPH_C64(0x8000000000008003),\n+        SPH_C64(0x8000000000008002), SPH_C64(0x8000000000000080),\n+        SPH_C64(0x000000000000800A), SPH_C64(0x800000008000000A),\n+        SPH_C64(0x8000000080008081), SPH_C64(0x8000000000008080),\n+        SPH_C64(0x0000000080000001), SPH_C64(0x8000000080008008)\n+};\n+ \n+#if SPH_KECCAK_NOCOPY\n+ \n+#define a00   (kc->u.wide[ 0])\n+#define a10   (kc->u.wide[ 1])\n+#define a20   (kc->u.wide[ 2])\n+#define a30   (kc->u.wide[ 3])\n+#define a40   (kc->u.wide[ 4])\n+#define a01   (kc->u.wide[ 5])\n+#define a11   (kc->u.wide[ 6])\n+#define a21   (kc->u.wide[ 7])\n+#define a31   (kc->u.wide[ 8])\n+#define a41   (kc->u.wide[ 9])\n+#define a02   (kc->u.wide[10])\n+#define a12   (kc->u.wide[11])\n+#define a22   (kc->u.wide[12])\n+#define a32   (kc->u.wide[13])\n+#define a42   (kc->u.wide[14])\n+#define a03   (kc->u.wide[15])\n+#define a13   (kc->u.wide[16])\n+#define a23   (kc->u.wide[17])\n+#define a33   (kc->u.wide[18])\n+#define a43   (kc->u.wide[19])\n+#define a04   (kc->u.wide[20])\n+#define a14   (kc->u.wide[21])\n+#define a24   (kc->u.wide[22])\n+#define a34   (kc->u.wide[23])\n+#define a44   (kc->u.wide[24])\n+ \n+#define DECL_STATE\n+#define READ_STATE(sc)\n+#define WRITE_STATE(sc)\n+ \n+#define INPUT_BUF(size)   do { \\\n+                size_t j; \\\n+                for (j = 0; j < (size); j += 8) { \\\n+                        kc->u.wide[j >> 3] ^= sph_dec64le_aligned(buf + j); \\\n+                } \\\n+        } while (0)\n+ \n+#define INPUT_BUF144   INPUT_BUF(144)\n+#define INPUT_BUF136   INPUT_BUF(136)\n+#define INPUT_BUF104   INPUT_BUF(104)\n+#define INPUT_BUF72    INPUT_BUF(72)\n+ \n+#else\n+ \n+#define DECL_STATE \\\n+        sph_u64 a00, a01, a02, a03, a04; \\\n+        sph_u64 a10, a11, a12, a13, a14; \\\n+        sph_u64 a20, a21, a22, a23, a24; \\\n+        sph_u64 a30, a31, a32, a33, a34; \\\n+        sph_u64 a40, a41, a42, a43, a44;\n+ \n+#define READ_STATE(state)   do { \\\n+                a00 = (state)->u.wide[ 0]; \\\n+                a10 = (state)->u.wide[ 1]; \\\n+                a20 = (state)->u.wide[ 2]; \\\n+                a30 = (state)->u.wide[ 3]; \\\n+                a40 = (state)->u.wide[ 4]; \\\n+                a01 = (state)->u.wide[ 5]; \\\n+                a11 = (state)->u.wide[ 6]; \\\n+                a21 = (state)->u.wide[ 7]; \\\n+                a31 = (state)->u.wide[ 8]; \\\n+                a41 = (state)->u.wide[ 9]; \\\n+                a02 = (state)->u.wide[10]; \\\n+                a12 = (state)->u.wide[11]; \\\n+                a22 = (state)->u.wide[12]; \\\n+                a32 = (state)->u.wide[13]; \\\n+                a42 = (state)->u.wide[14]; \\\n+                a03 = (state)->u.wide[15]; \\\n+                a13 = (state)->u.wide[16]; \\\n+                a23 = (state)->u.wide[17]; \\\n+                a33 = (state)->u.wide[18]; \\\n+                a43 = (state)->u.wide[19]; \\\n+                a04 = (state)->u.wide[20]; \\\n+                a14 = (state)->u.wide[21]; \\\n+                a24 = (state)->u.wide[22]; \\\n+                a34 = (state)->u.wide[23]; \\\n+                a44 = (state)->u.wide[24]; \\\n+        } while (0)\n+ \n+#define WRITE_STATE(state)   do { \\\n+                (state)->u.wide[ 0] = a00; \\\n+                (state)->u.wide[ 1] = a10; \\\n+                (state)->u.wide[ 2] = a20; \\\n+                (state)->u.wide[ 3] = a30; \\\n+                (state)->u.wide[ 4] = a40; \\\n+                (state)->u.wide[ 5] = a01; \\\n+                (state)->u.wide[ 6] = a11; \\\n+                (state)->u.wide[ 7] = a21; \\\n+                (state)->u.wide[ 8] = a31; \\\n+                (state)->u.wide[ 9] = a41; \\\n+                (state)->u.wide[10] = a02; \\\n+                (state)->u.wide[11] = a12; \\\n+                (state)->u.wide[12] = a22; \\\n+                (state)->u.wide[13] = a32; \\\n+                (state)->u.wide[14] = a42; \\\n+                (state)->u.wide[15] = a03; \\\n+                (state)->u.wide[16] = a13; \\\n+                (state)->u.wide[17] = a23; \\\n+                (state)->u.wide[18] = a33; \\\n+                (state)->u.wide[19] = a43; \\\n+                (state)->u.wide[20] = a04; \\\n+                (state)->u.wide[21] = a14; \\\n+                (state)->u.wide[22] = a24; \\\n+                (state)->u.wide[23] = a34; \\\n+                (state)->u.wide[24] = a44; \\\n+        } while (0)\n+ \n+#define INPUT_BUF144   do { \\\n+                a00 ^= sph_dec64le_aligned(buf +   0); \\\n+                a10 ^= sph_dec64le_aligned(buf +   8); \\\n+                a20 ^= sph_dec64le_aligned(buf +  16); \\\n+                a30 ^= sph_dec64le_aligned(buf +  24); \\\n+                a40 ^= sph_dec64le_aligned(buf +  32); \\\n+                a01 ^= sph_dec64le_aligned(buf +  40); \\\n+                a11 ^= sph_dec64le_aligned(buf +  48); \\\n+                a21 ^= sph_dec64le_aligned(buf +  56); \\\n+                a31 ^= sph_dec64le_aligned(buf +  64); \\\n+                a41 ^= sph_dec64le_aligned(buf +  72); \\\n+                a02 ^= sph_dec64le_aligned(buf +  80); \\\n+                a12 ^= sph_dec64le_aligned(buf +  88); \\\n+                a22 ^= sph_dec64le_aligned(buf +  96); \\\n+                a32 ^= sph_dec64le_aligned(buf + 104); \\\n+                a42 ^= sph_dec64le_aligned(buf + 112); \\\n+                a03 ^= sph_dec64le_aligned(buf + 120); \\\n+                a13 ^= sph_dec64le_aligned(buf + 128); \\\n+                a23 ^= sph_dec64le_aligned(buf + 136); \\\n+        } while (0)\n+ \n+#define INPUT_BUF136   do { \\\n+                a00 ^= sph_dec64le_aligned(buf +   0); \\\n+                a10 ^= sph_dec64le_aligned(buf +   8); \\\n+                a20 ^= sph_dec64le_aligned(buf +  16); \\\n+                a30 ^= sph_dec64le_aligned(buf +  24); \\\n+                a40 ^= sph_dec64le_aligned(buf +  32); \\\n+                a01 ^= sph_dec64le_aligned(buf +  40); \\\n+                a11 ^= sph_dec64le_aligned(buf +  48); \\\n+                a21 ^= sph_dec64le_aligned(buf +  56); \\\n+                a31 ^= sph_dec64le_aligned(buf +  64); \\\n+                a41 ^= sph_dec64le_aligned(buf +  72); \\\n+                a02 ^= sph_dec64le_aligned(buf +  80); \\\n+                a12 ^= sph_dec64le_aligned(buf +  88); \\\n+                a22 ^= sph_dec64le_aligned(buf +  96); \\\n+                a32 ^= sph_dec64le_aligned(buf + 104); \\\n+                a42 ^= sph_dec64le_aligned(buf + 112); \\\n+                a03 ^= sph_dec64le_aligned(buf + 120); \\\n+                a13 ^= sph_dec64le_aligned(buf + 128); \\\n+        } while (0)\n+ \n+#define INPUT_BUF104   do { \\\n+                a00 ^= sph_dec64le_aligned(buf +   0); \\\n+                a10 ^= sph_dec64le_aligned(buf +   8); \\\n+                a20 ^= sph_dec64le_aligned(buf +  16); \\\n+                a30 ^= sph_dec64le_aligned(buf +  24); \\\n+                a40 ^= sph_dec64le_aligned(buf +  32); \\\n+                a01 ^= sph_dec64le_aligned(buf +  40); \\\n+                a11 ^= sph_dec64le_aligned(buf +  48); \\\n+                a21 ^= sph_dec64le_aligned(buf +  56); \\\n+                a31 ^= sph_dec64le_aligned(buf +  64); \\\n+                a41 ^= sph_dec64le_aligned(buf +  72); \\\n+                a02 ^= sph_dec64le_aligned(buf +  80); \\\n+                a12 ^= sph_dec64le_aligned(buf +  88); \\\n+                a22 ^= sph_dec64le_aligned(buf +  96); \\\n+        } while (0)\n+ \n+#define INPUT_BUF72   do { \\\n+                a00 ^= sph_dec64le_aligned(buf +   0); \\\n+                a10 ^= sph_dec64le_aligned(buf +   8); \\\n+                a20 ^= sph_dec64le_aligned(buf +  16); \\\n+                a30 ^= sph_dec64le_aligned(buf +  24); \\\n+                a40 ^= sph_dec64le_aligned(buf +  32); \\\n+                a01 ^= sph_dec64le_aligned(buf +  40); \\\n+                a11 ^= sph_dec64le_aligned(buf +  48); \\\n+                a21 ^= sph_dec64le_aligned(buf +  56); \\\n+                a31 ^= sph_dec64le_aligned(buf +  64); \\\n+        } while (0)\n+ \n+#define INPUT_BUF(lim)   do { \\\n+                a00 ^= sph_dec64le_aligned(buf +   0); \\\n+                a10 ^= sph_dec64le_aligned(buf +   8); \\\n+                a20 ^= sph_dec64le_aligned(buf +  16); \\\n+                a30 ^= sph_dec64le_aligned(buf +  24); \\\n+                a40 ^= sph_dec64le_aligned(buf +  32); \\\n+                a01 ^= sph_dec64le_aligned(buf +  40); \\\n+                a11 ^= sph_dec64le_aligned(buf +  48); \\\n+                a21 ^= sph_dec64le_aligned(buf +  56); \\\n+                a31 ^= sph_dec64le_aligned(buf +  64); \\\n+                if ((lim) == 72) \\\n+                        break; \\\n+                a41 ^= sph_dec64le_aligned(buf +  72); \\\n+                a02 ^= sph_dec64le_aligned(buf +  80); \\\n+                a12 ^= sph_dec64le_aligned(buf +  88); \\\n+                a22 ^= sph_dec64le_aligned(buf +  96); \\\n+                if ((lim) == 104) \\\n+                        break; \\\n+                a32 ^= sph_dec64le_aligned(buf + 104); \\\n+                a42 ^= sph_dec64le_aligned(buf + 112); \\\n+                a03 ^= sph_dec64le_aligned(buf + 120); \\\n+                a13 ^= sph_dec64le_aligned(buf + 128); \\\n+                if ((lim) == 136) \\\n+                        break; \\\n+                a23 ^= sph_dec64le_aligned(buf + 136); \\\n+        } while (0)\n+ \n+#endif\n+ \n+#define DECL64(x)        sph_u64 x\n+#define MOV64(d, s)      (d = s)\n+#define XOR64(d, a, b)   (d = a ^ b)\n+#define AND64(d, a, b)   (d = a & b)\n+#define OR64(d, a, b)    (d = a | b)\n+#define NOT64(d, s)      (d = SPH_T64(~s))\n+#define ROL64(d, v, n)   (d = SPH_ROTL64(v, n))\n+#define XOR64_IOTA       XOR64\n+ \n+#else\n+ \n+static const struct {\n+        sph_u32 high, low;\n+} RC[] = {\n+#if SPH_KECCAK_INTERLEAVE\n+        { SPH_C32(0x00000000), SPH_C32(0x00000001) },\n+        { SPH_C32(0x00000089), SPH_C32(0x00000000) },\n+        { SPH_C32(0x8000008B), SPH_C32(0x00000000) },\n+        { SPH_C32(0x80008080), SPH_C32(0x00000000) },\n+        { SPH_C32(0x0000008B), SPH_C32(0x00000001) },\n+        { SPH_C32(0x00008000), SPH_C32(0x00000001) },\n+        { SPH_C32(0x80008088), SPH_C32(0x00000001) },\n+        { SPH_C32(0x80000082), SPH_C32(0x00000001) },\n+        { SPH_C32(0x0000000B), SPH_C32(0x00000000) },\n+        { SPH_C32(0x0000000A), SPH_C32(0x00000000) },\n+        { SPH_C32(0x00008082), SPH_C32(0x00000001) },\n+        { SPH_C32(0x00008003), SPH_C32(0x00000000) },\n+        { SPH_C32(0x0000808B), SPH_C32(0x00000001) },\n+        { SPH_C32(0x8000000B), SPH_C32(0x00000001) },\n+        { SPH_C32(0x8000008A), SPH_C32(0x00000001) },\n+        { SPH_C32(0x80000081), SPH_C32(0x00000001) },\n+        { SPH_C32(0x80000081), SPH_C32(0x00000000) },\n+        { SPH_C32(0x80000008), SPH_C32(0x00000000) },\n+        { SPH_C32(0x00000083), SPH_C32(0x00000000) },\n+        { SPH_C32(0x80008003), SPH_C32(0x00000000) },\n+        { SPH_C32(0x80008088), SPH_C32(0x00000001) },\n+        { SPH_C32(0x80000088), SPH_C32(0x00000000) },\n+        { SPH_C32(0x00008000), SPH_C32(0x00000001) },\n+        { SPH_C32(0x80008082), SPH_C32(0x00000000) }\n+#else\n+        { SPH_C32(0x00000000), SPH_C32(0x00000001) },\n+        { SPH_C32(0x00000000), SPH_C32(0x00008082) },\n+        { SPH_C32(0x80000000), SPH_C32(0x0000808A) },\n+        { SPH_C32(0x80000000), SPH_C32(0x80008000) },\n+        { SPH_C32(0x00000000), SPH_C32(0x0000808B) },\n+        { SPH_C32(0x00000000), SPH_C32(0x80000001) },\n+        { SPH_C32(0x80000000), SPH_C32(0x80008081) },\n+        { SPH_C32(0x80000000), SPH_C32(0x00008009) },\n+        { SPH_C32(0x00000000), SPH_C32(0x0000008A) },\n+        { SPH_C32(0x00000000), SPH_C32(0x00000088) },\n+        { SPH_C32(0x00000000), SPH_C32(0x80008009) },\n+        { SPH_C32(0x00000000), SPH_C32(0x8000000A) },\n+        { SPH_C32(0x00000000), SPH_C32(0x8000808B) },\n+        { SPH_C32(0x80000000), SPH_C32(0x0000008B) },\n+        { SPH_C32(0x80000000), SPH_C32(0x00008089) },\n+        { SPH_C32(0x80000000), SPH_C32(0x00008003) },\n+        { SPH_C32(0x80000000), SPH_C32(0x00008002) },\n+        { SPH_C32(0x80000000), SPH_C32(0x00000080) },\n+        { SPH_C32(0x00000000), SPH_C32(0x0000800A) },\n+        { SPH_C32(0x80000000), SPH_C32(0x8000000A) },\n+        { SPH_C32(0x80000000), SPH_C32(0x80008081) },\n+        { SPH_C32(0x80000000), SPH_C32(0x00008080) },\n+        { SPH_C32(0x00000000), SPH_C32(0x80000001) },\n+        { SPH_C32(0x80000000), SPH_C32(0x80008008) }\n+#endif\n+};\n+ \n+#if SPH_KECCAK_INTERLEAVE\n+ \n+#define INTERLEAVE(xl, xh)   do { \\\n+                sph_u32 l, h, t; \\\n+                l = (xl); h = (xh); \\\n+                t = (l ^ (l >> 1)) & SPH_C32(0x22222222); l ^= t ^ (t << 1); \\\n+                t = (h ^ (h >> 1)) & SPH_C32(0x22222222); h ^= t ^ (t << 1); \\\n+                t = (l ^ (l >> 2)) & SPH_C32(0x0C0C0C0C); l ^= t ^ (t << 2); \\\n+                t = (h ^ (h >> 2)) & SPH_C32(0x0C0C0C0C); h ^= t ^ (t << 2); \\\n+                t = (l ^ (l >> 4)) & SPH_C32(0x00F000F0); l ^= t ^ (t << 4); \\\n+                t = (h ^ (h >> 4)) & SPH_C32(0x00F000F0); h ^= t ^ (t << 4); \\\n+                t = (l ^ (l >> 8)) & SPH_C32(0x0000FF00); l ^= t ^ (t << 8); \\\n+                t = (h ^ (h >> 8)) & SPH_C32(0x0000FF00); h ^= t ^ (t << 8); \\\n+                t = (l ^ SPH_T32(h << 16)) & SPH_C32(0xFFFF0000); \\\n+                l ^= t; h ^= t >> 16; \\\n+                (xl) = l; (xh) = h; \\\n+        } while (0)\n+ \n+#define UNINTERLEAVE(xl, xh)   do { \\\n+                sph_u32 l, h, t; \\\n+                l = (xl); h = (xh); \\\n+                t = (l ^ SPH_T32(h << 16)) & SPH_C32(0xFFFF0000); \\\n+                l ^= t; h ^= t >> 16; \\\n+                t = (l ^ (l >> 8)) & SPH_C32(0x0000FF00); l ^= t ^ (t << 8); \\\n+                t = (h ^ (h >> 8)) & SPH_C32(0x0000FF00); h ^= t ^ (t << 8); \\\n+                t = (l ^ (l >> 4)) & SPH_C32(0x00F000F0); l ^= t ^ (t << 4); \\\n+                t = (h ^ (h >> 4)) & SPH_C32(0x00F000F0); h ^= t ^ (t << 4); \\\n+                t = (l ^ (l >> 2)) & SPH_C32(0x0C0C0C0C); l ^= t ^ (t << 2); \\\n+                t = (h ^ (h >> 2)) & SPH_C32(0x0C0C0C0C); h ^= t ^ (t << 2); \\\n+                t = (l ^ (l >> 1)) & SPH_C32(0x22222222); l ^= t ^ (t << 1); \\\n+                t = (h ^ (h >> 1)) & SPH_C32(0x22222222); h ^= t ^ (t << 1); \\\n+                (xl) = l; (xh) = h; \\\n+        } while (0)\n+ \n+#else\n+ \n+#define INTERLEAVE(l, h)\n+#define UNINTERLEAVE(l, h)\n+ \n+#endif\n+ \n+#if SPH_KECCAK_NOCOPY\n+ \n+#define a00l   (kc->u.narrow[2 *  0 + 0])\n+#define a00h   (kc->u.narrow[2 *  0 + 1])\n+#define a10l   (kc->u.narrow[2 *  1 + 0])\n+#define a10h   (kc->u.narrow[2 *  1 + 1])\n+#define a20l   (kc->u.narrow[2 *  2 + 0])\n+#define a20h   (kc->u.narrow[2 *  2 + 1])\n+#define a30l   (kc->u.narrow[2 *  3 + 0])\n+#define a30h   (kc->u.narrow[2 *  3 + 1])\n+#define a40l   (kc->u.narrow[2 *  4 + 0])\n+#define a40h   (kc->u.narrow[2 *  4 + 1])\n+#define a01l   (kc->u.narrow[2 *  5 + 0])\n+#define a01h   (kc->u.narrow[2 *  5 + 1])\n+#define a11l   (kc->u.narrow[2 *  6 + 0])\n+#define a11h   (kc->u.narrow[2 *  6 + 1])\n+#define a21l   (kc->u.narrow[2 *  7 + 0])\n+#define a21h   (kc->u.narrow[2 *  7 + 1])\n+#define a31l   (kc->u.narrow[2 *  8 + 0])\n+#define a31h   (kc->u.narrow[2 *  8 + 1])\n+#define a41l   (kc->u.narrow[2 *  9 + 0])\n+#define a41h   (kc->u.narrow[2 *  9 + 1])\n+#define a02l   (kc->u.narrow[2 * 10 + 0])\n+#define a02h   (kc->u.narrow[2 * 10 + 1])\n+#define a12l   (kc->u.narrow[2 * 11 + 0])\n+#define a12h   (kc->u.narrow[2 * 11 + 1])\n+#define a22l   (kc->u.narrow[2 * 12 + 0])\n+#define a22h   (kc->u.narrow[2 * 12 + 1])\n+#define a32l   (kc->u.narrow[2 * 13 + 0])\n+#define a32h   (kc->u.narrow[2 * 13 + 1])\n+#define a42l   (kc->u.narrow[2 * 14 + 0])\n+#define a42h   (kc->u.narrow[2 * 14 + 1])\n+#define a03l   (kc->u.narrow[2 * 15 + 0])\n+#define a03h   (kc->u.narrow[2 * 15 + 1])\n+#define a13l   (kc->u.narrow[2 * 16 + 0])\n+#define a13h   (kc->u.narrow[2 * 16 + 1])\n+#define a23l   (kc->u.narrow[2 * 17 + 0])\n+#define a23h   (kc->u.narrow[2 * 17 + 1])\n+#define a33l   (kc->u.narrow[2 * 18 + 0])\n+#define a33h   (kc->u.narrow[2 * 18 + 1])\n+#define a43l   (kc->u.narrow[2 * 19 + 0])\n+#define a43h   (kc->u.narrow[2 * 19 + 1])\n+#define a04l   (kc->u.narrow[2 * 20 + 0])\n+#define a04h   (kc->u.narrow[2 * 20 + 1])\n+#define a14l   (kc->u.narrow[2 * 21 + 0])\n+#define a14h   (kc->u.narrow[2 * 21 + 1])\n+#define a24l   (kc->u.narrow[2 * 22 + 0])\n+#define a24h   (kc->u.narrow[2 * 22 + 1])\n+#define a34l   (kc->u.narrow[2 * 23 + 0])\n+#define a34h   (kc->u.narrow[2 * 23 + 1])\n+#define a44l   (kc->u.narrow[2 * 24 + 0])\n+#define a44h   (kc->u.narrow[2 * 24 + 1])\n+ \n+#define DECL_STATE\n+#define READ_STATE(state)\n+#define WRITE_STATE(state)\n+ \n+#define INPUT_BUF(size)   do { \\\n+                size_t j; \\\n+                for (j = 0; j < (size); j += 8) { \\\n+                        sph_u32 tl, th; \\\n+                        tl = sph_dec32le_aligned(buf + j + 0); \\\n+                        th = sph_dec32le_aligned(buf + j + 4); \\\n+                        INTERLEAVE(tl, th); \\\n+                        kc->u.narrow[(j >> 2) + 0] ^= tl; \\\n+                        kc->u.narrow[(j >> 2) + 1] ^= th; \\\n+                } \\\n+        } while (0)\n+ \n+#define INPUT_BUF144   INPUT_BUF(144)\n+#define INPUT_BUF136   INPUT_BUF(136)\n+#define INPUT_BUF104   INPUT_BUF(104)\n+#define INPUT_BUF72    INPUT_BUF(72)\n+ \n+#else\n+ \n+#define DECL_STATE \\\n+        sph_u32 a00l, a00h, a01l, a01h, a02l, a02h, a03l, a03h, a04l, a04h; \\\n+        sph_u32 a10l, a10h, a11l, a11h, a12l, a12h, a13l, a13h, a14l, a14h; \\\n+        sph_u32 a20l, a20h, a21l, a21h, a22l, a22h, a23l, a23h, a24l, a24h; \\\n+        sph_u32 a30l, a30h, a31l, a31h, a32l, a32h, a33l, a33h, a34l, a34h; \\\n+        sph_u32 a40l, a40h, a41l, a41h, a42l, a42h, a43l, a43h, a44l, a44h;\n+ \n+#define READ_STATE(state)   do { \\\n+                a00l = (state)->u.narrow[2 *  0 + 0]; \\\n+                a00h = (state)->u.narrow[2 *  0 + 1]; \\\n+                a10l = (state)->u.narrow[2 *  1 + 0]; \\\n+                a10h = (state)->u.narrow[2 *  1 + 1]; \\\n+                a20l = (state)->u.narrow[2 *  2 + 0]; \\\n+                a20h = (state)->u.narrow[2 *  2 + 1]; \\\n+                a30l = (state)->u.narrow[2 *  3 + 0]; \\\n+                a30h = (state)->u.narrow[2 *  3 + 1]; \\\n+                a40l = (state)->u.narrow[2 *  4 + 0]; \\\n+                a40h = (state)->u.narrow[2 *  4 + 1]; \\\n+                a01l = (state)->u.narrow[2 *  5 + 0]; \\\n+                a01h = (state)->u.narrow[2 *  5 + 1]; \\\n+                a11l = (state)->u.narrow[2 *  6 + 0]; \\\n+                a11h = (state)->u.narrow[2 *  6 + 1]; \\\n+                a21l = (state)->u.narrow[2 *  7 + 0]; \\\n+                a21h = (state)->u.narrow[2 *  7 + 1]; \\\n+                a31l = (state)->u.narrow[2 *  8 + 0]; \\\n+                a31h = (state)->u.narrow[2 *  8 + 1]; \\\n+                a41l = (state)->u.narrow[2 *  9 + 0]; \\\n+                a41h = (state)->u.narrow[2 *  9 + 1]; \\\n+                a02l = (state)->u.narrow[2 * 10 + 0]; \\\n+                a02h = (state)->u.narrow[2 * 10 + 1]; \\\n+                a12l = (state)->u.narrow[2 * 11 + 0]; \\\n+                a12h = (state)->u.narrow[2 * 11 + 1]; \\\n+                a22l = (state)->u.narrow[2 * 12 + 0]; \\\n+                a22h = (state)->u.narrow[2 * 12 + 1]; \\\n+                a32l = (state)->u.narrow[2 * 13 + 0]; \\\n+                a32h = (state)->u.narrow[2 * 13 + 1]; \\\n+                a42l = (state)->u.narrow[2 * 14 + 0]; \\\n+                a42h = (state)->u.narrow[2 * 14 + 1]; \\\n+                a03l = (state)->u.narrow[2 * 15 + 0]; \\\n+                a03h = (state)->u.narrow[2 * 15 + 1]; \\\n+                a13l = (state)->u.narrow[2 * 16 + 0]; \\\n+                a13h = (state)->u.narrow[2 * 16 + 1]; \\\n+                a23l = (state)->u.narrow[2 * 17 + 0]; \\\n+                a23h = (state)->u.narrow[2 * 17 + 1]; \\\n+                a33l = (state)->u.narrow[2 * 18 + 0]; \\\n+                a33h = (state)->u.narrow[2 * 18 + 1]; \\\n+                a43l = (state)->u.narrow[2 * 19 + 0]; \\\n+                a43h = (state)->u.narrow[2 * 19 + 1]; \\\n+                a04l = (state)->u.narrow[2 * 20 + 0]; \\\n+                a04h = (state)->u.narrow[2 * 20 + 1]; \\\n+                a14l = (state)->u.narrow[2 * 21 + 0]; \\\n+                a14h = (state)->u.narrow[2 * 21 + 1]; \\\n+                a24l = (state)->u.narrow[2 * 22 + 0]; \\\n+                a24h = (state)->u.narrow[2 * 22 + 1]; \\\n+                a34l = (state)->u.narrow[2 * 23 + 0]; \\\n+                a34h = (state)->u.narrow[2 * 23 + 1]; \\\n+                a44l = (state)->u.narrow[2 * 24 + 0]; \\\n+                a44h = (state)->u.narrow[2 * 24 + 1]; \\\n+        } while (0)\n+ \n+#define WRITE_STATE(state)   do { \\\n+                (state)->u.narrow[2 *  0 + 0] = a00l; \\\n+                (state)->u.narrow[2 *  0 + 1] = a00h; \\\n+                (state)->u.narrow[2 *  1 + 0] = a10l; \\\n+                (state)->u.narrow[2 *  1 + 1] = a10h; \\\n+                (state)->u.narrow[2 *  2 + 0] = a20l; \\\n+                (state)->u.narrow[2 *  2 + 1] = a20h; \\\n+                (state)->u.narrow[2 *  3 + 0] = a30l; \\\n+                (state)->u.narrow[2 *  3 + 1] = a30h; \\\n+                (state)->u.narrow[2 *  4 + 0] = a40l; \\\n+                (state)->u.narrow[2 *  4 + 1] = a40h; \\\n+                (state)->u.narrow[2 *  5 + 0] = a01l; \\\n+                (state)->u.narrow[2 *  5 + 1] = a01h; \\\n+                (state)->u.narrow[2 *  6 + 0] = a11l; \\\n+                (state)->u.narrow[2 *  6 + 1] = a11h; \\\n+                (state)->u.narrow[2 *  7 + 0] = a21l; \\\n+                (state)->u.narrow[2 *  7 + 1] = a21h; \\\n+                (state)->u.narrow[2 *  8 + 0] = a31l; \\\n+                (state)->u.narrow[2 *  8 + 1] = a31h; \\\n+                (state)->u.narrow[2 *  9 + 0] = a41l; \\\n+                (state)->u.narrow[2 *  9 + 1] = a41h; \\\n+                (state)->u.narrow[2 * 10 + 0] = a02l; \\\n+                (state)->u.narrow[2 * 10 + 1] = a02h; \\\n+                (state)->u.narrow[2 * 11 + 0] = a12l; \\\n+                (state)->u.narrow[2 * 11 + 1] = a12h; \\\n+                (state)->u.narrow[2 * 12 + 0] = a22l; \\\n+                (state)->u.narrow[2 * 12 + 1] = a22h; \\\n+                (state)->u.narrow[2 * 13 + 0] = a32l; \\\n+                (state)->u.narrow[2 * 13 + 1] = a32h; \\\n+                (state)->u.narrow[2 * 14 + 0] = a42l; \\\n+                (state)->u.narrow[2 * 14 + 1] = a42h; \\\n+                (state)->u.narrow[2 * 15 + 0] = a03l; \\\n+                (state)->u.narrow[2 * 15 + 1] = a03h; \\\n+                (state)->u.narrow[2 * 16 + 0] = a13l; \\\n+                (state)->u.narrow[2 * 16 + 1] = a13h; \\\n+                (state)->u.narrow[2 * 17 + 0] = a23l; \\\n+                (state)->u.narrow[2 * 17 + 1] = a23h; \\\n+                (state)->u.narrow[2 * 18 + 0] = a33l; \\\n+                (state)->u.narrow[2 * 18 + 1] = a33h; \\\n+                (state)->u.narrow[2 * 19 + 0] = a43l; \\\n+                (state)->u.narrow[2 * 19 + 1] = a43h; \\\n+                (state)->u.narrow[2 * 20 + 0] = a04l; \\\n+                (state)->u.narrow[2 * 20 + 1] = a04h; \\\n+                (state)->u.narrow[2 * 21 + 0] = a14l; \\\n+                (state)->u.narrow[2 * 21 + 1] = a14h; \\\n+                (state)->u.narrow[2 * 22 + 0] = a24l; \\\n+                (state)->u.narrow[2 * 22 + 1] = a24h; \\\n+                (state)->u.narrow[2 * 23 + 0] = a34l; \\\n+                (state)->u.narrow[2 * 23 + 1] = a34h; \\\n+                (state)->u.narrow[2 * 24 + 0] = a44l; \\\n+                (state)->u.narrow[2 * 24 + 1] = a44h; \\\n+        } while (0)\n+ \n+#define READ64(d, off)   do { \\\n+                sph_u32 tl, th; \\\n+                tl = sph_dec32le_aligned(buf + (off)); \\\n+                th = sph_dec32le_aligned(buf + (off) + 4); \\\n+                INTERLEAVE(tl, th); \\\n+                d ## l ^= tl; \\\n+                d ## h ^= th; \\\n+        } while (0)\n+ \n+#define INPUT_BUF144   do { \\\n+                READ64(a00,   0); \\\n+                READ64(a10,   8); \\\n+                READ64(a20,  16); \\\n+                READ64(a30,  24); \\\n+                READ64(a40,  32); \\\n+                READ64(a01,  40); \\\n+                READ64(a11,  48); \\\n+                READ64(a21,  56); \\\n+                READ64(a31,  64); \\\n+                READ64(a41,  72); \\\n+                READ64(a02,  80); \\\n+                READ64(a12,  88); \\\n+                READ64(a22,  96); \\\n+                READ64(a32, 104); \\\n+                READ64(a42, 112); \\\n+                READ64(a03, 120); \\\n+                READ64(a13, 128); \\\n+                READ64(a23, 136); \\\n+        } while (0)\n+ \n+#define INPUT_BUF136   do { \\\n+                READ64(a00,   0); \\\n+                READ64(a10,   8); \\\n+                READ64(a20,  16); \\\n+                READ64(a30,  24); \\\n+                READ64(a40,  32); \\\n+                READ64(a01,  40); \\\n+                READ64(a11,  48); \\\n+                READ64(a21,  56); \\\n+                READ64(a31,  64); \\\n+                READ64(a41,  72); \\\n+                READ64(a02,  80); \\\n+                READ64(a12,  88); \\\n+                READ64(a22,  96); \\\n+                READ64(a32, 104); \\\n+                READ64(a42, 112); \\\n+                READ64(a03, 120); \\\n+                READ64(a13, 128); \\\n+        } while (0)\n+ \n+#define INPUT_BUF104   do { \\\n+                READ64(a00,   0); \\\n+                READ64(a10,   8); \\\n+                READ64(a20,  16); \\\n+                READ64(a30,  24); \\\n+                READ64(a40,  32); \\\n+                READ64(a01,  40); \\\n+                READ64(a11,  48); \\\n+                READ64(a21,  56); \\\n+                READ64(a31,  64); \\\n+                READ64(a41,  72); \\\n+                READ64(a02,  80); \\\n+                READ64(a12,  88); \\\n+                READ64(a22,  96); \\\n+        } while (0)\n+ \n+#define INPUT_BUF72   do { \\\n+                READ64(a00,   0); \\\n+                READ64(a10,   8); \\\n+                READ64(a20,  16); \\\n+                READ64(a30,  24); \\\n+                READ64(a40,  32); \\\n+                READ64(a01,  40); \\\n+                READ64(a11,  48); \\\n+                READ64(a21,  56); \\\n+                READ64(a31,  64); \\\n+        } while (0)\n+ \n+#define INPUT_BUF(lim)   do { \\\n+                READ64(a00,   0); \\\n+                READ64(a10,   8); \\\n+                READ64(a20,  16); \\\n+                READ64(a30,  24); \\\n+                READ64(a40,  32); \\\n+                READ64(a01,  40); \\\n+                READ64(a11,  48); \\\n+                READ64(a21,  56); \\\n+                READ64(a31,  64); \\\n+                if ((lim) == 72) \\\n+                        break; \\\n+                READ64(a41,  72); \\\n+                READ64(a02,  80); \\\n+                READ64(a12,  88); \\\n+                READ64(a22,  96); \\\n+                if ((lim) == 104) \\\n+                        break; \\\n+                READ64(a32, 104); \\\n+                READ64(a42, 112); \\\n+                READ64(a03, 120); \\\n+                READ64(a13, 128); \\\n+                if ((lim) == 136) \\\n+                        break; \\\n+                READ64(a23, 136); \\\n+        } while (0)\n+ \n+#endif\n+ \n+#define DECL64(x)        sph_u64 x ## l, x ## h\n+#define MOV64(d, s)      (d ## l = s ## l, d ## h = s ## h)\n+#define XOR64(d, a, b)   (d ## l = a ## l ^ b ## l, d ## h = a ## h ^ b ## h)\n+#define AND64(d, a, b)   (d ## l = a ## l & b ## l, d ## h = a ## h & b ## h)\n+#define OR64(d, a, b)    (d ## l = a ## l | b ## l, d ## h = a ## h | b ## h)\n+#define NOT64(d, s)      (d ## l = SPH_T32(~s ## l), d ## h = SPH_T32(~s ## h))\n+#define ROL64(d, v, n)   ROL64_ ## n(d, v)\n+ \n+#if SPH_KECCAK_INTERLEAVE\n+ \n+#define ROL64_odd1(d, v)   do { \\\n+                sph_u32 tmp; \\\n+                tmp = v ## l; \\\n+                d ## l = SPH_T32(v ## h << 1) | (v ## h >> 31); \\\n+                d ## h = tmp; \\\n+        } while (0)\n+ \n+#define ROL64_odd63(d, v)   do { \\\n+                sph_u32 tmp; \\\n+                tmp = SPH_T32(v ## l << 31) | (v ## l >> 1); \\\n+                d ## l = v ## h; \\\n+                d ## h = tmp; \\\n+        } while (0)\n+ \n+#define ROL64_odd(d, v, n)   do { \\\n+                sph_u32 tmp; \\\n+                tmp = SPH_T32(v ## l << (n - 1)) | (v ## l >> (33 - n)); \\\n+                d ## l = SPH_T32(v ## h << n) | (v ## h >> (32 - n)); \\\n+                d ## h = tmp; \\\n+        } while (0)\n+ \n+#define ROL64_even(d, v, n)   do { \\\n+                d ## l = SPH_T32(v ## l << n) | (v ## l >> (32 - n)); \\\n+                d ## h = SPH_T32(v ## h << n) | (v ## h >> (32 - n)); \\\n+        } while (0)\n+ \n+#define ROL64_0(d, v)\n+#define ROL64_1(d, v)    ROL64_odd1(d, v)\n+#define ROL64_2(d, v)    ROL64_even(d, v,  1)\n+#define ROL64_3(d, v)    ROL64_odd( d, v,  2)\n+#define ROL64_4(d, v)    ROL64_even(d, v,  2)\n+#define ROL64_5(d, v)    ROL64_odd( d, v,  3)\n+#define ROL64_6(d, v)    ROL64_even(d, v,  3)\n+#define ROL64_7(d, v)    ROL64_odd( d, v,  4)\n+#define ROL64_8(d, v)    ROL64_even(d, v,  4)\n+#define ROL64_9(d, v)    ROL64_odd( d, v,  5)\n+#define ROL64_10(d, v)   ROL64_even(d, v,  5)\n+#define ROL64_11(d, v)   ROL64_odd( d, v,  6)\n+#define ROL64_12(d, v)   ROL64_even(d, v,  6)\n+#define ROL64_13(d, v)   ROL64_odd( d, v,  7)\n+#define ROL64_14(d, v)   ROL64_even(d, v,  7)\n+#define ROL64_15(d, v)   ROL64_odd( d, v,  8)\n+#define ROL64_16(d, v)   ROL64_even(d, v,  8)\n+#define ROL64_17(d, v)   ROL64_odd( d, v,  9)\n+#define ROL64_18(d, v)   ROL64_even(d, v,  9)\n+#define ROL64_19(d, v)   ROL64_odd( d, v, 10)\n+#define ROL64_20(d, v)   ROL64_even(d, v, 10)\n+#define ROL64_21(d, v)   ROL64_odd( d, v, 11)\n+#define ROL64_22(d, v)   ROL64_even(d, v, 11)\n+#define ROL64_23(d, v)   ROL64_odd( d, v, 12)\n+#define ROL64_24(d, v)   ROL64_even(d, v, 12)\n+#define ROL64_25(d, v)   ROL64_odd( d, v, 13)\n+#define ROL64_26(d, v)   ROL64_even(d, v, 13)\n+#define ROL64_27(d, v)   ROL64_odd( d, v, 14)\n+#define ROL64_28(d, v)   ROL64_even(d, v, 14)\n+#define ROL64_29(d, v)   ROL64_odd( d, v, 15)\n+#define ROL64_30(d, v)   ROL64_even(d, v, 15)\n+#define ROL64_31(d, v)   ROL64_odd( d, v, 16)\n+#define ROL64_32(d, v)   ROL64_even(d, v, 16)\n+#define ROL64_33(d, v)   ROL64_odd( d, v, 17)\n+#define ROL64_34(d, v)   ROL64_even(d, v, 17)\n+#define ROL64_35(d, v)   ROL64_odd( d, v, 18)\n+#define ROL64_36(d, v)   ROL64_even(d, v, 18)\n+#define ROL64_37(d, v)   ROL64_odd( d, v, 19)\n+#define ROL64_38(d, v)   ROL64_even(d, v, 19)\n+#define ROL64_39(d, v)   ROL64_odd( d, v, 20)\n+#define ROL64_40(d, v)   ROL64_even(d, v, 20)\n+#define ROL64_41(d, v)   ROL64_odd( d, v, 21)\n+#define ROL64_42(d, v)   ROL64_even(d, v, 21)\n+#define ROL64_43(d, v)   ROL64_odd( d, v, 22)\n+#define ROL64_44(d, v)   ROL64_even(d, v, 22)\n+#define ROL64_45(d, v)   ROL64_odd( d, v, 23)\n+#define ROL64_46(d, v)   ROL64_even(d, v, 23)\n+#define ROL64_47(d, v)   ROL64_odd( d, v, 24)\n+#define ROL64_48(d, v)   ROL64_even(d, v, 24)\n+#define ROL64_49(d, v)   ROL64_odd( d, v, 25)\n+#define ROL64_50(d, v)   ROL64_even(d, v, 25)\n+#define ROL64_51(d, v)   ROL64_odd( d, v, 26)\n+#define ROL64_52(d, v)   ROL64_even(d, v, 26)\n+#define ROL64_53(d, v)   ROL64_odd( d, v, 27)\n+#define ROL64_54(d, v)   ROL64_even(d, v, 27)\n+#define ROL64_55(d, v)   ROL64_odd( d, v, 28)\n+#define ROL64_56(d, v)   ROL64_even(d, v, 28)\n+#define ROL64_57(d, v)   ROL64_odd( d, v, 29)\n+#define ROL64_58(d, v)   ROL64_even(d, v, 29)\n+#define ROL64_59(d, v)   ROL64_odd( d, v, 30)\n+#define ROL64_60(d, v)   ROL64_even(d, v, 30)\n+#define ROL64_61(d, v)   ROL64_odd( d, v, 31)\n+#define ROL64_62(d, v)   ROL64_even(d, v, 31)\n+#define ROL64_63(d, v)   ROL64_odd63(d, v)\n+ \n+#else\n+ \n+#define ROL64_small(d, v, n)   do { \\\n+                sph_u32 tmp; \\\n+                tmp = SPH_T32(v ## l << n) | (v ## h >> (32 - n)); \\\n+                d ## h = SPH_T32(v ## h << n) | (v ## l >> (32 - n)); \\\n+                d ## l = tmp; \\\n+        } while (0)\n+ \n+#define ROL64_0(d, v)    0\n+#define ROL64_1(d, v)    ROL64_small(d, v, 1)\n+#define ROL64_2(d, v)    ROL64_small(d, v, 2)\n+#define ROL64_3(d, v)    ROL64_small(d, v, 3)\n+#define ROL64_4(d, v)    ROL64_small(d, v, 4)\n+#define ROL64_5(d, v)    ROL64_small(d, v, 5)\n+#define ROL64_6(d, v)    ROL64_small(d, v, 6)\n+#define ROL64_7(d, v)    ROL64_small(d, v, 7)\n+#define ROL64_8(d, v)    ROL64_small(d, v, 8)\n+#define ROL64_9(d, v)    ROL64_small(d, v, 9)\n+#define ROL64_10(d, v)   ROL64_small(d, v, 10)\n+#define ROL64_11(d, v)   ROL64_small(d, v, 11)\n+#define ROL64_12(d, v)   ROL64_small(d, v, 12)\n+#define ROL64_13(d, v)   ROL64_small(d, v, 13)\n+#define ROL64_14(d, v)   ROL64_small(d, v, 14)\n+#define ROL64_15(d, v)   ROL64_small(d, v, 15)\n+#define ROL64_16(d, v)   ROL64_small(d, v, 16)\n+#define ROL64_17(d, v)   ROL64_small(d, v, 17)\n+#define ROL64_18(d, v)   ROL64_small(d, v, 18)\n+#define ROL64_19(d, v)   ROL64_small(d, v, 19)\n+#define ROL64_20(d, v)   ROL64_small(d, v, 20)\n+#define ROL64_21(d, v)   ROL64_small(d, v, 21)\n+#define ROL64_22(d, v)   ROL64_small(d, v, 22)\n+#define ROL64_23(d, v)   ROL64_small(d, v, 23)\n+#define ROL64_24(d, v)   ROL64_small(d, v, 24)\n+#define ROL64_25(d, v)   ROL64_small(d, v, 25)\n+#define ROL64_26(d, v)   ROL64_small(d, v, 26)\n+#define ROL64_27(d, v)   ROL64_small(d, v, 27)\n+#define ROL64_28(d, v)   ROL64_small(d, v, 28)\n+#define ROL64_29(d, v)   ROL64_small(d, v, 29)\n+#define ROL64_30(d, v)   ROL64_small(d, v, 30)\n+#define ROL64_31(d, v)   ROL64_small(d, v, 31)\n+ \n+#define ROL64_32(d, v)   do { \\\n+                sph_u32 tmp; \\\n+                tmp = v ## l; \\\n+                d ## l = v ## h; \\\n+                d ## h = tmp; \\\n+        } while (0)\n+ \n+#define ROL64_big(d, v, n)   do { \\\n+                sph_u32 trl, trh; \\\n+                ROL64_small(tr, v, n); \\\n+                d ## h = trl; \\\n+                d ## l = trh; \\\n+        } while (0)\n+ \n+#define ROL64_33(d, v)   ROL64_big(d, v, 1)\n+#define ROL64_34(d, v)   ROL64_big(d, v, 2)\n+#define ROL64_35(d, v)   ROL64_big(d, v, 3)\n+#define ROL64_36(d, v)   ROL64_big(d, v, 4)\n+#define ROL64_37(d, v)   ROL64_big(d, v, 5)\n+#define ROL64_38(d, v)   ROL64_big(d, v, 6)\n+#define ROL64_39(d, v)   ROL64_big(d, v, 7)\n+#define ROL64_40(d, v)   ROL64_big(d, v, 8)\n+#define ROL64_41(d, v)   ROL64_big(d, v, 9)\n+#define ROL64_42(d, v)   ROL64_big(d, v, 10)\n+#define ROL64_43(d, v)   ROL64_big(d, v, 11)\n+#define ROL64_44(d, v)   ROL64_big(d, v, 12)\n+#define ROL64_45(d, v)   ROL64_big(d, v, 13)\n+#define ROL64_46(d, v)   ROL64_big(d, v, 14)\n+#define ROL64_47(d, v)   ROL64_big(d, v, 15)\n+#define ROL64_48(d, v)   ROL64_big(d, v, 16)\n+#define ROL64_49(d, v)   ROL64_big(d, v, 17)\n+#define ROL64_50(d, v)   ROL64_big(d, v, 18)\n+#define ROL64_51(d, v)   ROL64_big(d, v, 19)\n+#define ROL64_52(d, v)   ROL64_big(d, v, 20)\n+#define ROL64_53(d, v)   ROL64_big(d, v, 21)\n+#define ROL64_54(d, v)   ROL64_big(d, v, 22)\n+#define ROL64_55(d, v)   ROL64_big(d, v, 23)\n+#define ROL64_56(d, v)   ROL64_big(d, v, 24)\n+#define ROL64_57(d, v)   ROL64_big(d, v, 25)\n+#define ROL64_58(d, v)   ROL64_big(d, v, 26)\n+#define ROL64_59(d, v)   ROL64_big(d, v, 27)\n+#define ROL64_60(d, v)   ROL64_big(d, v, 28)\n+#define ROL64_61(d, v)   ROL64_big(d, v, 29)\n+#define ROL64_62(d, v)   ROL64_big(d, v, 30)\n+#define ROL64_63(d, v)   ROL64_big(d, v, 31)\n+ \n+#endif\n+ \n+#define XOR64_IOTA(d, s, k) \\\n+        (d ## l = s ## l ^ k.low, d ## h = s ## h ^ k.high)\n+ \n+#endif\n+ \n+#define TH_ELT(t, c0, c1, c2, c3, c4, d0, d1, d2, d3, d4)   do { \\\n+                DECL64(tt0); \\\n+                DECL64(tt1); \\\n+                DECL64(tt2); \\\n+                DECL64(tt3); \\\n+                XOR64(tt0, d0, d1); \\\n+                XOR64(tt1, d2, d3); \\\n+                XOR64(tt0, tt0, d4); \\\n+                XOR64(tt0, tt0, tt1); \\\n+                ROL64(tt0, tt0, 1); \\\n+                XOR64(tt2, c0, c1); \\\n+                XOR64(tt3, c2, c3); \\\n+                XOR64(tt0, tt0, c4); \\\n+                XOR64(tt2, tt2, tt3); \\\n+                XOR64(t, tt0, tt2); \\\n+        } while (0)\n+ \n+#define THETA(b00, b01, b02, b03, b04, b10, b11, b12, b13, b14, \\\n+        b20, b21, b22, b23, b24, b30, b31, b32, b33, b34, \\\n+        b40, b41, b42, b43, b44) \\\n+        do { \\\n+                DECL64(t0); \\\n+                DECL64(t1); \\\n+                DECL64(t2); \\\n+                DECL64(t3); \\\n+                DECL64(t4); \\\n+                TH_ELT(t0, b40, b41, b42, b43, b44, b10, b11, b12, b13, b14); \\\n+                TH_ELT(t1, b00, b01, b02, b03, b04, b20, b21, b22, b23, b24); \\\n+                TH_ELT(t2, b10, b11, b12, b13, b14, b30, b31, b32, b33, b34); \\\n+                TH_ELT(t3, b20, b21, b22, b23, b24, b40, b41, b42, b43, b44); \\\n+                TH_ELT(t4, b30, b31, b32, b33, b34, b00, b01, b02, b03, b04); \\\n+                XOR64(b00, b00, t0); \\\n+                XOR64(b01, b01, t0); \\\n+                XOR64(b02, b02, t0); \\\n+                XOR64(b03, b03, t0); \\\n+                XOR64(b04, b04, t0); \\\n+                XOR64(b10, b10, t1); \\\n+                XOR64(b11, b11, t1); \\\n+                XOR64(b12, b12, t1); \\\n+                XOR64(b13, b13, t1); \\\n+                XOR64(b14, b14, t1); \\\n+                XOR64(b20, b20, t2); \\\n+                XOR64(b21, b21, t2); \\\n+                XOR64(b22, b22, t2); \\\n+                XOR64(b23, b23, t2); \\\n+                XOR64(b24, b24, t2); \\\n+                XOR64(b30, b30, t3); \\\n+                XOR64(b31, b31, t3); \\\n+                XOR64(b32, b32, t3); \\\n+                XOR64(b33, b33, t3); \\\n+                XOR64(b34, b34, t3); \\\n+                XOR64(b40, b40, t4); \\\n+                XOR64(b41, b41, t4); \\\n+                XOR64(b42, b42, t4); \\\n+                XOR64(b43, b43, t4); \\\n+                XOR64(b44, b44, t4); \\\n+        } while (0)\n+ \n+#define RHO(b00, b01, b02, b03, b04, b10, b11, b12, b13, b14, \\\n+        b20, b21, b22, b23, b24, b30, b31, b32, b33, b34, \\\n+        b40, b41, b42, b43, b44) \\\n+        do { \\\n+                /* ROL64(b00, b00,  0); */ \\\n+                ROL64(b01, b01, 36); \\\n+                ROL64(b02, b02,  3); \\\n+                ROL64(b03, b03, 41); \\\n+                ROL64(b04, b04, 18); \\\n+                ROL64(b10, b10,  1); \\\n+                ROL64(b11, b11, 44); \\\n+                ROL64(b12, b12, 10); \\\n+                ROL64(b13, b13, 45); \\\n+                ROL64(b14, b14,  2); \\\n+                ROL64(b20, b20, 62); \\\n+                ROL64(b21, b21,  6); \\\n+                ROL64(b22, b22, 43); \\\n+                ROL64(b23, b23, 15); \\\n+                ROL64(b24, b24, 61); \\\n+                ROL64(b30, b30, 28); \\\n+                ROL64(b31, b31, 55); \\\n+                ROL64(b32, b32, 25); \\\n+                ROL64(b33, b33, 21); \\\n+                ROL64(b34, b34, 56); \\\n+                ROL64(b40, b40, 27); \\\n+                ROL64(b41, b41, 20); \\\n+                ROL64(b42, b42, 39); \\\n+                ROL64(b43, b43,  8); \\\n+                ROL64(b44, b44, 14); \\\n+        } while (0)\n+ \n+/*\n+ * The KHI macro integrates the \"lane complement\" optimization. On input,\n+ * some words are complemented:\n+ *    a00 a01 a02 a04 a13 a20 a21 a22 a30 a33 a34 a43\n+ * On output, the following words are complemented:\n+ *    a04 a10 a20 a22 a23 a31\n+ *\n+ * The (implicit) permutation and the theta expansion will bring back\n+ * the input mask for the next round.\n+ */\n+ \n+#define KHI_XO(d, a, b, c)   do { \\\n+                DECL64(kt); \\\n+                OR64(kt, b, c); \\\n+                XOR64(d, a, kt); \\\n+        } while (0)\n+ \n+#define KHI_XA(d, a, b, c)   do { \\\n+                DECL64(kt); \\\n+                AND64(kt, b, c); \\\n+                XOR64(d, a, kt); \\\n+        } while (0)\n+ \n+#define KHI(b00, b01, b02, b03, b04, b10, b11, b12, b13, b14, \\\n+        b20, b21, b22, b23, b24, b30, b31, b32, b33, b34, \\\n+        b40, b41, b42, b43, b44) \\\n+        do { \\\n+                DECL64(c0); \\\n+                DECL64(c1); \\\n+                DECL64(c2); \\\n+                DECL64(c3); \\\n+                DECL64(c4); \\\n+                DECL64(bnn); \\\n+                NOT64(bnn, b20); \\\n+                KHI_XO(c0, b00, b10, b20); \\\n+                KHI_XO(c1, b10, bnn, b30); \\\n+                KHI_XA(c2, b20, b30, b40); \\\n+                KHI_XO(c3, b30, b40, b00); \\\n+                KHI_XA(c4, b40, b00, b10); \\\n+                MOV64(b00, c0); \\\n+                MOV64(b10, c1); \\\n+                MOV64(b20, c2); \\\n+                MOV64(b30, c3); \\\n+                MOV64(b40, c4); \\\n+                NOT64(bnn, b41); \\\n+                KHI_XO(c0, b01, b11, b21); \\\n+                KHI_XA(c1, b11, b21, b31); \\\n+                KHI_XO(c2, b21, b31, bnn); \\\n+                KHI_XO(c3, b31, b41, b01); \\\n+                KHI_XA(c4, b41, b01, b11); \\\n+                MOV64(b01, c0); \\\n+                MOV64(b11, c1); \\\n+                MOV64(b21, c2); \\\n+                MOV64(b31, c3); \\\n+                MOV64(b41, c4); \\\n+                NOT64(bnn, b32); \\\n+                KHI_XO(c0, b02, b12, b22); \\\n+                KHI_XA(c1, b12, b22, b32); \\\n+                KHI_XA(c2, b22, bnn, b42); \\\n+                KHI_XO(c3, bnn, b42, b02); \\\n+                KHI_XA(c4, b42, b02, b12); \\\n+                MOV64(b02, c0); \\\n+                MOV64(b12, c1); \\\n+                MOV64(b22, c2); \\\n+                MOV64(b32, c3); \\\n+                MOV64(b42, c4); \\\n+                NOT64(bnn, b33); \\\n+                KHI_XA(c0, b03, b13, b23); \\\n+                KHI_XO(c1, b13, b23, b33); \\\n+                KHI_XO(c2, b23, bnn, b43); \\\n+                KHI_XA(c3, bnn, b43, b03); \\\n+                KHI_XO(c4, b43, b03, b13); \\\n+                MOV64(b03, c0); \\\n+                MOV64(b13, c1); \\\n+                MOV64(b23, c2); \\\n+                MOV64(b33, c3); \\\n+                MOV64(b43, c4); \\\n+                NOT64(bnn, b14); \\\n+                KHI_XA(c0, b04, bnn, b24); \\\n+                KHI_XO(c1, bnn, b24, b34); \\\n+                KHI_XA(c2, b24, b34, b44); \\\n+                KHI_XO(c3, b34, b44, b04); \\\n+                KHI_XA(c4, b44, b04, b14); \\\n+                MOV64(b04, c0); \\\n+                MOV64(b14, c1); \\\n+                MOV64(b24, c2); \\\n+                MOV64(b34, c3); \\\n+                MOV64(b44, c4); \\\n+        } while (0)\n+ \n+#define IOTA(r)   XOR64_IOTA(a00, a00, r)\n+ \n+#define P0    a00, a01, a02, a03, a04, a10, a11, a12, a13, a14, a20, a21, \\\n+              a22, a23, a24, a30, a31, a32, a33, a34, a40, a41, a42, a43, a44\n+#define P1    a00, a30, a10, a40, a20, a11, a41, a21, a01, a31, a22, a02, \\\n+              a32, a12, a42, a33, a13, a43, a23, a03, a44, a24, a04, a34, a14\n+#define P2    a00, a33, a11, a44, a22, a41, a24, a02, a30, a13, a32, a10, \\\n+              a43, a21, a04, a23, a01, a34, a12, a40, a14, a42, a20, a03, a31\n+#define P3    a00, a23, a41, a14, a32, a24, a42, a10, a33, a01, a43, a11, \\\n+              a34, a02, a20, a12, a30, a03, a21, a44, a31, a04, a22, a40, a13\n+#define P4    a00, a12, a24, a31, a43, a42, a04, a11, a23, a30, a34, a41, \\\n+              a03, a10, a22, a21, a33, a40, a02, a14, a13, a20, a32, a44, a01\n+#define P5    a00, a21, a42, a13, a34, a04, a20, a41, a12, a33, a03, a24, \\\n+              a40, a11, a32, a02, a23, a44, a10, a31, a01, a22, a43, a14, a30\n+#define P6    a00, a02, a04, a01, a03, a20, a22, a24, a21, a23, a40, a42, \\\n+              a44, a41, a43, a10, a12, a14, a11, a13, a30, a32, a34, a31, a33\n+#define P7    a00, a10, a20, a30, a40, a22, a32, a42, a02, a12, a44, a04, \\\n+              a14, a24, a34, a11, a21, a31, a41, a01, a33, a43, a03, a13, a23\n+#define P8    a00, a11, a22, a33, a44, a32, a43, a04, a10, a21, a14, a20, \\\n+              a31, a42, a03, a41, a02, a13, a24, a30, a23, a34, a40, a01, a12\n+#define P9    a00, a41, a32, a23, a14, a43, a34, a20, a11, a02, a31, a22, \\\n+              a13, a04, a40, a24, a10, a01, a42, a33, a12, a03, a44, a30, a21\n+#define P10   a00, a24, a43, a12, a31, a34, a03, a22, a41, a10, a13, a32, \\\n+              a01, a20, a44, a42, a11, a30, a04, a23, a21, a40, a14, a33, a02\n+#define P11   a00, a42, a34, a21, a13, a03, a40, a32, a24, a11, a01, a43, \\\n+              a30, a22, a14, a04, a41, a33, a20, a12, a02, a44, a31, a23, a10\n+#define P12   a00, a04, a03, a02, a01, a40, a44, a43, a42, a41, a30, a34, \\\n+              a33, a32, a31, a20, a24, a23, a22, a21, a10, a14, a13, a12, a11\n+#define P13   a00, a20, a40, a10, a30, a44, a14, a34, a04, a24, a33, a03, \\\n+              a23, a43, a13, a22, a42, a12, a32, a02, a11, a31, a01, a21, a41\n+#define P14   a00, a22, a44, a11, a33, a14, a31, a03, a20, a42, a23, a40, \\\n+              a12, a34, a01, a32, a04, a21, a43, a10, a41, a13, a30, a02, a24\n+#define P15   a00, a32, a14, a41, a23, a31, a13, a40, a22, a04, a12, a44, \\\n+              a21, a03, a30, a43, a20, a02, a34, a11, a24, a01, a33, a10, a42\n+#define P16   a00, a43, a31, a24, a12, a13, a01, a44, a32, a20, a21, a14, \\\n+              a02, a40, a33, a34, a22, a10, a03, a41, a42, a30, a23, a11, a04\n+#define P17   a00, a34, a13, a42, a21, a01, a30, a14, a43, a22, a02, a31, \\\n+              a10, a44, a23, a03, a32, a11, a40, a24, a04, a33, a12, a41, a20\n+#define P18   a00, a03, a01, a04, a02, a30, a33, a31, a34, a32, a10, a13, \\\n+              a11, a14, a12, a40, a43, a41, a44, a42, a20, a23, a21, a24, a22\n+#define P19   a00, a40, a30, a20, a10, a33, a23, a13, a03, a43, a11, a01, \\\n+              a41, a31, a21, a44, a34, a24, a14, a04, a22, a12, a02, a42, a32\n+#define P20   a00, a44, a33, a22, a11, a23, a12, a01, a40, a34, a41, a30, \\\n+              a24, a13, a02, a14, a03, a42, a31, a20, a32, a21, a10, a04, a43\n+#define P21   a00, a14, a23, a32, a41, a12, a21, a30, a44, a03, a24, a33, \\\n+              a42, a01, a10, a31, a40, a04, a13, a22, a43, a02, a11, a20, a34\n+#define P22   a00, a31, a12, a43, a24, a21, a02, a33, a14, a40, a42, a23, \\\n+              a04, a30, a11, a13, a44, a20, a01, a32, a34, a10, a41, a22, a03\n+#define P23   a00, a13, a21, a34, a42, a02, a10, a23, a31, a44, a04, a12, \\\n+              a20, a33, a41, a01, a14, a22, a30, a43, a03, a11, a24, a32, a40\n+ \n+#define P1_TO_P0   do { \\\n+                DECL64(t); \\\n+                MOV64(t, a01); \\\n+                MOV64(a01, a30); \\\n+                MOV64(a30, a33); \\\n+                MOV64(a33, a23); \\\n+                MOV64(a23, a12); \\\n+                MOV64(a12, a21); \\\n+                MOV64(a21, a02); \\\n+                MOV64(a02, a10); \\\n+                MOV64(a10, a11); \\\n+                MOV64(a11, a41); \\\n+                MOV64(a41, a24); \\\n+                MOV64(a24, a42); \\\n+                MOV64(a42, a04); \\\n+                MOV64(a04, a20); \\\n+                MOV64(a20, a22); \\\n+                MOV64(a22, a32); \\\n+                MOV64(a32, a43); \\\n+                MOV64(a43, a34); \\\n+                MOV64(a34, a03); \\\n+                MOV64(a03, a40); \\\n+                MOV64(a40, a44); \\\n+                MOV64(a44, a14); \\\n+                MOV64(a14, a31); \\\n+                MOV64(a31, a13); \\\n+                MOV64(a13, t); \\\n+        } while (0)\n+ \n+#define P2_TO_P0   do { \\\n+                DECL64(t); \\\n+                MOV64(t, a01); \\\n+                MOV64(a01, a33); \\\n+                MOV64(a33, a12); \\\n+                MOV64(a12, a02); \\\n+                MOV64(a02, a11); \\\n+                MOV64(a11, a24); \\\n+                MOV64(a24, a04); \\\n+                MOV64(a04, a22); \\\n+                MOV64(a22, a43); \\\n+                MOV64(a43, a03); \\\n+                MOV64(a03, a44); \\\n+                MOV64(a44, a31); \\\n+                MOV64(a31, t); \\\n+                MOV64(t, a10); \\\n+                MOV64(a10, a41); \\\n+                MOV64(a41, a42); \\\n+                MOV64(a42, a20); \\\n+                MOV64(a20, a32); \\\n+                MOV64(a32, a34); \\\n+                MOV64(a34, a40); \\\n+                MOV64(a40, a14); \\\n+                MOV64(a14, a13); \\\n+                MOV64(a13, a30); \\\n+                MOV64(a30, a23); \\\n+                MOV64(a23, a21); \\\n+                MOV64(a21, t); \\\n+        } while (0)\n+ \n+#define P4_TO_P0   do { \\\n+                DECL64(t); \\\n+                MOV64(t, a01); \\\n+                MOV64(a01, a12); \\\n+                MOV64(a12, a11); \\\n+                MOV64(a11, a04); \\\n+                MOV64(a04, a43); \\\n+                MOV64(a43, a44); \\\n+                MOV64(a44, t); \\\n+                MOV64(t, a02); \\\n+                MOV64(a02, a24); \\\n+                MOV64(a24, a22); \\\n+                MOV64(a22, a03); \\\n+                MOV64(a03, a31); \\\n+                MOV64(a31, a33); \\\n+                MOV64(a33, t); \\\n+                MOV64(t, a10); \\\n+                MOV64(a10, a42); \\\n+                MOV64(a42, a32); \\\n+                MOV64(a32, a40); \\\n+                MOV64(a40, a13); \\\n+                MOV64(a13, a23); \\\n+                MOV64(a23, t); \\\n+                MOV64(t, a14); \\\n+                MOV64(a14, a30); \\\n+                MOV64(a30, a21); \\\n+                MOV64(a21, a41); \\\n+                MOV64(a41, a20); \\\n+                MOV64(a20, a34); \\\n+                MOV64(a34, t); \\\n+        } while (0)\n+ \n+#define P6_TO_P0   do { \\\n+                DECL64(t); \\\n+                MOV64(t, a01); \\\n+                MOV64(a01, a02); \\\n+                MOV64(a02, a04); \\\n+                MOV64(a04, a03); \\\n+                MOV64(a03, t); \\\n+                MOV64(t, a10); \\\n+                MOV64(a10, a20); \\\n+                MOV64(a20, a40); \\\n+                MOV64(a40, a30); \\\n+                MOV64(a30, t); \\\n+                MOV64(t, a11); \\\n+                MOV64(a11, a22); \\\n+                MOV64(a22, a44); \\\n+                MOV64(a44, a33); \\\n+                MOV64(a33, t); \\\n+                MOV64(t, a12); \\\n+                MOV64(a12, a24); \\\n+                MOV64(a24, a43); \\\n+                MOV64(a43, a31); \\\n+                MOV64(a31, t); \\\n+                MOV64(t, a13); \\\n+                MOV64(a13, a21); \\\n+                MOV64(a21, a42); \\\n+                MOV64(a42, a34); \\\n+                MOV64(a34, t); \\\n+                MOV64(t, a14); \\\n+                MOV64(a14, a23); \\\n+                MOV64(a23, a41); \\\n+                MOV64(a41, a32); \\\n+                MOV64(a32, t); \\\n+        } while (0)\n+ \n+#define P8_TO_P0   do { \\\n+                DECL64(t); \\\n+                MOV64(t, a01); \\\n+                MOV64(a01, a11); \\\n+                MOV64(a11, a43); \\\n+                MOV64(a43, t); \\\n+                MOV64(t, a02); \\\n+                MOV64(a02, a22); \\\n+                MOV64(a22, a31); \\\n+                MOV64(a31, t); \\\n+                MOV64(t, a03); \\\n+                MOV64(a03, a33); \\\n+                MOV64(a33, a24); \\\n+                MOV64(a24, t); \\\n+                MOV64(t, a04); \\\n+                MOV64(a04, a44); \\\n+                MOV64(a44, a12); \\\n+                MOV64(a12, t); \\\n+                MOV64(t, a10); \\\n+                MOV64(a10, a32); \\\n+                MOV64(a32, a13); \\\n+                MOV64(a13, t); \\\n+                MOV64(t, a14); \\\n+                MOV64(a14, a21); \\\n+                MOV64(a21, a20); \\\n+                MOV64(a20, t); \\\n+                MOV64(t, a23); \\\n+                MOV64(a23, a42); \\\n+                MOV64(a42, a40); \\\n+                MOV64(a40, t); \\\n+                MOV64(t, a30); \\\n+                MOV64(a30, a41); \\\n+                MOV64(a41, a34); \\\n+                MOV64(a34, t); \\\n+        } while (0)\n+ \n+#define P12_TO_P0   do { \\\n+                DECL64(t); \\\n+                MOV64(t, a01); \\\n+                MOV64(a01, a04); \\\n+                MOV64(a04, t); \\\n+                MOV64(t, a02); \\\n+                MOV64(a02, a03); \\\n+                MOV64(a03, t); \\\n+                MOV64(t, a10); \\\n+                MOV64(a10, a40); \\\n+                MOV64(a40, t); \\\n+                MOV64(t, a11); \\\n+                MOV64(a11, a44); \\\n+                MOV64(a44, t); \\\n+                MOV64(t, a12); \\\n+                MOV64(a12, a43); \\\n+                MOV64(a43, t); \\\n+                MOV64(t, a13); \\\n+                MOV64(a13, a42); \\\n+                MOV64(a42, t); \\\n+                MOV64(t, a14); \\\n+                MOV64(a14, a41); \\\n+                MOV64(a41, t); \\\n+                MOV64(t, a20); \\\n+                MOV64(a20, a30); \\\n+                MOV64(a30, t); \\\n+                MOV64(t, a21); \\\n+                MOV64(a21, a34); \\\n+                MOV64(a34, t); \\\n+                MOV64(t, a22); \\\n+                MOV64(a22, a33); \\\n+                MOV64(a33, t); \\\n+                MOV64(t, a23); \\\n+                MOV64(a23, a32); \\\n+                MOV64(a32, t); \\\n+                MOV64(t, a24); \\\n+                MOV64(a24, a31); \\\n+                MOV64(a31, t); \\\n+        } while (0)\n+ \n+#define LPAR   (\n+#define RPAR   )\n+ \n+#define KF_ELT(r, s, k)   do { \\\n+                THETA LPAR P ## r RPAR; \\\n+                RHO LPAR P ## r RPAR; \\\n+                KHI LPAR P ## s RPAR; \\\n+                IOTA(k); \\\n+        } while (0)\n+ \n+#define DO(x)   x\n+ \n+#define KECCAK_F_1600   DO(KECCAK_F_1600_)\n+ \n+#if SPH_KECCAK_UNROLL == 1\n+ \n+#define KECCAK_F_1600_   do { \\\n+                int j; \\\n+                for (j = 0; j < 24; j ++) { \\\n+                        KF_ELT( 0,  1, RC[j + 0]); \\\n+                        P1_TO_P0; \\\n+                } \\\n+        } while (0)\n+ \n+#elif SPH_KECCAK_UNROLL == 2\n+ \n+#define KECCAK_F_1600_   do { \\\n+                int j; \\\n+                for (j = 0; j < 24; j += 2) { \\\n+                        KF_ELT( 0,  1, RC[j + 0]); \\\n+                        KF_ELT( 1,  2, RC[j + 1]); \\\n+                        P2_TO_P0; \\\n+                } \\\n+        } while (0)\n+ \n+#elif SPH_KECCAK_UNROLL == 4\n+ \n+#define KECCAK_F_1600_   do { \\\n+                int j; \\\n+                for (j = 0; j < 24; j += 4) { \\\n+                        KF_ELT( 0,  1, RC[j + 0]); \\\n+                        KF_ELT( 1,  2, RC[j + 1]); \\\n+                        KF_ELT( 2,  3, RC[j + 2]); \\\n+                        KF_ELT( 3,  4, RC[j + 3]); \\\n+                        P4_TO_P0; \\\n+                } \\\n+        } while (0)\n+ \n+#elif SPH_KECCAK_UNROLL == 6\n+ \n+#define KECCAK_F_1600_   do { \\\n+                int j; \\\n+                for (j = 0; j < 24; j += 6) { \\\n+                        KF_ELT( 0,  1, RC[j + 0]); \\\n+                        KF_ELT( 1,  2, RC[j + 1]); \\\n+                        KF_ELT( 2,  3, RC[j + 2]); \\\n+                        KF_ELT( 3,  4, RC[j + 3]); \\\n+                        KF_ELT( 4,  5, RC[j + 4]); \\\n+                        KF_ELT( 5,  6, RC[j + 5]); \\\n+                        P6_TO_P0; \\\n+                } \\\n+        } while (0)\n+ \n+#elif SPH_KECCAK_UNROLL == 8\n+ \n+#define KECCAK_F_1600_   do { \\\n+                int j; \\\n+                for (j = 0; j < 24; j += 8) { \\\n+                        KF_ELT( 0,  1, RC[j + 0]); \\\n+                        KF_ELT( 1,  2, RC[j + 1]); \\\n+                        KF_ELT( 2,  3, RC[j + 2]); \\\n+                        KF_ELT( 3,  4, RC[j + 3]); \\\n+                        KF_ELT( 4,  5, RC[j + 4]); \\\n+                        KF_ELT( 5,  6, RC[j + 5]); \\\n+                        KF_ELT( 6,  7, RC[j + 6]); \\\n+                        KF_ELT( 7,  8, RC[j + 7]); \\\n+                        P8_TO_P0; \\\n+                } \\\n+        } while (0)\n+ \n+#elif SPH_KECCAK_UNROLL == 12\n+ \n+#define KECCAK_F_1600_   do { \\\n+                int j; \\\n+                for (j = 0; j < 24; j += 12) { \\\n+                        KF_ELT( 0,  1, RC[j +  0]); \\\n+                        KF_ELT( 1,  2, RC[j +  1]); \\\n+                        KF_ELT( 2,  3, RC[j +  2]); \\\n+                        KF_ELT( 3,  4, RC[j +  3]); \\\n+                        KF_ELT( 4,  5, RC[j +  4]); \\\n+                        KF_ELT( 5,  6, RC[j +  5]); \\\n+                        KF_ELT( 6,  7, RC[j +  6]); \\\n+                        KF_ELT( 7,  8, RC[j +  7]); \\\n+                        KF_ELT( 8,  9, RC[j +  8]); \\\n+                        KF_ELT( 9, 10, RC[j +  9]); \\\n+                        KF_ELT(10, 11, RC[j + 10]); \\\n+                        KF_ELT(11, 12, RC[j + 11]); \\\n+                        P12_TO_P0; \\\n+                } \\\n+        } while (0)\n+ \n+#elif SPH_KECCAK_UNROLL == 0\n+ \n+#define KECCAK_F_1600_   do { \\\n+                KF_ELT( 0,  1, RC[ 0]); \\\n+                KF_ELT( 1,  2, RC[ 1]); \\\n+                KF_ELT( 2,  3, RC[ 2]); \\\n+                KF_ELT( 3,  4, RC[ 3]); \\\n+                KF_ELT( 4,  5, RC[ 4]); \\\n+                KF_ELT( 5,  6, RC[ 5]); \\\n+                KF_ELT( 6,  7, RC[ 6]); \\\n+                KF_ELT( 7,  8, RC[ 7]); \\\n+                KF_ELT( 8,  9, RC[ 8]); \\\n+                KF_ELT( 9, 10, RC[ 9]); \\\n+                KF_ELT(10, 11, RC[10]); \\\n+                KF_ELT(11, 12, RC[11]); \\\n+                KF_ELT(12, 13, RC[12]); \\\n+                KF_ELT(13, 14, RC[13]); \\\n+                KF_ELT(14, 15, RC[14]); \\\n+                KF_ELT(15, 16, RC[15]); \\\n+                KF_ELT(16, 17, RC[16]); \\\n+                KF_ELT(17, 18, RC[17]); \\\n+                KF_ELT(18, 19, RC[18]); \\\n+                KF_ELT(19, 20, RC[19]); \\\n+                KF_ELT(20, 21, RC[20]); \\\n+                KF_ELT(21, 22, RC[21]); \\\n+                KF_ELT(22, 23, RC[22]); \\\n+                KF_ELT(23,  0, RC[23]); \\\n+        } while (0)\n+ \n+#else\n+ \n+#error Unimplemented unroll count for Keccak.\n+ \n+#endif\n+ \n+static void\n+keccak_init(void *kcv, unsigned out_size)\n+{\n+        sph_keccak_context* kc = (sph_keccak_context*)kcv;\n+        int i;\n+ \n+#if SPH_KECCAK_64\n+        for (i = 0; i < 25; i ++)\n+                kc->u.wide[i] = 0;\n+        /*\n+         * Initialization for the \"lane complement\".\n+         */\n+        kc->u.wide[ 1] = SPH_C64(0xFFFFFFFFFFFFFFFF);\n+        kc->u.wide[ 2] = SPH_C64(0xFFFFFFFFFFFFFFFF);\n+        kc->u.wide[ 8] = SPH_C64(0xFFFFFFFFFFFFFFFF);\n+        kc->u.wide[12] = SPH_C64(0xFFFFFFFFFFFFFFFF);\n+        kc->u.wide[17] = SPH_C64(0xFFFFFFFFFFFFFFFF);\n+        kc->u.wide[20] = SPH_C64(0xFFFFFFFFFFFFFFFF);\n+#else\n+ \n+        for (i = 0; i < 50; i ++)\n+                kc->u.narrow[i] = 0;\n+        /*\n+         * Initialization for the \"lane complement\".\n+         * Note: since we set to all-one full 64-bit words,\n+         * interleaving (if applicable) is a no-op.\n+         */\n+        kc->u.narrow[ 2] = SPH_C32(0xFFFFFFFF);\n+        kc->u.narrow[ 3] = SPH_C32(0xFFFFFFFF);\n+        kc->u.narrow[ 4] = SPH_C32(0xFFFFFFFF);\n+        kc->u.narrow[ 5] = SPH_C32(0xFFFFFFFF);\n+        kc->u.narrow[16] = SPH_C32(0xFFFFFFFF);\n+        kc->u.narrow[17] = SPH_C32(0xFFFFFFFF);\n+        kc->u.narrow[24] = SPH_C32(0xFFFFFFFF);\n+        kc->u.narrow[25] = SPH_C32(0xFFFFFFFF);\n+        kc->u.narrow[34] = SPH_C32(0xFFFFFFFF);\n+        kc->u.narrow[35] = SPH_C32(0xFFFFFFFF);\n+        kc->u.narrow[40] = SPH_C32(0xFFFFFFFF);\n+        kc->u.narrow[41] = SPH_C32(0xFFFFFFFF);\n+#endif\n+        kc->ptr = 0;\n+        kc->lim = 200 - (out_size >> 2);\n+}\n+ \n+static void\n+keccak_core(void *kcv, const void *data, size_t len, size_t lim)\n+{\n+ \n+        sph_keccak_context* kc = (sph_keccak_context*)kcv;\n+        unsigned char *buf;\n+        size_t ptr;\n+        DECL_STATE\n+ \n+        buf = kc->buf;\n+        ptr = kc->ptr;\n+ \n+        if (len < (lim - ptr)) {\n+                memcpy(buf + ptr, data, len);\n+                kc->ptr = ptr + len;\n+                return;\n+        }\n+ \n+        READ_STATE(kc);\n+        while (len > 0) {\n+                size_t clen;\n+ \n+                clen = (lim - ptr);\n+                if (clen > len)\n+                        clen = len;\n+                memcpy(buf + ptr, data, clen);\n+                ptr += clen;\n+                data = (const unsigned char *)data + clen;\n+                len -= clen;\n+                if (ptr == lim) {\n+                        INPUT_BUF(lim);\n+                        KECCAK_F_1600;\n+                        ptr = 0;\n+                }\n+        }\n+        WRITE_STATE(kc);\n+        kc->ptr = ptr;\n+}\n+ \n+#if SPH_KECCAK_64\n+ \n+#define DEFCLOSE(d, lim) \\\n+        static void keccak_close ## d( \\\n+                void *kcv, unsigned ub, unsigned n, void *dst) \\\n+        { \\\n+                sph_keccak_context* kc = (sph_keccak_context*)kcv; \\\n+                unsigned eb; \\\n+                union { \\\n+                        unsigned char tmp[lim + 1]; \\\n+                        sph_u64 dummy;   /* for alignment */ \\\n+                } u; \\\n+                size_t j; \\\n+ \\\n+                eb = (0x100 | (ub & 0xFF)) >> (8 - n); \\\n+                if (kc->ptr == (lim - 1)) { \\\n+                        if (n == 7) { \\\n+                                u.tmp[0] = eb; \\\n+                                memset(u.tmp + 1, 0, lim - 1); \\\n+                                u.tmp[lim] = 0x80; \\\n+                                j = 1 + lim; \\\n+                        } else { \\\n+                                u.tmp[0] = eb | 0x80; \\\n+                                j = 1; \\\n+                        } \\\n+                } else { \\\n+                        j = lim - kc->ptr; \\\n+                        u.tmp[0] = eb; \\\n+                        memset(u.tmp + 1, 0, j - 2); \\\n+                        u.tmp[j - 1] = 0x80; \\\n+                } \\\n+                keccak_core(kc, u.tmp, j, lim); \\\n+                /* Finalize the \"lane complement\" */ \\\n+                kc->u.wide[ 1] = ~kc->u.wide[ 1]; \\\n+                kc->u.wide[ 2] = ~kc->u.wide[ 2]; \\\n+                kc->u.wide[ 8] = ~kc->u.wide[ 8]; \\\n+                kc->u.wide[12] = ~kc->u.wide[12]; \\\n+                kc->u.wide[17] = ~kc->u.wide[17]; \\\n+                kc->u.wide[20] = ~kc->u.wide[20]; \\\n+                for (j = 0; j < d; j += 8) \\\n+                        sph_enc64le_aligned(u.tmp + j, kc->u.wide[j >> 3]); \\\n+                memcpy(dst, u.tmp, d); \\\n+                keccak_init(kc, (unsigned)d << 3); \\\n+        } \\\n+ \n+#else\n+ \n+#define DEFCLOSE(d, lim) \\\n+        static void keccak_close ## d( \\\n+                void *kcv, unsigned ub, unsigned n, void *dst) \\\n+        { \\\n+                sph_keccak_context* kc = (sph_keccak_context*)kcv; \\\n+                unsigned eb; \\\n+                union { \\\n+                        unsigned char tmp[lim + 1]; \\\n+                        sph_u64 dummy;   /* for alignment */ \\\n+                } u; \\\n+                size_t j; \\\n+ \\\n+                eb = (0x100 | (ub & 0xFF)) >> (8 - n); \\\n+                if (kc->ptr == (lim - 1)) { \\\n+                        if (n == 7) { \\\n+                                u.tmp[0] = eb; \\\n+                                memset(u.tmp + 1, 0, lim - 1); \\\n+                                u.tmp[lim] = 0x80; \\\n+                                j = 1 + lim; \\\n+                        } else { \\\n+                                u.tmp[0] = eb | 0x80; \\\n+                                j = 1; \\\n+                        } \\\n+                } else { \\\n+                        j = lim - kc->ptr; \\\n+                        u.tmp[0] = eb; \\\n+                        memset(u.tmp + 1, 0, j - 2); \\\n+                        u.tmp[j - 1] = 0x80; \\\n+                } \\\n+                keccak_core(kc, u.tmp, j, lim); \\\n+                /* Finalize the \"lane complement\" */ \\\n+                kc->u.narrow[ 2] = ~kc->u.narrow[ 2]; \\\n+                kc->u.narrow[ 3] = ~kc->u.narrow[ 3]; \\\n+                kc->u.narrow[ 4] = ~kc->u.narrow[ 4]; \\\n+                kc->u.narrow[ 5] = ~kc->u.narrow[ 5]; \\\n+                kc->u.narrow[16] = ~kc->u.narrow[16]; \\\n+                kc->u.narrow[17] = ~kc->u.narrow[17]; \\\n+                kc->u.narrow[24] = ~kc->u.narrow[24]; \\\n+                kc->u.narrow[25] = ~kc->u.narrow[25]; \\\n+                kc->u.narrow[34] = ~kc->u.narrow[34]; \\\n+                kc->u.narrow[35] = ~kc->u.narrow[35]; \\\n+                kc->u.narrow[40] = ~kc->u.narrow[40]; \\\n+                kc->u.narrow[41] = ~kc->u.narrow[41]; \\\n+                /* un-interleave */ \\\n+                for (j = 0; j < 50; j += 2) \\\n+                        UNINTERLEAVE(kc->u.narrow[j], kc->u.narrow[j + 1]); \\\n+                for (j = 0; j < d; j += 4) \\\n+                        sph_enc32le_aligned(u.tmp + j, kc->u.narrow[j >> 2]); \\\n+                memcpy(dst, u.tmp, d); \\\n+                keccak_init(kc, (unsigned)d << 3); \\\n+        } \\\n+ \n+#endif\n+ \n+DEFCLOSE(28, 144)\n+DEFCLOSE(32, 136)\n+DEFCLOSE(48, 104)\n+DEFCLOSE(64, 72)\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak224_init(void *cc)\n+{\n+        keccak_init(cc, 224);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak224(void *cc, const void *data, size_t len)\n+{\n+        keccak_core(cc, data, len, 144);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak224_close(void *cc, void *dst)\n+{\n+        sph_keccak224_addbits_and_close(cc, 0, 0, dst);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak224_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)\n+{\n+        keccak_close28(cc, ub, n, dst);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak256_init(void *cc)\n+{\n+        keccak_init(cc, 256);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak256(void *cc, const void *data, size_t len)\n+{\n+        keccak_core(cc, data, len, 136);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak256_close(void *cc, void *dst)\n+{\n+        sph_keccak256_addbits_and_close(cc, 0, 0, dst);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak256_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)\n+{\n+        keccak_close32(cc, ub, n, dst);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak384_init(void *cc)\n+{\n+        keccak_init(cc, 384);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak384(void *cc, const void *data, size_t len)\n+{\n+        keccak_core(cc, data, len, 104);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak384_close(void *cc, void *dst)\n+{\n+        sph_keccak384_addbits_and_close(cc, 0, 0, dst);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak384_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)\n+{\n+        keccak_close48(cc, ub, n, dst);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak512_init(void *cc)\n+{\n+        keccak_init(cc, 512);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak512(void *cc, const void *data, size_t len)\n+{\n+        keccak_core(cc, data, len, 72);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak512_close(void *cc, void *dst)\n+{\n+        sph_keccak512_addbits_and_close(cc, 0, 0, dst);\n+}\n+ \n+/* see sph_keccak.h */\n+void\n+sph_keccak512_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)\n+{\n+        keccak_close64(cc, ub, n, dst);\n+}\n+ \n+ \n+#ifdef __cplusplus\n+}\n+#endif"
      },
      {
        "sha": "7353a0bc57b66720b994fd34d3b9280732e8d416",
        "filename": "src/primitives/block.cpp",
        "status": "modified",
        "additions": 16,
        "deletions": 1,
        "changes": 17,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/8d3a84c242598ef3cdc733e99dddebfecdad84a6/src/primitives/block.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/8d3a84c242598ef3cdc733e99dddebfecdad84a6/src/primitives/block.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/primitives/block.cpp?ref=8d3a84c242598ef3cdc733e99dddebfecdad84a6",
        "patch": "@@ -9,10 +9,25 @@\n #include \"tinyformat.h\"\n #include \"utilstrencodings.h\"\n #include \"crypto/common.h\"\n+#include \"sph_keccak.h\"\n+#include \"streams.h\"\n \n uint256 CBlockHeader::GetHash() const\n {\n-    return SerializeHash(*this);\n+    if (nTime < 0x57100000)  // 2016 Apr 14 20:39:28 UTC\n+        return SerializeHash(*this);\n+\n+    CDataStream ss(SER_GETHASH, PROTOCOL_VERSION);\n+    ss << *this;\n+\n+    sph_keccak256_context ctx_keccak;\n+    uint256 hash;\n+\n+    sph_keccak256_init(&ctx_keccak);\n+    sph_keccak256(&ctx_keccak, (void*)&*ss.begin(), ss.size());\n+    sph_keccak256_close(&ctx_keccak, static_cast<void*>(&hash));\n+\n+    return hash;\n }\n \n std::string CBlock::ToString() const"
      },
      {
        "sha": "bdafdb88db020e381ab3b8ad544201b0c08da062",
        "filename": "src/sph_keccak.h",
        "status": "added",
        "additions": 293,
        "deletions": 0,
        "changes": 293,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/8d3a84c242598ef3cdc733e99dddebfecdad84a6/src/sph_keccak.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/8d3a84c242598ef3cdc733e99dddebfecdad84a6/src/sph_keccak.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/sph_keccak.h?ref=8d3a84c242598ef3cdc733e99dddebfecdad84a6",
        "patch": "@@ -0,0 +1,293 @@\n+/* $Id: sph_keccak.h 216 2010-06-08 09:46:57Z tp $ */\n+/**\n+ * Keccak interface. This is the interface for Keccak with the\n+ * recommended parameters for SHA-3, with output lengths 224, 256,\n+ * 384 and 512 bits.\n+ *\n+ * ==========================(LICENSE BEGIN)============================\n+ *\n+ * Copyright (c) 2007-2010  Projet RNRT SAPHIR\n+ * \n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files (the\n+ * \"Software\"), to deal in the Software without restriction, including\n+ * without limitation the rights to use, copy, modify, merge, publish,\n+ * distribute, sublicense, and/or sell copies of the Software, and to\n+ * permit persons to whom the Software is furnished to do so, subject to\n+ * the following conditions:\n+ * \n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ * \n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ *\n+ * ===========================(LICENSE END)=============================\n+ *\n+ * @file     sph_keccak.h\n+ * @author   Thomas Pornin <thomas.pornin@cryptolog.com>\n+ */\n+\n+#ifndef SPH_KECCAK_H__\n+#define SPH_KECCAK_H__\n+\n+#ifdef __cplusplus\n+extern \"C\"{\n+#endif\n+\n+#include <stddef.h>\n+#include \"sph_types.h\"\n+\n+/**\n+ * Output size (in bits) for Keccak-224.\n+ */\n+#define SPH_SIZE_keccak224   224\n+\n+/**\n+ * Output size (in bits) for Keccak-256.\n+ */\n+#define SPH_SIZE_keccak256   256\n+\n+/**\n+ * Output size (in bits) for Keccak-384.\n+ */\n+#define SPH_SIZE_keccak384   384\n+\n+/**\n+ * Output size (in bits) for Keccak-512.\n+ */\n+#define SPH_SIZE_keccak512   512\n+\n+/**\n+ * This structure is a context for Keccak computations: it contains the\n+ * intermediate values and some data from the last entered block. Once a\n+ * Keccak computation has been performed, the context can be reused for\n+ * another computation.\n+ *\n+ * The contents of this structure are private. A running Keccak computation\n+ * can be cloned by copying the context (e.g. with a simple\n+ * <code>memcpy()</code>).\n+ */\n+typedef struct {\n+#ifndef DOXYGEN_IGNORE\n+\tunsigned char buf[144];    /* first field, for alignment */\n+\tsize_t ptr, lim;\n+\tunion {\n+#if SPH_64\n+\t\tsph_u64 wide[25];\n+#endif\n+\t\tsph_u32 narrow[50];\n+\t} u;\n+#endif\n+} sph_keccak_context;\n+\n+/**\n+ * Type for a Keccak-224 context (identical to the common context).\n+ */\n+typedef sph_keccak_context sph_keccak224_context;\n+\n+/**\n+ * Type for a Keccak-256 context (identical to the common context).\n+ */\n+typedef sph_keccak_context sph_keccak256_context;\n+\n+/**\n+ * Type for a Keccak-384 context (identical to the common context).\n+ */\n+typedef sph_keccak_context sph_keccak384_context;\n+\n+/**\n+ * Type for a Keccak-512 context (identical to the common context).\n+ */\n+typedef sph_keccak_context sph_keccak512_context;\n+\n+/**\n+ * Initialize a Keccak-224 context. This process performs no memory allocation.\n+ *\n+ * @param cc   the Keccak-224 context (pointer to a\n+ *             <code>sph_keccak224_context</code>)\n+ */\n+void sph_keccak224_init(void *cc);\n+\n+/**\n+ * Process some data bytes. It is acceptable that <code>len</code> is zero\n+ * (in which case this function does nothing).\n+ *\n+ * @param cc     the Keccak-224 context\n+ * @param data   the input data\n+ * @param len    the input data length (in bytes)\n+ */\n+void sph_keccak224(void *cc, const void *data, size_t len);\n+\n+/**\n+ * Terminate the current Keccak-224 computation and output the result into\n+ * the provided buffer. The destination buffer must be wide enough to\n+ * accomodate the result (28 bytes). The context is automatically\n+ * reinitialized.\n+ *\n+ * @param cc    the Keccak-224 context\n+ * @param dst   the destination buffer\n+ */\n+void sph_keccak224_close(void *cc, void *dst);\n+\n+/**\n+ * Add a few additional bits (0 to 7) to the current computation, then\n+ * terminate it and output the result in the provided buffer, which must\n+ * be wide enough to accomodate the result (28 bytes). If bit number i\n+ * in <code>ub</code> has value 2^i, then the extra bits are those\n+ * numbered 7 downto 8-n (this is the big-endian convention at the byte\n+ * level). The context is automatically reinitialized.\n+ *\n+ * @param cc    the Keccak-224 context\n+ * @param ub    the extra bits\n+ * @param n     the number of extra bits (0 to 7)\n+ * @param dst   the destination buffer\n+ */\n+void sph_keccak224_addbits_and_close(\n+\tvoid *cc, unsigned ub, unsigned n, void *dst);\n+\n+/**\n+ * Initialize a Keccak-256 context. This process performs no memory allocation.\n+ *\n+ * @param cc   the Keccak-256 context (pointer to a\n+ *             <code>sph_keccak256_context</code>)\n+ */\n+void sph_keccak256_init(void *cc);\n+\n+/**\n+ * Process some data bytes. It is acceptable that <code>len</code> is zero\n+ * (in which case this function does nothing).\n+ *\n+ * @param cc     the Keccak-256 context\n+ * @param data   the input data\n+ * @param len    the input data length (in bytes)\n+ */\n+void sph_keccak256(void *cc, const void *data, size_t len);\n+\n+/**\n+ * Terminate the current Keccak-256 computation and output the result into\n+ * the provided buffer. The destination buffer must be wide enough to\n+ * accomodate the result (32 bytes). The context is automatically\n+ * reinitialized.\n+ *\n+ * @param cc    the Keccak-256 context\n+ * @param dst   the destination buffer\n+ */\n+void sph_keccak256_close(void *cc, void *dst);\n+\n+/**\n+ * Add a few additional bits (0 to 7) to the current computation, then\n+ * terminate it and output the result in the provided buffer, which must\n+ * be wide enough to accomodate the result (32 bytes). If bit number i\n+ * in <code>ub</code> has value 2^i, then the extra bits are those\n+ * numbered 7 downto 8-n (this is the big-endian convention at the byte\n+ * level). The context is automatically reinitialized.\n+ *\n+ * @param cc    the Keccak-256 context\n+ * @param ub    the extra bits\n+ * @param n     the number of extra bits (0 to 7)\n+ * @param dst   the destination buffer\n+ */\n+void sph_keccak256_addbits_and_close(\n+\tvoid *cc, unsigned ub, unsigned n, void *dst);\n+\n+/**\n+ * Initialize a Keccak-384 context. This process performs no memory allocation.\n+ *\n+ * @param cc   the Keccak-384 context (pointer to a\n+ *             <code>sph_keccak384_context</code>)\n+ */\n+void sph_keccak384_init(void *cc);\n+\n+/**\n+ * Process some data bytes. It is acceptable that <code>len</code> is zero\n+ * (in which case this function does nothing).\n+ *\n+ * @param cc     the Keccak-384 context\n+ * @param data   the input data\n+ * @param len    the input data length (in bytes)\n+ */\n+void sph_keccak384(void *cc, const void *data, size_t len);\n+\n+/**\n+ * Terminate the current Keccak-384 computation and output the result into\n+ * the provided buffer. The destination buffer must be wide enough to\n+ * accomodate the result (48 bytes). The context is automatically\n+ * reinitialized.\n+ *\n+ * @param cc    the Keccak-384 context\n+ * @param dst   the destination buffer\n+ */\n+void sph_keccak384_close(void *cc, void *dst);\n+\n+/**\n+ * Add a few additional bits (0 to 7) to the current computation, then\n+ * terminate it and output the result in the provided buffer, which must\n+ * be wide enough to accomodate the result (48 bytes). If bit number i\n+ * in <code>ub</code> has value 2^i, then the extra bits are those\n+ * numbered 7 downto 8-n (this is the big-endian convention at the byte\n+ * level). The context is automatically reinitialized.\n+ *\n+ * @param cc    the Keccak-384 context\n+ * @param ub    the extra bits\n+ * @param n     the number of extra bits (0 to 7)\n+ * @param dst   the destination buffer\n+ */\n+void sph_keccak384_addbits_and_close(\n+\tvoid *cc, unsigned ub, unsigned n, void *dst);\n+\n+/**\n+ * Initialize a Keccak-512 context. This process performs no memory allocation.\n+ *\n+ * @param cc   the Keccak-512 context (pointer to a\n+ *             <code>sph_keccak512_context</code>)\n+ */\n+void sph_keccak512_init(void *cc);\n+\n+/**\n+ * Process some data bytes. It is acceptable that <code>len</code> is zero\n+ * (in which case this function does nothing).\n+ *\n+ * @param cc     the Keccak-512 context\n+ * @param data   the input data\n+ * @param len    the input data length (in bytes)\n+ */\n+void sph_keccak512(void *cc, const void *data, size_t len);\n+\n+/**\n+ * Terminate the current Keccak-512 computation and output the result into\n+ * the provided buffer. The destination buffer must be wide enough to\n+ * accomodate the result (64 bytes). The context is automatically\n+ * reinitialized.\n+ *\n+ * @param cc    the Keccak-512 context\n+ * @param dst   the destination buffer\n+ */\n+void sph_keccak512_close(void *cc, void *dst);\n+\n+/**\n+ * Add a few additional bits (0 to 7) to the current computation, then\n+ * terminate it and output the result in the provided buffer, which must\n+ * be wide enough to accomodate the result (64 bytes). If bit number i\n+ * in <code>ub</code> has value 2^i, then the extra bits are those\n+ * numbered 7 downto 8-n (this is the big-endian convention at the byte\n+ * level). The context is automatically reinitialized.\n+ *\n+ * @param cc    the Keccak-512 context\n+ * @param ub    the extra bits\n+ * @param n     the number of extra bits (0 to 7)\n+ * @param dst   the destination buffer\n+ */\n+void sph_keccak512_addbits_and_close(\n+\tvoid *cc, unsigned ub, unsigned n, void *dst);\n+\n+#ifdef __cplusplus\n+}\n+#endif\n+\n+#endif"
      },
      {
        "sha": "7295b0b37097af5e94e5478496975e59c2c060a5",
        "filename": "src/sph_types.h",
        "status": "added",
        "additions": 1976,
        "deletions": 0,
        "changes": 1976,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/8d3a84c242598ef3cdc733e99dddebfecdad84a6/src/sph_types.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/8d3a84c242598ef3cdc733e99dddebfecdad84a6/src/sph_types.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/sph_types.h?ref=8d3a84c242598ef3cdc733e99dddebfecdad84a6",
        "patch": "@@ -0,0 +1,1976 @@\n+/* $Id: sph_types.h 260 2011-07-21 01:02:38Z tp $ */\n+/**\n+ * Basic type definitions.\n+ *\n+ * This header file defines the generic integer types that will be used\n+ * for the implementation of hash functions; it also contains helper\n+ * functions which encode and decode multi-byte integer values, using\n+ * either little-endian or big-endian conventions.\n+ *\n+ * This file contains a compile-time test on the size of a byte\n+ * (the <code>unsigned char</code> C type). If bytes are not octets,\n+ * i.e. if they do not have a size of exactly 8 bits, then compilation\n+ * is aborted. Architectures where bytes are not octets are relatively\n+ * rare, even in the embedded devices market. We forbid non-octet bytes\n+ * because there is no clear convention on how octet streams are encoded\n+ * on such systems.\n+ *\n+ * ==========================(LICENSE BEGIN)============================\n+ *\n+ * Copyright (c) 2007-2010  Projet RNRT SAPHIR\n+ * \n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files (the\n+ * \"Software\"), to deal in the Software without restriction, including\n+ * without limitation the rights to use, copy, modify, merge, publish,\n+ * distribute, sublicense, and/or sell copies of the Software, and to\n+ * permit persons to whom the Software is furnished to do so, subject to\n+ * the following conditions:\n+ * \n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ * \n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ *\n+ * ===========================(LICENSE END)=============================\n+ *\n+ * @file     sph_types.h\n+ * @author   Thomas Pornin <thomas.pornin@cryptolog.com>\n+ */\n+\n+#ifndef SPH_TYPES_H__\n+#define SPH_TYPES_H__\n+\n+#include <limits.h>\n+\n+/*\n+ * All our I/O functions are defined over octet streams. We do not know\n+ * how to handle input data if bytes are not octets.\n+ */\n+#if CHAR_BIT != 8\n+#error This code requires 8-bit bytes\n+#endif\n+\n+/* ============= BEGIN documentation block for Doxygen ============ */\n+\n+#ifdef DOXYGEN_IGNORE\n+\n+/** @mainpage sphlib C code documentation\n+ *\n+ * @section overview Overview\n+ *\n+ * <code>sphlib</code> is a library which contains implementations of\n+ * various cryptographic hash functions. These pages have been generated\n+ * with <a href=\"http://www.doxygen.org/index.html\">doxygen</a> and\n+ * document the API for the C implementations.\n+ *\n+ * The API is described in appropriate header files, which are available\n+ * in the \"Files\" section. Each hash function family has its own header,\n+ * whose name begins with <code>\"sph_\"</code> and contains the family\n+ * name. For instance, the API for the RIPEMD hash functions is available\n+ * in the header file <code>sph_ripemd.h</code>.\n+ *\n+ * @section principles API structure and conventions\n+ *\n+ * @subsection io Input/output conventions\n+ *\n+ * In all generality, hash functions operate over strings of bits.\n+ * Individual bits are rarely encountered in C programming or actual\n+ * communication protocols; most protocols converge on the ubiquitous\n+ * \"octet\" which is a group of eight bits. Data is thus expressed as a\n+ * stream of octets. The C programming language contains the notion of a\n+ * \"byte\", which is a data unit managed under the type <code>\"unsigned\n+ * char\"</code>. The C standard prescribes that a byte should hold at\n+ * least eight bits, but possibly more. Most modern architectures, even\n+ * in the embedded world, feature eight-bit bytes, i.e. map bytes to\n+ * octets.\n+ *\n+ * Nevertheless, for some of the implemented hash functions, an extra\n+ * API has been added, which allows the input of arbitrary sequences of\n+ * bits: when the computation is about to be closed, 1 to 7 extra bits\n+ * can be added. The functions for which this API is implemented include\n+ * the SHA-2 functions and all SHA-3 candidates.\n+ *\n+ * <code>sphlib</code> defines hash function which may hash octet streams,\n+ * i.e. streams of bits where the number of bits is a multiple of eight.\n+ * The data input functions in the <code>sphlib</code> API expect data\n+ * as anonymous pointers (<code>\"const void *\"</code>) with a length\n+ * (of type <code>\"size_t\"</code>) which gives the input data chunk length\n+ * in bytes. A byte is assumed to be an octet; the <code>sph_types.h</code>\n+ * header contains a compile-time test which prevents compilation on\n+ * architectures where this property is not met.\n+ *\n+ * The hash function output is also converted into bytes. All currently\n+ * implemented hash functions have an output width which is a multiple of\n+ * eight, and this is likely to remain true for new designs.\n+ *\n+ * Most hash functions internally convert input data into 32-bit of 64-bit\n+ * words, using either little-endian or big-endian conversion. The hash\n+ * output also often consists of such words, which are encoded into output\n+ * bytes with a similar endianness convention. Some hash functions have\n+ * been only loosely specified on that subject; when necessary,\n+ * <code>sphlib</code> has been tested against published \"reference\"\n+ * implementations in order to use the same conventions.\n+ *\n+ * @subsection shortname Function short name\n+ *\n+ * Each implemented hash function has a \"short name\" which is used\n+ * internally to derive the identifiers for the functions and context\n+ * structures which the function uses. For instance, MD5 has the short\n+ * name <code>\"md5\"</code>. Short names are listed in the next section,\n+ * for the implemented hash functions. In subsequent sections, the\n+ * short name will be assumed to be <code>\"XXX\"</code>: replace with the\n+ * actual hash function name to get the C identifier.\n+ *\n+ * Note: some functions within the same family share the same core\n+ * elements, such as update function or context structure. Correspondingly,\n+ * some of the defined types or functions may actually be macros which\n+ * transparently evaluate to another type or function name.\n+ *\n+ * @subsection context Context structure\n+ *\n+ * Each implemented hash fonction has its own context structure, available\n+ * under the type name <code>\"sph_XXX_context\"</code> for the hash function\n+ * with short name <code>\"XXX\"</code>. This structure holds all needed\n+ * state for a running hash computation.\n+ *\n+ * The contents of these structures are meant to be opaque, and private\n+ * to the implementation. However, these contents are specified in the\n+ * header files so that application code which uses <code>sphlib</code>\n+ * may access the size of those structures.\n+ *\n+ * The caller is responsible for allocating the context structure,\n+ * whether by dynamic allocation (<code>malloc()</code> or equivalent),\n+ * static allocation (a global permanent variable), as an automatic\n+ * variable (\"on the stack\"), or by any other mean which ensures proper\n+ * structure alignment. <code>sphlib</code> code performs no dynamic\n+ * allocation by itself.\n+ *\n+ * The context must be initialized before use, using the\n+ * <code>sph_XXX_init()</code> function. This function sets the context\n+ * state to proper initial values for hashing.\n+ *\n+ * Since all state data is contained within the context structure,\n+ * <code>sphlib</code> is thread-safe and reentrant: several hash\n+ * computations may be performed in parallel, provided that they do not\n+ * operate on the same context. Moreover, a running computation can be\n+ * cloned by copying the context (with a simple <code>memcpy()</code>):\n+ * the context and its clone are then independant and may be updated\n+ * with new data and/or closed without interfering with each other.\n+ * Similarly, a context structure can be moved in memory at will:\n+ * context structures contain no pointer, in particular no pointer to\n+ * themselves.\n+ *\n+ * @subsection dataio Data input\n+ *\n+ * Hashed data is input with the <code>sph_XXX()</code> fonction, which\n+ * takes as parameters a pointer to the context, a pointer to the data\n+ * to hash, and the number of data bytes to hash. The context is updated\n+ * with the new data.\n+ *\n+ * Data can be input in one or several calls, with arbitrary input lengths.\n+ * However, it is best, performance wise, to input data by relatively big\n+ * chunks (say a few kilobytes), because this allows <code>sphlib</code> to\n+ * optimize things and avoid internal copying.\n+ *\n+ * When all data has been input, the context can be closed with\n+ * <code>sph_XXX_close()</code>. The hash output is computed and written\n+ * into the provided buffer. The caller must take care to provide a\n+ * buffer of appropriate length; e.g., when using SHA-1, the output is\n+ * a 20-byte word, therefore the output buffer must be at least 20-byte\n+ * long.\n+ *\n+ * For some hash functions, the <code>sph_XXX_addbits_and_close()</code>\n+ * function can be used instead of <code>sph_XXX_close()</code>. This\n+ * function can take a few extra <strong>bits</strong> to be added at\n+ * the end of the input message. This allows hashing messages with a\n+ * bit length which is not a multiple of 8. The extra bits are provided\n+ * as an unsigned integer value, and a bit count. The bit count must be\n+ * between 0 and 7, inclusive. The extra bits are provided as bits 7 to\n+ * 0 (bits of numerical value 128, 64, 32... downto 0), in that order.\n+ * For instance, to add three bits of value 1, 1 and 0, the unsigned\n+ * integer will have value 192 (1*128 + 1*64 + 0*32) and the bit count\n+ * will be 3.\n+ *\n+ * The <code>SPH_SIZE_XXX</code> macro is defined for each hash function;\n+ * it evaluates to the function output size, expressed in bits. For instance,\n+ * <code>SPH_SIZE_sha1</code> evaluates to <code>160</code>.\n+ *\n+ * When closed, the context is automatically reinitialized and can be\n+ * immediately used for another computation. It is not necessary to call\n+ * <code>sph_XXX_init()</code> after a close. Note that\n+ * <code>sph_XXX_init()</code> can still be called to \"reset\" a context,\n+ * i.e. forget previously input data, and get back to the initial state.\n+ *\n+ * @subsection alignment Data alignment\n+ *\n+ * \"Alignment\" is a property of data, which is said to be \"properly\n+ * aligned\" when its emplacement in memory is such that the data can\n+ * be optimally read by full words. This depends on the type of access;\n+ * basically, some hash functions will read data by 32-bit or 64-bit\n+ * words. <code>sphlib</code> does not mandate such alignment for input\n+ * data, but using aligned data can substantially improve performance.\n+ *\n+ * As a rule, it is best to input data by chunks whose length (in bytes)\n+ * is a multiple of eight, and which begins at \"generally aligned\"\n+ * addresses, such as the base address returned by a call to\n+ * <code>malloc()</code>.\n+ *\n+ * @section functions Implemented functions\n+ *\n+ * We give here the list of implemented functions. They are grouped by\n+ * family; to each family corresponds a specific header file. Each\n+ * individual function has its associated \"short name\". Please refer to\n+ * the documentation for that header file to get details on the hash\n+ * function denomination and provenance.\n+ *\n+ * Note: the functions marked with a '(64)' in the list below are\n+ * available only if the C compiler provides an integer type of length\n+ * 64 bits or more. Such a type is mandatory in the latest C standard\n+ * (ISO 9899:1999, aka \"C99\") and is present in several older compilers\n+ * as well, so chances are that such a type is available.\n+ *\n+ * - HAVAL family: file <code>sph_haval.h</code>\n+ *   - HAVAL-128/3 (128-bit, 3 passes): short name: <code>haval128_3</code>\n+ *   - HAVAL-128/4 (128-bit, 4 passes): short name: <code>haval128_4</code>\n+ *   - HAVAL-128/5 (128-bit, 5 passes): short name: <code>haval128_5</code>\n+ *   - HAVAL-160/3 (160-bit, 3 passes): short name: <code>haval160_3</code>\n+ *   - HAVAL-160/4 (160-bit, 4 passes): short name: <code>haval160_4</code>\n+ *   - HAVAL-160/5 (160-bit, 5 passes): short name: <code>haval160_5</code>\n+ *   - HAVAL-192/3 (192-bit, 3 passes): short name: <code>haval192_3</code>\n+ *   - HAVAL-192/4 (192-bit, 4 passes): short name: <code>haval192_4</code>\n+ *   - HAVAL-192/5 (192-bit, 5 passes): short name: <code>haval192_5</code>\n+ *   - HAVAL-224/3 (224-bit, 3 passes): short name: <code>haval224_3</code>\n+ *   - HAVAL-224/4 (224-bit, 4 passes): short name: <code>haval224_4</code>\n+ *   - HAVAL-224/5 (224-bit, 5 passes): short name: <code>haval224_5</code>\n+ *   - HAVAL-256/3 (256-bit, 3 passes): short name: <code>haval256_3</code>\n+ *   - HAVAL-256/4 (256-bit, 4 passes): short name: <code>haval256_4</code>\n+ *   - HAVAL-256/5 (256-bit, 5 passes): short name: <code>haval256_5</code>\n+ * - MD2: file <code>sph_md2.h</code>, short name: <code>md2</code>\n+ * - MD4: file <code>sph_md4.h</code>, short name: <code>md4</code>\n+ * - MD5: file <code>sph_md5.h</code>, short name: <code>md5</code>\n+ * - PANAMA: file <code>sph_panama.h</code>, short name: <code>panama</code>\n+ * - RadioGatun family: file <code>sph_radiogatun.h</code>\n+ *   - RadioGatun[32]: short name: <code>radiogatun32</code>\n+ *   - RadioGatun[64]: short name: <code>radiogatun64</code> (64)\n+ * - RIPEMD family: file <code>sph_ripemd.h</code>\n+ *   - RIPEMD: short name: <code>ripemd</code>\n+ *   - RIPEMD-128: short name: <code>ripemd128</code>\n+ *   - RIPEMD-160: short name: <code>ripemd160</code>\n+ * - SHA-0: file <code>sph_sha0.h</code>, short name: <code>sha0</code>\n+ * - SHA-1: file <code>sph_sha1.h</code>, short name: <code>sha1</code>\n+ * - SHA-2 family, 32-bit hashes: file <code>sph_sha2.h</code>\n+ *   - SHA-224: short name: <code>sha224</code>\n+ *   - SHA-256: short name: <code>sha256</code>\n+ *   - SHA-384: short name: <code>sha384</code> (64)\n+ *   - SHA-512: short name: <code>sha512</code> (64)\n+ * - Tiger family: file <code>sph_tiger.h</code>\n+ *   - Tiger: short name: <code>tiger</code> (64)\n+ *   - Tiger2: short name: <code>tiger2</code> (64)\n+ * - WHIRLPOOL family: file <code>sph_whirlpool.h</code>\n+ *   - WHIRLPOOL-0: short name: <code>whirlpool0</code> (64)\n+ *   - WHIRLPOOL-1: short name: <code>whirlpool1</code> (64)\n+ *   - WHIRLPOOL: short name: <code>whirlpool</code> (64)\n+ *\n+ * The fourteen second-round SHA-3 candidates are also implemented;\n+ * when applicable, the implementations follow the \"final\" specifications\n+ * as published for the third round of the SHA-3 competition (BLAKE,\n+ * Groestl, JH, Keccak and Skein have been tweaked for third round).\n+ *\n+ * - BLAKE family: file <code>sph_blake.h</code>\n+ *   - BLAKE-224: short name: <code>blake224</code>\n+ *   - BLAKE-256: short name: <code>blake256</code>\n+ *   - BLAKE-384: short name: <code>blake384</code>\n+ *   - BLAKE-512: short name: <code>blake512</code>\n+ * - BMW (Blue Midnight Wish) family: file <code>sph_bmw.h</code>\n+ *   - BMW-224: short name: <code>bmw224</code>\n+ *   - BMW-256: short name: <code>bmw256</code>\n+ *   - BMW-384: short name: <code>bmw384</code> (64)\n+ *   - BMW-512: short name: <code>bmw512</code> (64)\n+ * - CubeHash family: file <code>sph_cubehash.h</code> (specified as\n+ *   CubeHash16/32 in the CubeHash specification)\n+ *   - CubeHash-224: short name: <code>cubehash224</code>\n+ *   - CubeHash-256: short name: <code>cubehash256</code>\n+ *   - CubeHash-384: short name: <code>cubehash384</code>\n+ *   - CubeHash-512: short name: <code>cubehash512</code>\n+ * - ECHO family: file <code>sph_echo.h</code>\n+ *   - ECHO-224: short name: <code>echo224</code>\n+ *   - ECHO-256: short name: <code>echo256</code>\n+ *   - ECHO-384: short name: <code>echo384</code>\n+ *   - ECHO-512: short name: <code>echo512</code>\n+ * - Fugue family: file <code>sph_fugue.h</code>\n+ *   - Fugue-224: short name: <code>fugue224</code>\n+ *   - Fugue-256: short name: <code>fugue256</code>\n+ *   - Fugue-384: short name: <code>fugue384</code>\n+ *   - Fugue-512: short name: <code>fugue512</code>\n+ * - Groestl family: file <code>sph_groestl.h</code>\n+ *   - Groestl-224: short name: <code>groestl224</code>\n+ *   - Groestl-256: short name: <code>groestl256</code>\n+ *   - Groestl-384: short name: <code>groestl384</code>\n+ *   - Groestl-512: short name: <code>groestl512</code>\n+ * - Hamsi family: file <code>sph_hamsi.h</code>\n+ *   - Hamsi-224: short name: <code>hamsi224</code>\n+ *   - Hamsi-256: short name: <code>hamsi256</code>\n+ *   - Hamsi-384: short name: <code>hamsi384</code>\n+ *   - Hamsi-512: short name: <code>hamsi512</code>\n+ * - JH family: file <code>sph_jh.h</code>\n+ *   - JH-224: short name: <code>jh224</code>\n+ *   - JH-256: short name: <code>jh256</code>\n+ *   - JH-384: short name: <code>jh384</code>\n+ *   - JH-512: short name: <code>jh512</code>\n+ * - Keccak family: file <code>sph_keccak.h</code>\n+ *   - Keccak-224: short name: <code>keccak224</code>\n+ *   - Keccak-256: short name: <code>keccak256</code>\n+ *   - Keccak-384: short name: <code>keccak384</code>\n+ *   - Keccak-512: short name: <code>keccak512</code>\n+ * - Luffa family: file <code>sph_luffa.h</code>\n+ *   - Luffa-224: short name: <code>luffa224</code>\n+ *   - Luffa-256: short name: <code>luffa256</code>\n+ *   - Luffa-384: short name: <code>luffa384</code>\n+ *   - Luffa-512: short name: <code>luffa512</code>\n+ * - Shabal family: file <code>sph_shabal.h</code>\n+ *   - Shabal-192: short name: <code>shabal192</code>\n+ *   - Shabal-224: short name: <code>shabal224</code>\n+ *   - Shabal-256: short name: <code>shabal256</code>\n+ *   - Shabal-384: short name: <code>shabal384</code>\n+ *   - Shabal-512: short name: <code>shabal512</code>\n+ * - SHAvite-3 family: file <code>sph_shavite.h</code>\n+ *   - SHAvite-224 (nominally \"SHAvite-3 with 224-bit output\"):\n+ *     short name: <code>shabal224</code>\n+ *   - SHAvite-256 (nominally \"SHAvite-3 with 256-bit output\"):\n+ *     short name: <code>shabal256</code>\n+ *   - SHAvite-384 (nominally \"SHAvite-3 with 384-bit output\"):\n+ *     short name: <code>shabal384</code>\n+ *   - SHAvite-512 (nominally \"SHAvite-3 with 512-bit output\"):\n+ *     short name: <code>shabal512</code>\n+ * - SIMD family: file <code>sph_simd.h</code>\n+ *   - SIMD-224: short name: <code>simd224</code>\n+ *   - SIMD-256: short name: <code>simd256</code>\n+ *   - SIMD-384: short name: <code>simd384</code>\n+ *   - SIMD-512: short name: <code>simd512</code>\n+ * - Skein family: file <code>sph_skein.h</code>\n+ *   - Skein-224 (nominally specified as Skein-512-224): short name:\n+ *     <code>skein224</code> (64)\n+ *   - Skein-256 (nominally specified as Skein-512-256): short name:\n+ *     <code>skein256</code> (64)\n+ *   - Skein-384 (nominally specified as Skein-512-384): short name:\n+ *     <code>skein384</code> (64)\n+ *   - Skein-512 (nominally specified as Skein-512-512): short name:\n+ *     <code>skein512</code> (64)\n+ *\n+ * For the second-round SHA-3 candidates, the functions are as specified\n+ * for round 2, i.e. with the \"tweaks\" that some candidates added\n+ * between round 1 and round 2. Also, some of the submitted packages for\n+ * round 2 contained errors, in the specification, reference code, or\n+ * both. <code>sphlib</code> implements the corrected versions.\n+ */\n+\n+/** @hideinitializer\n+ * Unsigned integer type whose length is at least 32 bits; on most\n+ * architectures, it will have a width of exactly 32 bits. Unsigned C\n+ * types implement arithmetics modulo a power of 2; use the\n+ * <code>SPH_T32()</code> macro to ensure that the value is truncated\n+ * to exactly 32 bits. Unless otherwise specified, all macros and\n+ * functions which accept <code>sph_u32</code> values assume that these\n+ * values fit on 32 bits, i.e. do not exceed 2^32-1, even on architectures\n+ * where <code>sph_u32</code> is larger than that.\n+ */\n+typedef __arch_dependant__ sph_u32;\n+\n+/** @hideinitializer\n+ * Signed integer type corresponding to <code>sph_u32</code>; it has\n+ * width 32 bits or more.\n+ */\n+typedef __arch_dependant__ sph_s32;\n+\n+/** @hideinitializer\n+ * Unsigned integer type whose length is at least 64 bits; on most\n+ * architectures which feature such a type, it will have a width of\n+ * exactly 64 bits. C99-compliant platform will have this type; it\n+ * is also defined when the GNU compiler (gcc) is used, and on\n+ * platforms where <code>unsigned long</code> is large enough. If this\n+ * type is not available, then some hash functions which depends on\n+ * a 64-bit type will not be available (most notably SHA-384, SHA-512,\n+ * Tiger and WHIRLPOOL).\n+ */\n+typedef __arch_dependant__ sph_u64;\n+\n+/** @hideinitializer\n+ * Signed integer type corresponding to <code>sph_u64</code>; it has\n+ * width 64 bits or more.\n+ */\n+typedef __arch_dependant__ sph_s64;\n+\n+/**\n+ * This macro expands the token <code>x</code> into a suitable\n+ * constant expression of type <code>sph_u32</code>. Depending on\n+ * how this type is defined, a suffix such as <code>UL</code> may\n+ * be appended to the argument.\n+ *\n+ * @param x   the token to expand into a suitable constant expression\n+ */\n+#define SPH_C32(x)\n+\n+/**\n+ * Truncate a 32-bit value to exactly 32 bits. On most systems, this is\n+ * a no-op, recognized as such by the compiler.\n+ *\n+ * @param x   the value to truncate (of type <code>sph_u32</code>)\n+ */\n+#define SPH_T32(x)\n+\n+/**\n+ * Rotate a 32-bit value by a number of bits to the left. The rotate\n+ * count must reside between 1 and 31. This macro assumes that its\n+ * first argument fits in 32 bits (no extra bit allowed on machines where\n+ * <code>sph_u32</code> is wider); both arguments may be evaluated\n+ * several times.\n+ *\n+ * @param x   the value to rotate (of type <code>sph_u32</code>)\n+ * @param n   the rotation count (between 1 and 31, inclusive)\n+ */\n+#define SPH_ROTL32(x, n)\n+\n+/**\n+ * Rotate a 32-bit value by a number of bits to the left. The rotate\n+ * count must reside between 1 and 31. This macro assumes that its\n+ * first argument fits in 32 bits (no extra bit allowed on machines where\n+ * <code>sph_u32</code> is wider); both arguments may be evaluated\n+ * several times.\n+ *\n+ * @param x   the value to rotate (of type <code>sph_u32</code>)\n+ * @param n   the rotation count (between 1 and 31, inclusive)\n+ */\n+#define SPH_ROTR32(x, n)\n+\n+/**\n+ * This macro is defined on systems for which a 64-bit type has been\n+ * detected, and is used for <code>sph_u64</code>.\n+ */\n+#define SPH_64\n+\n+/**\n+ * This macro is defined on systems for the \"native\" integer size is\n+ * 64 bits (64-bit values fit in one register).\n+ */\n+#define SPH_64_TRUE\n+\n+/**\n+ * This macro expands the token <code>x</code> into a suitable\n+ * constant expression of type <code>sph_u64</code>. Depending on\n+ * how this type is defined, a suffix such as <code>ULL</code> may\n+ * be appended to the argument. This macro is defined only if a\n+ * 64-bit type was detected and used for <code>sph_u64</code>.\n+ *\n+ * @param x   the token to expand into a suitable constant expression\n+ */\n+#define SPH_C64(x)\n+\n+/**\n+ * Truncate a 64-bit value to exactly 64 bits. On most systems, this is\n+ * a no-op, recognized as such by the compiler. This macro is defined only\n+ * if a 64-bit type was detected and used for <code>sph_u64</code>.\n+ *\n+ * @param x   the value to truncate (of type <code>sph_u64</code>)\n+ */\n+#define SPH_T64(x)\n+\n+/**\n+ * Rotate a 64-bit value by a number of bits to the left. The rotate\n+ * count must reside between 1 and 63. This macro assumes that its\n+ * first argument fits in 64 bits (no extra bit allowed on machines where\n+ * <code>sph_u64</code> is wider); both arguments may be evaluated\n+ * several times. This macro is defined only if a 64-bit type was detected\n+ * and used for <code>sph_u64</code>.\n+ *\n+ * @param x   the value to rotate (of type <code>sph_u64</code>)\n+ * @param n   the rotation count (between 1 and 63, inclusive)\n+ */\n+#define SPH_ROTL64(x, n)\n+\n+/**\n+ * Rotate a 64-bit value by a number of bits to the left. The rotate\n+ * count must reside between 1 and 63. This macro assumes that its\n+ * first argument fits in 64 bits (no extra bit allowed on machines where\n+ * <code>sph_u64</code> is wider); both arguments may be evaluated\n+ * several times. This macro is defined only if a 64-bit type was detected\n+ * and used for <code>sph_u64</code>.\n+ *\n+ * @param x   the value to rotate (of type <code>sph_u64</code>)\n+ * @param n   the rotation count (between 1 and 63, inclusive)\n+ */\n+#define SPH_ROTR64(x, n)\n+\n+/**\n+ * This macro evaluates to <code>inline</code> or an equivalent construction,\n+ * if available on the compilation platform, or to nothing otherwise. This\n+ * is used to declare inline functions, for which the compiler should\n+ * endeavour to include the code directly in the caller. Inline functions\n+ * are typically defined in header files as replacement for macros.\n+ */\n+#define SPH_INLINE\n+\n+/**\n+ * This macro is defined if the platform has been detected as using\n+ * little-endian convention. This implies that the <code>sph_u32</code>\n+ * type (and the <code>sph_u64</code> type also, if it is defined) has\n+ * an exact width (i.e. exactly 32-bit, respectively 64-bit).\n+ */\n+#define SPH_LITTLE_ENDIAN\n+\n+/**\n+ * This macro is defined if the platform has been detected as using\n+ * big-endian convention. This implies that the <code>sph_u32</code>\n+ * type (and the <code>sph_u64</code> type also, if it is defined) has\n+ * an exact width (i.e. exactly 32-bit, respectively 64-bit).\n+ */\n+#define SPH_BIG_ENDIAN\n+\n+/**\n+ * This macro is defined if 32-bit words (and 64-bit words, if defined)\n+ * can be read from and written to memory efficiently in little-endian\n+ * convention. This is the case for little-endian platforms, and also\n+ * for the big-endian platforms which have special little-endian access\n+ * opcodes (e.g. Ultrasparc).\n+ */\n+#define SPH_LITTLE_FAST\n+\n+/**\n+ * This macro is defined if 32-bit words (and 64-bit words, if defined)\n+ * can be read from and written to memory efficiently in big-endian\n+ * convention. This is the case for little-endian platforms, and also\n+ * for the little-endian platforms which have special big-endian access\n+ * opcodes.\n+ */\n+#define SPH_BIG_FAST\n+\n+/**\n+ * On some platforms, this macro is defined to an unsigned integer type\n+ * into which pointer values may be cast. The resulting value can then\n+ * be tested for being a multiple of 2, 4 or 8, indicating an aligned\n+ * pointer for, respectively, 16-bit, 32-bit or 64-bit memory accesses.\n+ */\n+#define SPH_UPTR\n+\n+/**\n+ * When defined, this macro indicates that unaligned memory accesses\n+ * are possible with only a minor penalty, and thus should be prefered\n+ * over strategies which first copy data to an aligned buffer.\n+ */\n+#define SPH_UNALIGNED\n+\n+/**\n+ * Byte-swap a 32-bit word (i.e. <code>0x12345678</code> becomes\n+ * <code>0x78563412</code>). This is an inline function which resorts\n+ * to inline assembly on some platforms, for better performance.\n+ *\n+ * @param x   the 32-bit value to byte-swap\n+ * @return  the byte-swapped value\n+ */\n+static inline sph_u32 sph_bswap32(sph_u32 x);\n+\n+/**\n+ * Byte-swap a 64-bit word. This is an inline function which resorts\n+ * to inline assembly on some platforms, for better performance. This\n+ * function is defined only if a suitable 64-bit type was found for\n+ * <code>sph_u64</code>\n+ *\n+ * @param x   the 64-bit value to byte-swap\n+ * @return  the byte-swapped value\n+ */\n+static inline sph_u64 sph_bswap64(sph_u64 x);\n+\n+/**\n+ * Decode a 16-bit unsigned value from memory, in little-endian convention\n+ * (least significant byte comes first).\n+ *\n+ * @param src   the source address\n+ * @return  the decoded value\n+ */\n+static inline unsigned sph_dec16le(const void *src);\n+\n+/**\n+ * Encode a 16-bit unsigned value into memory, in little-endian convention\n+ * (least significant byte comes first).\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the value to encode\n+ */\n+static inline void sph_enc16le(void *dst, unsigned val);\n+\n+/**\n+ * Decode a 16-bit unsigned value from memory, in big-endian convention\n+ * (most significant byte comes first).\n+ *\n+ * @param src   the source address\n+ * @return  the decoded value\n+ */\n+static inline unsigned sph_dec16be(const void *src);\n+\n+/**\n+ * Encode a 16-bit unsigned value into memory, in big-endian convention\n+ * (most significant byte comes first).\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the value to encode\n+ */\n+static inline void sph_enc16be(void *dst, unsigned val);\n+\n+/**\n+ * Decode a 32-bit unsigned value from memory, in little-endian convention\n+ * (least significant byte comes first).\n+ *\n+ * @param src   the source address\n+ * @return  the decoded value\n+ */\n+static inline sph_u32 sph_dec32le(const void *src);\n+\n+/**\n+ * Decode a 32-bit unsigned value from memory, in little-endian convention\n+ * (least significant byte comes first). This function assumes that the\n+ * source address is suitably aligned for a direct access, if the platform\n+ * supports such things; it can thus be marginally faster than the generic\n+ * <code>sph_dec32le()</code> function.\n+ *\n+ * @param src   the source address\n+ * @return  the decoded value\n+ */\n+static inline sph_u32 sph_dec32le_aligned(const void *src);\n+\n+/**\n+ * Encode a 32-bit unsigned value into memory, in little-endian convention\n+ * (least significant byte comes first).\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the value to encode\n+ */\n+static inline void sph_enc32le(void *dst, sph_u32 val);\n+\n+/**\n+ * Encode a 32-bit unsigned value into memory, in little-endian convention\n+ * (least significant byte comes first). This function assumes that the\n+ * destination address is suitably aligned for a direct access, if the\n+ * platform supports such things; it can thus be marginally faster than\n+ * the generic <code>sph_enc32le()</code> function.\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the value to encode\n+ */\n+static inline void sph_enc32le_aligned(void *dst, sph_u32 val);\n+\n+/**\n+ * Decode a 32-bit unsigned value from memory, in big-endian convention\n+ * (most significant byte comes first).\n+ *\n+ * @param src   the source address\n+ * @return  the decoded value\n+ */\n+static inline sph_u32 sph_dec32be(const void *src);\n+\n+/**\n+ * Decode a 32-bit unsigned value from memory, in big-endian convention\n+ * (most significant byte comes first). This function assumes that the\n+ * source address is suitably aligned for a direct access, if the platform\n+ * supports such things; it can thus be marginally faster than the generic\n+ * <code>sph_dec32be()</code> function.\n+ *\n+ * @param src   the source address\n+ * @return  the decoded value\n+ */\n+static inline sph_u32 sph_dec32be_aligned(const void *src);\n+\n+/**\n+ * Encode a 32-bit unsigned value into memory, in big-endian convention\n+ * (most significant byte comes first).\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the value to encode\n+ */\n+static inline void sph_enc32be(void *dst, sph_u32 val);\n+\n+/**\n+ * Encode a 32-bit unsigned value into memory, in big-endian convention\n+ * (most significant byte comes first). This function assumes that the\n+ * destination address is suitably aligned for a direct access, if the\n+ * platform supports such things; it can thus be marginally faster than\n+ * the generic <code>sph_enc32be()</code> function.\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the value to encode\n+ */\n+static inline void sph_enc32be_aligned(void *dst, sph_u32 val);\n+\n+/**\n+ * Decode a 64-bit unsigned value from memory, in little-endian convention\n+ * (least significant byte comes first). This function is defined only\n+ * if a suitable 64-bit type was detected and used for <code>sph_u64</code>.\n+ *\n+ * @param src   the source address\n+ * @return  the decoded value\n+ */\n+static inline sph_u64 sph_dec64le(const void *src);\n+\n+/**\n+ * Decode a 64-bit unsigned value from memory, in little-endian convention\n+ * (least significant byte comes first). This function assumes that the\n+ * source address is suitably aligned for a direct access, if the platform\n+ * supports such things; it can thus be marginally faster than the generic\n+ * <code>sph_dec64le()</code> function. This function is defined only\n+ * if a suitable 64-bit type was detected and used for <code>sph_u64</code>.\n+ *\n+ * @param src   the source address\n+ * @return  the decoded value\n+ */\n+static inline sph_u64 sph_dec64le_aligned(const void *src);\n+\n+/**\n+ * Encode a 64-bit unsigned value into memory, in little-endian convention\n+ * (least significant byte comes first). This function is defined only\n+ * if a suitable 64-bit type was detected and used for <code>sph_u64</code>.\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the value to encode\n+ */\n+static inline void sph_enc64le(void *dst, sph_u64 val);\n+\n+/**\n+ * Encode a 64-bit unsigned value into memory, in little-endian convention\n+ * (least significant byte comes first). This function assumes that the\n+ * destination address is suitably aligned for a direct access, if the\n+ * platform supports such things; it can thus be marginally faster than\n+ * the generic <code>sph_enc64le()</code> function. This function is defined\n+ * only if a suitable 64-bit type was detected and used for\n+ * <code>sph_u64</code>.\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the value to encode\n+ */\n+static inline void sph_enc64le_aligned(void *dst, sph_u64 val);\n+\n+/**\n+ * Decode a 64-bit unsigned value from memory, in big-endian convention\n+ * (most significant byte comes first). This function is defined only\n+ * if a suitable 64-bit type was detected and used for <code>sph_u64</code>.\n+ *\n+ * @param src   the source address\n+ * @return  the decoded value\n+ */\n+static inline sph_u64 sph_dec64be(const void *src);\n+\n+/**\n+ * Decode a 64-bit unsigned value from memory, in big-endian convention\n+ * (most significant byte comes first). This function assumes that the\n+ * source address is suitably aligned for a direct access, if the platform\n+ * supports such things; it can thus be marginally faster than the generic\n+ * <code>sph_dec64be()</code> function. This function is defined only\n+ * if a suitable 64-bit type was detected and used for <code>sph_u64</code>.\n+ *\n+ * @param src   the source address\n+ * @return  the decoded value\n+ */\n+static inline sph_u64 sph_dec64be_aligned(const void *src);\n+\n+/**\n+ * Encode a 64-bit unsigned value into memory, in big-endian convention\n+ * (most significant byte comes first). This function is defined only\n+ * if a suitable 64-bit type was detected and used for <code>sph_u64</code>.\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the value to encode\n+ */\n+static inline void sph_enc64be(void *dst, sph_u64 val);\n+\n+/**\n+ * Encode a 64-bit unsigned value into memory, in big-endian convention\n+ * (most significant byte comes first). This function assumes that the\n+ * destination address is suitably aligned for a direct access, if the\n+ * platform supports such things; it can thus be marginally faster than\n+ * the generic <code>sph_enc64be()</code> function. This function is defined\n+ * only if a suitable 64-bit type was detected and used for\n+ * <code>sph_u64</code>.\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the value to encode\n+ */\n+static inline void sph_enc64be_aligned(void *dst, sph_u64 val);\n+\n+#endif\n+\n+/* ============== END documentation block for Doxygen ============= */\n+\n+#ifndef DOXYGEN_IGNORE\n+\n+/*\n+ * We want to define the types \"sph_u32\" and \"sph_u64\" which hold\n+ * unsigned values of at least, respectively, 32 and 64 bits. These\n+ * tests should select appropriate types for most platforms. The\n+ * macro \"SPH_64\" is defined if the 64-bit is supported.\n+ */\n+\n+#undef SPH_64\n+#undef SPH_64_TRUE\n+\n+#if defined __STDC__ && __STDC_VERSION__ >= 199901L\n+\n+/*\n+ * On C99 implementations, we can use <stdint.h> to get an exact 64-bit\n+ * type, if any, or otherwise use a wider type (which must exist, for\n+ * C99 conformance).\n+ */\n+\n+#include <stdint.h>\n+\n+#ifdef UINT32_MAX\n+typedef uint32_t sph_u32;\n+typedef int32_t sph_s32;\n+#else\n+typedef uint_fast32_t sph_u32;\n+typedef int_fast32_t sph_s32;\n+#endif\n+#if !SPH_NO_64\n+#ifdef UINT64_MAX\n+typedef uint64_t sph_u64;\n+typedef int64_t sph_s64;\n+#else\n+typedef uint_fast64_t sph_u64;\n+typedef int_fast64_t sph_s64;\n+#endif\n+#endif\n+\n+#define SPH_C32(x)    ((sph_u32)(x))\n+#if !SPH_NO_64\n+#define SPH_C64(x)    ((sph_u64)(x))\n+#define SPH_64  1\n+#endif\n+\n+#else\n+\n+/*\n+ * On non-C99 systems, we use \"unsigned int\" if it is wide enough,\n+ * \"unsigned long\" otherwise. This supports all \"reasonable\" architectures.\n+ * We have to be cautious: pre-C99 preprocessors handle constants\n+ * differently in '#if' expressions. Hence the shifts to test UINT_MAX.\n+ */\n+\n+#if ((UINT_MAX >> 11) >> 11) >= 0x3FF\n+\n+typedef unsigned int sph_u32;\n+typedef int sph_s32;\n+\n+#define SPH_C32(x)    ((sph_u32)(x ## U))\n+\n+#else\n+\n+typedef unsigned long sph_u32;\n+typedef long sph_s32;\n+\n+#define SPH_C32(x)    ((sph_u32)(x ## UL))\n+\n+#endif\n+\n+#if !SPH_NO_64\n+\n+/*\n+ * We want a 64-bit type. We use \"unsigned long\" if it is wide enough (as\n+ * is common on 64-bit architectures such as AMD64, Alpha or Sparcv9),\n+ * \"unsigned long long\" otherwise, if available. We use ULLONG_MAX to\n+ * test whether \"unsigned long long\" is available; we also know that\n+ * gcc features this type, even if the libc header do not know it.\n+ */\n+\n+#if ((ULONG_MAX >> 31) >> 31) >= 3\n+\n+typedef unsigned long sph_u64;\n+typedef long sph_s64;\n+\n+#define SPH_C64(x)    ((sph_u64)(x ## UL))\n+\n+#define SPH_64  1\n+\n+#elif ((ULLONG_MAX >> 31) >> 31) >= 3 || defined __GNUC__\n+\n+typedef unsigned long long sph_u64;\n+typedef long long sph_s64;\n+\n+#define SPH_C64(x)    ((sph_u64)(x ## ULL))\n+\n+#define SPH_64  1\n+\n+#else\n+\n+/*\n+ * No 64-bit type...\n+ */\n+\n+#endif\n+\n+#endif\n+\n+#endif\n+\n+/*\n+ * If the \"unsigned long\" type has length 64 bits or more, then this is\n+ * a \"true\" 64-bit architectures. This is also true with Visual C on\n+ * amd64, even though the \"long\" type is limited to 32 bits.\n+ */\n+#if SPH_64 && (((ULONG_MAX >> 31) >> 31) >= 3 || defined _M_X64)\n+#define SPH_64_TRUE   1\n+#endif\n+\n+/*\n+ * Implementation note: some processors have specific opcodes to perform\n+ * a rotation. Recent versions of gcc recognize the expression above and\n+ * use the relevant opcodes, when appropriate.\n+ */\n+\n+#define SPH_T32(x)    ((x) & SPH_C32(0xFFFFFFFF))\n+#define SPH_ROTL32(x, n)   SPH_T32(((x) << (n)) | ((x) >> (32 - (n))))\n+#define SPH_ROTR32(x, n)   SPH_ROTL32(x, (32 - (n)))\n+\n+#if SPH_64\n+\n+#define SPH_T64(x)    ((x) & SPH_C64(0xFFFFFFFFFFFFFFFF))\n+#define SPH_ROTL64(x, n)   SPH_T64(((x) << (n)) | ((x) >> (64 - (n))))\n+#define SPH_ROTR64(x, n)   SPH_ROTL64(x, (64 - (n)))\n+\n+#endif\n+\n+#ifndef DOXYGEN_IGNORE\n+/*\n+ * Define SPH_INLINE to be an \"inline\" qualifier, if available. We define\n+ * some small macro-like functions which benefit greatly from being inlined.\n+ */\n+#if (defined __STDC__ && __STDC_VERSION__ >= 199901L) || defined __GNUC__\n+#define SPH_INLINE inline\n+#elif defined _MSC_VER\n+#define SPH_INLINE __inline\n+#else\n+#define SPH_INLINE\n+#endif\n+#endif\n+\n+/*\n+ * We define some macros which qualify the architecture. These macros\n+ * may be explicit set externally (e.g. as compiler parameters). The\n+ * code below sets those macros if they are not already defined.\n+ *\n+ * Most macros are boolean, thus evaluate to either zero or non-zero.\n+ * The SPH_UPTR macro is special, in that it evaluates to a C type,\n+ * or is not defined.\n+ *\n+ * SPH_UPTR             if defined: unsigned type to cast pointers into\n+ *\n+ * SPH_UNALIGNED        non-zero if unaligned accesses are efficient\n+ * SPH_LITTLE_ENDIAN    non-zero if architecture is known to be little-endian\n+ * SPH_BIG_ENDIAN       non-zero if architecture is known to be big-endian\n+ * SPH_LITTLE_FAST      non-zero if little-endian decoding is fast\n+ * SPH_BIG_FAST         non-zero if big-endian decoding is fast\n+ *\n+ * If SPH_UPTR is defined, then encoding and decoding of 32-bit and 64-bit\n+ * values will try to be \"smart\". Either SPH_LITTLE_ENDIAN or SPH_BIG_ENDIAN\n+ * _must_ be non-zero in those situations. The 32-bit and 64-bit types\n+ * _must_ also have an exact width.\n+ *\n+ * SPH_SPARCV9_GCC_32   UltraSPARC-compatible with gcc, 32-bit mode\n+ * SPH_SPARCV9_GCC_64   UltraSPARC-compatible with gcc, 64-bit mode\n+ * SPH_SPARCV9_GCC      UltraSPARC-compatible with gcc\n+ * SPH_I386_GCC         x86-compatible (32-bit) with gcc\n+ * SPH_I386_MSVC        x86-compatible (32-bit) with Microsoft Visual C\n+ * SPH_AMD64_GCC        x86-compatible (64-bit) with gcc\n+ * SPH_AMD64_MSVC       x86-compatible (64-bit) with Microsoft Visual C\n+ * SPH_PPC32_GCC        PowerPC, 32-bit, with gcc\n+ * SPH_PPC64_GCC        PowerPC, 64-bit, with gcc\n+ *\n+ * TODO: enhance automatic detection, for more architectures and compilers.\n+ * Endianness is the most important. SPH_UNALIGNED and SPH_UPTR help with\n+ * some very fast functions (e.g. MD4) when using unaligned input data.\n+ * The CPU-specific-with-GCC macros are useful only for inline assembly,\n+ * normally restrained to this header file.\n+ */\n+\n+/*\n+ * 32-bit x86, aka \"i386 compatible\".\n+ */\n+#if defined __i386__ || defined _M_IX86\n+\n+#define SPH_DETECT_UNALIGNED         1\n+#define SPH_DETECT_LITTLE_ENDIAN     1\n+#define SPH_DETECT_UPTR              sph_u32\n+#ifdef __GNUC__\n+#define SPH_DETECT_I386_GCC          1\n+#endif\n+#ifdef _MSC_VER\n+#define SPH_DETECT_I386_MSVC         1\n+#endif\n+\n+/*\n+ * 64-bit x86, hereafter known as \"amd64\".\n+ */\n+#elif defined __x86_64 || defined _M_X64\n+\n+#define SPH_DETECT_UNALIGNED         1\n+#define SPH_DETECT_LITTLE_ENDIAN     1\n+#define SPH_DETECT_UPTR              sph_u64\n+#ifdef __GNUC__\n+#define SPH_DETECT_AMD64_GCC         1\n+#endif\n+#ifdef _MSC_VER\n+#define SPH_DETECT_AMD64_MSVC        1\n+#endif\n+\n+/*\n+ * 64-bit Sparc architecture (implies v9).\n+ */\n+#elif ((defined __sparc__ || defined __sparc) && defined __arch64__) \\\n+\t|| defined __sparcv9\n+\n+#define SPH_DETECT_BIG_ENDIAN        1\n+#define SPH_DETECT_UPTR              sph_u64\n+#ifdef __GNUC__\n+#define SPH_DETECT_SPARCV9_GCC_64    1\n+#define SPH_DETECT_LITTLE_FAST       1\n+#endif\n+\n+/*\n+ * 32-bit Sparc.\n+ */\n+#elif (defined __sparc__ || defined __sparc) \\\n+\t&& !(defined __sparcv9 || defined __arch64__)\n+\n+#define SPH_DETECT_BIG_ENDIAN        1\n+#define SPH_DETECT_UPTR              sph_u32\n+#if defined __GNUC__ && defined __sparc_v9__\n+#define SPH_DETECT_SPARCV9_GCC_32    1\n+#define SPH_DETECT_LITTLE_FAST       1\n+#endif\n+\n+/*\n+ * ARM, little-endian.\n+ */\n+#elif defined __arm__ && __ARMEL__\n+\n+#define SPH_DETECT_LITTLE_ENDIAN     1\n+\n+/*\n+ * MIPS, little-endian.\n+ */\n+#elif MIPSEL || _MIPSEL || __MIPSEL || __MIPSEL__\n+\n+#define SPH_DETECT_LITTLE_ENDIAN     1\n+\n+/*\n+ * MIPS, big-endian.\n+ */\n+#elif MIPSEB || _MIPSEB || __MIPSEB || __MIPSEB__\n+\n+#define SPH_DETECT_BIG_ENDIAN        1\n+\n+/*\n+ * PowerPC.\n+ */\n+#elif defined __powerpc__ || defined __POWERPC__ || defined __ppc__ \\\n+\t|| defined _ARCH_PPC\n+\n+/*\n+ * Note: we do not declare cross-endian access to be \"fast\": even if\n+ * using inline assembly, implementation should still assume that\n+ * keeping the decoded word in a temporary is faster than decoding\n+ * it again.\n+ */\n+#if defined __GNUC__\n+#if SPH_64_TRUE\n+#define SPH_DETECT_PPC64_GCC         1\n+#else\n+#define SPH_DETECT_PPC32_GCC         1\n+#endif\n+#endif\n+\n+#if defined __BIG_ENDIAN__ || defined _BIG_ENDIAN\n+#define SPH_DETECT_BIG_ENDIAN        1\n+#elif defined __LITTLE_ENDIAN__ || defined _LITTLE_ENDIAN\n+#define SPH_DETECT_LITTLE_ENDIAN     1\n+#endif\n+\n+/*\n+ * Itanium, 64-bit.\n+ */\n+#elif defined __ia64 || defined __ia64__ \\\n+\t|| defined __itanium__ || defined _M_IA64\n+\n+#if defined __BIG_ENDIAN__ || defined _BIG_ENDIAN\n+#define SPH_DETECT_BIG_ENDIAN        1\n+#else\n+#define SPH_DETECT_LITTLE_ENDIAN     1\n+#endif\n+#if defined __LP64__ || defined _LP64\n+#define SPH_DETECT_UPTR              sph_u64\n+#else\n+#define SPH_DETECT_UPTR              sph_u32\n+#endif\n+\n+#endif\n+\n+#if defined SPH_DETECT_SPARCV9_GCC_32 || defined SPH_DETECT_SPARCV9_GCC_64\n+#define SPH_DETECT_SPARCV9_GCC       1\n+#endif\n+\n+#if defined SPH_DETECT_UNALIGNED && !defined SPH_UNALIGNED\n+#define SPH_UNALIGNED         SPH_DETECT_UNALIGNED\n+#endif\n+#if defined SPH_DETECT_UPTR && !defined SPH_UPTR\n+#define SPH_UPTR              SPH_DETECT_UPTR\n+#endif\n+#if defined SPH_DETECT_LITTLE_ENDIAN && !defined SPH_LITTLE_ENDIAN\n+#define SPH_LITTLE_ENDIAN     SPH_DETECT_LITTLE_ENDIAN\n+#endif\n+#if defined SPH_DETECT_BIG_ENDIAN && !defined SPH_BIG_ENDIAN\n+#define SPH_BIG_ENDIAN        SPH_DETECT_BIG_ENDIAN\n+#endif\n+#if defined SPH_DETECT_LITTLE_FAST && !defined SPH_LITTLE_FAST\n+#define SPH_LITTLE_FAST       SPH_DETECT_LITTLE_FAST\n+#endif\n+#if defined SPH_DETECT_BIG_FAST && !defined SPH_BIG_FAST\n+#define SPH_BIG_FAST    SPH_DETECT_BIG_FAST\n+#endif\n+#if defined SPH_DETECT_SPARCV9_GCC_32 && !defined SPH_SPARCV9_GCC_32\n+#define SPH_SPARCV9_GCC_32    SPH_DETECT_SPARCV9_GCC_32\n+#endif\n+#if defined SPH_DETECT_SPARCV9_GCC_64 && !defined SPH_SPARCV9_GCC_64\n+#define SPH_SPARCV9_GCC_64    SPH_DETECT_SPARCV9_GCC_64\n+#endif\n+#if defined SPH_DETECT_SPARCV9_GCC && !defined SPH_SPARCV9_GCC\n+#define SPH_SPARCV9_GCC       SPH_DETECT_SPARCV9_GCC\n+#endif\n+#if defined SPH_DETECT_I386_GCC && !defined SPH_I386_GCC\n+#define SPH_I386_GCC          SPH_DETECT_I386_GCC\n+#endif\n+#if defined SPH_DETECT_I386_MSVC && !defined SPH_I386_MSVC\n+#define SPH_I386_MSVC         SPH_DETECT_I386_MSVC\n+#endif\n+#if defined SPH_DETECT_AMD64_GCC && !defined SPH_AMD64_GCC\n+#define SPH_AMD64_GCC         SPH_DETECT_AMD64_GCC\n+#endif\n+#if defined SPH_DETECT_AMD64_MSVC && !defined SPH_AMD64_MSVC\n+#define SPH_AMD64_MSVC        SPH_DETECT_AMD64_MSVC\n+#endif\n+#if defined SPH_DETECT_PPC32_GCC && !defined SPH_PPC32_GCC\n+#define SPH_PPC32_GCC         SPH_DETECT_PPC32_GCC\n+#endif\n+#if defined SPH_DETECT_PPC64_GCC && !defined SPH_PPC64_GCC\n+#define SPH_PPC64_GCC         SPH_DETECT_PPC64_GCC\n+#endif\n+\n+#if SPH_LITTLE_ENDIAN && !defined SPH_LITTLE_FAST\n+#define SPH_LITTLE_FAST              1\n+#endif\n+#if SPH_BIG_ENDIAN && !defined SPH_BIG_FAST\n+#define SPH_BIG_FAST                 1\n+#endif\n+\n+#if defined SPH_UPTR && !(SPH_LITTLE_ENDIAN || SPH_BIG_ENDIAN)\n+#error SPH_UPTR defined, but endianness is not known.\n+#endif\n+\n+#if SPH_I386_GCC && !SPH_NO_ASM\n+\n+/*\n+ * On x86 32-bit, with gcc, we use the bswapl opcode to byte-swap 32-bit\n+ * values.\n+ */\n+\n+static SPH_INLINE sph_u32\n+sph_bswap32(sph_u32 x)\n+{\n+\t__asm__ __volatile__ (\"bswapl %0\" : \"=r\" (x) : \"0\" (x));\n+\treturn x;\n+}\n+\n+#if SPH_64\n+\n+static SPH_INLINE sph_u64\n+sph_bswap64(sph_u64 x)\n+{\n+\treturn ((sph_u64)sph_bswap32((sph_u32)x) << 32)\n+\t\t| (sph_u64)sph_bswap32((sph_u32)(x >> 32));\n+}\n+\n+#endif\n+\n+#elif SPH_AMD64_GCC && !SPH_NO_ASM\n+\n+/*\n+ * On x86 64-bit, with gcc, we use the bswapl opcode to byte-swap 32-bit\n+ * and 64-bit values.\n+ */\n+\n+static SPH_INLINE sph_u32\n+sph_bswap32(sph_u32 x)\n+{\n+\t__asm__ __volatile__ (\"bswapl %0\" : \"=r\" (x) : \"0\" (x));\n+\treturn x;\n+}\n+\n+#if SPH_64\n+\n+static SPH_INLINE sph_u64\n+sph_bswap64(sph_u64 x)\n+{\n+\t__asm__ __volatile__ (\"bswapq %0\" : \"=r\" (x) : \"0\" (x));\n+\treturn x;\n+}\n+\n+#endif\n+\n+/*\n+ * Disabled code. Apparently, Microsoft Visual C 2005 is smart enough\n+ * to generate proper opcodes for endianness swapping with the pure C\n+ * implementation below.\n+ *\n+\n+#elif SPH_I386_MSVC && !SPH_NO_ASM\n+\n+static __inline sph_u32 __declspec(naked) __fastcall\n+sph_bswap32(sph_u32 x)\n+{\n+\t__asm {\n+\t\tbswap  ecx\n+\t\tmov    eax,ecx\n+\t\tret\n+\t}\n+}\n+\n+#if SPH_64\n+\n+static SPH_INLINE sph_u64\n+sph_bswap64(sph_u64 x)\n+{\n+\treturn ((sph_u64)sph_bswap32((sph_u32)x) << 32)\n+\t\t| (sph_u64)sph_bswap32((sph_u32)(x >> 32));\n+}\n+\n+#endif\n+\n+ *\n+ * [end of disabled code]\n+ */\n+\n+#else\n+\n+static SPH_INLINE sph_u32\n+sph_bswap32(sph_u32 x)\n+{\n+\tx = SPH_T32((x << 16) | (x >> 16));\n+\tx = ((x & SPH_C32(0xFF00FF00)) >> 8)\n+\t\t| ((x & SPH_C32(0x00FF00FF)) << 8);\n+\treturn x;\n+}\n+\n+#if SPH_64\n+\n+/**\n+ * Byte-swap a 64-bit value.\n+ *\n+ * @param x   the input value\n+ * @return  the byte-swapped value\n+ */\n+static SPH_INLINE sph_u64\n+sph_bswap64(sph_u64 x)\n+{\n+\tx = SPH_T64((x << 32) | (x >> 32));\n+\tx = ((x & SPH_C64(0xFFFF0000FFFF0000)) >> 16)\n+\t\t| ((x & SPH_C64(0x0000FFFF0000FFFF)) << 16);\n+\tx = ((x & SPH_C64(0xFF00FF00FF00FF00)) >> 8)\n+\t\t| ((x & SPH_C64(0x00FF00FF00FF00FF)) << 8);\n+\treturn x;\n+}\n+\n+#endif\n+\n+#endif\n+\n+#if SPH_SPARCV9_GCC && !SPH_NO_ASM\n+\n+/*\n+ * On UltraSPARC systems, native ordering is big-endian, but it is\n+ * possible to perform little-endian read accesses by specifying the\n+ * address space 0x88 (ASI_PRIMARY_LITTLE). Basically, either we use\n+ * the opcode \"lda [%reg]0x88,%dst\", where %reg is the register which\n+ * contains the source address and %dst is the destination register,\n+ * or we use \"lda [%reg+imm]%asi,%dst\", which uses the %asi register\n+ * to get the address space name. The latter format is better since it\n+ * combines an addition and the actual access in a single opcode; but\n+ * it requires the setting (and subsequent resetting) of %asi, which is\n+ * slow. Some operations (i.e. MD5 compression function) combine many\n+ * successive little-endian read accesses, which may share the same\n+ * %asi setting. The macros below contain the appropriate inline\n+ * assembly.\n+ */\n+\n+#define SPH_SPARCV9_SET_ASI   \\\n+\tsph_u32 sph_sparcv9_asi; \\\n+\t__asm__ __volatile__ ( \\\n+\t\t\"rd %%asi,%0\\n\\twr %%g0,0x88,%%asi\" : \"=r\" (sph_sparcv9_asi));\n+\n+#define SPH_SPARCV9_RESET_ASI  \\\n+\t__asm__ __volatile__ (\"wr %%g0,%0,%%asi\" : : \"r\" (sph_sparcv9_asi));\n+\n+#define SPH_SPARCV9_DEC32LE(base, idx)   ({ \\\n+\t\tsph_u32 sph_sparcv9_tmp; \\\n+\t\t__asm__ __volatile__ (\"lda [%1+\" #idx \"*4]%%asi,%0\" \\\n+\t\t\t: \"=r\" (sph_sparcv9_tmp) : \"r\" (base)); \\\n+\t\tsph_sparcv9_tmp; \\\n+\t})\n+\n+#endif\n+\n+static SPH_INLINE void\n+sph_enc16be(void *dst, unsigned val)\n+{\n+\t((unsigned char *)dst)[0] = (val >> 8);\n+\t((unsigned char *)dst)[1] = val;\n+}\n+\n+static SPH_INLINE unsigned\n+sph_dec16be(const void *src)\n+{\n+\treturn ((unsigned)(((const unsigned char *)src)[0]) << 8)\n+\t\t| (unsigned)(((const unsigned char *)src)[1]);\n+}\n+\n+static SPH_INLINE void\n+sph_enc16le(void *dst, unsigned val)\n+{\n+\t((unsigned char *)dst)[0] = val;\n+\t((unsigned char *)dst)[1] = val >> 8;\n+}\n+\n+static SPH_INLINE unsigned\n+sph_dec16le(const void *src)\n+{\n+\treturn (unsigned)(((const unsigned char *)src)[0])\n+\t\t| ((unsigned)(((const unsigned char *)src)[1]) << 8);\n+}\n+\n+/**\n+ * Encode a 32-bit value into the provided buffer (big endian convention).\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the 32-bit value to encode\n+ */\n+static SPH_INLINE void\n+sph_enc32be(void *dst, sph_u32 val)\n+{\n+#if defined SPH_UPTR\n+#if SPH_UNALIGNED\n+#if SPH_LITTLE_ENDIAN\n+\tval = sph_bswap32(val);\n+#endif\n+\t*(sph_u32 *)dst = val;\n+#else\n+\tif (((SPH_UPTR)dst & 3) == 0) {\n+#if SPH_LITTLE_ENDIAN\n+\t\tval = sph_bswap32(val);\n+#endif\n+\t\t*(sph_u32 *)dst = val;\n+\t} else {\n+\t\t((unsigned char *)dst)[0] = (val >> 24);\n+\t\t((unsigned char *)dst)[1] = (val >> 16);\n+\t\t((unsigned char *)dst)[2] = (val >> 8);\n+\t\t((unsigned char *)dst)[3] = val;\n+\t}\n+#endif\n+#else\n+\t((unsigned char *)dst)[0] = (val >> 24);\n+\t((unsigned char *)dst)[1] = (val >> 16);\n+\t((unsigned char *)dst)[2] = (val >> 8);\n+\t((unsigned char *)dst)[3] = val;\n+#endif\n+}\n+\n+/**\n+ * Encode a 32-bit value into the provided buffer (big endian convention).\n+ * The destination buffer must be properly aligned.\n+ *\n+ * @param dst   the destination buffer (32-bit aligned)\n+ * @param val   the value to encode\n+ */\n+static SPH_INLINE void\n+sph_enc32be_aligned(void *dst, sph_u32 val)\n+{\n+#if SPH_LITTLE_ENDIAN\n+\t*(sph_u32 *)dst = sph_bswap32(val);\n+#elif SPH_BIG_ENDIAN\n+\t*(sph_u32 *)dst = val;\n+#else\n+\t((unsigned char *)dst)[0] = (val >> 24);\n+\t((unsigned char *)dst)[1] = (val >> 16);\n+\t((unsigned char *)dst)[2] = (val >> 8);\n+\t((unsigned char *)dst)[3] = val;\n+#endif\n+}\n+\n+/**\n+ * Decode a 32-bit value from the provided buffer (big endian convention).\n+ *\n+ * @param src   the source buffer\n+ * @return  the decoded value\n+ */\n+static SPH_INLINE sph_u32\n+sph_dec32be(const void *src)\n+{\n+#if defined SPH_UPTR\n+#if SPH_UNALIGNED\n+#if SPH_LITTLE_ENDIAN\n+\treturn sph_bswap32(*(const sph_u32 *)src);\n+#else\n+\treturn *(const sph_u32 *)src;\n+#endif\n+#else\n+\tif (((SPH_UPTR)src & 3) == 0) {\n+#if SPH_LITTLE_ENDIAN\n+\t\treturn sph_bswap32(*(const sph_u32 *)src);\n+#else\n+\t\treturn *(const sph_u32 *)src;\n+#endif\n+\t} else {\n+\t\treturn ((sph_u32)(((const unsigned char *)src)[0]) << 24)\n+\t\t\t| ((sph_u32)(((const unsigned char *)src)[1]) << 16)\n+\t\t\t| ((sph_u32)(((const unsigned char *)src)[2]) << 8)\n+\t\t\t| (sph_u32)(((const unsigned char *)src)[3]);\n+\t}\n+#endif\n+#else\n+\treturn ((sph_u32)(((const unsigned char *)src)[0]) << 24)\n+\t\t| ((sph_u32)(((const unsigned char *)src)[1]) << 16)\n+\t\t| ((sph_u32)(((const unsigned char *)src)[2]) << 8)\n+\t\t| (sph_u32)(((const unsigned char *)src)[3]);\n+#endif\n+}\n+\n+/**\n+ * Decode a 32-bit value from the provided buffer (big endian convention).\n+ * The source buffer must be properly aligned.\n+ *\n+ * @param src   the source buffer (32-bit aligned)\n+ * @return  the decoded value\n+ */\n+static SPH_INLINE sph_u32\n+sph_dec32be_aligned(const void *src)\n+{\n+#if SPH_LITTLE_ENDIAN\n+\treturn sph_bswap32(*(const sph_u32 *)src);\n+#elif SPH_BIG_ENDIAN\n+\treturn *(const sph_u32 *)src;\n+#else\n+\treturn ((sph_u32)(((const unsigned char *)src)[0]) << 24)\n+\t\t| ((sph_u32)(((const unsigned char *)src)[1]) << 16)\n+\t\t| ((sph_u32)(((const unsigned char *)src)[2]) << 8)\n+\t\t| (sph_u32)(((const unsigned char *)src)[3]);\n+#endif\n+}\n+\n+/**\n+ * Encode a 32-bit value into the provided buffer (little endian convention).\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the 32-bit value to encode\n+ */\n+static SPH_INLINE void\n+sph_enc32le(void *dst, sph_u32 val)\n+{\n+#if defined SPH_UPTR\n+#if SPH_UNALIGNED\n+#if SPH_BIG_ENDIAN\n+\tval = sph_bswap32(val);\n+#endif\n+\t*(sph_u32 *)dst = val;\n+#else\n+\tif (((SPH_UPTR)dst & 3) == 0) {\n+#if SPH_BIG_ENDIAN\n+\t\tval = sph_bswap32(val);\n+#endif\n+\t\t*(sph_u32 *)dst = val;\n+\t} else {\n+\t\t((unsigned char *)dst)[0] = val;\n+\t\t((unsigned char *)dst)[1] = (val >> 8);\n+\t\t((unsigned char *)dst)[2] = (val >> 16);\n+\t\t((unsigned char *)dst)[3] = (val >> 24);\n+\t}\n+#endif\n+#else\n+\t((unsigned char *)dst)[0] = val;\n+\t((unsigned char *)dst)[1] = (val >> 8);\n+\t((unsigned char *)dst)[2] = (val >> 16);\n+\t((unsigned char *)dst)[3] = (val >> 24);\n+#endif\n+}\n+\n+/**\n+ * Encode a 32-bit value into the provided buffer (little endian convention).\n+ * The destination buffer must be properly aligned.\n+ *\n+ * @param dst   the destination buffer (32-bit aligned)\n+ * @param val   the value to encode\n+ */\n+static SPH_INLINE void\n+sph_enc32le_aligned(void *dst, sph_u32 val)\n+{\n+#if SPH_LITTLE_ENDIAN\n+\t*(sph_u32 *)dst = val;\n+#elif SPH_BIG_ENDIAN\n+\t*(sph_u32 *)dst = sph_bswap32(val);\n+#else\n+\t((unsigned char *)dst)[0] = val;\n+\t((unsigned char *)dst)[1] = (val >> 8);\n+\t((unsigned char *)dst)[2] = (val >> 16);\n+\t((unsigned char *)dst)[3] = (val >> 24);\n+#endif\n+}\n+\n+/**\n+ * Decode a 32-bit value from the provided buffer (little endian convention).\n+ *\n+ * @param src   the source buffer\n+ * @return  the decoded value\n+ */\n+static SPH_INLINE sph_u32\n+sph_dec32le(const void *src)\n+{\n+#if defined SPH_UPTR\n+#if SPH_UNALIGNED\n+#if SPH_BIG_ENDIAN\n+\treturn sph_bswap32(*(const sph_u32 *)src);\n+#else\n+\treturn *(const sph_u32 *)src;\n+#endif\n+#else\n+\tif (((SPH_UPTR)src & 3) == 0) {\n+#if SPH_BIG_ENDIAN\n+#if SPH_SPARCV9_GCC && !SPH_NO_ASM\n+\t\tsph_u32 tmp;\n+\n+\t\t/*\n+\t\t * \"__volatile__\" is needed here because without it,\n+\t\t * gcc-3.4.3 miscompiles the code and performs the\n+\t\t * access before the test on the address, thus triggering\n+\t\t * a bus error...\n+\t\t */\n+\t\t__asm__ __volatile__ (\n+\t\t\t\"lda [%1]0x88,%0\" : \"=r\" (tmp) : \"r\" (src));\n+\t\treturn tmp;\n+/*\n+ * On PowerPC, this turns out not to be worth the effort: the inline\n+ * assembly makes GCC optimizer uncomfortable, which tends to nullify\n+ * the decoding gains.\n+ *\n+ * For most hash functions, using this inline assembly trick changes\n+ * hashing speed by less than 5% and often _reduces_ it. The biggest\n+ * gains are for MD4 (+11%) and CubeHash (+30%). For all others, it is\n+ * less then 10%. The speed gain on CubeHash is probably due to the\n+ * chronic shortage of registers that CubeHash endures; for the other\n+ * functions, the generic code appears to be efficient enough already.\n+ *\n+#elif (SPH_PPC32_GCC || SPH_PPC64_GCC) && !SPH_NO_ASM\n+\t\tsph_u32 tmp;\n+\n+\t\t__asm__ __volatile__ (\n+\t\t\t\"lwbrx %0,0,%1\" : \"=r\" (tmp) : \"r\" (src));\n+\t\treturn tmp;\n+ */\n+#else\n+\t\treturn sph_bswap32(*(const sph_u32 *)src);\n+#endif\n+#else\n+\t\treturn *(const sph_u32 *)src;\n+#endif\n+\t} else {\n+\t\treturn (sph_u32)(((const unsigned char *)src)[0])\n+\t\t\t| ((sph_u32)(((const unsigned char *)src)[1]) << 8)\n+\t\t\t| ((sph_u32)(((const unsigned char *)src)[2]) << 16)\n+\t\t\t| ((sph_u32)(((const unsigned char *)src)[3]) << 24);\n+\t}\n+#endif\n+#else\n+\treturn (sph_u32)(((const unsigned char *)src)[0])\n+\t\t| ((sph_u32)(((const unsigned char *)src)[1]) << 8)\n+\t\t| ((sph_u32)(((const unsigned char *)src)[2]) << 16)\n+\t\t| ((sph_u32)(((const unsigned char *)src)[3]) << 24);\n+#endif\n+}\n+\n+/**\n+ * Decode a 32-bit value from the provided buffer (little endian convention).\n+ * The source buffer must be properly aligned.\n+ *\n+ * @param src   the source buffer (32-bit aligned)\n+ * @return  the decoded value\n+ */\n+static SPH_INLINE sph_u32\n+sph_dec32le_aligned(const void *src)\n+{\n+#if SPH_LITTLE_ENDIAN\n+\treturn *(const sph_u32 *)src;\n+#elif SPH_BIG_ENDIAN\n+#if SPH_SPARCV9_GCC && !SPH_NO_ASM\n+\tsph_u32 tmp;\n+\n+\t__asm__ __volatile__ (\"lda [%1]0x88,%0\" : \"=r\" (tmp) : \"r\" (src));\n+\treturn tmp;\n+/*\n+ * Not worth it generally.\n+ *\n+#elif (SPH_PPC32_GCC || SPH_PPC64_GCC) && !SPH_NO_ASM\n+\tsph_u32 tmp;\n+\n+\t__asm__ __volatile__ (\"lwbrx %0,0,%1\" : \"=r\" (tmp) : \"r\" (src));\n+\treturn tmp;\n+ */\n+#else\n+\treturn sph_bswap32(*(const sph_u32 *)src);\n+#endif\n+#else\n+\treturn (sph_u32)(((const unsigned char *)src)[0])\n+\t\t| ((sph_u32)(((const unsigned char *)src)[1]) << 8)\n+\t\t| ((sph_u32)(((const unsigned char *)src)[2]) << 16)\n+\t\t| ((sph_u32)(((const unsigned char *)src)[3]) << 24);\n+#endif\n+}\n+\n+#if SPH_64\n+\n+/**\n+ * Encode a 64-bit value into the provided buffer (big endian convention).\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the 64-bit value to encode\n+ */\n+static SPH_INLINE void\n+sph_enc64be(void *dst, sph_u64 val)\n+{\n+#if defined SPH_UPTR\n+#if SPH_UNALIGNED\n+#if SPH_LITTLE_ENDIAN\n+\tval = sph_bswap64(val);\n+#endif\n+\t*(sph_u64 *)dst = val;\n+#else\n+\tif (((SPH_UPTR)dst & 7) == 0) {\n+#if SPH_LITTLE_ENDIAN\n+\t\tval = sph_bswap64(val);\n+#endif\n+\t\t*(sph_u64 *)dst = val;\n+\t} else {\n+\t\t((unsigned char *)dst)[0] = (val >> 56);\n+\t\t((unsigned char *)dst)[1] = (val >> 48);\n+\t\t((unsigned char *)dst)[2] = (val >> 40);\n+\t\t((unsigned char *)dst)[3] = (val >> 32);\n+\t\t((unsigned char *)dst)[4] = (val >> 24);\n+\t\t((unsigned char *)dst)[5] = (val >> 16);\n+\t\t((unsigned char *)dst)[6] = (val >> 8);\n+\t\t((unsigned char *)dst)[7] = val;\n+\t}\n+#endif\n+#else\n+\t((unsigned char *)dst)[0] = (val >> 56);\n+\t((unsigned char *)dst)[1] = (val >> 48);\n+\t((unsigned char *)dst)[2] = (val >> 40);\n+\t((unsigned char *)dst)[3] = (val >> 32);\n+\t((unsigned char *)dst)[4] = (val >> 24);\n+\t((unsigned char *)dst)[5] = (val >> 16);\n+\t((unsigned char *)dst)[6] = (val >> 8);\n+\t((unsigned char *)dst)[7] = val;\n+#endif\n+}\n+\n+/**\n+ * Encode a 64-bit value into the provided buffer (big endian convention).\n+ * The destination buffer must be properly aligned.\n+ *\n+ * @param dst   the destination buffer (64-bit aligned)\n+ * @param val   the value to encode\n+ */\n+static SPH_INLINE void\n+sph_enc64be_aligned(void *dst, sph_u64 val)\n+{\n+#if SPH_LITTLE_ENDIAN\n+\t*(sph_u64 *)dst = sph_bswap64(val);\n+#elif SPH_BIG_ENDIAN\n+\t*(sph_u64 *)dst = val;\n+#else\n+\t((unsigned char *)dst)[0] = (val >> 56);\n+\t((unsigned char *)dst)[1] = (val >> 48);\n+\t((unsigned char *)dst)[2] = (val >> 40);\n+\t((unsigned char *)dst)[3] = (val >> 32);\n+\t((unsigned char *)dst)[4] = (val >> 24);\n+\t((unsigned char *)dst)[5] = (val >> 16);\n+\t((unsigned char *)dst)[6] = (val >> 8);\n+\t((unsigned char *)dst)[7] = val;\n+#endif\n+}\n+\n+/**\n+ * Decode a 64-bit value from the provided buffer (big endian convention).\n+ *\n+ * @param src   the source buffer\n+ * @return  the decoded value\n+ */\n+static SPH_INLINE sph_u64\n+sph_dec64be(const void *src)\n+{\n+#if defined SPH_UPTR\n+#if SPH_UNALIGNED\n+#if SPH_LITTLE_ENDIAN\n+\treturn sph_bswap64(*(const sph_u64 *)src);\n+#else\n+\treturn *(const sph_u64 *)src;\n+#endif\n+#else\n+\tif (((SPH_UPTR)src & 7) == 0) {\n+#if SPH_LITTLE_ENDIAN\n+\t\treturn sph_bswap64(*(const sph_u64 *)src);\n+#else\n+\t\treturn *(const sph_u64 *)src;\n+#endif\n+\t} else {\n+\t\treturn ((sph_u64)(((const unsigned char *)src)[0]) << 56)\n+\t\t\t| ((sph_u64)(((const unsigned char *)src)[1]) << 48)\n+\t\t\t| ((sph_u64)(((const unsigned char *)src)[2]) << 40)\n+\t\t\t| ((sph_u64)(((const unsigned char *)src)[3]) << 32)\n+\t\t\t| ((sph_u64)(((const unsigned char *)src)[4]) << 24)\n+\t\t\t| ((sph_u64)(((const unsigned char *)src)[5]) << 16)\n+\t\t\t| ((sph_u64)(((const unsigned char *)src)[6]) << 8)\n+\t\t\t| (sph_u64)(((const unsigned char *)src)[7]);\n+\t}\n+#endif\n+#else\n+\treturn ((sph_u64)(((const unsigned char *)src)[0]) << 56)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[1]) << 48)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[2]) << 40)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[3]) << 32)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[4]) << 24)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[5]) << 16)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[6]) << 8)\n+\t\t| (sph_u64)(((const unsigned char *)src)[7]);\n+#endif\n+}\n+\n+/**\n+ * Decode a 64-bit value from the provided buffer (big endian convention).\n+ * The source buffer must be properly aligned.\n+ *\n+ * @param src   the source buffer (64-bit aligned)\n+ * @return  the decoded value\n+ */\n+static SPH_INLINE sph_u64\n+sph_dec64be_aligned(const void *src)\n+{\n+#if SPH_LITTLE_ENDIAN\n+\treturn sph_bswap64(*(const sph_u64 *)src);\n+#elif SPH_BIG_ENDIAN\n+\treturn *(const sph_u64 *)src;\n+#else\n+\treturn ((sph_u64)(((const unsigned char *)src)[0]) << 56)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[1]) << 48)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[2]) << 40)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[3]) << 32)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[4]) << 24)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[5]) << 16)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[6]) << 8)\n+\t\t| (sph_u64)(((const unsigned char *)src)[7]);\n+#endif\n+}\n+\n+/**\n+ * Encode a 64-bit value into the provided buffer (little endian convention).\n+ *\n+ * @param dst   the destination buffer\n+ * @param val   the 64-bit value to encode\n+ */\n+static SPH_INLINE void\n+sph_enc64le(void *dst, sph_u64 val)\n+{\n+#if defined SPH_UPTR\n+#if SPH_UNALIGNED\n+#if SPH_BIG_ENDIAN\n+\tval = sph_bswap64(val);\n+#endif\n+\t*(sph_u64 *)dst = val;\n+#else\n+\tif (((SPH_UPTR)dst & 7) == 0) {\n+#if SPH_BIG_ENDIAN\n+\t\tval = sph_bswap64(val);\n+#endif\n+\t\t*(sph_u64 *)dst = val;\n+\t} else {\n+\t\t((unsigned char *)dst)[0] = val;\n+\t\t((unsigned char *)dst)[1] = (val >> 8);\n+\t\t((unsigned char *)dst)[2] = (val >> 16);\n+\t\t((unsigned char *)dst)[3] = (val >> 24);\n+\t\t((unsigned char *)dst)[4] = (val >> 32);\n+\t\t((unsigned char *)dst)[5] = (val >> 40);\n+\t\t((unsigned char *)dst)[6] = (val >> 48);\n+\t\t((unsigned char *)dst)[7] = (val >> 56);\n+\t}\n+#endif\n+#else\n+\t((unsigned char *)dst)[0] = val;\n+\t((unsigned char *)dst)[1] = (val >> 8);\n+\t((unsigned char *)dst)[2] = (val >> 16);\n+\t((unsigned char *)dst)[3] = (val >> 24);\n+\t((unsigned char *)dst)[4] = (val >> 32);\n+\t((unsigned char *)dst)[5] = (val >> 40);\n+\t((unsigned char *)dst)[6] = (val >> 48);\n+\t((unsigned char *)dst)[7] = (val >> 56);\n+#endif\n+}\n+\n+/**\n+ * Encode a 64-bit value into the provided buffer (little endian convention).\n+ * The destination buffer must be properly aligned.\n+ *\n+ * @param dst   the destination buffer (64-bit aligned)\n+ * @param val   the value to encode\n+ */\n+static SPH_INLINE void\n+sph_enc64le_aligned(void *dst, sph_u64 val)\n+{\n+#if SPH_LITTLE_ENDIAN\n+\t*(sph_u64 *)dst = val;\n+#elif SPH_BIG_ENDIAN\n+\t*(sph_u64 *)dst = sph_bswap64(val);\n+#else\n+\t((unsigned char *)dst)[0] = val;\n+\t((unsigned char *)dst)[1] = (val >> 8);\n+\t((unsigned char *)dst)[2] = (val >> 16);\n+\t((unsigned char *)dst)[3] = (val >> 24);\n+\t((unsigned char *)dst)[4] = (val >> 32);\n+\t((unsigned char *)dst)[5] = (val >> 40);\n+\t((unsigned char *)dst)[6] = (val >> 48);\n+\t((unsigned char *)dst)[7] = (val >> 56);\n+#endif\n+}\n+\n+/**\n+ * Decode a 64-bit value from the provided buffer (little endian convention).\n+ *\n+ * @param src   the source buffer\n+ * @return  the decoded value\n+ */\n+static SPH_INLINE sph_u64\n+sph_dec64le(const void *src)\n+{\n+#if defined SPH_UPTR\n+#if SPH_UNALIGNED\n+#if SPH_BIG_ENDIAN\n+\treturn sph_bswap64(*(const sph_u64 *)src);\n+#else\n+\treturn *(const sph_u64 *)src;\n+#endif\n+#else\n+\tif (((SPH_UPTR)src & 7) == 0) {\n+#if SPH_BIG_ENDIAN\n+#if SPH_SPARCV9_GCC_64 && !SPH_NO_ASM\n+\t\tsph_u64 tmp;\n+\n+\t\t__asm__ __volatile__ (\n+\t\t\t\"ldxa [%1]0x88,%0\" : \"=r\" (tmp) : \"r\" (src));\n+\t\treturn tmp;\n+/*\n+ * Not worth it generally.\n+ *\n+#elif SPH_PPC32_GCC && !SPH_NO_ASM\n+\t\treturn (sph_u64)sph_dec32le_aligned(src)\n+\t\t\t| ((sph_u64)sph_dec32le_aligned(\n+\t\t\t\t(const char *)src + 4) << 32);\n+#elif SPH_PPC64_GCC && !SPH_NO_ASM\n+\t\tsph_u64 tmp;\n+\n+\t\t__asm__ __volatile__ (\n+\t\t\t\"ldbrx %0,0,%1\" : \"=r\" (tmp) : \"r\" (src));\n+\t\treturn tmp;\n+ */\n+#else\n+\t\treturn sph_bswap64(*(const sph_u64 *)src);\n+#endif\n+#else\n+\t\treturn *(const sph_u64 *)src;\n+#endif\n+\t} else {\n+\t\treturn (sph_u64)(((const unsigned char *)src)[0])\n+\t\t\t| ((sph_u64)(((const unsigned char *)src)[1]) << 8)\n+\t\t\t| ((sph_u64)(((const unsigned char *)src)[2]) << 16)\n+\t\t\t| ((sph_u64)(((const unsigned char *)src)[3]) << 24)\n+\t\t\t| ((sph_u64)(((const unsigned char *)src)[4]) << 32)\n+\t\t\t| ((sph_u64)(((const unsigned char *)src)[5]) << 40)\n+\t\t\t| ((sph_u64)(((const unsigned char *)src)[6]) << 48)\n+\t\t\t| ((sph_u64)(((const unsigned char *)src)[7]) << 56);\n+\t}\n+#endif\n+#else\n+\treturn (sph_u64)(((const unsigned char *)src)[0])\n+\t\t| ((sph_u64)(((const unsigned char *)src)[1]) << 8)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[2]) << 16)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[3]) << 24)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[4]) << 32)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[5]) << 40)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[6]) << 48)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[7]) << 56);\n+#endif\n+}\n+\n+/**\n+ * Decode a 64-bit value from the provided buffer (little endian convention).\n+ * The source buffer must be properly aligned.\n+ *\n+ * @param src   the source buffer (64-bit aligned)\n+ * @return  the decoded value\n+ */\n+static SPH_INLINE sph_u64\n+sph_dec64le_aligned(const void *src)\n+{\n+#if SPH_LITTLE_ENDIAN\n+\treturn *(const sph_u64 *)src;\n+#elif SPH_BIG_ENDIAN\n+#if SPH_SPARCV9_GCC_64 && !SPH_NO_ASM\n+\tsph_u64 tmp;\n+\n+\t__asm__ __volatile__ (\"ldxa [%1]0x88,%0\" : \"=r\" (tmp) : \"r\" (src));\n+\treturn tmp;\n+/*\n+ * Not worth it generally.\n+ *\n+#elif SPH_PPC32_GCC && !SPH_NO_ASM\n+\treturn (sph_u64)sph_dec32le_aligned(src)\n+\t\t| ((sph_u64)sph_dec32le_aligned((const char *)src + 4) << 32);\n+#elif SPH_PPC64_GCC && !SPH_NO_ASM\n+\tsph_u64 tmp;\n+\n+\t__asm__ __volatile__ (\"ldbrx %0,0,%1\" : \"=r\" (tmp) : \"r\" (src));\n+\treturn tmp;\n+ */\n+#else\n+\treturn sph_bswap64(*(const sph_u64 *)src);\n+#endif\n+#else\n+\treturn (sph_u64)(((const unsigned char *)src)[0])\n+\t\t| ((sph_u64)(((const unsigned char *)src)[1]) << 8)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[2]) << 16)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[3]) << 24)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[4]) << 32)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[5]) << 40)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[6]) << 48)\n+\t\t| ((sph_u64)(((const unsigned char *)src)[7]) << 56);\n+#endif\n+}\n+\n+#endif\n+\n+#endif /* Doxygen excluded block */\n+\n+#endif"
      }
    ]
  },
  {
    "sha": "1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7",
    "node_id": "MDY6Q29tbWl0MTE4MTkyNzoxYTgzZTRmM2YxZDdkOTc3ZTUxMzk2YzU3M2UyYTNkZTJlZDNlYmQ3",
    "commit": {
      "author": {
        "name": "Luke Dashjr",
        "email": "luke-jr+git@utopios.org",
        "date": "2016-01-16T03:32:57Z"
      },
      "committer": {
        "name": "Luke Dashjr",
        "email": "luke-jr+git@utopios.org",
        "date": "2016-01-16T03:38:29Z"
      },
      "message": "HARDFORK: Reduce required target by 256 beginning 2016 Apr 14",
      "tree": {
        "sha": "d11f19aaf00749439a616b58420686d2845b581d",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/git/trees/d11f19aaf00749439a616b58420686d2845b581d"
      },
      "url": "https://api.github.com/repos/bitcoin/bitcoin/git/commits/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7",
      "comment_count": 0,
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "signature": null,
        "payload": null
      }
    },
    "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7",
    "html_url": "https://github.com/bitcoin/bitcoin/commit/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7",
    "comments_url": "https://api.github.com/repos/bitcoin/bitcoin/commits/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7/comments",
    "author": {
      "login": "luke-jr",
      "id": 1095675,
      "node_id": "MDQ6VXNlcjEwOTU2NzU=",
      "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/luke-jr",
      "html_url": "https://github.com/luke-jr",
      "followers_url": "https://api.github.com/users/luke-jr/followers",
      "following_url": "https://api.github.com/users/luke-jr/following{/other_user}",
      "gists_url": "https://api.github.com/users/luke-jr/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/luke-jr/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
      "organizations_url": "https://api.github.com/users/luke-jr/orgs",
      "repos_url": "https://api.github.com/users/luke-jr/repos",
      "events_url": "https://api.github.com/users/luke-jr/events{/privacy}",
      "received_events_url": "https://api.github.com/users/luke-jr/received_events",
      "type": "User",
      "site_admin": false
    },
    "committer": {
      "login": "luke-jr",
      "id": 1095675,
      "node_id": "MDQ6VXNlcjEwOTU2NzU=",
      "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/luke-jr",
      "html_url": "https://github.com/luke-jr",
      "followers_url": "https://api.github.com/users/luke-jr/followers",
      "following_url": "https://api.github.com/users/luke-jr/following{/other_user}",
      "gists_url": "https://api.github.com/users/luke-jr/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/luke-jr/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
      "organizations_url": "https://api.github.com/users/luke-jr/orgs",
      "repos_url": "https://api.github.com/users/luke-jr/repos",
      "events_url": "https://api.github.com/users/luke-jr/events{/privacy}",
      "received_events_url": "https://api.github.com/users/luke-jr/received_events",
      "type": "User",
      "site_admin": false
    },
    "parents": [
      {
        "sha": "8d3a84c242598ef3cdc733e99dddebfecdad84a6",
        "url": "https://api.github.com/repos/bitcoin/bitcoin/commits/8d3a84c242598ef3cdc733e99dddebfecdad84a6",
        "html_url": "https://github.com/bitcoin/bitcoin/commit/8d3a84c242598ef3cdc733e99dddebfecdad84a6"
      }
    ],
    "stats": {
      "total": 17,
      "additions": 10,
      "deletions": 7
    },
    "files": [
      {
        "sha": "65ae94ee615f10a24d0cf37b89f9b9481354c8f3",
        "filename": "src/main.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7/src/main.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7/src/main.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/main.cpp?ref=1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7",
        "patch": "@@ -1361,7 +1361,7 @@ bool ReadBlockFromDisk(CBlock& block, const CDiskBlockPos& pos, const Consensus:\n     }\n \n     // Check the header\n-    if (!CheckProofOfWork(block.GetHash(), block.nBits, consensusParams))\n+    if (!CheckProofOfWork(block.GetHash(), block.nBits, block.nTime, consensusParams))\n         return error(\"ReadBlockFromDisk: Errors in block header at %s\", pos.ToString());\n \n     return true;\n@@ -2933,7 +2933,7 @@ bool FindUndoPos(CValidationState &state, int nFile, CDiskBlockPos &pos, unsigne\n bool CheckBlockHeader(const CBlockHeader& block, CValidationState& state, bool fCheckPOW)\n {\n     // Check proof of work matches claimed amount\n-    if (fCheckPOW && !CheckProofOfWork(block.GetHash(), block.nBits, Params().GetConsensus()))\n+    if (fCheckPOW && !CheckProofOfWork(block.GetHash(), block.nBits, block.nTime, Params().GetConsensus()))\n         return state.DoS(50, error(\"CheckBlockHeader(): proof of work failed\"),\n                          REJECT_INVALID, \"high-hash\");\n "
      },
      {
        "sha": "e256dee15041ede8498c87333ea6a0c226fc113d",
        "filename": "src/pow.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7/src/pow.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7/src/pow.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/pow.cpp?ref=1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7",
        "patch": "@@ -84,14 +84,17 @@ unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nF\n     return bnNew.GetCompact();\n }\n \n-bool CheckProofOfWork(uint256 hash, unsigned int nBits, const Consensus::Params& params)\n+bool CheckProofOfWork(uint256 hash, unsigned int nBits, uint32_t nTime, const Consensus::Params& params)\n {\n     bool fNegative;\n     bool fOverflow;\n     arith_uint256 bnTarget;\n \n     bnTarget.SetCompact(nBits, &fNegative, &fOverflow);\n \n+    if (nTime >= 0x57100000)  // 2016 Apr 14 20:39:28 UTC\n+        bnTarget >>= 0x100;\n+\n     // Check range\n     if (fNegative || bnTarget == 0 || fOverflow || bnTarget > UintToArith256(params.powLimit))\n         return error(\"CheckProofOfWork(): nBits below minimum work\");"
      },
      {
        "sha": "9b7951cf7f88663942c62fb9bad19a28be7f3a87",
        "filename": "src/pow.h",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7/src/pow.h",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7/src/pow.h",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/pow.h?ref=1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7",
        "patch": "@@ -19,7 +19,7 @@ unsigned int GetNextWorkRequired(const CBlockIndex* pindexLast, const CBlockHead\n unsigned int CalculateNextWorkRequired(const CBlockIndex* pindexLast, int64_t nFirstBlockTime, const Consensus::Params&);\n \n /** Check whether a block hash satisfies the proof-of-work requirement specified by nBits */\n-bool CheckProofOfWork(uint256 hash, unsigned int nBits, const Consensus::Params&);\n+bool CheckProofOfWork(uint256 hash, unsigned int nBits, uint32_t nTime, const Consensus::Params&);\n arith_uint256 GetBlockProof(const CBlockIndex& block);\n \n /** Return the time it would take to redo the work difference between from and to, assuming the current hashrate corresponds to the difficulty at tip, in seconds. */"
      },
      {
        "sha": "18195581032bcb603960d2762b0b084cb632785d",
        "filename": "src/rpcmining.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7/src/rpcmining.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7/src/rpcmining.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/rpcmining.cpp?ref=1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7",
        "patch": "@@ -165,7 +165,7 @@ UniValue generate(const UniValue& params, bool fHelp)\n             LOCK(cs_main);\n             IncrementExtraNonce(pblock, chainActive.Tip(), nExtraNonce);\n         }\n-        while (!CheckProofOfWork(pblock->GetHash(), pblock->nBits, Params().GetConsensus())) {\n+        while (!CheckProofOfWork(pblock->GetHash(), pblock->nBits, pblock->nTime, Params().GetConsensus())) {\n             // Yes, there is a chance every nonce could fail to satisfy the -regtest\n             // target -- 1 in 2^(2^32). That ain't gonna happen.\n             ++pblock->nNonce;"
      },
      {
        "sha": "70054737b3e585cdf33f073db8836fd030f34198",
        "filename": "src/test/test_bitcoin.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7/src/test/test_bitcoin.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7/src/test/test_bitcoin.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/test/test_bitcoin.cpp?ref=1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7",
        "patch": "@@ -128,7 +128,7 @@ TestChain100Setup::CreateAndProcessBlock(const std::vector<CMutableTransaction>&\n     unsigned int extraNonce = 0;\n     IncrementExtraNonce(&block, chainActive.Tip(), extraNonce);\n \n-    while (!CheckProofOfWork(block.GetHash(), block.nBits, chainparams.GetConsensus())) ++block.nNonce;\n+    while (!CheckProofOfWork(block.GetHash(), block.nBits, block.nTime, chainparams.GetConsensus())) ++block.nNonce;\n \n     CValidationState state;\n     ProcessNewBlock(state, chainparams, NULL, &block, true, NULL);"
      },
      {
        "sha": "fe2d195e96b10f146f05122572e0412a7c234671",
        "filename": "src/txdb.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "blob_url": "https://github.com/bitcoin/bitcoin/blob/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7/src/txdb.cpp",
        "raw_url": "https://github.com/bitcoin/bitcoin/raw/1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7/src/txdb.cpp",
        "contents_url": "https://api.github.com/repos/bitcoin/bitcoin/contents/src/txdb.cpp?ref=1a83e4f3f1d7d977e51396c573e2a3de2ed3ebd7",
        "patch": "@@ -203,7 +203,7 @@ bool CBlockTreeDB::LoadBlockIndexGuts()\n                 pindexNew->nStatus        = diskindex.nStatus;\n                 pindexNew->nTx            = diskindex.nTx;\n \n-                if (!CheckProofOfWork(pindexNew->GetBlockHash(), pindexNew->nBits, Params().GetConsensus()))\n+                if (!CheckProofOfWork(pindexNew->GetBlockHash(), pindexNew->nBits, pindexNew->nTime, Params().GetConsensus()))\n                     return error(\"LoadBlockIndex(): CheckProofOfWork failed: %s\", pindexNew->ToString());\n \n                 pcursor->Next();"
      }
    ]
  }
]